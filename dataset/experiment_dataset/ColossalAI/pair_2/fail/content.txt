Requested labels: gpu-h20-10
Job defined at: hpcaitech/ColossalAI/.github/workflows/build_on_pr.yml@refs/pull/6254/merge
Waiting for a runner to pick up this job...
Job is about to start running on the runner: gpu-h20-10 (repository)
Current runner version: '2.323.0'
Runner name: 'gpu-h20-10'
Runner group name: 'Default'
Machine name: 'gpu-h20-10'
##[group]GITHUB_TOKEN Permissions
Actions: read
read
read
read
read
read
read


read
read
read
read
read
read
##[endgroup]
None
Runner is running behind proxy server 'http://vpn.luchentech.com:32171' for all HTTP requests.
Runner is running behind proxy server 'http://vpn.luchentech.com:32171' for all HTTPS requests.



'actions/checkout@v2' ee0669bd1cc54295c223e0bb666b733df41de1c5
'actions/upload-artifact@v4' ea165f8d65b6e75b540449e92b4886f43607fa02
Complete job name: Build and Test Colossal-AI
##[group]Checking docker version
##[command]/usr/bin/docker version --format '{{.Server.APIVersion}}'
'1.41'
Docker daemon API version: '1.41'
##[command]/usr/bin/docker version --format '{{.Client.APIVersion}}'
'1.41'
Docker client API version: '1.41'
##[endgroup]
##[group]Clean up resources from previous jobs
##[command]/usr/bin/docker ps --all --quiet --no-trunc --filter "label=81704a"
##[command]/usr/bin/docker network prune --force --filter "label=81704a"
##[endgroup]
##[group]Create local container network
##[command]/usr/bin/docker network create --label 81704a github_network_94998f4534db421f86753cfbbb2319dd
b3822b338ad3014696b5524895dc654196f57c01b7ad8f25a871d12f60d28c99
##[endgroup]
##[group]Starting job container
##[command]/usr/bin/docker pull image-cloud.luchentech.com/hpcaitech/pytorch-cuda:2.2.2-12.1.0
2.2.2-12.1.0: Pulling from hpcaitech/pytorch-cuda
c2a777c1361b156ea23925ab311db35b1b450443d25dc292205058aeffea7e64
Status: Image is up to date for image-cloud.luchentech.com/hpcaitech/pytorch-cuda:2.2.2-12.1.0
image-cloud.luchentech.com/hpcaitech/pytorch-cuda:2.2.2-12.1.0
##[command]/usr/bin/docker create --name 7d6bacf453dd49bc8aea383939f91c8e_imagecloudluchentechcomhpcaitechpytorchcuda2221210_f16ff6 --label 81704a --workdir /__w/ColossalAI/ColossalAI --network github_network_94998f4534db421f86753cfbbb2319dd --gpus all --shm-size=2g --rm -v /dev/shm -v /data/scratch:/data/scratch -e "HTTP_PROXY=http://vpn.luchentech.com:32171" -e "http_proxy=http://vpn.luchentech.com:32171" -e "HTTPS_PROXY=http://vpn.luchentech.com:32171" -e "https_proxy=http://vpn.luchentech.com:32171" -e "HOME=/github/home" -e GITHUB_ACTIONS=true -e CI=true -v "/var/run/docker.sock":"/var/run/docker.sock" -v "/root/actions-runner/github-gpu":"/__w" -v "/root/actions-runner/externals":"/__e":ro -v "/root/actions-runner/github-gpu/_temp":"/__w/_temp" -v "/root/actions-runner/github-gpu/_actions":"/__w/_actions" -v "/root/actions-runner/github-gpu/_tool":"/__w/_tool" -v "/root/actions-runner/github-gpu/_temp/_github_home":"/github/home" -v "/root/actions-runner/github-gpu/_temp/_github_workflow":"/github/workflow" --entrypoint "tail" image-cloud.luchentech.com/hpcaitech/pytorch-cuda:2.2.2-12.1.0 "-f" "/dev/null"
920416532ddad0112e30de68b3d3249d8793b2fa61e8416f9de3eb685dff9f0f
##[command]/usr/bin/docker start 920416532ddad0112e30de68b3d3249d8793b2fa61e8416f9de3eb685dff9f0f
920416532ddad0112e30de68b3d3249d8793b2fa61e8416f9de3eb685dff9f0f
##[command]/usr/bin/docker ps --all --filter id=920416532ddad0112e30de68b3d3249d8793b2fa61e8416f9de3eb685dff9f0f --filter status=running --no-trunc --format "{{.ID}} {{.Status}}"
920416532ddad0112e30de68b3d3249d8793b2fa61e8416f9de3eb685dff9f0f Up Less than a second
##[command]/usr/bin/docker inspect --format "{{range .Config.Env}}{{println .}}{{end}}" 920416532ddad0112e30de68b3d3249d8793b2fa61e8416f9de3eb685dff9f0f
http_proxy=http://vpn.luchentech.com:32171
HTTPS_PROXY=http://vpn.luchentech.com:32171
https_proxy=http://vpn.luchentech.com:32171
HOME=/github/home
GITHUB_ACTIONS=true
CI=true
HTTP_PROXY=http://vpn.luchentech.com:32171
PATH=/opt/conda/envs/pytorch/bin:/opt/conda/bin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
NVARCH=x86_64
NVIDIA_REQUIRE_CUDA=cuda>=12.1 brand=tesla,driver>=470,driver<471 brand=unknown,driver>=470,driver<471 brand=nvidia,driver>=470,driver<471 brand=nvidiartx,driver>=470,driver<471 brand=geforce,driver>=470,driver<471 brand=geforcertx,driver>=470,driver<471 brand=quadro,driver>=470,driver<471 brand=quadrortx,driver>=470,driver<471 brand=titan,driver>=470,driver<471 brand=titanrtx,driver>=470,driver<471 brand=tesla,driver>=525,driver<526 brand=unknown,driver>=525,driver<526 brand=nvidia,driver>=525,driver<526 brand=nvidiartx,driver>=525,driver<526 brand=geforce,driver>=525,driver<526 brand=geforcertx,driver>=525,driver<526 brand=quadro,driver>=525,driver<526 brand=quadrortx,driver>=525,driver<526 brand=titan,driver>=525,driver<526 brand=titanrtx,driver>=525,driver<526
NV_CUDA_CUDART_VERSION=12.1.55-1
NV_CUDA_COMPAT_PACKAGE=cuda-compat-12-1
CUDA_VERSION=12.1.0
LD_LIBRARY_PATH=/usr/local/nvidia/lib:/usr/local/nvidia/lib64
NVIDIA_VISIBLE_DEVICES=all
NVIDIA_DRIVER_CAPABILITIES=compute,utility
NV_CUDA_LIB_VERSION=12.1.0-1
NV_NVTX_VERSION=12.1.66-1
NV_LIBNPP_VERSION=12.0.2.50-1
NV_LIBNPP_PACKAGE=libnpp-12-1=12.0.2.50-1
NV_LIBCUSPARSE_VERSION=12.0.2.55-1
NV_LIBCUBLAS_PACKAGE_NAME=libcublas-12-1
NV_LIBCUBLAS_VERSION=12.1.0.26-1
NV_LIBCUBLAS_PACKAGE=libcublas-12-1=12.1.0.26-1
NV_LIBNCCL_PACKAGE_NAME=libnccl2
NV_LIBNCCL_PACKAGE_VERSION=2.17.1-1
NCCL_VERSION=2.17.1-1
NV_LIBNCCL_PACKAGE=libnccl2=2.17.1-1+cuda12.1
NVIDIA_PRODUCT_NAME=CUDA
NVIDIA_CUDA_END_OF_LIFE=1
NV_CUDA_CUDART_DEV_VERSION=12.1.55-1
NV_NVML_DEV_VERSION=12.1.55-1
NV_LIBCUSPARSE_DEV_VERSION=12.0.2.55-1
NV_LIBNPP_DEV_VERSION=12.0.2.50-1
NV_LIBNPP_DEV_PACKAGE=libnpp-dev-12-1=12.0.2.50-1
NV_LIBCUBLAS_DEV_VERSION=12.1.0.26-1
NV_LIBCUBLAS_DEV_PACKAGE_NAME=libcublas-dev-12-1
NV_LIBCUBLAS_DEV_PACKAGE=libcublas-dev-12-1=12.1.0.26-1
NV_CUDA_NSIGHT_COMPUTE_VERSION=12.1.0-1
NV_CUDA_NSIGHT_COMPUTE_DEV_PACKAGE=cuda-nsight-compute-12-1=12.1.0-1
NV_NVPROF_VERSION=12.1.55-1
NV_NVPROF_DEV_PACKAGE=cuda-nvprof-12-1=12.1.55-1
NV_LIBNCCL_DEV_PACKAGE_NAME=libnccl-dev
NV_LIBNCCL_DEV_PACKAGE_VERSION=2.17.1-1
NV_LIBNCCL_DEV_PACKAGE=libnccl-dev=2.17.1-1+cuda12.1
LIBRARY_PATH=/usr/local/cuda/lib64/stubs
NV_CUDNN_VERSION=8.9.0.131
NV_CUDNN_PACKAGE_NAME=libcudnn8
NV_CUDNN_PACKAGE=libcudnn8=8.9.0.131-1+cuda12.1
NV_CUDNN_PACKAGE_DEV=libcudnn8-dev=8.9.0.131-1+cuda12.1
CUDA_HOME=/usr/local/cuda
##[endgroup]
##[group]Waiting for all services to be ready
##[endgroup]
actions/checkout@v2

hpcaitech/TensorNVMe
TensorNVMe


true

1

false

##[endgroup]
##[command]/usr/bin/docker exec  920416532ddad0112e30de68b3d3249d8793b2fa61e8416f9de3eb685dff9f0f sh -c "cat /etc/*release | grep ^ID"
hpcaitech/TensorNVMe

Working directory is '/__w/ColossalAI/ColossalAI/TensorNVMe'

2 25 1
##[endgroup]
Temporarily overriding HOME='/__w/_temp/fbfe2622-2566-432d-b025-a8650df70e60' before making global git config changes

[command]/usr/bin/git config --global --add safe.directory /__w/ColossalAI/ColossalAI/TensorNVMe

[command]/usr/bin/git init /__w/ColossalAI/ColossalAI/TensorNVMe
Initialized empty Git repository in /__w/ColossalAI/ColossalAI/TensorNVMe/.git/
https://github.com/hpcaitech/TensorNVMe
##[endgroup]

[command]/usr/bin/git config --local gc.auto 0
##[endgroup]

core\.sshCommand
'core\.sshCommand' 'core.sshCommand'
http\.https\:\/\/github\.com\/\.extraheader
'http\.https\:\/\/github\.com\/\.extraheader' 'http.https://github.com/.extraheader'
--local
##[endgroup]
##[group]Determining the default branch
Retrieving the default branch name
Default branch 'main'
##[endgroup]

[command]/usr/bin/git -c protocol.version=2 fetch --no-tags --prune --progress --no-recurse-submodules --depth=1 origin +refs/heads/main:refs/remotes/origin/main
Enumerating 60, done.
Counting 1% (1/60)
Counting 3% (2/60)
Counting 5% (3/60)
Counting 6% (4/60)
Counting 8% (5/60)
Counting 10% (6/60)
Counting 11% (7/60)
Counting 13% (8/60)
Counting 15% (9/60)
Counting 16% (10/60)
Counting 18% (11/60)
Counting 20% (12/60)
Counting 21% (13/60)
Counting 23% (14/60)
Counting 25% (15/60)
Counting 26% (16/60)
Counting 28% (17/60)
Counting 30% (18/60)
Counting 31% (19/60)
Counting 33% (20/60)
Counting 35% (21/60)
Counting 36% (22/60)
Counting 38% (23/60)
Counting 40% (24/60)
Counting 41% (25/60)
Counting 43% (26/60)
Counting 45% (27/60)
Counting 46% (28/60)
Counting 48% (29/60)
Counting 50% (30/60)
Counting 51% (31/60)
Counting 53% (32/60)
Counting 55% (33/60)
Counting 56% (34/60)
Counting 58% (35/60)
Counting 60% (36/60)
Counting 61% (37/60)
Counting 63% (38/60)
Counting 65% (39/60)
Counting 66% (40/60)
Counting 68% (41/60)
Counting 70% (42/60)
Counting 71% (43/60)
Counting 73% (44/60)
Counting 75% (45/60)
Counting 76% (46/60)
Counting 78% (47/60)
Counting 80% (48/60)
Counting 81% (49/60)
Counting 83% (50/60)
Counting 85% (51/60)
Counting 86% (52/60)
Counting 88% (53/60)
Counting 90% (54/60)
Counting 91% (55/60)
Counting 93% (56/60)
Counting 95% (57/60)
Counting 96% (58/60)
Counting 98% (59/60)
Counting 100% (60/60)
Counting 100 60 60
Compressing 1% (1/51)
Compressing 3% (2/51)
Compressing 5% (3/51)
Compressing 7% (4/51)
Compressing 9% (5/51)
Compressing 11% (6/51)
Compressing 13% (7/51)
Compressing 15% (8/51)
Compressing 17% (9/51)
Compressing 19% (10/51)
Compressing 21% (11/51)
Compressing 23% (12/51)
Compressing 25% (13/51)
Compressing 27% (14/51)
Compressing 29% (15/51)
Compressing 31% (16/51)
Compressing 33% (17/51)
Compressing 35% (18/51)
Compressing 37% (19/51)
Compressing 39% (20/51)
Compressing 41% (21/51)
Compressing 43% (22/51)
Compressing 45% (23/51)
Compressing 47% (24/51)
Compressing 49% (25/51)
Compressing 50% (26/51)
Compressing 52% (27/51)
Compressing 54% (28/51)
Compressing 56% (29/51)
Compressing 58% (30/51)
Compressing 60% (31/51)
Compressing 62% (32/51)
Compressing 64% (33/51)
Compressing 66% (34/51)
Compressing 68% (35/51)
Compressing 70% (36/51)
Compressing 72% (37/51)
Compressing 74% (38/51)
Compressing 76% (39/51)
Compressing 78% (40/51)
Compressing 80% (41/51)
Compressing 82% (42/51)
Compressing 84% (43/51)
Compressing 86% (44/51)
Compressing 88% (45/51)
Compressing 90% (46/51)
Compressing 92% (47/51)
Compressing 94% (48/51)
Compressing 96% (49/51)
Compressing 98% (50/51)
Compressing 100% (51/51)
Compressing 100 51 51
60 6 24 1 0 0
https://github.com/hpcaitech/TensorNVMe
branch] main origin/main
##[endgroup]

##[endgroup]

main refs/remotes/origin/main

Branch 'main' set up to track remote branch 'main' from 'origin'.
##[endgroup]
-1 --format='%H'
6403388f7c8929f43e407c8f7f39c6453d78c5d4
##[group]Run if [ -d /github/home/tensornvme_cache ] && [ ! -z "$(ls -A /github/home/tensornvme_cache/)" ]; then
[36;1mif [ -d /github/home/tensornvme_cache ] && [ ! -z "$(ls -A /github/home/tensornvme_cache/)" ]; then[0m
[36;1m  cp -p -r /github/home/tensornvme_cache/* /__w/ColossalAI/ColossalAI/TensorNVMe[0m
36
shell: bash --noprofile --norc -e -o pipefail {0}
##[endgroup]
##[group]Run cd TensorNVMe
[36;1mcd TensorNVMe[0m
[36;1mconda install cmake[0m
[36;1mpip install -r requirements.txt[0m
[36;1mDISABLE_URING=1 pip install -v --no-cache-dir .[0m
shell: bash --noprofile --norc -e -o pipefail {0}
##[endgroup]
Channels:
- defaults
Platform: linux-64
Collecting package metadata (repodata.json): ...working... done
Solving environment: ...working... done
2025-04-11T03:53:38.7304100Z
## Package Plan ##
2025-04-11T03:53:38.7304807Z
environment location: /opt/conda
2025-04-11T03:53:38.7305107Z
added / updated specs:
- cmake
2025-04-11T03:53:38.7305625Z
2025-04-11T03:53:38.7305631Z
The following packages will be downloaded:
2025-04-11T03:53:38.7306136Z
package                    |            build
---------------------------|-----------------
archspec-0.2.3             |     pyhd3eb1b0_0          47 KB
ca-certificates-2025.2.25  |       h06a4308_0         129 KB
certifi-2025.1.31          |  py311h06a4308_0         163 KB
cmake-3.31.2               |       h27e300b_0        20.9 MB
conda-24.11.3              |  py311h06a4308_0         1.2 MB
expat-2.6.4                |       h6a678d5_0         180 KB
frozendict-2.4.2           |  py311h06a4308_0          37 KB
libuv-1.48.0               |       h5eee18b_0         950 KB
openssl-3.0.16             |       h5eee18b_0         5.2 MB
rhash-1.4.3                |       hdbd6064_0         220 KB
xz-5.6.4                   |       h5eee18b_1         567 KB
------------------------------------------------------------
Total:        29.5 MB
2025-04-11T03:53:38.7314107Z
The following NEW packages will be INSTALLED:
2025-04-11T03:53:38.7314656Z
cmake              pkgs/main/linux-64::cmake-3.31.2-h27e300b_0
expat              pkgs/main/linux-64::expat-2.6.4-h6a678d5_0
frozendict         pkgs/main/linux-64::frozendict-2.4.2-py311h06a4308_0
libuv              pkgs/main/linux-64::libuv-1.48.0-h5eee18b_0
rhash              pkgs/main/linux-64::rhash-1.4.3-hdbd6064_0
2025-04-11T03:53:38.7322095Z
The following packages will be UPDATED:
2025-04-11T03:53:38.7322550Z
archspec                               0.2.1-pyhd3eb1b0_0 --> 0.2.3-pyhd3eb1b0_0
ca-certificates                     2023.12.12-h06a4308_0 --> 2025.2.25-h06a4308_0
certifi                        2023.11.17-py311h06a4308_0 --> 2025.1.31-py311h06a4308_0
conda                             23.11.0-py311h06a4308_0 --> 24.11.3-py311h06a4308_0
openssl                                 3.0.12-h7f8727e_0 --> 3.0.16-h5eee18b_0
xz                                       5.4.5-h5eee18b_0 --> 5.6.4-h5eee18b_1
2025-04-11T03:53:38.7326128Z
2025-04-11T03:53:38.7326137Z
Proceed ([y]/n)?
2025-04-11T03:53:41.2830046Z
Downloading and Extracting Packages: ...working... done
Preparing transaction: ...working... done
Verifying transaction: ...working... done
Executing transaction: ...working... done
Requirement already satisfied: packaging in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from -r requirements.txt (line 1)) (24.1)
Collecting click (from -r requirements.txt (line 2))
click-8.1.8-py3-none-any.whl.metadata (2.3
Requirement already satisfied: torch in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from -r requirements.txt (line 3)) (2.2.2)
Requirement already satisfied: filelock in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from torch->-r requirements.txt (line 3)) (3.13.1)
Requirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from torch->-r requirements.txt (line 3)) (4.11.0)
Requirement already satisfied: sympy in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from torch->-r requirements.txt (line 3)) (1.12)
Requirement already satisfied: networkx in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from torch->-r requirements.txt (line 3)) (3.2.1)
Requirement already satisfied: jinja2 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from torch->-r requirements.txt (line 3)) (3.1.4)
Requirement already satisfied: fsspec in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from torch->-r requirements.txt (line 3)) (2024.6.1)
Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from jinja2->torch->-r requirements.txt (line 3)) (2.1.3)
Requirement already satisfied: mpmath>=0.19 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from sympy->torch->-r requirements.txt (line 3)) (1.3.0)
click-8.1.8-py3-none-any.whl (98
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 98.2/98.2 kB 474.7 kB/s eta 0:00:00
Installing collected packages: click
Successfully installed click-8.1.8
WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv
Using pip 24.0 from /opt/conda/envs/pytorch/lib/python3.10/site-packages/pip (python 3.10)
Processing /__w/ColossalAI/ColossalAI/TensorNVMe
Preparing metadata (setup.py): started
Running command python setup.py egg_info
running egg_info
creating /tmp/pip-pip-egg-info-xdkr1szy/tensornvme.egg-info
writing /tmp/pip-pip-egg-info-xdkr1szy/tensornvme.egg-info/PKG-INFO
writing dependency_links to /tmp/pip-pip-egg-info-xdkr1szy/tensornvme.egg-info/dependency_links.txt
writing entry points to /tmp/pip-pip-egg-info-xdkr1szy/tensornvme.egg-info/entry_points.txt
writing requirements to /tmp/pip-pip-egg-info-xdkr1szy/tensornvme.egg-info/requires.txt
writing top-level names to /tmp/pip-pip-egg-info-xdkr1szy/tensornvme.egg-info/top_level.txt
writing manifest file '/tmp/pip-pip-egg-info-xdkr1szy/tensornvme.egg-info/SOURCES.txt'
reading manifest file '/tmp/pip-pip-egg-info-xdkr1szy/tensornvme.egg-info/SOURCES.txt'
reading manifest template 'MANIFEST.in'
writing manifest file '/tmp/pip-pip-egg-info-xdkr1szy/tensornvme.egg-info/SOURCES.txt'
Preparing metadata (setup.py): finished with status 'done'
Requirement already satisfied: packaging in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from tensornvme==0.1.0) (24.1)
Requirement already satisfied: click in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from tensornvme==0.1.0) (8.1.8)
Requirement already satisfied: torch in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from tensornvme==0.1.0) (2.2.2)
Requirement already satisfied: filelock in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from torch->tensornvme==0.1.0) (3.13.1)
Requirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from torch->tensornvme==0.1.0) (4.11.0)
Requirement already satisfied: sympy in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from torch->tensornvme==0.1.0) (1.12)
Requirement already satisfied: networkx in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from torch->tensornvme==0.1.0) (3.2.1)
Requirement already satisfied: jinja2 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from torch->tensornvme==0.1.0) (3.1.4)
Requirement already satisfied: fsspec in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from torch->tensornvme==0.1.0) (2024.6.1)
Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from jinja2->torch->tensornvme==0.1.0) (2.1.3)
Requirement already satisfied: mpmath>=0.19 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from sympy->torch->tensornvme==0.1.0) (1.3.0)
Building wheels for collected packages: tensornvme
Building wheel for tensornvme (setup.py): started
Running command python setup.py bdist_wheel
CXX 9 4 0
CXX
CXX
CXX /usr/bin/c++
CXX
CXX
liburing is not found, install in /github/home/.tensornvme
libaio is not found, install in /github/home/.tensornvme
-- Configuring done (0.2s)
-- Generating done (0.0s)
-- Build files have been written to: /__w/ColossalAI/ColossalAI/TensorNVMe/cmake-build
[ 12%] Creating directories for 'extern_aio'
[ 25%] Performing download step (git clone) for 'extern_aio'
Cloning into 'libaio'...
HEAD is now at 1b18bfa bump libaio version
[ 37%] No update step for 'extern_aio'
[ 50%] No patch step for 'extern_aio'
[ 62%] No configure step for 'extern_aio'
[ 75%] Performing build step for 'extern_aio'
ar: creating libaio.a
[ 87%] No install step for 'extern_aio'
[100%] Completed 'extern_aio'
[100%] Built target extern_aio
/github/home/.bashrc is changed, please source it.
running bdist_wheel
running build
running build_py
creating build
creating build/lib.linux-x86_64-cpython-310
creating build/lib.linux-x86_64-cpython-310/tensornvme
copying tensornvme/offload.py -> build/lib.linux-x86_64-cpython-310/tensornvme
copying tensornvme/__init__.py -> build/lib.linux-x86_64-cpython-310/tensornvme
copying tensornvme/async_file_io.py -> build/lib.linux-x86_64-cpython-310/tensornvme
creating build/lib.linux-x86_64-cpython-310/tensornvme/cli
copying tensornvme/cli/check.py -> build/lib.linux-x86_64-cpython-310/tensornvme/cli
copying tensornvme/cli/__init__.py -> build/lib.linux-x86_64-cpython-310/tensornvme/cli
copying tensornvme/cli/cli.py -> build/lib.linux-x86_64-cpython-310/tensornvme/cli
running build_ext
building 'tensornvme._C' extension
creating /__w/ColossalAI/ColossalAI/TensorNVMe/build/temp.linux-x86_64-cpython-310
creating /__w/ColossalAI/ColossalAI/TensorNVMe/build/temp.linux-x86_64-cpython-310/__w
creating /__w/ColossalAI/ColossalAI/TensorNVMe/build/temp.linux-x86_64-cpython-310/__w/ColossalAI
creating /__w/ColossalAI/ColossalAI/TensorNVMe/build/temp.linux-x86_64-cpython-310/__w/ColossalAI/ColossalAI
creating /__w/ColossalAI/ColossalAI/TensorNVMe/build/temp.linux-x86_64-cpython-310/__w/ColossalAI/ColossalAI/TensorNVMe
creating /__w/ColossalAI/ColossalAI/TensorNVMe/build/temp.linux-x86_64-cpython-310/__w/ColossalAI/ColossalAI/TensorNVMe/csrc
Emitting ninja build file /__w/ColossalAI/ColossalAI/TensorNVMe/build/temp.linux-x86_64-cpython-310/build.ninja...
Compiling objects...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
[1/7] c++ -MMD -MF /__w/ColossalAI/ColossalAI/TensorNVMe/build/temp.linux-x86_64-cpython-310/__w/ColossalAI/ColossalAI/TensorNVMe/csrc/space_mgr.o.d -pthread -B /opt/conda/envs/pytorch/compiler_compat -Wno-unused-result -Wsign-compare -DNDEBUG -fwrapv -O2 -Wall -fPIC -O2 -isystem /opt/conda/envs/pytorch/include -fPIC -O2 -isystem /opt/conda/envs/pytorch/include -fPIC -DDISABLE_URING -I/__w/ColossalAI/ColossalAI/TensorNVMe/csrc -I/__w/ColossalAI/ColossalAI/TensorNVMe/include -I/github/home/.tensornvme/include -I/usr/local/cuda/include -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include/TH -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include/THC -I/opt/conda/envs/pytorch/include/python3.10 -c -c /__w/ColossalAI/ColossalAI/TensorNVMe/csrc/space_mgr.cpp -o /__w/ColossalAI/ColossalAI/TensorNVMe/build/temp.linux-x86_64-cpython-310/__w/ColossalAI/ColossalAI/TensorNVMe/csrc/space_mgr.o -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE="_gcc"' '-DPYBIND11_STDLIB="_libstdcpp"' '-DPYBIND11_BUILD_ABI="_cxxabi1011"' -DTORCH_EXTENSION_NAME=_C -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++17
[2/7] c++ -MMD -MF /__w/ColossalAI/ColossalAI/TensorNVMe/build/temp.linux-x86_64-cpython-310/__w/ColossalAI/ColossalAI/TensorNVMe/csrc/aio.o.d -pthread -B /opt/conda/envs/pytorch/compiler_compat -Wno-unused-result -Wsign-compare -DNDEBUG -fwrapv -O2 -Wall -fPIC -O2 -isystem /opt/conda/envs/pytorch/include -fPIC -O2 -isystem /opt/conda/envs/pytorch/include -fPIC -DDISABLE_URING -I/__w/ColossalAI/ColossalAI/TensorNVMe/csrc -I/__w/ColossalAI/ColossalAI/TensorNVMe/include -I/github/home/.tensornvme/include -I/usr/local/cuda/include -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include/TH -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include/THC -I/opt/conda/envs/pytorch/include/python3.10 -c -c /__w/ColossalAI/ColossalAI/TensorNVMe/csrc/aio.cpp -o /__w/ColossalAI/ColossalAI/TensorNVMe/build/temp.linux-x86_64-cpython-310/__w/ColossalAI/ColossalAI/TensorNVMe/csrc/aio.o -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE="_gcc"' '-DPYBIND11_STDLIB="_libstdcpp"' '-DPYBIND11_BUILD_ABI="_cxxabi1011"' -DTORCH_EXTENSION_NAME=_C -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++17
[3/7] c++ -MMD -MF /__w/ColossalAI/ColossalAI/TensorNVMe/build/temp.linux-x86_64-cpython-310/__w/ColossalAI/ColossalAI/TensorNVMe/csrc/async_file_io.o.d -pthread -B /opt/conda/envs/pytorch/compiler_compat -Wno-unused-result -Wsign-compare -DNDEBUG -fwrapv -O2 -Wall -fPIC -O2 -isystem /opt/conda/envs/pytorch/include -fPIC -O2 -isystem /opt/conda/envs/pytorch/include -fPIC -DDISABLE_URING -I/__w/ColossalAI/ColossalAI/TensorNVMe/csrc -I/__w/ColossalAI/ColossalAI/TensorNVMe/include -I/github/home/.tensornvme/include -I/usr/local/cuda/include -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include/TH -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include/THC -I/opt/conda/envs/pytorch/include/python3.10 -c -c /__w/ColossalAI/ColossalAI/TensorNVMe/csrc/async_file_io.cpp -o /__w/ColossalAI/ColossalAI/TensorNVMe/build/temp.linux-x86_64-cpython-310/__w/ColossalAI/ColossalAI/TensorNVMe/csrc/async_file_io.o -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE="_gcc"' '-DPYBIND11_STDLIB="_libstdcpp"' '-DPYBIND11_BUILD_ABI="_cxxabi1011"' -DTORCH_EXTENSION_NAME=_C -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++17
[4/7] c++ -MMD -MF /__w/ColossalAI/ColossalAI/TensorNVMe/build/temp.linux-x86_64-cpython-310/__w/ColossalAI/ColossalAI/TensorNVMe/csrc/offload.o.d -pthread -B /opt/conda/envs/pytorch/compiler_compat -Wno-unused-result -Wsign-compare -DNDEBUG -fwrapv -O2 -Wall -fPIC -O2 -isystem /opt/conda/envs/pytorch/include -fPIC -O2 -isystem /opt/conda/envs/pytorch/include -fPIC -DDISABLE_URING -I/__w/ColossalAI/ColossalAI/TensorNVMe/csrc -I/__w/ColossalAI/ColossalAI/TensorNVMe/include -I/github/home/.tensornvme/include -I/usr/local/cuda/include -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include/TH -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include/THC -I/opt/conda/envs/pytorch/include/python3.10 -c -c /__w/ColossalAI/ColossalAI/TensorNVMe/csrc/offload.cpp -o /__w/ColossalAI/ColossalAI/TensorNVMe/build/temp.linux-x86_64-cpython-310/__w/ColossalAI/ColossalAI/TensorNVMe/csrc/offload.o -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE="_gcc"' '-DPYBIND11_STDLIB="_libstdcpp"' '-DPYBIND11_BUILD_ABI="_cxxabi1011"' -DTORCH_EXTENSION_NAME=_C -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++17
[5/7] c++ -MMD -MF /__w/ColossalAI/ColossalAI/TensorNVMe/build/temp.linux-x86_64-cpython-310/__w/ColossalAI/ColossalAI/TensorNVMe/csrc/backend.o.d -pthread -B /opt/conda/envs/pytorch/compiler_compat -Wno-unused-result -Wsign-compare -DNDEBUG -fwrapv -O2 -Wall -fPIC -O2 -isystem /opt/conda/envs/pytorch/include -fPIC -O2 -isystem /opt/conda/envs/pytorch/include -fPIC -DDISABLE_URING -I/__w/ColossalAI/ColossalAI/TensorNVMe/csrc -I/__w/ColossalAI/ColossalAI/TensorNVMe/include -I/github/home/.tensornvme/include -I/usr/local/cuda/include -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include/TH -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include/THC -I/opt/conda/envs/pytorch/include/python3.10 -c -c /__w/ColossalAI/ColossalAI/TensorNVMe/csrc/backend.cpp -o /__w/ColossalAI/ColossalAI/TensorNVMe/build/temp.linux-x86_64-cpython-310/__w/ColossalAI/ColossalAI/TensorNVMe/csrc/backend.o -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE="_gcc"' '-DPYBIND11_STDLIB="_libstdcpp"' '-DPYBIND11_BUILD_ABI="_cxxabi1011"' -DTORCH_EXTENSION_NAME=_C -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++17
In file included from /__w/ColossalAI/ColossalAI/TensorNVMe/csrc/backend.cpp:15:
/__w/ColossalAI/ColossalAI/TensorNVMe/include/pthread_backend.h: In constructor ‘PthreadAsyncIO::PthreadAsyncIO(unsigned int, unsigned int)’:
/__w/ColossalAI/ColossalAI/TensorNVMe/include/pthread_backend.h:38:18: warning: ‘PthreadAsyncIO::total_tasks’ will be initialized after [-Wreorder]
38 |     unsigned int total_tasks;
^~~~~~~~~~~
/__w/ColossalAI/ColossalAI/TensorNVMe/include/pthread_backend.h:29:18: warning:   ‘unsigned int PthreadAsyncIO::total_h2d’ [-Wreorder]
29 |     unsigned int total_h2d;
^~~~~~~~~
/__w/ColossalAI/ColossalAI/TensorNVMe/include/pthread_backend.h:41:5: warning:   when initialized here [-Wreorder]
41 |     PthreadAsyncIO(unsigned int n_entries, unsigned int n_tasks)
^~~~~~~~~~~~~~
[6/7] c++ -MMD -MF /__w/ColossalAI/ColossalAI/TensorNVMe/build/temp.linux-x86_64-cpython-310/__w/ColossalAI/ColossalAI/TensorNVMe/csrc/pthread_backend.o.d -pthread -B /opt/conda/envs/pytorch/compiler_compat -Wno-unused-result -Wsign-compare -DNDEBUG -fwrapv -O2 -Wall -fPIC -O2 -isystem /opt/conda/envs/pytorch/include -fPIC -O2 -isystem /opt/conda/envs/pytorch/include -fPIC -DDISABLE_URING -I/__w/ColossalAI/ColossalAI/TensorNVMe/csrc -I/__w/ColossalAI/ColossalAI/TensorNVMe/include -I/github/home/.tensornvme/include -I/usr/local/cuda/include -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include/TH -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include/THC -I/opt/conda/envs/pytorch/include/python3.10 -c -c /__w/ColossalAI/ColossalAI/TensorNVMe/csrc/pthread_backend.cpp -o /__w/ColossalAI/ColossalAI/TensorNVMe/build/temp.linux-x86_64-cpython-310/__w/ColossalAI/ColossalAI/TensorNVMe/csrc/pthread_backend.o -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE="_gcc"' '-DPYBIND11_STDLIB="_libstdcpp"' '-DPYBIND11_BUILD_ABI="_cxxabi1011"' -DTORCH_EXTENSION_NAME=_C -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++17
In file included from /__w/ColossalAI/ColossalAI/TensorNVMe/csrc/pthread_backend.cpp:1:
/__w/ColossalAI/ColossalAI/TensorNVMe/include/pthread_backend.h: In constructor ‘PthreadAsyncIO::PthreadAsyncIO(unsigned int, unsigned int)’:
/__w/ColossalAI/ColossalAI/TensorNVMe/include/pthread_backend.h:38:18: warning: ‘PthreadAsyncIO::total_tasks’ will be initialized after [-Wreorder]
38 |     unsigned int total_tasks;
^~~~~~~~~~~
/__w/ColossalAI/ColossalAI/TensorNVMe/include/pthread_backend.h:29:18: warning:   ‘unsigned int PthreadAsyncIO::total_h2d’ [-Wreorder]
29 |     unsigned int total_h2d;
^~~~~~~~~
/__w/ColossalAI/ColossalAI/TensorNVMe/include/pthread_backend.h:41:5: warning:   when initialized here [-Wreorder]
41 |     PthreadAsyncIO(unsigned int n_entries, unsigned int n_tasks)
^~~~~~~~~~~~~~
[7/7] c++ -MMD -MF /__w/ColossalAI/ColossalAI/TensorNVMe/build/temp.linux-x86_64-cpython-310/__w/ColossalAI/ColossalAI/TensorNVMe/csrc/py_api.o.d -pthread -B /opt/conda/envs/pytorch/compiler_compat -Wno-unused-result -Wsign-compare -DNDEBUG -fwrapv -O2 -Wall -fPIC -O2 -isystem /opt/conda/envs/pytorch/include -fPIC -O2 -isystem /opt/conda/envs/pytorch/include -fPIC -DDISABLE_URING -I/__w/ColossalAI/ColossalAI/TensorNVMe/csrc -I/__w/ColossalAI/ColossalAI/TensorNVMe/include -I/github/home/.tensornvme/include -I/usr/local/cuda/include -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include/TH -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include/THC -I/opt/conda/envs/pytorch/include/python3.10 -c -c /__w/ColossalAI/ColossalAI/TensorNVMe/csrc/py_api.cpp -o /__w/ColossalAI/ColossalAI/TensorNVMe/build/temp.linux-x86_64-cpython-310/__w/ColossalAI/ColossalAI/TensorNVMe/csrc/py_api.o -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE="_gcc"' '-DPYBIND11_STDLIB="_libstdcpp"' '-DPYBIND11_BUILD_ABI="_cxxabi1011"' -DTORCH_EXTENSION_NAME=_C -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++17
g++ -pthread -B /opt/conda/envs/pytorch/compiler_compat -shared -Wl,-rpath,/opt/conda/envs/pytorch/lib -Wl,-rpath-link,/opt/conda/envs/pytorch/lib -L/opt/conda/envs/pytorch/lib -Wl,-rpath,/opt/conda/envs/pytorch/lib -Wl,-rpath-link,/opt/conda/envs/pytorch/lib -L/opt/conda/envs/pytorch/lib /__w/ColossalAI/ColossalAI/TensorNVMe/build/temp.linux-x86_64-cpython-310/__w/ColossalAI/ColossalAI/TensorNVMe/csrc/aio.o /__w/ColossalAI/ColossalAI/TensorNVMe/build/temp.linux-x86_64-cpython-310/__w/ColossalAI/ColossalAI/TensorNVMe/csrc/async_file_io.o /__w/ColossalAI/ColossalAI/TensorNVMe/build/temp.linux-x86_64-cpython-310/__w/ColossalAI/ColossalAI/TensorNVMe/csrc/backend.o /__w/ColossalAI/ColossalAI/TensorNVMe/build/temp.linux-x86_64-cpython-310/__w/ColossalAI/ColossalAI/TensorNVMe/csrc/offload.o /__w/ColossalAI/ColossalAI/TensorNVMe/build/temp.linux-x86_64-cpython-310/__w/ColossalAI/ColossalAI/TensorNVMe/csrc/pthread_backend.o /__w/ColossalAI/ColossalAI/TensorNVMe/build/temp.linux-x86_64-cpython-310/__w/ColossalAI/ColossalAI/TensorNVMe/csrc/py_api.o /__w/ColossalAI/ColossalAI/TensorNVMe/build/temp.linux-x86_64-cpython-310/__w/ColossalAI/ColossalAI/TensorNVMe/csrc/space_mgr.o -L/github/home/.tensornvme/lib -L/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib -laio -lc10 -ltorch -ltorch_cpu -ltorch_python -o build/lib.linux-x86_64-cpython-310/tensornvme/_C.cpython-310-x86_64-linux-gnu.so
/opt/conda/envs/pytorch/lib/python3.10/site-packages/setuptools/_distutils/cmd.py:66: SetuptoolsDeprecationWarning: setup.py install is deprecated.
!!
2025-04-11T03:54:09.7310233Z
********************************************************************************
Please avoid running ``setup.py`` directly.
Instead, use pypa/build, pypa/installer or other
standards-based tools.
2025-04-11T03:54:09.7312818Z
See https://blog.ganssle.io/articles/2021/10/setup-py-deprecated.html for details.
********************************************************************************
2025-04-11T03:54:09.7315661Z
!!
self.initialize_options()
installing to build/bdist.linux-x86_64/wheel
running install
running install_lib
creating build/bdist.linux-x86_64
creating build/bdist.linux-x86_64/wheel
creating build/bdist.linux-x86_64/wheel/tensornvme
copying build/lib.linux-x86_64-cpython-310/tensornvme/offload.py -> build/bdist.linux-x86_64/wheel/tensornvme
copying build/lib.linux-x86_64-cpython-310/tensornvme/__init__.py -> build/bdist.linux-x86_64/wheel/tensornvme
copying build/lib.linux-x86_64-cpython-310/tensornvme/_C.cpython-310-x86_64-linux-gnu.so -> build/bdist.linux-x86_64/wheel/tensornvme
creating build/bdist.linux-x86_64/wheel/tensornvme/cli
copying build/lib.linux-x86_64-cpython-310/tensornvme/cli/check.py -> build/bdist.linux-x86_64/wheel/tensornvme/cli
copying build/lib.linux-x86_64-cpython-310/tensornvme/cli/__init__.py -> build/bdist.linux-x86_64/wheel/tensornvme/cli
copying build/lib.linux-x86_64-cpython-310/tensornvme/cli/cli.py -> build/bdist.linux-x86_64/wheel/tensornvme/cli
copying build/lib.linux-x86_64-cpython-310/tensornvme/async_file_io.py -> build/bdist.linux-x86_64/wheel/tensornvme
running install_egg_info
running egg_info
creating tensornvme.egg-info
writing tensornvme.egg-info/PKG-INFO
writing dependency_links to tensornvme.egg-info/dependency_links.txt
writing entry points to tensornvme.egg-info/entry_points.txt
writing requirements to tensornvme.egg-info/requires.txt
writing top-level names to tensornvme.egg-info/top_level.txt
writing manifest file 'tensornvme.egg-info/SOURCES.txt'
reading manifest file 'tensornvme.egg-info/SOURCES.txt'
reading manifest template 'MANIFEST.in'
writing manifest file 'tensornvme.egg-info/SOURCES.txt'
Copying tensornvme.egg-info to build/bdist.linux-x86_64/wheel/tensornvme-0.1.0-py3.10.egg-info
running install_scripts
creating build/bdist.linux-x86_64/wheel/tensornvme-0.1.0.dist-info/WHEEL
creating '/tmp/pip-wheel-mebhwq1p/tensornvme-0.1.0-cp310-cp310-linux_x86_64.whl' and adding 'build/bdist.linux-x86_64/wheel' to it
adding 'tensornvme/_C.cpython-310-x86_64-linux-gnu.so'
adding 'tensornvme/__init__.py'
adding 'tensornvme/async_file_io.py'
adding 'tensornvme/offload.py'
adding 'tensornvme/cli/__init__.py'
adding 'tensornvme/cli/check.py'
adding 'tensornvme/cli/cli.py'
adding 'tensornvme-0.1.0.dist-info/METADATA'
adding 'tensornvme-0.1.0.dist-info/WHEEL'
adding 'tensornvme-0.1.0.dist-info/entry_points.txt'
adding 'tensornvme-0.1.0.dist-info/top_level.txt'
adding 'tensornvme-0.1.0.dist-info/RECORD'
removing build/bdist.linux-x86_64/wheel
Building wheel for tensornvme (setup.py): finished with status 'done'
Created wheel for tensornvme: filename=tensornvme-0.1.0-cp310-cp310-linux_x86_64.whl size=147126 sha256=98ec0dac63584137f1bbdb4b3e3d7ff01d6db14c7eba9fb6e23f56b07e7d651d
Stored in directory: /tmp/pip-ephem-wheel-cache-8_46q4or/wheels/cb/3c/a4/391c1ae2f1a321082a466078f0646d215673629d6cbb874a65
Successfully built tensornvme
Installing collected packages: tensornvme
changing mode of /opt/conda/envs/pytorch/bin/tensornvme to 755
Successfully installed tensornvme-0.1.0
WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv
##[group]Run cd TensorNVMe
[36;1mcd TensorNVMe[0m
[36;1mcp -p -r ./build /github/home/tensornvme_cache/[0m
[36;1mcp -p -r ./cmake-build /github/home/tensornvme_cache/[0m
shell: bash --noprofile --norc -e -o pipefail {0}
##[endgroup]
actions/checkout@v2

hpcaitech/ColossalAI


true

1

false

##[endgroup]
##[command]/usr/bin/docker exec  920416532ddad0112e30de68b3d3249d8793b2fa61e8416f9de3eb685dff9f0f sh -c "cat /etc/*release | grep ^ID"
hpcaitech/ColossalAI

Working directory is '/__w/ColossalAI/ColossalAI'

2 25 1
##[endgroup]
Temporarily overriding HOME='/__w/_temp/10676da8-a365-4caf-9d27-e077551f087c' before making global git config changes

[command]/usr/bin/git config --global --add safe.directory /__w/ColossalAI/ColossalAI
[command]/usr/bin/git config --local --get remote.origin.url
https://github.com/hpcaitech/ColossalAI
##[group]Removing previously created refs, to avoid conflicts
[command]/usr/bin/git rev-parse --symbolic-full-name --verify --quiet HEAD
HEAD
[command]/usr/bin/git rev-parse --symbolic-full-name --branches
##[endgroup]
##[group]Cleaning the repository
[command]/usr/bin/git clean -ffdx
Removing TensorNVMe/
[command]/usr/bin/git reset --hard HEAD
HEAD is now at dc60efe1 [pre-commit.ci] auto fixes from pre-commit.com hooks
##[endgroup]

[command]/usr/bin/git config --local gc.auto 0
##[endgroup]

core\.sshCommand
'core\.sshCommand' 'core.sshCommand'
http\.https\:\/\/github\.com\/\.extraheader
'http\.https\:\/\/github\.com\/\.extraheader' 'http.https://github.com/.extraheader'
--local
##[endgroup]

2 1 178be2e194a3ace2a537d7496efc179ea913eaee 6254
Enumerating 16, done.
Counting 6% (1/16)
Counting 12% (2/16)
Counting 18% (3/16)
Counting 25% (4/16)
Counting 31% (5/16)
Counting 37% (6/16)
Counting 43% (7/16)
Counting 50% (8/16)
Counting 56% (9/16)
Counting 62% (10/16)
Counting 68% (11/16)
Counting 75% (12/16)
Counting 81% (13/16)
Counting 87% (14/16)
Counting 93% (15/16)
Counting 100% (16/16)
Counting 100 16 16
Compressing 100% (1/1)
Compressing 100 1 1
6 4 5 4 0 0
https://github.com/hpcaitech/ColossalAI
+ 05b5218a...178be2e1 178be2e194a3ace2a537d7496efc179ea913eaee -> pull/6254/merge  (forced update)
##[endgroup]

##[endgroup]

6254
Warning: you are leaving 44 commits behind, not connected to
any of your branches:
2025-04-11T03:54:12.1222563Z
dc60efe1 [pre-commit.ci] auto fixes from pre-commit.com hooks
fd69a821 fix
db4c73f6 fix
0950b07a [pre-commit.ci] auto fixes from pre-commit.com hooks
... and 40 more.
2025-04-11T03:54:12.1223926Z
If you want to keep them by creating a new branch, this may be a good time
to do so with:
2025-04-11T03:54:12.1224529Z
git branch <new-branch-name> dc60efe1
2025-04-11T03:54:12.1224815Z
178be2e1 dc60efe1545b4eb9fa84ba2816d45af499f22b40 44d4053fec005fe0b06b6bc755fdc962463145df
##[endgroup]
-1 --format='%H'
178be2e194a3ace2a537d7496efc179ea913eaee
##[group]Run # -p flag is required to preserve the file timestamp to avoid ninja rebuild
[36;1m# -p flag is required to preserve the file timestamp to avoid ninja rebuild[0m
[36;1mif [ -d /github/home/cuda_ext_cache ] && [ ! -z "$(ls -A /github/home/cuda_ext_cache/)" ]; then[0m
[36;1m  cp -p -r /github/home/cuda_ext_cache/* /__w/ColossalAI/ColossalAI/[0m
36
shell: bash --noprofile --norc -e -o pipefail {0}
##[endgroup]
##[group]Run BUILD_EXT=1 pip install -v -e .
[36;1mBUILD_EXT=1 pip install -v -e .[0m
[36;1mpip install --no-cache-dir -r requirements/requirements-test.txt[0m
shell: bash --noprofile --norc -e -o pipefail {0}
##[endgroup]
Using pip 24.0 from /opt/conda/envs/pytorch/lib/python3.10/site-packages/pip (python 3.10)
Obtaining file:///__w/ColossalAI/ColossalAI
Preparing metadata (setup.py): started
Running command python setup.py egg_info
[extension] Building extensionscpu_adam_x86, layernorm_cuda, moe_cuda, fused_optim_cuda, inference_ops_cuda, scaled_masked_softmax_cuda, scaled_upper_triangle_masked_softmax_cuda
running egg_info
creating /tmp/pip-pip-egg-info-86f3rf9l/colossalai.egg-info
writing /tmp/pip-pip-egg-info-86f3rf9l/colossalai.egg-info/PKG-INFO
writing dependency_links to /tmp/pip-pip-egg-info-86f3rf9l/colossalai.egg-info/dependency_links.txt
writing entry points to /tmp/pip-pip-egg-info-86f3rf9l/colossalai.egg-info/entry_points.txt
writing requirements to /tmp/pip-pip-egg-info-86f3rf9l/colossalai.egg-info/requires.txt
writing top-level names to /tmp/pip-pip-egg-info-86f3rf9l/colossalai.egg-info/top_level.txt
writing manifest file '/tmp/pip-pip-egg-info-86f3rf9l/colossalai.egg-info/SOURCES.txt'
reading manifest file '/tmp/pip-pip-egg-info-86f3rf9l/colossalai.egg-info/SOURCES.txt'
reading manifest template 'MANIFEST.in'
warning: no files found matching '*.tr' under directory 'colossalai'
warning: no files found matching '*.cc' under directory 'colossalai'
warning: no files found matching '*.pyi' under directory 'colossalai'
warning: no files found matching '*.tr' under directory 'extensions'
warning: no files found matching '*.cc' under directory 'extensions'
warning: no files found matching '*.pyi' under directory 'extensions'
adding license file 'LICENSE'
writing manifest file '/tmp/pip-pip-egg-info-86f3rf9l/colossalai.egg-info/SOURCES.txt'
Preparing metadata (setup.py): finished with status 'done'
Requirement already satisfied: numpy in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from colossalai==0.4.9) (1.26.4)
tqdm colossalai==0.4.9)
Obtaining dependency information for tqdm from https://files.pythonhosted.org/packages/d0/30/dc54f88dd4a2b5dc8a0279bdd7270e735851848b762aeb1c1184ed1f6b14/tqdm-4.67.1-py3-none-any.whl.metadata
tqdm-4.67.1-py3-none-any.whl.metadata (57
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 57.7/57.7 kB 689.2 kB/s eta 0:00:00
psutil colossalai==0.4.9)
Obtaining dependency information for psutil from https://files.pythonhosted.org/packages/bf/b9/b0eb3f3cbcb734d930fdf839431606844a825b23eaf9a6ab371edac8162c/psutil-7.0.0-cp36-abi3-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata
Downloading psutil-7.0.0-cp36-abi3-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (22 kB)
Requirement already satisfied: packaging in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from colossalai==0.4.9) (24.1)
pre-commit colossalai==0.4.9)
Obtaining dependency information for pre-commit from https://files.pythonhosted.org/packages/88/74/a88bf1b1efeae488a0c0b7bdf71429c313722d1fc0f377537fbe554e6180/pre_commit-4.2.0-py2.py3-none-any.whl.metadata
pre commit-4.2.0-py2.py3-none-any.whl.metadata (1.3
rich colossalai==0.4.9)
Obtaining dependency information for rich from https://files.pythonhosted.org/packages/0d/9b/63f4c7ebc259242c89b3acafdb37b41d1185c07ff0011164674e9076b491/rich-14.0.0-py3-none-any.whl.metadata
rich-14.0.0-py3-none-any.whl.metadata (18
Requirement already satisfied: click in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from colossalai==0.4.9) (8.1.8)
fabric colossalai==0.4.9)
Obtaining dependency information for fabric from https://files.pythonhosted.org/packages/d6/1f/e99e23ee01847147fa194e8d41cfcf2535a2dbfcb51414c541cadb15c5d7/fabric-3.2.2-py3-none-any.whl.metadata
fabric-3.2.2-py3-none-any.whl.metadata (3.5
contexttimer colossalai==0.4.9)
contexttimer-0.3.3.tar.gz (4.9
Preparing metadata (setup.py): started
Running command python setup.py egg_info
running egg_info
creating /tmp/pip-pip-egg-info-0wwqld_8/contexttimer.egg-info
writing /tmp/pip-pip-egg-info-0wwqld_8/contexttimer.egg-info/PKG-INFO
writing dependency_links to /tmp/pip-pip-egg-info-0wwqld_8/contexttimer.egg-info/dependency_links.txt
writing top-level names to /tmp/pip-pip-egg-info-0wwqld_8/contexttimer.egg-info/top_level.txt
writing manifest file '/tmp/pip-pip-egg-info-0wwqld_8/contexttimer.egg-info/SOURCES.txt'
reading manifest file '/tmp/pip-pip-egg-info-0wwqld_8/contexttimer.egg-info/SOURCES.txt'
writing manifest file '/tmp/pip-pip-egg-info-0wwqld_8/contexttimer.egg-info/SOURCES.txt'
Preparing metadata (setup.py): finished with status 'done'
Requirement already satisfied: ninja in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from colossalai==0.4.9) (1.11.1.1)
Requirement already satisfied: torch<=2.5.1,>=2.2.0 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from colossalai==0.4.9) (2.2.2)
safetensors colossalai==0.4.9)
Obtaining dependency information for safetensors from https://files.pythonhosted.org/packages/a6/f8/dae3421624fcc87a89d42e1898a798bc7ff72c61f38973a65d60df8f124c/safetensors-0.5.3-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata
Downloading safetensors-0.5.3-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)
einops colossalai==0.4.9)
Obtaining dependency information for einops from https://files.pythonhosted.org/packages/87/62/9773de14fe6c45c23649e98b83231fffd7b9892b6cf863251dc2afa73643/einops-0.8.1-py3-none-any.whl.metadata
einops-0.8.1-py3-none-any.whl.metadata (13
pydantic colossalai==0.4.9)
Obtaining dependency information for pydantic from https://files.pythonhosted.org/packages/b0/1d/407b29780a289868ed696d1616f4aad49d6388e5a77f567dcd2629dcd7b8/pydantic-2.11.3-py3-none-any.whl.metadata
pydantic-2.11.3-py3-none-any.whl.metadata (65
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 65.2/65.2 kB 978.3 kB/s eta 0:00:00
ray colossalai==0.4.9)
Obtaining dependency information for ray from https://files.pythonhosted.org/packages/93/f1/9108c4f878e3cacb767b7dfbbc3a26537c79ab516d2530b9f63b558ba4bb/ray-2.44.1-cp310-cp310-manylinux2014_x86_64.whl.metadata
Downloading ray-2.44.1-cp310-cp310-manylinux2014_x86_64.whl.metadata (19 kB)
sentencepiece colossalai==0.4.9)
Obtaining dependency information for sentencepiece from https://files.pythonhosted.org/packages/a6/27/33019685023221ca8ed98e8ceb7ae5e166032686fa3662c68f1f1edf334e/sentencepiece-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata
Downloading sentencepiece-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)
google colossalai==0.4.9)
Obtaining dependency information for google from https://files.pythonhosted.org/packages/ac/35/17c9141c4ae21e9a29a43acdfd848e3e468a810517f862cad07977bf8fe9/google-3.0.0-py2.py3-none-any.whl.metadata
Downloading google-3.0.0-py2.py3-none-any.whl.metadata (627 bytes)
protobuf colossalai==0.4.9)
Obtaining dependency information for protobuf from https://files.pythonhosted.org/packages/28/50/1925de813499546bc8ab3ae857e3ec84efe7d2f19b34529d0c7c3d02d11d/protobuf-6.30.2-cp39-abi3-manylinux2014_x86_64.whl.metadata
Downloading protobuf-6.30.2-cp39-abi3-manylinux2014_x86_64.whl.metadata (593 bytes)
transformers==4.39.3 colossalai==0.4.9)
Obtaining dependency information for transformers==4.39.3 from https://files.pythonhosted.org/packages/15/fc/7b6dd7e1adc0a6407b845ed4be1999e98b6917d0694e57316d140cc85484/transformers-4.39.3-py3-none-any.whl.metadata
transformers-4.39.3-py3-none-any.whl.metadata (134
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 134.8/134.8 kB 2.2 MB/s eta 0:00:00
peft<=0.13.2,>=0.7.1 colossalai==0.4.9)
Obtaining dependency information for peft<=0.13.2,>=0.7.1 from https://files.pythonhosted.org/packages/78/9d/5f95bfb298c8d3b4e3a107701f9a4e7774a0d4d1f8eb0c9d5420b80f7c9d/peft-0.13.2-py3-none-any.whl.metadata
peft-0.13.2-py3-none-any.whl.metadata (13
bitsandbytes>=0.39.0 colossalai==0.4.9)
Obtaining dependency information for bitsandbytes>=0.39.0 from https://files.pythonhosted.org/packages/07/b7/cb5ce4d1a382cf53c19ef06c5fc29e85f5e129b4da6527dd207d90a5b8ad/bitsandbytes-0.45.5-py3-none-manylinux_2_24_x86_64.whl.metadata
Downloading bitsandbytes-0.45.5-py3-none-manylinux_2_24_x86_64.whl.metadata (5.0 kB)
rpyc==6.0.0 colossalai==0.4.9)
Obtaining dependency information for rpyc==6.0.0 from https://files.pythonhosted.org/packages/f6/e7/38cd5f2d8ab0d259648f6672fd19de30688a22ae769f87616c6a2fe92581/rpyc-6.0.0-py3-none-any.whl.metadata
rpyc-6.0.0-py3-none-any.whl.metadata (3.5
fastapi colossalai==0.4.9)
Obtaining dependency information for fastapi from https://files.pythonhosted.org/packages/50/b3/b51f09c2ba432a576fe63758bddc81f78f0c6309d9e5c10d194313bf021e/fastapi-0.115.12-py3-none-any.whl.metadata
fastapi-0.115.12-py3-none-any.whl.metadata (27
uvicorn==0.29.0 colossalai==0.4.9)
Obtaining dependency information for uvicorn==0.29.0 from https://files.pythonhosted.org/packages/73/f5/cbb16fcbe277c1e0b8b3ddd188f2df0e0947f545c49119b589643632d156/uvicorn-0.29.0-py3-none-any.whl.metadata
uvicorn-0.29.0-py3-none-any.whl.metadata (6.3
Collecting galore_torch (from colossalai==0.4.9)
Obtaining dependency information for galore_torch from https://files.pythonhosted.org/packages/2b/b9/e9e88f989c62edefaa45df07198ce280ac0d373f5e2842686c6ece2ddb1e/galore_torch-1.0-py3-none-any.whl.metadata
Downloading galore_torch-1.0-py3-none-any.whl.metadata (355 bytes)
diffusers==0.29.0 colossalai==0.4.9)
Obtaining dependency information for diffusers==0.29.0 from https://files.pythonhosted.org/packages/d5/68/a84d929518c9fbd65febcedd7810203bcac393f9cddd0603ec58df7f93f7/diffusers-0.29.0-py3-none-any.whl.metadata
diffusers-0.29.0-py3-none-any.whl.metadata (19
importlib-metadata diffusers==0.29.0->colossalai==0.4.9)
Obtaining dependency information for importlib-metadata from https://files.pythonhosted.org/packages/79/9d/0fb148dc4d6fa4a7dd1d8378168d9b4cd8d4560a6fbf6f0121c5fc34eb68/importlib_metadata-8.6.1-py3-none-any.whl.metadata
importlib metadata-8.6.1-py3-none-any.whl.metadata (4.7
Requirement already satisfied: filelock in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from diffusers==0.29.0->colossalai==0.4.9) (3.13.1)
huggingface-hub>=0.23.2 diffusers==0.29.0->colossalai==0.4.9)
Obtaining dependency information for huggingface-hub>=0.23.2 from https://files.pythonhosted.org/packages/93/27/1fb384a841e9661faad1c31cbfa62864f59632e876df5d795234da51c395/huggingface_hub-0.30.2-py3-none-any.whl.metadata
huggingface hub-0.30.2-py3-none-any.whl.metadata (13
regex!=2019.12.17 diffusers==0.29.0->colossalai==0.4.9)
Obtaining dependency information for regex!=2019.12.17 from https://files.pythonhosted.org/packages/f2/98/26d3830875b53071f1f0ae6d547f1d98e964dd29ad35cbf94439120bb67a/regex-2024.11.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata
Downloading regex-2024.11.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 40.5/40.5 kB 541.3 kB/s eta 0:00:00
Requirement already satisfied: requests in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from diffusers==0.29.0->colossalai==0.4.9) (2.32.2)
Requirement already satisfied: Pillow in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from diffusers==0.29.0->colossalai==0.4.9) (10.3.0)
plumbum rpyc==6.0.0->colossalai==0.4.9)
Obtaining dependency information for plumbum from https://files.pythonhosted.org/packages/4f/9d/d03542c93bb3d448406731b80f39c3d5601282f778328c22c77d270f4ed4/plumbum-1.9.0-py3-none-any.whl.metadata
plumbum-1.9.0-py3-none-any.whl.metadata (10
Requirement already satisfied: pyyaml>=5.1 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from transformers==4.39.3->colossalai==0.4.9) (6.0.1)
tokenizers<0.19,>=0.14 transformers==4.39.3->colossalai==0.4.9)
Obtaining dependency information for tokenizers<0.19,>=0.14 from https://files.pythonhosted.org/packages/1c/5d/cf5e122ce4f1a29f165b2a69dc33d1ff30bce303343d58a54775ddba5d51/tokenizers-0.15.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata
Downloading tokenizers-0.15.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)
h11>=0.8 uvicorn==0.29.0->colossalai==0.4.9)
Obtaining dependency information for h11>=0.8 from https://files.pythonhosted.org/packages/95/04/ff642e65ad6b90db43e668d70ffb6736436c7ce41fcc549f4e9472234127/h11-0.14.0-py3-none-any.whl.metadata
h11-0.14.0-py3-none-any.whl.metadata (8.2
Requirement already satisfied: typing-extensions>=4.0 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from uvicorn==0.29.0->colossalai==0.4.9) (4.11.0)
accelerate>=0.21.0 peft<=0.13.2,>=0.7.1->colossalai==0.4.9)
Obtaining dependency information for accelerate>=0.21.0 from https://files.pythonhosted.org/packages/63/b1/8198e3cdd11a426b1df2912e3381018c4a4a55368f6d0857ba3ca418ef93/accelerate-1.6.0-py3-none-any.whl.metadata
accelerate-1.6.0-py3-none-any.whl.metadata (19
Requirement already satisfied: sympy in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from torch<=2.5.1,>=2.2.0->colossalai==0.4.9) (1.12)
Requirement already satisfied: networkx in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from torch<=2.5.1,>=2.2.0->colossalai==0.4.9) (3.2.1)
Requirement already satisfied: jinja2 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from torch<=2.5.1,>=2.2.0->colossalai==0.4.9) (3.1.4)
Requirement already satisfied: fsspec in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from torch<=2.5.1,>=2.2.0->colossalai==0.4.9) (2024.6.1)
invoke>=2.0 fabric->colossalai==0.4.9)
Obtaining dependency information for invoke>=2.0 from https://files.pythonhosted.org/packages/0a/66/7f8c48009c72d73bc6bbe6eb87ac838d6a526146f7dab14af671121eb379/invoke-2.2.0-py3-none-any.whl.metadata
invoke-2.2.0-py3-none-any.whl.metadata (3.3
paramiko>=2.4 fabric->colossalai==0.4.9)
Obtaining dependency information for paramiko>=2.4 from https://files.pythonhosted.org/packages/15/f8/c7bd0ef12954a81a1d3cea60a13946bd9a49a0036a5927770c461eade7ae/paramiko-3.5.1-py3-none-any.whl.metadata
paramiko-3.5.1-py3-none-any.whl.metadata (4.6
decorator>=5 fabric->colossalai==0.4.9)
Obtaining dependency information for decorator>=5 from https://files.pythonhosted.org/packages/4e/8c/f3147f5c4b73e7550fe5f9352eaa956ae838d5c51eb58e7a25b9f3e2643b/decorator-5.2.1-py3-none-any.whl.metadata
decorator-5.2.1-py3-none-any.whl.metadata (3.9
deprecated>=1.2 fabric->colossalai==0.4.9)
Obtaining dependency information for deprecated>=1.2 from https://files.pythonhosted.org/packages/6e/c6/ac0b6c1e2d138f1002bcf799d330bd6d85084fece321e662a14223794041/Deprecated-1.2.18-py2.py3-none-any.whl.metadata
Deprecated-1.2.18-py2.py3-none-any.whl.metadata (5.7
starlette<0.47.0,>=0.40.0 fastapi->colossalai==0.4.9)
Obtaining dependency information for starlette<0.47.0,>=0.40.0 from https://files.pythonhosted.org/packages/a0/4b/528ccf7a982216885a1ff4908e886b8fb5f19862d1962f56a3fce2435a70/starlette-0.46.1-py3-none-any.whl.metadata
starlette-0.46.1-py3-none-any.whl.metadata (6.2
annotated-types>=0.6.0 pydantic->colossalai==0.4.9)
Obtaining dependency information for annotated-types>=0.6.0 from https://files.pythonhosted.org/packages/78/b6/6307fbef88d9b5ee7421e68d78a9f162e0da4900bc5f5793f6d3d0e34fb8/annotated_types-0.7.0-py3-none-any.whl.metadata
annotated types-0.7.0-py3-none-any.whl.metadata (15
pydantic-core==2.33.1 pydantic->colossalai==0.4.9)
Obtaining dependency information for pydantic-core==2.33.1 from https://files.pythonhosted.org/packages/f2/68/866ce83a51dd37e7c604ce0050ff6ad26de65a7799df89f4db87dd93d1d6/pydantic_core-2.33.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata
Downloading pydantic_core-2.33.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)
typing-extensions>=4.0 uvicorn==0.29.0->colossalai==0.4.9)
Obtaining dependency information for typing-extensions>=4.0 from https://files.pythonhosted.org/packages/8b/54/b1ae86c0973cc6f0210b53d508ca3641fb6d0c56823f288d108bc7ab3cc8/typing_extensions-4.13.2-py3-none-any.whl.metadata
typing extensions-4.13.2-py3-none-any.whl.metadata (3.0
typing-inspection>=0.4.0 pydantic->colossalai==0.4.9)
Obtaining dependency information for typing-inspection>=0.4.0 from https://files.pythonhosted.org/packages/31/08/aa4fdfb71f7de5176385bd9e90852eaf6b5d622735020ad600f2bab54385/typing_inspection-0.4.0-py3-none-any.whl.metadata
typing inspection-0.4.0-py3-none-any.whl.metadata (2.6
beautifulsoup4 google->colossalai==0.4.9)
Obtaining dependency information for beautifulsoup4 from https://files.pythonhosted.org/packages/f9/49/6abb616eb3cbab6a7cca303dc02fdf3836de2e0b834bf966a7f5271a34d8/beautifulsoup4-4.13.3-py3-none-any.whl.metadata
beautifulsoup4-4.13.3-py3-none-any.whl.metadata (3.8
cfgv>=2.0.0 pre-commit->colossalai==0.4.9)
Obtaining dependency information for cfgv>=2.0.0 from https://files.pythonhosted.org/packages/c5/55/51844dd50c4fc7a33b653bfaba4c2456f06955289ca770a5dbd5fd267374/cfgv-3.4.0-py2.py3-none-any.whl.metadata
cfgv-3.4.0-py2.py3-none-any.whl.metadata (8.5
identify>=1.0.0 pre-commit->colossalai==0.4.9)
Obtaining dependency information for identify>=1.0.0 from https://files.pythonhosted.org/packages/07/ce/0845144ed1f0e25db5e7a79c2354c1da4b5ce392b8966449d5db8dca18f1/identify-2.6.9-py2.py3-none-any.whl.metadata
identify-2.6.9-py2.py3-none-any.whl.metadata (4.4
nodeenv>=0.11.1 pre-commit->colossalai==0.4.9)
Obtaining dependency information for nodeenv>=0.11.1 from https://files.pythonhosted.org/packages/d2/1d/1b658dbd2b9fa9c4c9f32accbfc0205d532c8c6194dc0f2a4c0428e7128a/nodeenv-1.9.1-py2.py3-none-any.whl.metadata
nodeenv-1.9.1-py2.py3-none-any.whl.metadata (21
virtualenv>=20.10.0 pre-commit->colossalai==0.4.9)
Obtaining dependency information for virtualenv>=20.10.0 from https://files.pythonhosted.org/packages/4c/ed/3cfeb48175f0671ec430ede81f628f9fb2b1084c9064ca67ebe8c0ed6a05/virtualenv-20.30.0-py3-none-any.whl.metadata
virtualenv-20.30.0-py3-none-any.whl.metadata (4.5
jsonschema ray->colossalai==0.4.9)
Obtaining dependency information for jsonschema from https://files.pythonhosted.org/packages/69/4a/4f9dbeb84e8850557c02365a0eee0649abe5eb1d84af92a25731c6c0f922/jsonschema-4.23.0-py3-none-any.whl.metadata
jsonschema-4.23.0-py3-none-any.whl.metadata (7.9
msgpack<2.0.0,>=1.0.0 ray->colossalai==0.4.9)
Obtaining dependency information for msgpack<2.0.0,>=1.0.0 from https://files.pythonhosted.org/packages/ff/75/09081792db60470bef19d9c2be89f024d366b1e1973c197bb59e6aabc647/msgpack-1.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata
Downloading msgpack-1.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.4 kB)
aiosignal ray->colossalai==0.4.9)
Obtaining dependency information for aiosignal from https://files.pythonhosted.org/packages/ec/6a/bc7e17a3e87a2985d3e8f4da4cd0f481060eb78fb08596c42be62c90a4d9/aiosignal-1.3.2-py2.py3-none-any.whl.metadata
aiosignal-1.3.2-py2.py3-none-any.whl.metadata (3.8
frozenlist ray->colossalai==0.4.9)
Obtaining dependency information for frozenlist from https://files.pythonhosted.org/packages/ee/59/928322800306f6529d1852323014ee9008551e9bb027cc38d276cbc0b0e7/frozenlist-1.5.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata
Downloading frozenlist-1.5.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)
markdown-it-py>=2.2.0 rich->colossalai==0.4.9)
Obtaining dependency information for markdown-it-py>=2.2.0 from https://files.pythonhosted.org/packages/42/d7/1ec15b46af6af88f19b8e5ffea08fa375d433c998b8a7639e76935c14f1f/markdown_it_py-3.0.0-py3-none-any.whl.metadata
Downloading markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)
pygments<3.0.0,>=2.13.0 rich->colossalai==0.4.9)
Obtaining dependency information for pygments<3.0.0,>=2.13.0 from https://files.pythonhosted.org/packages/8a/0b/9fcc47d19c48b59121088dd6da2488a49d5f72dacf8262e2790a1d2c7d15/pygments-2.19.1-py3-none-any.whl.metadata
pygments-2.19.1-py3-none-any.whl.metadata (2.5
wrapt<2,>=1.10 deprecated>=1.2->fabric->colossalai==0.4.9)
Obtaining dependency information for wrapt<2,>=1.10 from https://files.pythonhosted.org/packages/90/ec/00759565518f268ed707dcc40f7eeec38637d46b098a1f5143bff488fe97/wrapt-1.17.2-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata
Downloading wrapt-1.17.2-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.4 kB)
mdurl~=0.1 markdown-it-py>=2.2.0->rich->colossalai==0.4.9)
Obtaining dependency information for mdurl~=0.1 from https://files.pythonhosted.org/packages/b3/38/89ba8ad64ae25be8de66a6d463314cf1eb366222074cfda9ee839c56a4b4/mdurl-0.1.2-py3-none-any.whl.metadata
mdurl-0.1.2-py3-none-any.whl.metadata (1.6
bcrypt>=3.2 paramiko>=2.4->fabric->colossalai==0.4.9)
Obtaining dependency information for bcrypt>=3.2 from https://files.pythonhosted.org/packages/cb/c6/8fedca4c2ada1b6e889c52d2943b2f968d3427e5d65f595620ec4c06fa2f/bcrypt-4.3.0-cp39-abi3-manylinux_2_28_x86_64.whl.metadata
Downloading bcrypt-4.3.0-cp39-abi3-manylinux_2_28_x86_64.whl.metadata (10 kB)
cryptography>=3.3 paramiko>=2.4->fabric->colossalai==0.4.9)
Obtaining dependency information for cryptography>=3.3 from https://files.pythonhosted.org/packages/78/2b/999b2a1e1ba2206f2d3bca267d68f350beb2b048a41ea827e08ce7260098/cryptography-44.0.2-cp39-abi3-manylinux_2_28_x86_64.whl.metadata
Downloading cryptography-44.0.2-cp39-abi3-manylinux_2_28_x86_64.whl.metadata (5.7 kB)
pynacl>=1.5 paramiko>=2.4->fabric->colossalai==0.4.9)
Obtaining dependency information for pynacl>=1.5 from https://files.pythonhosted.org/packages/ee/87/f1bb6a595f14a327e8285b9eb54d41fef76c585a0edef0a45f6fc95de125/PyNaCl-1.5.0-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl.metadata
Downloading PyNaCl-1.5.0-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl.metadata (8.6 kB)
anyio<5,>=3.6.2 starlette<0.47.0,>=0.40.0->fastapi->colossalai==0.4.9)
Obtaining dependency information for anyio<5,>=3.6.2 from https://files.pythonhosted.org/packages/a1/ee/48ca1a7c89ffec8b6a0c5d02b89c305671d5ffd8d3c94acf8b8c408575bb/anyio-4.9.0-py3-none-any.whl.metadata
anyio-4.9.0-py3-none-any.whl.metadata (4.7
distlib<1,>=0.3.7 virtualenv>=20.10.0->pre-commit->colossalai==0.4.9)
Obtaining dependency information for distlib<1,>=0.3.7 from https://files.pythonhosted.org/packages/91/a1/cf2472db20f7ce4a6be1253a81cfdf85ad9c7885ffbed7047fb72c24cf87/distlib-0.3.9-py2.py3-none-any.whl.metadata
distlib-0.3.9-py2.py3-none-any.whl.metadata (5.2
platformdirs<5,>=3.9.1 virtualenv>=20.10.0->pre-commit->colossalai==0.4.9)
Obtaining dependency information for platformdirs<5,>=3.9.1 from https://files.pythonhosted.org/packages/6d/45/59578566b3275b8fd9157885918fcd0c4d74162928a5310926887b856a51/platformdirs-4.3.7-py3-none-any.whl.metadata
platformdirs-4.3.7-py3-none-any.whl.metadata (11
soupsieve>1.2 beautifulsoup4->google->colossalai==0.4.9)
Obtaining dependency information for soupsieve>1.2 from https://files.pythonhosted.org/packages/d1/c2/fe97d779f3ef3b15f05c94a2f1e3d21732574ed441687474db9d342a7315/soupsieve-2.6-py3-none-any.whl.metadata
soupsieve-2.6-py3-none-any.whl.metadata (4.6
zipp>=3.20 importlib-metadata->diffusers==0.29.0->colossalai==0.4.9)
Obtaining dependency information for zipp>=3.20 from https://files.pythonhosted.org/packages/b7/1a/7e4798e9339adc931158c9d69ecc34f5e6791489d469f5e50ec15e35f458/zipp-3.21.0-py3-none-any.whl.metadata
zipp-3.21.0-py3-none-any.whl.metadata (3.7
Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from jinja2->torch<=2.5.1,>=2.2.0->colossalai==0.4.9) (2.1.3)
attrs>=22.2.0 jsonschema->ray->colossalai==0.4.9)
Obtaining dependency information for attrs>=22.2.0 from https://files.pythonhosted.org/packages/77/06/bb80f5f86020c4551da315d78b3ab75e8228f89f0162f2c3a819e407941a/attrs-25.3.0-py3-none-any.whl.metadata
attrs-25.3.0-py3-none-any.whl.metadata (10
jsonschema-specifications>=2023.03.6 jsonschema->ray->colossalai==0.4.9)
Obtaining dependency information for jsonschema-specifications>=2023.03.6 from https://files.pythonhosted.org/packages/d1/0f/8910b19ac0670a0f80ce1008e5e751c4a57e14d2c4c13a482aa6079fa9d6/jsonschema_specifications-2024.10.1-py3-none-any.whl.metadata
jsonschema specifications-2024.10.1-py3-none-any.whl.metadata (3.0
referencing>=0.28.4 jsonschema->ray->colossalai==0.4.9)
Obtaining dependency information for referencing>=0.28.4 from https://files.pythonhosted.org/packages/c1/b1/3baf80dc6d2b7bc27a95a67752d0208e410351e3feb4eb78de5f77454d8d/referencing-0.36.2-py3-none-any.whl.metadata
referencing-0.36.2-py3-none-any.whl.metadata (2.8
rpds-py>=0.7.1 jsonschema->ray->colossalai==0.4.9)
Obtaining dependency information for rpds-py>=0.7.1 from https://files.pythonhosted.org/packages/a7/a7/6d04d438f53d8bb2356bb000bea9cf5c96a9315e405b577117e344cc7404/rpds_py-0.24.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata
Downloading rpds_py-0.24.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)
Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from requests->diffusers==0.29.0->colossalai==0.4.9) (2.0.4)
Requirement already satisfied: idna<4,>=2.5 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from requests->diffusers==0.29.0->colossalai==0.4.9) (3.7)
Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from requests->diffusers==0.29.0->colossalai==0.4.9) (2.2.2)
Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from requests->diffusers==0.29.0->colossalai==0.4.9) (2024.6.2)
Requirement already satisfied: mpmath>=0.19 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from sympy->torch<=2.5.1,>=2.2.0->colossalai==0.4.9) (1.3.0)
exceptiongroup>=1.0.2 anyio<5,>=3.6.2->starlette<0.47.0,>=0.40.0->fastapi->colossalai==0.4.9)
Obtaining dependency information for exceptiongroup>=1.0.2 from https://files.pythonhosted.org/packages/02/cc/b7e31358aac6ed1ef2bb790a9746ac2c69bcb3c8588b41616914eb106eaf/exceptiongroup-1.2.2-py3-none-any.whl.metadata
exceptiongroup-1.2.2-py3-none-any.whl.metadata (6.6
sniffio>=1.1 anyio<5,>=3.6.2->starlette<0.47.0,>=0.40.0->fastapi->colossalai==0.4.9)
Obtaining dependency information for sniffio>=1.1 from https://files.pythonhosted.org/packages/e9/44/75a9c9421471a6c4805dbf2356f7c181a29c1879239abab1ea2cc8f38b40/sniffio-1.3.1-py3-none-any.whl.metadata
sniffio-1.3.1-py3-none-any.whl.metadata (3.9
cffi>=1.12 cryptography>=3.3->paramiko>=2.4->fabric->colossalai==0.4.9)
Obtaining dependency information for cffi>=1.12 from https://files.pythonhosted.org/packages/8d/fb/4da72871d177d63649ac449aec2e8a29efe0274035880c7af59101ca2232/cffi-1.17.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata
Downloading cffi-1.17.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.5 kB)
pycparser cffi>=1.12->cryptography>=3.3->paramiko>=2.4->fabric->colossalai==0.4.9)
Obtaining dependency information for pycparser from https://files.pythonhosted.org/packages/13/a3/a812df4e2dd5696d1f351d58b8fe16a405b234ad2886a0dab9183fb78109/pycparser-2.22-py3-none-any.whl.metadata
Downloading pycparser-2.22-py3-none-any.whl.metadata (943 bytes)
Downloading diffusers-0.29.0-py3-none-any.whl (2.2 MB)
2 2 2 2 MB 6 5 0 00 00
rpyc-6.0.0-py3-none-any.whl (74
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 74.5/74.5 kB 928.9 kB/s eta 0:00:00
Downloading transformers-4.39.3-py3-none-any.whl (8.8 MB)
8 8 8 8 MB 32 2 0 00 00
uvicorn-0.29.0-py3-none-any.whl (60
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 60.8/60.8 kB 731.5 kB/s eta 0:00:00
Downloading bitsandbytes-0.45.5-py3-none-manylinux_2_24_x86_64.whl (76.1 MB)
76 1 76 1 MB 17 3 0 00 00
peft-0.13.2-py3-none-any.whl (320
320 7 320 7 kB 4 5 0 00 00
Downloading safetensors-0.5.3-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (471 kB)
471 6 471 6 kB 6 5 0 00 00
tqdm-4.67.1-py3-none-any.whl (78
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 78.5/78.5 kB 987.4 kB/s eta 0:00:00
einops-0.8.1-py3-none-any.whl (64
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 64.4/64.4 kB 781.6 kB/s eta 0:00:00
fabric-3.2.2-py3-none-any.whl (59
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 59.4/59.4 kB 713.9 kB/s eta 0:00:00
fastapi-0.115.12-py3-none-any.whl (95
95 2 95 2 kB 1 2 0 00 00
pydantic-2.11.3-py3-none-any.whl (443
443 6 443 6 kB 6 2 0 00 00
Downloading pydantic_core-2.33.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)
2 0 2 0 MB 24 8 0 00 00
galore torch-1.0-py3-none-any.whl (13
google-3.0.0-py2.py3-none-any.whl (45
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 45.3/45.3 kB 508.6 kB/s eta 0:00:00
pre commit-4.2.0-py2.py3-none-any.whl (220
220 7 220 7 kB 3 0 0 00 00
Downloading protobuf-6.30.2-cp39-abi3-manylinux2014_x86_64.whl (316 kB)
316 2 316 2 kB 4 4 0 00 00
Downloading psutil-7.0.0-cp36-abi3-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (277 kB)
278 0 278 0 kB 3 9 0 00 00
Downloading ray-2.44.1-cp310-cp310-manylinux2014_x86_64.whl (67.9 MB)
67 9 67 9 MB 22 0 0 00 00
rich-14.0.0-py3-none-any.whl (243
243 2 243 2 kB 3 3 0 00 00
Downloading sentencepiece-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)
1 3 1 3 MB 17 9 0 00 00
accelerate-1.6.0-py3-none-any.whl (354
354 7 354 7 kB 5 0 0 00 00
annotated types-0.7.0-py3-none-any.whl (13
cfgv-3.4.0-py2.py3-none-any.whl (7.2
decorator-5.2.1-py3-none-any.whl (9.2
Deprecated-1.2.18-py2.py3-none-any.whl (10.0
h11-0.14.0-py3-none-any.whl (58
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 58.3/58.3 kB 695.5 kB/s eta 0:00:00
huggingface hub-0.30.2-py3-none-any.whl (481
481 4 481 4 kB 6 7 0 00 00
identify-2.6.9-py2.py3-none-any.whl (99
99 1 99 1 kB 1 3 0 00 00
invoke-2.2.0-py3-none-any.whl (160
160 3 160 3 kB 2 2 0 00 00
Downloading markdown_it_py-3.0.0-py3-none-any.whl (87 kB)
87 5 87 5 kB 1 1 0 00 00
Downloading msgpack-1.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (378 kB)
378 0 378 0 kB 5 3 0 00 00
nodeenv-1.9.1-py2.py3-none-any.whl (22
paramiko-3.5.1-py3-none-any.whl (227
227 3 227 3 kB 3 1 0 00 00
Downloading pygments-2.19.1-py3-none-any.whl (1.2 MB)
1 2 1 2 MB 16 8 0 00 00
Downloading regex-2024.11.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (781 kB)
781 7 781 7 kB 10 9 0 00 00
starlette-0.46.1-py3-none-any.whl (71
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 72.0/72.0 kB 895.9 kB/s eta 0:00:00
Downloading tokenizers-0.15.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)
3 6 3 6 MB 31 4 0 00 00
typing extensions-4.13.2-py3-none-any.whl (45
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 45.8/45.8 kB 515.0 kB/s eta 0:00:00
typing inspection-0.4.0-py3-none-any.whl (14
20 30 0 4 3
4 3 4 3 MB 31 8 0 00 00
aiosignal-1.3.2-py2.py3-none-any.whl (7.6
Downloading frozenlist-1.5.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (241 kB)
241 9 241 9 kB 3 3 0 00 00
beautifulsoup4-4.13.3-py3-none-any.whl (186
186 0 186 0 kB 2 5 0 00 00
importlib metadata-8.6.1-py3-none-any.whl (26
jsonschema-4.23.0-py3-none-any.whl (88
88 5 88 5 kB 1 1 0 00 00
plumbum-1.9.0-py3-none-any.whl (127
128 0 128 0 kB 1 7 0 00 00
anyio-4.9.0-py3-none-any.whl (100
100 9 100 9 kB 1 3 0 00 00
attrs-25.3.0-py3-none-any.whl (63
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 63.8/63.8 kB 776.0 kB/s eta 0:00:00
Downloading bcrypt-4.3.0-cp39-abi3-manylinux_2_28_x86_64.whl (284 kB)
284 5 284 5 kB 3 9 0 00 00
Downloading cryptography-44.0.2-cp39-abi3-manylinux_2_28_x86_64.whl (4.2 MB)
4 2 4 2 MB 16 0 0 00 00
distlib-0.3.9-py2.py3-none-any.whl (468
469 0 469 0 kB 6 6 0 00 00
jsonschema specifications-2024.10.1-py3-none-any.whl (18
mdurl-0.1.2-py3-none-any.whl (10.0
platformdirs-4.3.7-py3-none-any.whl (18
Downloading PyNaCl-1.5.0-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl (856 kB)
856 7 856 7 kB 7 3 0 00 00
referencing-0.36.2-py3-none-any.whl (26
Downloading rpds_py-0.24.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (389 kB)
389 5 389 5 kB 5 4 0 00 00
soupsieve-2.6-py3-none-any.whl (36
Downloading wrapt-1.17.2-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (82 kB)
82 8 82 8 kB 1 1 0 00 00
zipp-3.21.0-py3-none-any.whl (9.6
Downloading cffi-1.17.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (446 kB)
446 2 446 2 kB 6 2 0 00 00
exceptiongroup-1.2.2-py3-none-any.whl (16
sniffio-1.3.1-py3-none-any.whl (10
pycparser-2.22-py3-none-any.whl (117
117 6 117 6 kB 1 6 0 00 00
Building wheels for collected packages: contexttimer
Building wheel for contexttimer (setup.py): started
Running command python setup.py bdist_wheel
running bdist_wheel
running build
running build_py
creating build
creating build/lib
creating build/lib/contexttimer
copying contexttimer/__init__.py -> build/lib/contexttimer
copying contexttimer/timeout.py -> build/lib/contexttimer
/opt/conda/envs/pytorch/lib/python3.10/site-packages/setuptools/_distutils/cmd.py:66: SetuptoolsDeprecationWarning: setup.py install is deprecated.
!!
2025-04-11T03:54:56.5767569Z
********************************************************************************
Please avoid running ``setup.py`` directly.
Instead, use pypa/build, pypa/installer or other
standards-based tools.
2025-04-11T03:54:56.5769948Z
See https://blog.ganssle.io/articles/2021/10/setup-py-deprecated.html for details.
********************************************************************************
2025-04-11T03:54:56.5772909Z
!!
self.initialize_options()
installing to build/bdist.linux-x86_64/wheel
running install
running install_lib
creating build/bdist.linux-x86_64
creating build/bdist.linux-x86_64/wheel
creating build/bdist.linux-x86_64/wheel/contexttimer
copying build/lib/contexttimer/__init__.py -> build/bdist.linux-x86_64/wheel/contexttimer
copying build/lib/contexttimer/timeout.py -> build/bdist.linux-x86_64/wheel/contexttimer
running install_egg_info
running egg_info
writing contexttimer.egg-info/PKG-INFO
writing dependency_links to contexttimer.egg-info/dependency_links.txt
writing top-level names to contexttimer.egg-info/top_level.txt
reading manifest file 'contexttimer.egg-info/SOURCES.txt'
writing manifest file 'contexttimer.egg-info/SOURCES.txt'
Copying contexttimer.egg-info to build/bdist.linux-x86_64/wheel/contexttimer-0.3.3-py3.10.egg-info
running install_scripts
creating build/bdist.linux-x86_64/wheel/contexttimer-0.3.3.dist-info/WHEEL
creating '/tmp/pip-wheel-4e9r1l_e/contexttimer-0.3.3-py3-none-any.whl' and adding 'build/bdist.linux-x86_64/wheel' to it
adding 'contexttimer/__init__.py'
adding 'contexttimer/timeout.py'
adding 'contexttimer-0.3.3.dist-info/METADATA'
adding 'contexttimer-0.3.3.dist-info/WHEEL'
adding 'contexttimer-0.3.3.dist-info/top_level.txt'
adding 'contexttimer-0.3.3.dist-info/RECORD'
removing build/bdist.linux-x86_64/wheel
Building wheel for contexttimer (setup.py): finished with status 'done'
Created wheel for contexttimer: filename=contexttimer-0.3.3-py3-none-any.whl size=5804 sha256=ccdc87db8df71092a69c43fa7fbfca4e2715eee68f9137797a7d5496c0a20364
Stored in directory: /github/home/.cache/pip/wheels/72/1c/da/cfd97201d88ccce214427fa84a5caeb91fef7c5a1b4c4312b4
Successfully built contexttimer
Installing collected packages: sentencepiece, distlib, contexttimer, zipp, wrapt, typing-extensions, tqdm, soupsieve, sniffio, safetensors, rpds-py, regex, pygments, pycparser, psutil, protobuf, plumbum, platformdirs, nodeenv, msgpack, mdurl, invoke, identify, h11, frozenlist, exceptiongroup, einops, decorator, cfgv, bcrypt, attrs, annotated-types, virtualenv, uvicorn, typing-inspection, rpyc, referencing, pydantic-core, markdown-it-py, importlib-metadata, huggingface-hub, deprecated, cffi, beautifulsoup4, anyio, aiosignal, tokenizers, starlette, rich, pynacl, pydantic, pre-commit, jsonschema-specifications, google, diffusers, cryptography, bitsandbytes, accelerate, transformers, paramiko, jsonschema, fastapi, ray, peft, galore_torch, fabric, colossalai
Attempting uninstall: typing-extensions
Found existing installation: typing_extensions 4.11.0
Uninstalling typing_extensions-4.11.0:
Removing file or directory /opt/conda/envs/pytorch/lib/python3.10/site-packages/__pycache__/typing_extensions.cpython-310.pyc
Removing file or directory /opt/conda/envs/pytorch/lib/python3.10/site-packages/typing_extensions-4.11.0.dist-info/
Removing file or directory /opt/conda/envs/pytorch/lib/python3.10/site-packages/typing_extensions.py
Successfully uninstalled typing_extensions-4.11.0
changing mode of /opt/conda/envs/pytorch/bin/tqdm to 755
changing mode of /opt/conda/envs/pytorch/bin/pygmentize to 755
changing mode of /opt/conda/envs/pytorch/bin/nodeenv to 755
changing mode of /opt/conda/envs/pytorch/bin/inv to 755
changing mode of /opt/conda/envs/pytorch/bin/invoke to 755
changing mode of /opt/conda/envs/pytorch/bin/identify-cli to 755
changing mode of /opt/conda/envs/pytorch/bin/virtualenv to 755
changing mode of /opt/conda/envs/pytorch/bin/uvicorn to 755
changing mode of /opt/conda/envs/pytorch/bin/rpyc_classic to 755
changing mode of /opt/conda/envs/pytorch/bin/rpyc_classic.py to 755
changing mode of /opt/conda/envs/pytorch/bin/rpyc_registry to 755
changing mode of /opt/conda/envs/pytorch/bin/rpyc_registry.py to 755
changing mode of /opt/conda/envs/pytorch/bin/markdown-it to 755
changing mode of /opt/conda/envs/pytorch/bin/huggingface-cli to 755
changing mode of /opt/conda/envs/pytorch/bin/pre-commit to 755
changing mode of /opt/conda/envs/pytorch/bin/diffusers-cli to 755
changing mode of /opt/conda/envs/pytorch/bin/accelerate to 755
changing mode of /opt/conda/envs/pytorch/bin/accelerate-config to 755
changing mode of /opt/conda/envs/pytorch/bin/accelerate-estimate-memory to 755
changing mode of /opt/conda/envs/pytorch/bin/accelerate-launch to 755
changing mode of /opt/conda/envs/pytorch/bin/accelerate-merge-weights to 755
changing mode of /opt/conda/envs/pytorch/bin/transformers-cli to 755
changing mode of /opt/conda/envs/pytorch/bin/jsonschema to 755
changing mode of /opt/conda/envs/pytorch/bin/fastapi to 755
changing mode of /opt/conda/envs/pytorch/bin/ray to 755
changing mode of /opt/conda/envs/pytorch/bin/serve to 755
changing mode of /opt/conda/envs/pytorch/bin/tune to 755
changing mode of /opt/conda/envs/pytorch/bin/fab to 755
Running setup.py develop for colossalai
Running command python setup.py develop
[extension] Building extensionscpu_adam_x86, layernorm_cuda, moe_cuda, fused_optim_cuda, inference_ops_cuda, scaled_masked_softmax_cuda, scaled_upper_triangle_masked_softmax_cuda
running develop
/opt/conda/envs/pytorch/lib/python3.10/site-packages/setuptools/command/develop.py:40: EasyInstallDeprecationWarning: easy_install command is deprecated.
!!
2025-04-11T03:55:11.8573168Z
********************************************************************************
Please avoid running ``setup.py`` and ``easy_install``.
Instead, use pypa/build, pypa/installer or other
standards-based tools.
2025-04-11T03:55:11.8577714Z
See https://github.com/pypa/setuptools/issues/917 for details.
********************************************************************************
2025-04-11T03:55:11.8580637Z
!!
easy_install.initialize_options(self)
/opt/conda/envs/pytorch/lib/python3.10/site-packages/setuptools/_distutils/cmd.py:66: SetuptoolsDeprecationWarning: setup.py install is deprecated.
!!
2025-04-11T03:55:11.8635124Z
********************************************************************************
Please avoid running ``setup.py`` directly.
Instead, use pypa/build, pypa/installer or other
standards-based tools.
2025-04-11T03:55:11.8639938Z
See https://blog.ganssle.io/articles/2021/10/setup-py-deprecated.html for details.
********************************************************************************
2025-04-11T03:55:11.8642772Z
!!
self.initialize_options()
running egg_info
creating colossalai.egg-info
writing colossalai.egg-info/PKG-INFO
writing dependency_links to colossalai.egg-info/dependency_links.txt
writing entry points to colossalai.egg-info/entry_points.txt
writing requirements to colossalai.egg-info/requires.txt
writing top-level names to colossalai.egg-info/top_level.txt
writing manifest file 'colossalai.egg-info/SOURCES.txt'
reading manifest file 'colossalai.egg-info/SOURCES.txt'
reading manifest template 'MANIFEST.in'
warning: no files found matching '*.tr' under directory 'colossalai'
warning: no files found matching '*.cc' under directory 'colossalai'
warning: no files found matching '*.pyi' under directory 'colossalai'
warning: no files found matching '*.tr' under directory 'extensions'
warning: no files found matching '*.cc' under directory 'extensions'
warning: no files found matching '*.pyi' under directory 'extensions'
adding license file 'LICENSE'
writing manifest file 'colossalai.egg-info/SOURCES.txt'
running build_ext
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/utils/cpp_extension.py:425: UserWarning: There are no g++ version bounds defined for CUDA version 12.1
warnings.warn(f'There are no {compiler_name} version bounds defined for CUDA version {cuda_str_version}')
building 'colossalai._C.cpu_adam_x86' extension
creating /__w/ColossalAI/ColossalAI/build
creating /__w/ColossalAI/ColossalAI/build/temp.linux-x86_64-cpython-310
creating /__w/ColossalAI/ColossalAI/build/temp.linux-x86_64-cpython-310/__w
creating /__w/ColossalAI/ColossalAI/build/temp.linux-x86_64-cpython-310/__w/ColossalAI
creating /__w/ColossalAI/ColossalAI/build/temp.linux-x86_64-cpython-310/__w/ColossalAI/ColossalAI
creating /__w/ColossalAI/ColossalAI/build/temp.linux-x86_64-cpython-310/__w/ColossalAI/ColossalAI/extensions
creating /__w/ColossalAI/ColossalAI/build/temp.linux-x86_64-cpython-310/__w/ColossalAI/ColossalAI/extensions/csrc
creating /__w/ColossalAI/ColossalAI/build/temp.linux-x86_64-cpython-310/__w/ColossalAI/ColossalAI/extensions/csrc/kernel
creating /__w/ColossalAI/ColossalAI/build/temp.linux-x86_64-cpython-310/__w/ColossalAI/ColossalAI/extensions/csrc/kernel/x86
Emitting ninja build file /__w/ColossalAI/ColossalAI/build/temp.linux-x86_64-cpython-310/build.ninja...
Compiling objects...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
[1/1] c++ -MMD -MF /__w/ColossalAI/ColossalAI/build/temp.linux-x86_64-cpython-310/__w/ColossalAI/ColossalAI/extensions/csrc/kernel/x86/cpu_adam.o.d -pthread -B /opt/conda/envs/pytorch/compiler_compat -Wno-unused-result -Wsign-compare -DNDEBUG -fwrapv -O2 -Wall -fPIC -O2 -isystem /opt/conda/envs/pytorch/include -fPIC -O2 -isystem /opt/conda/envs/pytorch/include -fPIC -I/__w/ColossalAI/ColossalAI/extensions/csrc/ -I/usr/local/cuda/include -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include/TH -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include/THC -I/usr/local/cuda/include -I/opt/conda/envs/pytorch/include/python3.10 -c -c /__w/ColossalAI/ColossalAI/extensions/csrc/kernel/x86/cpu_adam.cpp -o /__w/ColossalAI/ColossalAI/build/temp.linux-x86_64-cpython-310/__w/ColossalAI/ColossalAI/extensions/csrc/kernel/x86/cpu_adam.o -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -std=c++14 -std=c++17 -lcudart -lcublas -g -Wno-reorder -fopenmp -march=native -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE="_gcc"' '-DPYBIND11_STDLIB="_libstdcpp"' '-DPYBIND11_BUILD_ABI="_cxxabi1011"' -DTORCH_EXTENSION_NAME=cpu_adam_x86 -D_GLIBCXX_USE_CXX11_ABI=0
/__w/ColossalAI/ColossalAI/extensions/csrc/kernel/x86/cpu_adam.cpp:237: warning: ignoring #pragma unroll  [-Wunknown-pragmas]
237 #pragma unroll 4
|
/__w/ColossalAI/ColossalAI/extensions/csrc/kernel/x86/cpu_adam.cpp:352: warning: ignoring #pragma unroll  [-Wunknown-pragmas]
352 #pragma unroll 8
|
In file included from /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include/torch/csrc/Exceptions.h:14,
from /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include/torch/csrc/api/include/torch/python.h:11,
from /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include/torch/extension.h:9,
from /__w/ColossalAI/ColossalAI/extensions/csrc/kernel/x86/cpu_adam.h:29,
from /__w/ColossalAI/ColossalAI/extensions/csrc/kernel/x86/cpu_adam.cpp:22:
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include/pybind11/pybind11.h: In instantiation of ‘class pybind11::class_<Adam_Optimizer>’:
/__w/ColossalAI/ColossalAI/extensions/csrc/kernel/x86/cpu_adam.cpp:443:51:   required from here
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include/pybind11/pybind11.h:1496:7: warning: ‘pybind11::class_<Adam_Optimizer>’ declared with greater visibility than its base ‘pybind11::detail::generic_type’ [-Wattributes]
1496 | class class_ : public detail::generic_type {
^~~~~~
creating build/lib.linux-x86_64-cpython-310
creating build/lib.linux-x86_64-cpython-310/colossalai
creating build/lib.linux-x86_64-cpython-310/colossalai/_C
g++ -pthread -B /opt/conda/envs/pytorch/compiler_compat -shared -Wl,-rpath,/opt/conda/envs/pytorch/lib -Wl,-rpath-link,/opt/conda/envs/pytorch/lib -L/opt/conda/envs/pytorch/lib -Wl,-rpath,/opt/conda/envs/pytorch/lib -Wl,-rpath-link,/opt/conda/envs/pytorch/lib -L/opt/conda/envs/pytorch/lib /__w/ColossalAI/ColossalAI/build/temp.linux-x86_64-cpython-310/__w/ColossalAI/ColossalAI/extensions/csrc/kernel/x86/cpu_adam.o -L/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib -L/usr/local/cuda/lib64 -lc10 -ltorch -ltorch_cpu -ltorch_python -lcudart -lc10_cuda -ltorch_cuda -o build/lib.linux-x86_64-cpython-310/colossalai/_C/cpu_adam_x86.cpython-310-x86_64-linux-gnu.so
building 'colossalai._C.layernorm_cuda' extension
creating /__w/ColossalAI/ColossalAI/build/temp.linux-x86_64-cpython-310/__w/ColossalAI/ColossalAI/extensions/csrc/kernel/cuda
creating /__w/ColossalAI/ColossalAI/build/temp.linux-x86_64-cpython-310/__w/ColossalAI/ColossalAI/extensions/pybind
creating /__w/ColossalAI/ColossalAI/build/temp.linux-x86_64-cpython-310/__w/ColossalAI/ColossalAI/extensions/pybind/layernorm
Emitting ninja build file /__w/ColossalAI/ColossalAI/build/temp.linux-x86_64-cpython-310/build.ninja...
Compiling objects...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
[1/2] c++ -MMD -MF /__w/ColossalAI/ColossalAI/build/temp.linux-x86_64-cpython-310/__w/ColossalAI/ColossalAI/extensions/pybind/layernorm/layer_norm.o.d -pthread -B /opt/conda/envs/pytorch/compiler_compat -Wno-unused-result -Wsign-compare -DNDEBUG -fwrapv -O2 -Wall -fPIC -O2 -isystem /opt/conda/envs/pytorch/include -fPIC -O2 -isystem /opt/conda/envs/pytorch/include -fPIC -I/usr/local/cuda/include -I/__w/ColossalAI/ColossalAI/extensions/csrc/ -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include/TH -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include/THC -I/usr/local/cuda/include -I/opt/conda/envs/pytorch/include/python3.10 -c -c /__w/ColossalAI/ColossalAI/extensions/pybind/layernorm/layer_norm.cpp -o /__w/ColossalAI/ColossalAI/build/temp.linux-x86_64-cpython-310/__w/ColossalAI/ColossalAI/extensions/pybind/layernorm/layer_norm.o -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE="_gcc"' '-DPYBIND11_STDLIB="_libstdcpp"' '-DPYBIND11_BUILD_ABI="_cxxabi1011"' -DTORCH_EXTENSION_NAME=layernorm_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++17
[2/2] /usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output /__w/ColossalAI/ColossalAI/build/temp.linux-x86_64-cpython-310/__w/ColossalAI/ColossalAI/extensions/csrc/kernel/cuda/layer_norm_kernel.o.d -I/usr/local/cuda/include -I/__w/ColossalAI/ColossalAI/extensions/csrc/ -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include/TH -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include/THC -I/usr/local/cuda/include -I/opt/conda/envs/pytorch/include/python3.10 -c -c /__w/ColossalAI/ColossalAI/extensions/csrc/kernel/cuda/layer_norm_kernel.cu -o /__w/ColossalAI/ColossalAI/build/temp.linux-x86_64-cpython-310/__w/ColossalAI/ColossalAI/extensions/csrc/kernel/cuda/layer_norm_kernel.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''"'"'-fPIC'"'"'' -O3 --use_fast_math -maxrregcount=50 -gencode arch=compute_60,code=sm_60 -gencode arch=compute_61,code=sm_61 -gencode arch=compute_70,code=sm_70 -gencode arch=compute_75,code=sm_75 -gencode arch=compute_80,code=sm_80 -gencode arch=compute_86,code=sm_86 -gencode arch=compute_90,code=sm_90 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DCOLOSSAL_WITH_CUDA -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE="_gcc"' '-DPYBIND11_STDLIB="_libstdcpp"' '-DPYBIND11_BUILD_ABI="_cxxabi1011"' -DTORCH_EXTENSION_NAME=layernorm_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++17
g++ -pthread -B /opt/conda/envs/pytorch/compiler_compat -shared -Wl,-rpath,/opt/conda/envs/pytorch/lib -Wl,-rpath-link,/opt/conda/envs/pytorch/lib -L/opt/conda/envs/pytorch/lib -Wl,-rpath,/opt/conda/envs/pytorch/lib -Wl,-rpath-link,/opt/conda/envs/pytorch/lib -L/opt/conda/envs/pytorch/lib /__w/ColossalAI/ColossalAI/build/temp.linux-x86_64-cpython-310/__w/ColossalAI/ColossalAI/extensions/csrc/kernel/cuda/layer_norm_kernel.o /__w/ColossalAI/ColossalAI/build/temp.linux-x86_64-cpython-310/__w/ColossalAI/ColossalAI/extensions/pybind/layernorm/layer_norm.o -L/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib -L/usr/local/cuda/lib64 -lc10 -ltorch -ltorch_cpu -ltorch_python -lcudart -lc10_cuda -ltorch_cuda -o build/lib.linux-x86_64-cpython-310/colossalai/_C/layernorm_cuda.cpython-310-x86_64-linux-gnu.so
building 'colossalai._C.moe_cuda' extension
creating /__w/ColossalAI/ColossalAI/build/temp.linux-x86_64-cpython-310/__w/ColossalAI/ColossalAI/extensions/pybind/moe
Emitting ninja build file /__w/ColossalAI/ColossalAI/build/temp.linux-x86_64-cpython-310/build.ninja...
Compiling objects...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
[1/2] c++ -MMD -MF /__w/ColossalAI/ColossalAI/build/temp.linux-x86_64-cpython-310/__w/ColossalAI/ColossalAI/extensions/pybind/moe/moe.o.d -pthread -B /opt/conda/envs/pytorch/compiler_compat -Wno-unused-result -Wsign-compare -DNDEBUG -fwrapv -O2 -Wall -fPIC -O2 -isystem /opt/conda/envs/pytorch/include -fPIC -O2 -isystem /opt/conda/envs/pytorch/include -fPIC -I/__w/ColossalAI/ColossalAI/extensions/csrc/ -I/usr/local/cuda/include -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include/TH -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include/THC -I/usr/local/cuda/include -I/opt/conda/envs/pytorch/include/python3.10 -c -c /__w/ColossalAI/ColossalAI/extensions/pybind/moe/moe.cpp -o /__w/ColossalAI/ColossalAI/build/temp.linux-x86_64-cpython-310/__w/ColossalAI/ColossalAI/extensions/pybind/moe/moe.o -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE="_gcc"' '-DPYBIND11_STDLIB="_libstdcpp"' '-DPYBIND11_BUILD_ABI="_cxxabi1011"' -DTORCH_EXTENSION_NAME=moe_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++17
[2/2] /usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output /__w/ColossalAI/ColossalAI/build/temp.linux-x86_64-cpython-310/__w/ColossalAI/ColossalAI/extensions/csrc/kernel/cuda/moe_kernel.o.d -I/__w/ColossalAI/ColossalAI/extensions/csrc/ -I/usr/local/cuda/include -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include/TH -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include/THC -I/usr/local/cuda/include -I/opt/conda/envs/pytorch/include/python3.10 -c -c /__w/ColossalAI/ColossalAI/extensions/csrc/kernel/cuda/moe_kernel.cu -o /__w/ColossalAI/ColossalAI/build/temp.linux-x86_64-cpython-310/__w/ColossalAI/ColossalAI/extensions/csrc/kernel/cuda/moe_kernel.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''"'"'-fPIC'"'"'' -O3 --use_fast_math -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ --expt-relaxed-constexpr --expt-extended-lambda -gencode arch=compute_60,code=sm_60 -gencode arch=compute_61,code=sm_61 -gencode arch=compute_70,code=sm_70 -gencode arch=compute_75,code=sm_75 -gencode arch=compute_80,code=sm_80 -gencode arch=compute_86,code=sm_86 -gencode arch=compute_90,code=sm_90 -DCOLOSSAL_WITH_CUDA -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE="_gcc"' '-DPYBIND11_STDLIB="_libstdcpp"' '-DPYBIND11_BUILD_ABI="_cxxabi1011"' -DTORCH_EXTENSION_NAME=moe_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++17
g++ -pthread -B /opt/conda/envs/pytorch/compiler_compat -shared -Wl,-rpath,/opt/conda/envs/pytorch/lib -Wl,-rpath-link,/opt/conda/envs/pytorch/lib -L/opt/conda/envs/pytorch/lib -Wl,-rpath,/opt/conda/envs/pytorch/lib -Wl,-rpath-link,/opt/conda/envs/pytorch/lib -L/opt/conda/envs/pytorch/lib /__w/ColossalAI/ColossalAI/build/temp.linux-x86_64-cpython-310/__w/ColossalAI/ColossalAI/extensions/csrc/kernel/cuda/moe_kernel.o /__w/ColossalAI/ColossalAI/build/temp.linux-x86_64-cpython-310/__w/ColossalAI/ColossalAI/extensions/pybind/moe/moe.o -L/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib -L/usr/local/cuda/lib64 -lc10 -ltorch -ltorch_cpu -ltorch_python -lcudart -lc10_cuda -ltorch_cuda -o build/lib.linux-x86_64-cpython-310/colossalai/_C/moe_cuda.cpython-310-x86_64-linux-gnu.so
building 'colossalai._C.fused_optim_cuda' extension
creating /__w/ColossalAI/ColossalAI/build/temp.linux-x86_64-cpython-310/__w/ColossalAI/ColossalAI/extensions/pybind/optimizer
Emitting ninja build file /__w/ColossalAI/ColossalAI/build/temp.linux-x86_64-cpython-310/build.ninja...
Compiling objects...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
[1/6] c++ -MMD -MF /__w/ColossalAI/ColossalAI/build/temp.linux-x86_64-cpython-310/__w/ColossalAI/ColossalAI/extensions/pybind/optimizer/optimizer.o.d -pthread -B /opt/conda/envs/pytorch/compiler_compat -Wno-unused-result -Wsign-compare -DNDEBUG -fwrapv -O2 -Wall -fPIC -O2 -isystem /opt/conda/envs/pytorch/include -fPIC -O2 -isystem /opt/conda/envs/pytorch/include -fPIC -I/__w/ColossalAI/ColossalAI/extensions/csrc/ -I/usr/local/cuda/include -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include/TH -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include/THC -I/usr/local/cuda/include -I/opt/conda/envs/pytorch/include/python3.10 -c -c /__w/ColossalAI/ColossalAI/extensions/pybind/optimizer/optimizer.cpp -o /__w/ColossalAI/ColossalAI/build/temp.linux-x86_64-cpython-310/__w/ColossalAI/ColossalAI/extensions/pybind/optimizer/optimizer.o -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE="_gcc"' '-DPYBIND11_STDLIB="_libstdcpp"' '-DPYBIND11_BUILD_ABI="_cxxabi1011"' -DTORCH_EXTENSION_NAME=fused_optim_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++17
[2/6] /usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output /__w/ColossalAI/ColossalAI/build/temp.linux-x86_64-cpython-310/__w/ColossalAI/ColossalAI/extensions/csrc/kernel/cuda/multi_tensor_lamb_kernel.o.d -I/__w/ColossalAI/ColossalAI/extensions/csrc/ -I/usr/local/cuda/include -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include/TH -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include/THC -I/usr/local/cuda/include -I/opt/conda/envs/pytorch/include/python3.10 -c -c /__w/ColossalAI/ColossalAI/extensions/csrc/kernel/cuda/multi_tensor_lamb_kernel.cu -o /__w/ColossalAI/ColossalAI/build/temp.linux-x86_64-cpython-310/__w/ColossalAI/ColossalAI/extensions/csrc/kernel/cuda/multi_tensor_lamb_kernel.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''"'"'-fPIC'"'"'' -O3 --use_fast_math -lineinfo -gencode arch=compute_60,code=sm_60 -gencode arch=compute_61,code=sm_61 -gencode arch=compute_70,code=sm_70 -gencode arch=compute_75,code=sm_75 -gencode arch=compute_80,code=sm_80 -gencode arch=compute_86,code=sm_86 -gencode arch=compute_90,code=sm_90 -DCOLOSSAL_WITH_CUDA -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE="_gcc"' '-DPYBIND11_STDLIB="_libstdcpp"' '-DPYBIND11_BUILD_ABI="_cxxabi1011"' -DTORCH_EXTENSION_NAME=fused_optim_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++17
[3/6] /usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output /__w/ColossalAI/ColossalAI/build/temp.linux-x86_64-cpython-310/__w/ColossalAI/ColossalAI/extensions/csrc/kernel/cuda/multi_tensor_scale_kernel.o.d -I/__w/ColossalAI/ColossalAI/extensions/csrc/ -I/usr/local/cuda/include -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include/TH -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include/THC -I/usr/local/cuda/include -I/opt/conda/envs/pytorch/include/python3.10 -c -c /__w/ColossalAI/ColossalAI/extensions/csrc/kernel/cuda/multi_tensor_scale_kernel.cu -o /__w/ColossalAI/ColossalAI/build/temp.linux-x86_64-cpython-310/__w/ColossalAI/ColossalAI/extensions/csrc/kernel/cuda/multi_tensor_scale_kernel.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''"'"'-fPIC'"'"'' -O3 --use_fast_math -lineinfo -gencode arch=compute_60,code=sm_60 -gencode arch=compute_61,code=sm_61 -gencode arch=compute_70,code=sm_70 -gencode arch=compute_75,code=sm_75 -gencode arch=compute_80,code=sm_80 -gencode arch=compute_86,code=sm_86 -gencode arch=compute_90,code=sm_90 -DCOLOSSAL_WITH_CUDA -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE="_gcc"' '-DPYBIND11_STDLIB="_libstdcpp"' '-DPYBIND11_BUILD_ABI="_cxxabi1011"' -DTORCH_EXTENSION_NAME=fused_optim_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++17
[4/6] /usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output /__w/ColossalAI/ColossalAI/build/temp.linux-x86_64-cpython-310/__w/ColossalAI/ColossalAI/extensions/csrc/kernel/cuda/multi_tensor_sgd_kernel.o.d -I/__w/ColossalAI/ColossalAI/extensions/csrc/ -I/usr/local/cuda/include -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include/TH -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include/THC -I/usr/local/cuda/include -I/opt/conda/envs/pytorch/include/python3.10 -c -c /__w/ColossalAI/ColossalAI/extensions/csrc/kernel/cuda/multi_tensor_sgd_kernel.cu -o /__w/ColossalAI/ColossalAI/build/temp.linux-x86_64-cpython-310/__w/ColossalAI/ColossalAI/extensions/csrc/kernel/cuda/multi_tensor_sgd_kernel.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''"'"'-fPIC'"'"'' -O3 --use_fast_math -lineinfo -gencode arch=compute_60,code=sm_60 -gencode arch=compute_61,code=sm_61 -gencode arch=compute_70,code=sm_70 -gencode arch=compute_75,code=sm_75 -gencode arch=compute_80,code=sm_80 -gencode arch=compute_86,code=sm_86 -gencode arch=compute_90,code=sm_90 -DCOLOSSAL_WITH_CUDA -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE="_gcc"' '-DPYBIND11_STDLIB="_libstdcpp"' '-DPYBIND11_BUILD_ABI="_cxxabi1011"' -DTORCH_EXTENSION_NAME=fused_optim_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++17
[5/6] /usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output /__w/ColossalAI/ColossalAI/build/temp.linux-x86_64-cpython-310/__w/ColossalAI/ColossalAI/extensions/csrc/kernel/cuda/multi_tensor_l2norm_kernel.o.d -I/__w/ColossalAI/ColossalAI/extensions/csrc/ -I/usr/local/cuda/include -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include/TH -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include/THC -I/usr/local/cuda/include -I/opt/conda/envs/pytorch/include/python3.10 -c -c /__w/ColossalAI/ColossalAI/extensions/csrc/kernel/cuda/multi_tensor_l2norm_kernel.cu -o /__w/ColossalAI/ColossalAI/build/temp.linux-x86_64-cpython-310/__w/ColossalAI/ColossalAI/extensions/csrc/kernel/cuda/multi_tensor_l2norm_kernel.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''"'"'-fPIC'"'"'' -O3 --use_fast_math -lineinfo -gencode arch=compute_60,code=sm_60 -gencode arch=compute_61,code=sm_61 -gencode arch=compute_70,code=sm_70 -gencode arch=compute_75,code=sm_75 -gencode arch=compute_80,code=sm_80 -gencode arch=compute_86,code=sm_86 -gencode arch=compute_90,code=sm_90 -DCOLOSSAL_WITH_CUDA -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE="_gcc"' '-DPYBIND11_STDLIB="_libstdcpp"' '-DPYBIND11_BUILD_ABI="_cxxabi1011"' -DTORCH_EXTENSION_NAME=fused_optim_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++17
[6/6] /usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output /__w/ColossalAI/ColossalAI/build/temp.linux-x86_64-cpython-310/__w/ColossalAI/ColossalAI/extensions/csrc/kernel/cuda/multi_tensor_adam_kernel.o.d -I/__w/ColossalAI/ColossalAI/extensions/csrc/ -I/usr/local/cuda/include -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include/TH -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include/THC -I/usr/local/cuda/include -I/opt/conda/envs/pytorch/include/python3.10 -c -c /__w/ColossalAI/ColossalAI/extensions/csrc/kernel/cuda/multi_tensor_adam_kernel.cu -o /__w/ColossalAI/ColossalAI/build/temp.linux-x86_64-cpython-310/__w/ColossalAI/ColossalAI/extensions/csrc/kernel/cuda/multi_tensor_adam_kernel.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''"'"'-fPIC'"'"'' -O3 --use_fast_math -lineinfo -gencode arch=compute_60,code=sm_60 -gencode arch=compute_61,code=sm_61 -gencode arch=compute_70,code=sm_70 -gencode arch=compute_75,code=sm_75 -gencode arch=compute_80,code=sm_80 -gencode arch=compute_86,code=sm_86 -gencode arch=compute_90,code=sm_90 -DCOLOSSAL_WITH_CUDA -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE="_gcc"' '-DPYBIND11_STDLIB="_libstdcpp"' '-DPYBIND11_BUILD_ABI="_cxxabi1011"' -DTORCH_EXTENSION_NAME=fused_optim_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++17
g++ -pthread -B /opt/conda/envs/pytorch/compiler_compat -shared -Wl,-rpath,/opt/conda/envs/pytorch/lib -Wl,-rpath-link,/opt/conda/envs/pytorch/lib -L/opt/conda/envs/pytorch/lib -Wl,-rpath,/opt/conda/envs/pytorch/lib -Wl,-rpath-link,/opt/conda/envs/pytorch/lib -L/opt/conda/envs/pytorch/lib /__w/ColossalAI/ColossalAI/build/temp.linux-x86_64-cpython-310/__w/ColossalAI/ColossalAI/extensions/csrc/kernel/cuda/multi_tensor_adam_kernel.o /__w/ColossalAI/ColossalAI/build/temp.linux-x86_64-cpython-310/__w/ColossalAI/ColossalAI/extensions/csrc/kernel/cuda/multi_tensor_l2norm_kernel.o /__w/ColossalAI/ColossalAI/build/temp.linux-x86_64-cpython-310/__w/ColossalAI/ColossalAI/extensions/csrc/kernel/cuda/multi_tensor_lamb_kernel.o /__w/ColossalAI/ColossalAI/build/temp.linux-x86_64-cpython-310/__w/ColossalAI/ColossalAI/extensions/csrc/kernel/cuda/multi_tensor_scale_kernel.o /__w/ColossalAI/ColossalAI/build/temp.linux-x86_64-cpython-310/__w/ColossalAI/ColossalAI/extensions/csrc/kernel/cuda/multi_tensor_sgd_kernel.o /__w/ColossalAI/ColossalAI/build/temp.linux-x86_64-cpython-310/__w/ColossalAI/ColossalAI/extensions/pybind/optimizer/optimizer.o -L/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib -L/usr/local/cuda/lib64 -lc10 -ltorch -ltorch_cpu -ltorch_python -lcudart -lc10_cuda -ltorch_cuda -o build/lib.linux-x86_64-cpython-310/colossalai/_C/fused_optim_cuda.cpython-310-x86_64-linux-gnu.so
building 'colossalai._C.inference_ops_cuda' extension
creating /__w/ColossalAI/ColossalAI/build/temp.linux-x86_64-cpython-310/__w/ColossalAI/ColossalAI/extensions/pybind/inference
Emitting ninja build file /__w/ColossalAI/ColossalAI/build/temp.linux-x86_64-cpython-310/build.ninja...
Compiling objects...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
[1/9] c++ -MMD -MF /__w/ColossalAI/ColossalAI/build/temp.linux-x86_64-cpython-310/__w/ColossalAI/ColossalAI/extensions/pybind/inference/inference.o.d -pthread -B /opt/conda/envs/pytorch/compiler_compat -Wno-unused-result -Wsign-compare -DNDEBUG -fwrapv -O2 -Wall -fPIC -O2 -isystem /opt/conda/envs/pytorch/include -fPIC -O2 -isystem /opt/conda/envs/pytorch/include -fPIC -I/__w/ColossalAI/ColossalAI/extensions/csrc/ -I/usr/local/cuda/include -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include/TH -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include/THC -I/usr/local/cuda/include -I/opt/conda/envs/pytorch/include/python3.10 -c -c /__w/ColossalAI/ColossalAI/extensions/pybind/inference/inference.cpp -o /__w/ColossalAI/ColossalAI/build/temp.linux-x86_64-cpython-310/__w/ColossalAI/ColossalAI/extensions/pybind/inference/inference.o -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE="_gcc"' '-DPYBIND11_STDLIB="_libstdcpp"' '-DPYBIND11_BUILD_ABI="_cxxabi1011"' -DTORCH_EXTENSION_NAME=inference_ops_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++17
[2/9] /usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output /__w/ColossalAI/ColossalAI/build/temp.linux-x86_64-cpython-310/__w/ColossalAI/ColossalAI/extensions/csrc/kernel/cuda/get_cos_and_sin_kernel.o.d -I/__w/ColossalAI/ColossalAI/extensions/csrc/ -I/usr/local/cuda/include -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include/TH -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include/THC -I/usr/local/cuda/include -I/opt/conda/envs/pytorch/include/python3.10 -c -c /__w/ColossalAI/ColossalAI/extensions/csrc/kernel/cuda/get_cos_and_sin_kernel.cu -o /__w/ColossalAI/ColossalAI/build/temp.linux-x86_64-cpython-310/__w/ColossalAI/ColossalAI/extensions/csrc/kernel/cuda/get_cos_and_sin_kernel.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''"'"'-fPIC'"'"'' -O3 --use_fast_math -lineinfo -gencode arch=compute_60,code=sm_60 -gencode arch=compute_61,code=sm_61 -gencode arch=compute_70,code=sm_70 -gencode arch=compute_75,code=sm_75 -gencode arch=compute_80,code=sm_80 -gencode arch=compute_86,code=sm_86 -gencode arch=compute_90,code=sm_90 -DCOLOSSAL_WITH_CUDA -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE="_gcc"' '-DPYBIND11_STDLIB="_libstdcpp"' '-DPYBIND11_BUILD_ABI="_cxxabi1011"' -DTORCH_EXTENSION_NAME=inference_ops_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++17
[3/9] /usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output /__w/ColossalAI/ColossalAI/build/temp.linux-x86_64-cpython-310/__w/ColossalAI/ColossalAI/extensions/csrc/kernel/cuda/activation_kernel.o.d -I/__w/ColossalAI/ColossalAI/extensions/csrc/ -I/usr/local/cuda/include -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include/TH -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include/THC -I/usr/local/cuda/include -I/opt/conda/envs/pytorch/include/python3.10 -c -c /__w/ColossalAI/ColossalAI/extensions/csrc/kernel/cuda/activation_kernel.cu -o /__w/ColossalAI/ColossalAI/build/temp.linux-x86_64-cpython-310/__w/ColossalAI/ColossalAI/extensions/csrc/kernel/cuda/activation_kernel.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''"'"'-fPIC'"'"'' -O3 --use_fast_math -lineinfo -gencode arch=compute_60,code=sm_60 -gencode arch=compute_61,code=sm_61 -gencode arch=compute_70,code=sm_70 -gencode arch=compute_75,code=sm_75 -gencode arch=compute_80,code=sm_80 -gencode arch=compute_86,code=sm_86 -gencode arch=compute_90,code=sm_90 -DCOLOSSAL_WITH_CUDA -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE="_gcc"' '-DPYBIND11_STDLIB="_libstdcpp"' '-DPYBIND11_BUILD_ABI="_cxxabi1011"' -DTORCH_EXTENSION_NAME=inference_ops_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++17
[4/9] /usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output /__w/ColossalAI/ColossalAI/build/temp.linux-x86_64-cpython-310/__w/ColossalAI/ColossalAI/extensions/csrc/kernel/cuda/convert_fp8_kernel.o.d -I/__w/ColossalAI/ColossalAI/extensions/csrc/ -I/usr/local/cuda/include -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include/TH -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include/THC -I/usr/local/cuda/include -I/opt/conda/envs/pytorch/include/python3.10 -c -c /__w/ColossalAI/ColossalAI/extensions/csrc/kernel/cuda/convert_fp8_kernel.cu -o /__w/ColossalAI/ColossalAI/build/temp.linux-x86_64-cpython-310/__w/ColossalAI/ColossalAI/extensions/csrc/kernel/cuda/convert_fp8_kernel.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''"'"'-fPIC'"'"'' -O3 --use_fast_math -lineinfo -gencode arch=compute_60,code=sm_60 -gencode arch=compute_61,code=sm_61 -gencode arch=compute_70,code=sm_70 -gencode arch=compute_75,code=sm_75 -gencode arch=compute_80,code=sm_80 -gencode arch=compute_86,code=sm_86 -gencode arch=compute_90,code=sm_90 -DCOLOSSAL_WITH_CUDA -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE="_gcc"' '-DPYBIND11_STDLIB="_libstdcpp"' '-DPYBIND11_BUILD_ABI="_cxxabi1011"' -DTORCH_EXTENSION_NAME=inference_ops_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++17
[5/9] /usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output /__w/ColossalAI/ColossalAI/build/temp.linux-x86_64-cpython-310/__w/ColossalAI/ColossalAI/extensions/csrc/kernel/cuda/rms_layernorm_kernel.o.d -I/__w/ColossalAI/ColossalAI/extensions/csrc/ -I/usr/local/cuda/include -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include/TH -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include/THC -I/usr/local/cuda/include -I/opt/conda/envs/pytorch/include/python3.10 -c -c /__w/ColossalAI/ColossalAI/extensions/csrc/kernel/cuda/rms_layernorm_kernel.cu -o /__w/ColossalAI/ColossalAI/build/temp.linux-x86_64-cpython-310/__w/ColossalAI/ColossalAI/extensions/csrc/kernel/cuda/rms_layernorm_kernel.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''"'"'-fPIC'"'"'' -O3 --use_fast_math -lineinfo -gencode arch=compute_60,code=sm_60 -gencode arch=compute_61,code=sm_61 -gencode arch=compute_70,code=sm_70 -gencode arch=compute_75,code=sm_75 -gencode arch=compute_80,code=sm_80 -gencode arch=compute_86,code=sm_86 -gencode arch=compute_90,code=sm_90 -DCOLOSSAL_WITH_CUDA -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE="_gcc"' '-DPYBIND11_STDLIB="_libstdcpp"' '-DPYBIND11_BUILD_ABI="_cxxabi1011"' -DTORCH_EXTENSION_NAME=inference_ops_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++17
[6/9] /usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output /__w/ColossalAI/ColossalAI/build/temp.linux-x86_64-cpython-310/__w/ColossalAI/ColossalAI/extensions/csrc/kernel/cuda/context_kv_cache_memcpy_kernel.o.d -I/__w/ColossalAI/ColossalAI/extensions/csrc/ -I/usr/local/cuda/include -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include/TH -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include/THC -I/usr/local/cuda/include -I/opt/conda/envs/pytorch/include/python3.10 -c -c /__w/ColossalAI/ColossalAI/extensions/csrc/kernel/cuda/context_kv_cache_memcpy_kernel.cu -o /__w/ColossalAI/ColossalAI/build/temp.linux-x86_64-cpython-310/__w/ColossalAI/ColossalAI/extensions/csrc/kernel/cuda/context_kv_cache_memcpy_kernel.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''"'"'-fPIC'"'"'' -O3 --use_fast_math -lineinfo -gencode arch=compute_60,code=sm_60 -gencode arch=compute_61,code=sm_61 -gencode arch=compute_70,code=sm_70 -gencode arch=compute_75,code=sm_75 -gencode arch=compute_80,code=sm_80 -gencode arch=compute_86,code=sm_86 -gencode arch=compute_90,code=sm_90 -DCOLOSSAL_WITH_CUDA -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE="_gcc"' '-DPYBIND11_STDLIB="_libstdcpp"' '-DPYBIND11_BUILD_ABI="_cxxabi1011"' -DTORCH_EXTENSION_NAME=inference_ops_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++17
[7/9] /usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output /__w/ColossalAI/ColossalAI/build/temp.linux-x86_64-cpython-310/__w/ColossalAI/ColossalAI/extensions/csrc/kernel/cuda/decode_kv_cache_memcpy_kernel.o.d -I/__w/ColossalAI/ColossalAI/extensions/csrc/ -I/usr/local/cuda/include -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include/TH -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include/THC -I/usr/local/cuda/include -I/opt/conda/envs/pytorch/include/python3.10 -c -c /__w/ColossalAI/ColossalAI/extensions/csrc/kernel/cuda/decode_kv_cache_memcpy_kernel.cu -o /__w/ColossalAI/ColossalAI/build/temp.linux-x86_64-cpython-310/__w/ColossalAI/ColossalAI/extensions/csrc/kernel/cuda/decode_kv_cache_memcpy_kernel.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''"'"'-fPIC'"'"'' -O3 --use_fast_math -lineinfo -gencode arch=compute_60,code=sm_60 -gencode arch=compute_61,code=sm_61 -gencode arch=compute_70,code=sm_70 -gencode arch=compute_75,code=sm_75 -gencode arch=compute_80,code=sm_80 -gencode arch=compute_86,code=sm_86 -gencode arch=compute_90,code=sm_90 -DCOLOSSAL_WITH_CUDA -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE="_gcc"' '-DPYBIND11_STDLIB="_libstdcpp"' '-DPYBIND11_BUILD_ABI="_cxxabi1011"' -DTORCH_EXTENSION_NAME=inference_ops_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++17
[8/9] /usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output /__w/ColossalAI/ColossalAI/build/temp.linux-x86_64-cpython-310/__w/ColossalAI/ColossalAI/extensions/csrc/kernel/cuda/fused_rotary_emb_and_cache_kernel.o.d -I/__w/ColossalAI/ColossalAI/extensions/csrc/ -I/usr/local/cuda/include -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include/TH -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include/THC -I/usr/local/cuda/include -I/opt/conda/envs/pytorch/include/python3.10 -c -c /__w/ColossalAI/ColossalAI/extensions/csrc/kernel/cuda/fused_rotary_emb_and_cache_kernel.cu -o /__w/ColossalAI/ColossalAI/build/temp.linux-x86_64-cpython-310/__w/ColossalAI/ColossalAI/extensions/csrc/kernel/cuda/fused_rotary_emb_and_cache_kernel.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''"'"'-fPIC'"'"'' -O3 --use_fast_math -lineinfo -gencode arch=compute_60,code=sm_60 -gencode arch=compute_61,code=sm_61 -gencode arch=compute_70,code=sm_70 -gencode arch=compute_75,code=sm_75 -gencode arch=compute_80,code=sm_80 -gencode arch=compute_86,code=sm_86 -gencode arch=compute_90,code=sm_90 -DCOLOSSAL_WITH_CUDA -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE="_gcc"' '-DPYBIND11_STDLIB="_libstdcpp"' '-DPYBIND11_BUILD_ABI="_cxxabi1011"' -DTORCH_EXTENSION_NAME=inference_ops_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++17
[9/9] /usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output /__w/ColossalAI/ColossalAI/build/temp.linux-x86_64-cpython-310/__w/ColossalAI/ColossalAI/extensions/csrc/kernel/cuda/flash_decoding_attention_kernel.o.d -I/__w/ColossalAI/ColossalAI/extensions/csrc/ -I/usr/local/cuda/include -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include/TH -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include/THC -I/usr/local/cuda/include -I/opt/conda/envs/pytorch/include/python3.10 -c -c /__w/ColossalAI/ColossalAI/extensions/csrc/kernel/cuda/flash_decoding_attention_kernel.cu -o /__w/ColossalAI/ColossalAI/build/temp.linux-x86_64-cpython-310/__w/ColossalAI/ColossalAI/extensions/csrc/kernel/cuda/flash_decoding_attention_kernel.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''"'"'-fPIC'"'"'' -O3 --use_fast_math -lineinfo -gencode arch=compute_60,code=sm_60 -gencode arch=compute_61,code=sm_61 -gencode arch=compute_70,code=sm_70 -gencode arch=compute_75,code=sm_75 -gencode arch=compute_80,code=sm_80 -gencode arch=compute_86,code=sm_86 -gencode arch=compute_90,code=sm_90 -DCOLOSSAL_WITH_CUDA -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE="_gcc"' '-DPYBIND11_STDLIB="_libstdcpp"' '-DPYBIND11_BUILD_ABI="_cxxabi1011"' -DTORCH_EXTENSION_NAME=inference_ops_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++17
g++ -pthread -B /opt/conda/envs/pytorch/compiler_compat -shared -Wl,-rpath,/opt/conda/envs/pytorch/lib -Wl,-rpath-link,/opt/conda/envs/pytorch/lib -L/opt/conda/envs/pytorch/lib -Wl,-rpath,/opt/conda/envs/pytorch/lib -Wl,-rpath-link,/opt/conda/envs/pytorch/lib -L/opt/conda/envs/pytorch/lib /__w/ColossalAI/ColossalAI/build/temp.linux-x86_64-cpython-310/__w/ColossalAI/ColossalAI/extensions/csrc/kernel/cuda/activation_kernel.o /__w/ColossalAI/ColossalAI/build/temp.linux-x86_64-cpython-310/__w/ColossalAI/ColossalAI/extensions/csrc/kernel/cuda/context_kv_cache_memcpy_kernel.o /__w/ColossalAI/ColossalAI/build/temp.linux-x86_64-cpython-310/__w/ColossalAI/ColossalAI/extensions/csrc/kernel/cuda/convert_fp8_kernel.o /__w/ColossalAI/ColossalAI/build/temp.linux-x86_64-cpython-310/__w/ColossalAI/ColossalAI/extensions/csrc/kernel/cuda/decode_kv_cache_memcpy_kernel.o /__w/ColossalAI/ColossalAI/build/temp.linux-x86_64-cpython-310/__w/ColossalAI/ColossalAI/extensions/csrc/kernel/cuda/flash_decoding_attention_kernel.o /__w/ColossalAI/ColossalAI/build/temp.linux-x86_64-cpython-310/__w/ColossalAI/ColossalAI/extensions/csrc/kernel/cuda/fused_rotary_emb_and_cache_kernel.o /__w/ColossalAI/ColossalAI/build/temp.linux-x86_64-cpython-310/__w/ColossalAI/ColossalAI/extensions/csrc/kernel/cuda/get_cos_and_sin_kernel.o /__w/ColossalAI/ColossalAI/build/temp.linux-x86_64-cpython-310/__w/ColossalAI/ColossalAI/extensions/csrc/kernel/cuda/rms_layernorm_kernel.o /__w/ColossalAI/ColossalAI/build/temp.linux-x86_64-cpython-310/__w/ColossalAI/ColossalAI/extensions/pybind/inference/inference.o -L/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib -L/usr/local/cuda/lib64 -lc10 -ltorch -ltorch_cpu -ltorch_python -lcudart -lc10_cuda -ltorch_cuda -o build/lib.linux-x86_64-cpython-310/colossalai/_C/inference_ops_cuda.cpython-310-x86_64-linux-gnu.so
building 'colossalai._C.scaled_masked_softmax_cuda' extension
creating /__w/ColossalAI/ColossalAI/build/temp.linux-x86_64-cpython-310/__w/ColossalAI/ColossalAI/extensions/pybind/softmax
Emitting ninja build file /__w/ColossalAI/ColossalAI/build/temp.linux-x86_64-cpython-310/build.ninja...
Compiling objects...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
[1/2] c++ -MMD -MF /__w/ColossalAI/ColossalAI/build/temp.linux-x86_64-cpython-310/__w/ColossalAI/ColossalAI/extensions/pybind/softmax/scaled_masked_softmax.o.d -pthread -B /opt/conda/envs/pytorch/compiler_compat -Wno-unused-result -Wsign-compare -DNDEBUG -fwrapv -O2 -Wall -fPIC -O2 -isystem /opt/conda/envs/pytorch/include -fPIC -O2 -isystem /opt/conda/envs/pytorch/include -fPIC -I/__w/ColossalAI/ColossalAI/extensions/csrc/ -I/usr/local/cuda/include -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include/TH -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include/THC -I/usr/local/cuda/include -I/opt/conda/envs/pytorch/include/python3.10 -c -c /__w/ColossalAI/ColossalAI/extensions/pybind/softmax/scaled_masked_softmax.cpp -o /__w/ColossalAI/ColossalAI/build/temp.linux-x86_64-cpython-310/__w/ColossalAI/ColossalAI/extensions/pybind/softmax/scaled_masked_softmax.o -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE="_gcc"' '-DPYBIND11_STDLIB="_libstdcpp"' '-DPYBIND11_BUILD_ABI="_cxxabi1011"' -DTORCH_EXTENSION_NAME=scaled_masked_softmax_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++17
[2/2] /usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output /__w/ColossalAI/ColossalAI/build/temp.linux-x86_64-cpython-310/__w/ColossalAI/ColossalAI/extensions/csrc/kernel/cuda/scaled_masked_softmax_kernel.o.d -I/__w/ColossalAI/ColossalAI/extensions/csrc/ -I/usr/local/cuda/include -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include/TH -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include/THC -I/usr/local/cuda/include -I/opt/conda/envs/pytorch/include/python3.10 -c -c /__w/ColossalAI/ColossalAI/extensions/csrc/kernel/cuda/scaled_masked_softmax_kernel.cu -o /__w/ColossalAI/ColossalAI/build/temp.linux-x86_64-cpython-310/__w/ColossalAI/ColossalAI/extensions/csrc/kernel/cuda/scaled_masked_softmax_kernel.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''"'"'-fPIC'"'"'' -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -std=c++14 -std=c++17 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -DTHRUST_IGNORE_CUB_VERSION_CHECK -DCOLOSSAL_WITH_CUDA -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE="_gcc"' '-DPYBIND11_STDLIB="_libstdcpp"' '-DPYBIND11_BUILD_ABI="_cxxabi1011"' -DTORCH_EXTENSION_NAME=scaled_masked_softmax_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_90,code=compute_90 -gencode=arch=compute_90,code=sm_90
nvcc warning : incompatible redefinition for option 'std', the last value of this option was used
g++ -pthread -B /opt/conda/envs/pytorch/compiler_compat -shared -Wl,-rpath,/opt/conda/envs/pytorch/lib -Wl,-rpath-link,/opt/conda/envs/pytorch/lib -L/opt/conda/envs/pytorch/lib -Wl,-rpath,/opt/conda/envs/pytorch/lib -Wl,-rpath-link,/opt/conda/envs/pytorch/lib -L/opt/conda/envs/pytorch/lib /__w/ColossalAI/ColossalAI/build/temp.linux-x86_64-cpython-310/__w/ColossalAI/ColossalAI/extensions/csrc/kernel/cuda/scaled_masked_softmax_kernel.o /__w/ColossalAI/ColossalAI/build/temp.linux-x86_64-cpython-310/__w/ColossalAI/ColossalAI/extensions/pybind/softmax/scaled_masked_softmax.o -L/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib -L/usr/local/cuda/lib64 -lc10 -ltorch -ltorch_cpu -ltorch_python -lcudart -lc10_cuda -ltorch_cuda -o build/lib.linux-x86_64-cpython-310/colossalai/_C/scaled_masked_softmax_cuda.cpython-310-x86_64-linux-gnu.so
building 'colossalai._C.scaled_upper_triangle_masked_softmax_cuda' extension
Emitting ninja build file /__w/ColossalAI/ColossalAI/build/temp.linux-x86_64-cpython-310/build.ninja...
Compiling objects...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
[1/2] c++ -MMD -MF /__w/ColossalAI/ColossalAI/build/temp.linux-x86_64-cpython-310/__w/ColossalAI/ColossalAI/extensions/pybind/softmax/scaled_upper_triang_masked_softmax.o.d -pthread -B /opt/conda/envs/pytorch/compiler_compat -Wno-unused-result -Wsign-compare -DNDEBUG -fwrapv -O2 -Wall -fPIC -O2 -isystem /opt/conda/envs/pytorch/include -fPIC -O2 -isystem /opt/conda/envs/pytorch/include -fPIC -I/__w/ColossalAI/ColossalAI/extensions/csrc/ -I/usr/local/cuda/include -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include/TH -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include/THC -I/usr/local/cuda/include -I/opt/conda/envs/pytorch/include/python3.10 -c -c /__w/ColossalAI/ColossalAI/extensions/pybind/softmax/scaled_upper_triang_masked_softmax.cpp -o /__w/ColossalAI/ColossalAI/build/temp.linux-x86_64-cpython-310/__w/ColossalAI/ColossalAI/extensions/pybind/softmax/scaled_upper_triang_masked_softmax.o -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE="_gcc"' '-DPYBIND11_STDLIB="_libstdcpp"' '-DPYBIND11_BUILD_ABI="_cxxabi1011"' -DTORCH_EXTENSION_NAME=scaled_upper_triangle_masked_softmax_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++17
[2/2] /usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output /__w/ColossalAI/ColossalAI/build/temp.linux-x86_64-cpython-310/__w/ColossalAI/ColossalAI/extensions/csrc/kernel/cuda/scaled_upper_triang_masked_softmax_kernel.o.d -I/__w/ColossalAI/ColossalAI/extensions/csrc/ -I/usr/local/cuda/include -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include/TH -I/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/include/THC -I/usr/local/cuda/include -I/opt/conda/envs/pytorch/include/python3.10 -c -c /__w/ColossalAI/ColossalAI/extensions/csrc/kernel/cuda/scaled_upper_triang_masked_softmax_kernel.cu -o /__w/ColossalAI/ColossalAI/build/temp.linux-x86_64-cpython-310/__w/ColossalAI/ColossalAI/extensions/csrc/kernel/cuda/scaled_upper_triang_masked_softmax_kernel.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''"'"'-fPIC'"'"'' -O3 --use_fast_math -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ --expt-relaxed-constexpr --expt-extended-lambda -gencode arch=compute_60,code=sm_60 -gencode arch=compute_61,code=sm_61 -gencode arch=compute_70,code=sm_70 -gencode arch=compute_75,code=sm_75 -gencode arch=compute_80,code=sm_80 -gencode arch=compute_86,code=sm_86 -gencode arch=compute_90,code=sm_90 -DCOLOSSAL_WITH_CUDA -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE="_gcc"' '-DPYBIND11_STDLIB="_libstdcpp"' '-DPYBIND11_BUILD_ABI="_cxxabi1011"' -DTORCH_EXTENSION_NAME=scaled_upper_triangle_masked_softmax_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++17
g++ -pthread -B /opt/conda/envs/pytorch/compiler_compat -shared -Wl,-rpath,/opt/conda/envs/pytorch/lib -Wl,-rpath-link,/opt/conda/envs/pytorch/lib -L/opt/conda/envs/pytorch/lib -Wl,-rpath,/opt/conda/envs/pytorch/lib -Wl,-rpath-link,/opt/conda/envs/pytorch/lib -L/opt/conda/envs/pytorch/lib /__w/ColossalAI/ColossalAI/build/temp.linux-x86_64-cpython-310/__w/ColossalAI/ColossalAI/extensions/csrc/kernel/cuda/scaled_upper_triang_masked_softmax_kernel.o /__w/ColossalAI/ColossalAI/build/temp.linux-x86_64-cpython-310/__w/ColossalAI/ColossalAI/extensions/pybind/softmax/scaled_upper_triang_masked_softmax.o -L/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib -L/usr/local/cuda/lib64 -lc10 -ltorch -ltorch_cpu -ltorch_python -lcudart -lc10_cuda -ltorch_cuda -o build/lib.linux-x86_64-cpython-310/colossalai/_C/scaled_upper_triangle_masked_softmax_cuda.cpython-310-x86_64-linux-gnu.so
copying build/lib.linux-x86_64-cpython-310/colossalai/_C/cpu_adam_x86.cpython-310-x86_64-linux-gnu.so -> colossalai/_C
copying build/lib.linux-x86_64-cpython-310/colossalai/_C/layernorm_cuda.cpython-310-x86_64-linux-gnu.so -> colossalai/_C
copying build/lib.linux-x86_64-cpython-310/colossalai/_C/moe_cuda.cpython-310-x86_64-linux-gnu.so -> colossalai/_C
copying build/lib.linux-x86_64-cpython-310/colossalai/_C/fused_optim_cuda.cpython-310-x86_64-linux-gnu.so -> colossalai/_C
copying build/lib.linux-x86_64-cpython-310/colossalai/_C/inference_ops_cuda.cpython-310-x86_64-linux-gnu.so -> colossalai/_C
copying build/lib.linux-x86_64-cpython-310/colossalai/_C/scaled_masked_softmax_cuda.cpython-310-x86_64-linux-gnu.so -> colossalai/_C
copying build/lib.linux-x86_64-cpython-310/colossalai/_C/scaled_upper_triangle_masked_softmax_cuda.cpython-310-x86_64-linux-gnu.so -> colossalai/_C
Creating /opt/conda/envs/pytorch/lib/python3.10/site-packages/colossalai.egg-link (link to .)
Adding colossalai 0.4.9 to easy-install.pth file
Installing colossalai script to /opt/conda/envs/pytorch/bin
2025-04-11T04:10:18.9073150Z
Installed /__w/ColossalAI/ColossalAI
Successfully installed accelerate-1.6.0 aiosignal-1.3.2 annotated-types-0.7.0 anyio-4.9.0 attrs-25.3.0 bcrypt-4.3.0 beautifulsoup4-4.13.3 bitsandbytes-0.45.5 cffi-1.17.1 cfgv-3.4.0 colossalai-0.4.9 contexttimer-0.3.3 cryptography-44.0.2 decorator-5.2.1 deprecated-1.2.18 diffusers-0.29.0 distlib-0.3.9 einops-0.8.1 exceptiongroup-1.2.2 fabric-3.2.2 fastapi-0.115.12 frozenlist-1.5.0 galore_torch-1.0 google-3.0.0 h11-0.14.0 huggingface-hub-0.30.2 identify-2.6.9 importlib-metadata-8.6.1 invoke-2.2.0 jsonschema-4.23.0 jsonschema-specifications-2024.10.1 markdown-it-py-3.0.0 mdurl-0.1.2 msgpack-1.1.0 nodeenv-1.9.1 paramiko-3.5.1 peft-0.13.2 platformdirs-4.3.7 plumbum-1.9.0 pre-commit-4.2.0 protobuf-6.30.2 psutil-7.0.0 pycparser-2.22 pydantic-2.11.3 pydantic-core-2.33.1 pygments-2.19.1 pynacl-1.5.0 ray-2.44.1 referencing-0.36.2 regex-2024.11.6 rich-14.0.0 rpds-py-0.24.0 rpyc-6.0.0 safetensors-0.5.3 sentencepiece-0.2.0 sniffio-1.3.1 soupsieve-2.6 starlette-0.46.1 tokenizers-0.15.2 tqdm-4.67.1 transformers-4.39.3 typing-extensions-4.13.2 typing-inspection-0.4.0 uvicorn-0.29.0 virtualenv-20.30.0 wrapt-1.17.2 zipp-3.21.0
WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv
Collecting git+https://github.com/hpcaitech/pytest-testmon (from -r requirements/requirements-test.txt (line 3))
Cloning https://github.com/hpcaitech/pytest-testmon to /tmp/pip-req-build-pyk829u3
Running command git clone --filter=blob:none --quiet https://github.com/hpcaitech/pytest-testmon /tmp/pip-req-build-pyk829u3
Resolved https://github.com/hpcaitech/pytest-testmon to commit be30f4ac384656daae1a9157e6de97825caaf0ba
Installing build dependencies: started
Installing build dependencies: finished with status 'done'
Getting requirements to build wheel: started
Getting requirements to build wheel: finished with status 'done'
Preparing metadata (pyproject.toml): started
Preparing metadata (pyproject.toml): finished with status 'done'
Collecting pytest (from -r requirements/requirements-test.txt (line 1))
pytest-8.3.5-py3-none-any.whl.metadata (7.6
Collecting coverage==7.2.3 (from -r requirements/requirements-test.txt (line 2))
Downloading coverage-7.2.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.6 kB)
Requirement already satisfied: torchvision in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from -r requirements/requirements-test.txt (line 4)) (0.17.2)
Collecting timm (from -r requirements/requirements-test.txt (line 5))
timm-1.0.15-py3-none-any.whl.metadata (52
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 52.0/52.0 kB 733.8 kB/s eta 0:00:00
Collecting titans (from -r requirements/requirements-test.txt (line 6))
titans-0.0.7.tar.gz (38
Preparing metadata (setup.py): started
Preparing metadata (setup.py): finished with status 'done'
Requirement already satisfied: torchaudio>=0.13.1 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from -r requirements/requirements-test.txt (line 7)) (2.2.2)
Collecting torchx-nightly==2022.6.29 (from -r requirements/requirements-test.txt (line 8))
torchx nightly-2022.6.29-py3-none-any.whl.metadata (4.9
Collecting torchrec==0.2.0 (from -r requirements/requirements-test.txt (line 9))
torchrec-0.2.0-py39-none-any.whl.metadata (6.7
Requirement already satisfied: contexttimer in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from -r requirements/requirements-test.txt (line 10)) (0.3.3)
Requirement already satisfied: einops in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from -r requirements/requirements-test.txt (line 11)) (0.8.1)
Requirement already satisfied: triton in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from -r requirements/requirements-test.txt (line 12)) (2.2.0)
Collecting requests==2.27.1 (from -r requirements/requirements-test.txt (line 13))
requests-2.27.1-py2.py3-none-any.whl.metadata (5.0
Requirement already satisfied: SentencePiece in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from -r requirements/requirements-test.txt (line 14)) (0.2.0)
Requirement already satisfied: ninja in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from -r requirements/requirements-test.txt (line 15)) (1.11.1.1)
Collecting flash_attn (from -r requirements/requirements-test.txt (line 16))
Downloading flash_attn-2.7.4.post1.tar.gz (6.0 MB)
6 0 6 0 MB 10 6 0 00 00
Preparing metadata (setup.py): started
Preparing metadata (setup.py): finished with status 'done'
Collecting datasets (from -r requirements/requirements-test.txt (line 17))
datasets-3.5.0-py3-none-any.whl.metadata (19
Requirement already satisfied: pydantic in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from -r requirements/requirements-test.txt (line 18)) (2.11.3)
Requirement already satisfied: ray in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from -r requirements/requirements-test.txt (line 19)) (2.44.1)
Requirement already satisfied: peft>=0.7.1 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from -r requirements/requirements-test.txt (line 20)) (0.13.2)
Collecting pyre-extensions (from torchx-nightly==2022.6.29->-r requirements/requirements-test.txt (line 8))
pyre extensions-0.0.32-py3-none-any.whl.metadata (4.0
Collecting docstring-parser==0.8.1 (from torchx-nightly==2022.6.29->-r requirements/requirements-test.txt (line 8))
docstring parser-0.8.1.tar.gz (14
Installing build dependencies: started
Installing build dependencies: finished with status 'done'
Getting requirements to build wheel: started
Getting requirements to build wheel: finished with status 'done'
Preparing metadata (pyproject.toml): started
Preparing metadata (pyproject.toml): finished with status 'done'
Requirement already satisfied: pyyaml in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from torchx-nightly==2022.6.29->-r requirements/requirements-test.txt (line 8)) (6.0.1)
Collecting docker (from torchx-nightly==2022.6.29->-r requirements/requirements-test.txt (line 8))
docker-7.1.0-py3-none-any.whl.metadata (3.8
Requirement already satisfied: filelock in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from torchx-nightly==2022.6.29->-r requirements/requirements-test.txt (line 8)) (3.13.1)
Requirement already satisfied: fsspec in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from torchx-nightly==2022.6.29->-r requirements/requirements-test.txt (line 8)) (2024.6.1)
Collecting arrow (from torchrec==0.2.0->-r requirements/requirements-test.txt (line 9))
arrow-1.3.0-py3-none-any.whl.metadata (7.5
Requirement already satisfied: attrs in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from torchrec==0.2.0->-r requirements/requirements-test.txt (line 9)) (25.3.0)
Requirement already satisfied: certifi in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from torchrec==0.2.0->-r requirements/requirements-test.txt (line 9)) (2024.6.2)
Requirement already satisfied: charset-normalizer in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from torchrec==0.2.0->-r requirements/requirements-test.txt (line 9)) (2.0.4)
Collecting cmake (from torchrec==0.2.0->-r requirements/requirements-test.txt (line 9))
Downloading cmake-4.0.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.3 kB)
Collecting Cython (from torchrec==0.2.0->-r requirements/requirements-test.txt (line 9))
Downloading Cython-3.0.12-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.3 kB)
Collecting distro (from torchrec==0.2.0->-r requirements/requirements-test.txt (line 9))
distro-1.9.0-py3-none-any.whl.metadata (6.8
Collecting hypothesis (from torchrec==0.2.0->-r requirements/requirements-test.txt (line 9))
hypothesis-6.131.0-py3-none-any.whl.metadata (5.6
Requirement already satisfied: idna in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from torchrec==0.2.0->-r requirements/requirements-test.txt (line 9)) (3.7)
Collecting iopath (from torchrec==0.2.0->-r requirements/requirements-test.txt (line 9))
iopath-0.1.10.tar.gz (42
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 42.2/42.2 kB 564.4 kB/s eta 0:00:00
Preparing metadata (setup.py): started
Preparing metadata (setup.py): finished with status 'done'
Requirement already satisfied: Jinja2 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from torchrec==0.2.0->-r requirements/requirements-test.txt (line 9)) (3.1.4)
Requirement already satisfied: MarkupSafe in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from torchrec==0.2.0->-r requirements/requirements-test.txt (line 9)) (2.1.3)
Collecting mypy-extensions (from torchrec==0.2.0->-r requirements/requirements-test.txt (line 9))
mypy extensions-1.0.0-py3-none-any.whl.metadata (1.1
Requirement already satisfied: numpy in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from torchrec==0.2.0->-r requirements/requirements-test.txt (line 9)) (1.26.4)
Requirement already satisfied: packaging in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from torchrec==0.2.0->-r requirements/requirements-test.txt (line 9)) (24.1)
Collecting pandas (from torchrec==0.2.0->-r requirements/requirements-test.txt (line 9))
Downloading pandas-2.2.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (89 kB)
89 9 89 9 kB 1 4 0 00 00
Collecting portalocker (from torchrec==0.2.0->-r requirements/requirements-test.txt (line 9))
portalocker-3.1.1-py3-none-any.whl.metadata (8.6
Collecting pyarrow (from torchrec==0.2.0->-r requirements/requirements-test.txt (line 9))
Downloading pyarrow-19.0.1-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (3.3 kB)
Collecting pyDeprecate (from torchrec==0.2.0->-r requirements/requirements-test.txt (line 9))
pyDeprecate-0.3.2-py3-none-any.whl.metadata (10
Collecting pyparsing (from torchrec==0.2.0->-r requirements/requirements-test.txt (line 9))
pyparsing-3.2.3-py3-none-any.whl.metadata (5.0
Collecting pyre-extensions (from torchx-nightly==2022.6.29->-r requirements/requirements-test.txt (line 8))
pyre extensions-0.0.27-py3-none-any.whl.metadata (4.0
Collecting python-dateutil (from torchrec==0.2.0->-r requirements/requirements-test.txt (line 9))
python dateutil-2.9.0.post0-py2.py3-none-any.whl.metadata (8.4
Collecting pytz (from torchrec==0.2.0->-r requirements/requirements-test.txt (line 9))
pytz-2025.2-py2.py3-none-any.whl.metadata (22
Collecting scikit-build (from torchrec==0.2.0->-r requirements/requirements-test.txt (line 9))
scikit build-0.18.1-py3-none-any.whl.metadata (18
Collecting six (from torchrec==0.2.0->-r requirements/requirements-test.txt (line 9))
six-1.17.0-py2.py3-none-any.whl.metadata (1.7
Collecting sortedcontainers (from torchrec==0.2.0->-r requirements/requirements-test.txt (line 9))
sortedcontainers-2.4.0-py2.py3-none-any.whl.metadata (10
Collecting tabulate (from torchrec==0.2.0->-r requirements/requirements-test.txt (line 9))
tabulate-0.9.0-py3-none-any.whl.metadata (34
Collecting torchmetrics (from torchrec==0.2.0->-r requirements/requirements-test.txt (line 9))
torchmetrics-1.7.1-py3-none-any.whl.metadata (21
Requirement already satisfied: tqdm in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from torchrec==0.2.0->-r requirements/requirements-test.txt (line 9)) (4.67.1)
Collecting typing-inspect (from torchrec==0.2.0->-r requirements/requirements-test.txt (line 9))
typing inspect-0.9.0-py3-none-any.whl.metadata (1.5
Requirement already satisfied: typing-extensions in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from torchrec==0.2.0->-r requirements/requirements-test.txt (line 9)) (4.13.2)
Requirement already satisfied: urllib3 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from torchrec==0.2.0->-r requirements/requirements-test.txt (line 9)) (2.2.2)
Collecting usort (from torchrec==0.2.0->-r requirements/requirements-test.txt (line 9))
usort-1.0.8.post1-py3-none-any.whl.metadata (3.5
Collecting websocket-client (from torchrec==0.2.0->-r requirements/requirements-test.txt (line 9))
websocket client-1.8.0-py3-none-any.whl.metadata (8.0
Collecting fbgemm-gpu (from torchrec==0.2.0->-r requirements/requirements-test.txt (line 9))
Downloading fbgemm_gpu-1.1.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (2.8 kB)
Collecting urllib3 (from torchrec==0.2.0->-r requirements/requirements-test.txt (line 9))
urllib3-1.26.20-py2.py3-none-any.whl.metadata (50
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 50.1/50.1 kB 714.3 kB/s eta 0:00:00
Requirement already satisfied: exceptiongroup>=1.0.0rc8 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from pytest->-r requirements/requirements-test.txt (line 1)) (1.2.2)
Collecting iniconfig (from pytest->-r requirements/requirements-test.txt (line 1))
iniconfig-2.1.0-py3-none-any.whl.metadata (2.7
Collecting pluggy<2,>=1.5 (from pytest->-r requirements/requirements-test.txt (line 1))
pluggy-1.5.0-py3-none-any.whl.metadata (4.8
Collecting tomli>=1 (from pytest->-r requirements/requirements-test.txt (line 1))
tomli-2.2.1-py3-none-any.whl.metadata (10
Collecting pytest (from -r requirements/requirements-test.txt (line 1))
pytest-7.4.4-py3-none-any.whl.metadata (7.9
Collecting requirements-parser (from pytest-testmon==2.0.7b1->-r requirements/requirements-test.txt (line 3))
requirements parser-0.11.0-py3-none-any.whl.metadata (4.7
Requirement already satisfied: torch in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from torchvision->-r requirements/requirements-test.txt (line 4)) (2.2.2)
Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from torchvision->-r requirements/requirements-test.txt (line 4)) (10.3.0)
Requirement already satisfied: huggingface_hub in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from timm->-r requirements/requirements-test.txt (line 5)) (0.30.2)
Requirement already satisfied: safetensors in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from timm->-r requirements/requirements-test.txt (line 5)) (0.5.3)
Requirement already satisfied: colossalai in /__w/ColossalAI/ColossalAI (from titans->-r requirements/requirements-test.txt (line 6)) (0.4.9)
Collecting dill<0.3.9,>=0.3.0 (from datasets->-r requirements/requirements-test.txt (line 17))
dill-0.3.8-py3-none-any.whl.metadata (10
INFO: pip is looking at multiple versions of datasets to determine which version is compatible with other requirements. This could take a while.
Collecting datasets (from -r requirements/requirements-test.txt (line 17))
datasets-3.4.1-py3-none-any.whl.metadata (19
datasets-3.4.0-py3-none-any.whl.metadata (19
datasets-3.3.2-py3-none-any.whl.metadata (19
datasets-3.3.1-py3-none-any.whl.metadata (19
datasets-3.3.0-py3-none-any.whl.metadata (19
datasets-3.2.0-py3-none-any.whl.metadata (20
datasets-3.1.0-py3-none-any.whl.metadata (20
INFO: pip is still looking at multiple versions of datasets to determine which version is compatible with other requirements. This could take a while.
datasets-3.0.2-py3-none-any.whl.metadata (20
datasets-3.0.1-py3-none-any.whl.metadata (20
datasets-3.0.0-py3-none-any.whl.metadata (19
datasets-2.21.0-py3-none-any.whl.metadata (21
datasets-2.20.0-py3-none-any.whl.metadata (19
Collecting pyarrow-hotfix (from datasets->-r requirements/requirements-test.txt (line 17))
pyarrow hotfix-0.6-py3-none-any.whl.metadata (3.6
INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.
Collecting datasets (from -r requirements/requirements-test.txt (line 17))
datasets-2.19.2-py3-none-any.whl.metadata (19
datasets-2.19.1-py3-none-any.whl.metadata (19
Collecting xxhash (from datasets->-r requirements/requirements-test.txt (line 17))
Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)
Collecting multiprocess (from datasets->-r requirements/requirements-test.txt (line 17))
multiprocess-0.70.17-py310-none-any.whl.metadata (7.2
Collecting fsspec (from torchx-nightly==2022.6.29->-r requirements/requirements-test.txt (line 8))
fsspec-2024.3.1-py3-none-any.whl.metadata (6.8
Collecting aiohttp (from datasets->-r requirements/requirements-test.txt (line 17))
Downloading aiohttp-3.11.16-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)
Requirement already satisfied: annotated-types>=0.6.0 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from pydantic->-r requirements/requirements-test.txt (line 18)) (0.7.0)
Requirement already satisfied: pydantic-core==2.33.1 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from pydantic->-r requirements/requirements-test.txt (line 18)) (2.33.1)
Requirement already satisfied: typing-inspection>=0.4.0 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from pydantic->-r requirements/requirements-test.txt (line 18)) (0.4.0)
Requirement already satisfied: click>=7.0 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from ray->-r requirements/requirements-test.txt (line 19)) (8.1.8)
Requirement already satisfied: jsonschema in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from ray->-r requirements/requirements-test.txt (line 19)) (4.23.0)
Requirement already satisfied: msgpack<2.0.0,>=1.0.0 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from ray->-r requirements/requirements-test.txt (line 19)) (1.1.0)
Requirement already satisfied: protobuf!=3.19.5,>=3.15.3 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from ray->-r requirements/requirements-test.txt (line 19)) (6.30.2)
Requirement already satisfied: aiosignal in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from ray->-r requirements/requirements-test.txt (line 19)) (1.3.2)
Requirement already satisfied: frozenlist in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from ray->-r requirements/requirements-test.txt (line 19)) (1.5.0)
Requirement already satisfied: psutil in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from peft>=0.7.1->-r requirements/requirements-test.txt (line 20)) (7.0.0)
Requirement already satisfied: transformers in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from peft>=0.7.1->-r requirements/requirements-test.txt (line 20)) (4.39.3)
Requirement already satisfied: accelerate>=0.21.0 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from peft>=0.7.1->-r requirements/requirements-test.txt (line 20)) (1.6.0)
Collecting aiohappyeyeballs>=2.3.0 (from aiohttp->datasets->-r requirements/requirements-test.txt (line 17))
aiohappyeyeballs-2.6.1-py3-none-any.whl.metadata (5.9
Collecting async-timeout<6.0,>=4.0 (from aiohttp->datasets->-r requirements/requirements-test.txt (line 17))
async timeout-5.0.1-py3-none-any.whl.metadata (5.1
Collecting multidict<7.0,>=4.5 (from aiohttp->datasets->-r requirements/requirements-test.txt (line 17))
Downloading multidict-6.4.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.3 kB)
Collecting propcache>=0.2.0 (from aiohttp->datasets->-r requirements/requirements-test.txt (line 17))
Downloading propcache-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)
Collecting yarl<2.0,>=1.17.0 (from aiohttp->datasets->-r requirements/requirements-test.txt (line 17))
Downloading yarl-1.19.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (71 kB)
71 8 71 8 kB 1 1 0 00 00
Requirement already satisfied: sympy in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from torch->torchvision->-r requirements/requirements-test.txt (line 4)) (1.12)
Requirement already satisfied: networkx in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from torch->torchvision->-r requirements/requirements-test.txt (line 4)) (3.2.1)
Collecting types-python-dateutil>=2.8.10 (from arrow->torchrec==0.2.0->-r requirements/requirements-test.txt (line 9))
Downloading types_python_dateutil-2.9.0.20241206-py3-none-any.whl.metadata (2.1 kB)
Requirement already satisfied: pre-commit in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from colossalai->titans->-r requirements/requirements-test.txt (line 6)) (4.2.0)
Requirement already satisfied: rich in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from colossalai->titans->-r requirements/requirements-test.txt (line 6)) (14.0.0)
Requirement already satisfied: fabric in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from colossalai->titans->-r requirements/requirements-test.txt (line 6)) (3.2.2)
Requirement already satisfied: google in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from colossalai->titans->-r requirements/requirements-test.txt (line 6)) (3.0.0)
Requirement already satisfied: bitsandbytes>=0.39.0 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from colossalai->titans->-r requirements/requirements-test.txt (line 6)) (0.45.5)
Requirement already satisfied: rpyc==6.0.0 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from colossalai->titans->-r requirements/requirements-test.txt (line 6)) (6.0.0)
Requirement already satisfied: fastapi in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from colossalai->titans->-r requirements/requirements-test.txt (line 6)) (0.115.12)
Requirement already satisfied: uvicorn==0.29.0 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from colossalai->titans->-r requirements/requirements-test.txt (line 6)) (0.29.0)
Requirement already satisfied: galore_torch in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from colossalai->titans->-r requirements/requirements-test.txt (line 6)) (1.0)
Requirement already satisfied: diffusers==0.29.0 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from colossalai->titans->-r requirements/requirements-test.txt (line 6)) (0.29.0)
Requirement already satisfied: regex!=2019.12.17 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from transformers->peft>=0.7.1->-r requirements/requirements-test.txt (line 20)) (2024.11.6)
Requirement already satisfied: tokenizers<0.19,>=0.14 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from transformers->peft>=0.7.1->-r requirements/requirements-test.txt (line 20)) (0.15.2)
Requirement already satisfied: importlib-metadata in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from diffusers==0.29.0->colossalai->titans->-r requirements/requirements-test.txt (line 6)) (8.6.1)
Requirement already satisfied: plumbum in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from rpyc==6.0.0->colossalai->titans->-r requirements/requirements-test.txt (line 6)) (1.9.0)
Requirement already satisfied: h11>=0.8 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from uvicorn==0.29.0->colossalai->titans->-r requirements/requirements-test.txt (line 6)) (0.14.0)
Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from jsonschema->ray->-r requirements/requirements-test.txt (line 19)) (2024.10.1)
Requirement already satisfied: referencing>=0.28.4 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from jsonschema->ray->-r requirements/requirements-test.txt (line 19)) (0.36.2)
Requirement already satisfied: rpds-py>=0.7.1 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from jsonschema->ray->-r requirements/requirements-test.txt (line 19)) (0.24.0)
INFO: pip is looking at multiple versions of multiprocess to determine which version is compatible with other requirements. This could take a while.
Collecting multiprocess (from datasets->-r requirements/requirements-test.txt (line 17))
multiprocess-0.70.16-py310-none-any.whl.metadata (7.2
Collecting tzdata>=2022.7 (from pandas->torchrec==0.2.0->-r requirements/requirements-test.txt (line 9))
tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4
Collecting types-setuptools>=69.1.0 (from requirements-parser->pytest-testmon==2.0.7b1->-r requirements/requirements-test.txt (line 3))
types setuptools-78.1.0.20250329-py3-none-any.whl.metadata (2.2
Requirement already satisfied: setuptools>=42.0.0 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from scikit-build->torchrec==0.2.0->-r requirements/requirements-test.txt (line 9)) (69.5.1)
Requirement already satisfied: wheel>=0.32.0 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from scikit-build->torchrec==0.2.0->-r requirements/requirements-test.txt (line 9)) (0.43.0)
Collecting lightning-utilities>=0.8.0 (from torchmetrics->torchrec==0.2.0->-r requirements/requirements-test.txt (line 9))
lightning utilities-0.14.3-py3-none-any.whl.metadata (5.6
Collecting LibCST>=0.3.7 (from usort->torchrec==0.2.0->-r requirements/requirements-test.txt (line 9))
Downloading libcst-1.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (17 kB)
Collecting moreorless>=0.3.0 (from usort->torchrec==0.2.0->-r requirements/requirements-test.txt (line 9))
moreorless-0.4.0-py2.py3-none-any.whl.metadata (1.5
Collecting stdlibs>=2021.4.1 (from usort->torchrec==0.2.0->-r requirements/requirements-test.txt (line 9))
stdlibs-2025.4.4-py3-none-any.whl.metadata (5.0
Collecting toml>=0.10.0 (from usort->torchrec==0.2.0->-r requirements/requirements-test.txt (line 9))
toml-0.10.2-py2.py3-none-any.whl.metadata (7.1
Collecting trailrunner>=1.0 (from usort->torchrec==0.2.0->-r requirements/requirements-test.txt (line 9))
trailrunner-1.4.0-py3-none-any.whl.metadata (4.2
Collecting pathspec>=0.8.1 (from trailrunner>=1.0->usort->torchrec==0.2.0->-r requirements/requirements-test.txt (line 9))
pathspec-0.12.1-py3-none-any.whl.metadata (21
Requirement already satisfied: invoke>=2.0 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from fabric->colossalai->titans->-r requirements/requirements-test.txt (line 6)) (2.2.0)
Requirement already satisfied: paramiko>=2.4 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from fabric->colossalai->titans->-r requirements/requirements-test.txt (line 6)) (3.5.1)
Requirement already satisfied: decorator>=5 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from fabric->colossalai->titans->-r requirements/requirements-test.txt (line 6)) (5.2.1)
Requirement already satisfied: deprecated>=1.2 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from fabric->colossalai->titans->-r requirements/requirements-test.txt (line 6)) (1.2.18)
Requirement already satisfied: starlette<0.47.0,>=0.40.0 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from fastapi->colossalai->titans->-r requirements/requirements-test.txt (line 6)) (0.46.1)
Requirement already satisfied: beautifulsoup4 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from google->colossalai->titans->-r requirements/requirements-test.txt (line 6)) (4.13.3)
Requirement already satisfied: cfgv>=2.0.0 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from pre-commit->colossalai->titans->-r requirements/requirements-test.txt (line 6)) (3.4.0)
Requirement already satisfied: identify>=1.0.0 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from pre-commit->colossalai->titans->-r requirements/requirements-test.txt (line 6)) (2.6.9)
Requirement already satisfied: nodeenv>=0.11.1 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from pre-commit->colossalai->titans->-r requirements/requirements-test.txt (line 6)) (1.9.1)
Requirement already satisfied: virtualenv>=20.10.0 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from pre-commit->colossalai->titans->-r requirements/requirements-test.txt (line 6)) (20.30.0)
Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from rich->colossalai->titans->-r requirements/requirements-test.txt (line 6)) (3.0.0)
Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from rich->colossalai->titans->-r requirements/requirements-test.txt (line 6)) (2.19.1)
Requirement already satisfied: mpmath>=0.19 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from sympy->torch->torchvision->-r requirements/requirements-test.txt (line 4)) (1.3.0)
Requirement already satisfied: wrapt<2,>=1.10 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from deprecated>=1.2->fabric->colossalai->titans->-r requirements/requirements-test.txt (line 6)) (1.17.2)
Requirement already satisfied: mdurl~=0.1 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich->colossalai->titans->-r requirements/requirements-test.txt (line 6)) (0.1.2)
Requirement already satisfied: bcrypt>=3.2 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from paramiko>=2.4->fabric->colossalai->titans->-r requirements/requirements-test.txt (line 6)) (4.3.0)
Requirement already satisfied: cryptography>=3.3 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from paramiko>=2.4->fabric->colossalai->titans->-r requirements/requirements-test.txt (line 6)) (44.0.2)
Requirement already satisfied: pynacl>=1.5 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from paramiko>=2.4->fabric->colossalai->titans->-r requirements/requirements-test.txt (line 6)) (1.5.0)
Requirement already satisfied: anyio<5,>=3.6.2 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from starlette<0.47.0,>=0.40.0->fastapi->colossalai->titans->-r requirements/requirements-test.txt (line 6)) (4.9.0)
Requirement already satisfied: distlib<1,>=0.3.7 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from virtualenv>=20.10.0->pre-commit->colossalai->titans->-r requirements/requirements-test.txt (line 6)) (0.3.9)
Requirement already satisfied: platformdirs<5,>=3.9.1 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from virtualenv>=20.10.0->pre-commit->colossalai->titans->-r requirements/requirements-test.txt (line 6)) (4.3.7)
Requirement already satisfied: soupsieve>1.2 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from beautifulsoup4->google->colossalai->titans->-r requirements/requirements-test.txt (line 6)) (2.6)
Requirement already satisfied: zipp>=3.20 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from importlib-metadata->diffusers==0.29.0->colossalai->titans->-r requirements/requirements-test.txt (line 6)) (3.21.0)
Requirement already satisfied: sniffio>=1.1 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from anyio<5,>=3.6.2->starlette<0.47.0,>=0.40.0->fastapi->colossalai->titans->-r requirements/requirements-test.txt (line 6)) (1.3.1)
Requirement already satisfied: cffi>=1.12 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from cryptography>=3.3->paramiko>=2.4->fabric->colossalai->titans->-r requirements/requirements-test.txt (line 6)) (1.17.1)
Requirement already satisfied: pycparser in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from cffi>=1.12->cryptography>=3.3->paramiko>=2.4->fabric->colossalai->titans->-r requirements/requirements-test.txt (line 6)) (2.22)
Downloading coverage-7.2.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (227 kB)
228 0 228 0 kB 3 0 0 00 00
torchx nightly-2022.6.29-py3-none-any.whl (177
177 8 177 8 kB 2 3 0 00 00
torchrec-0.2.0-py39-none-any.whl (293
293 4 293 4 kB 4 0 0 00 00
requests-2.27.1-py2.py3-none-any.whl (63
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 63.1/63.1 kB 744.5 kB/s eta 0:00:00
pyre extensions-0.0.27-py3-none-any.whl (12
pytest-7.4.4-py3-none-any.whl (325
325 3 325 3 kB 4 4 0 00 00
Downloading timm-1.0.15-py3-none-any.whl (2.4 MB)
2 4 2 4 MB 10 7 0 00 00
datasets-2.19.1-py3-none-any.whl (542
542 0 542 0 kB 7 5 0 00 00
dill-0.3.8-py3-none-any.whl (116
116 3 116 3 kB 1 5 0 00 00
fsspec-2024.3.1-py3-none-any.whl (171
172 0 172 0 kB 2 3 0 00 00
Downloading aiohttp-3.11.16-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)
1 6 1 6 MB 11 9 0 00 00
pluggy-1.5.0-py3-none-any.whl (20
Downloading pyarrow-19.0.1-cp310-cp310-manylinux_2_28_x86_64.whl (42.1 MB)
42 1 42 1 MB 18 7 0 00 00
tomli-2.2.1-py3-none-any.whl (14
urllib3-1.26.20-py2.py3-none-any.whl (144
144 2 144 2 kB 1 9 0 00 00
arrow-1.3.0-py3-none-any.whl (66
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 66.4/66.4 kB 786.9 kB/s eta 0:00:00
python dateutil-2.9.0.post0-py2.py3-none-any.whl (229
229 9 229 9 kB 3 1 0 00 00
six-1.17.0-py2.py3-none-any.whl (11
Downloading cmake-4.0.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (27.9 MB)
27 9 27 9 MB 22 3 0 00 00
Downloading Cython-3.0.12-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)
3 6 3 6 MB 16 7 0 00 00
distro-1.9.0-py3-none-any.whl (20
docker-7.1.0-py3-none-any.whl (147
147 8 147 8 kB 1 9 0 00 00
Downloading fbgemm_gpu-1.1.0-cp310-cp310-manylinux_2_28_x86_64.whl (417.2 MB)
417 2 417 2 MB 26 7 0 00 00
hypothesis-6.131.0-py3-none-any.whl (495
495 7 495 7 kB 6 9 0 00 00
sortedcontainers-2.4.0-py2.py3-none-any.whl (29
iniconfig-2.1.0-py3-none-any.whl (6.0
multiprocess-0.70.16-py310-none-any.whl (134
134 8 134 8 kB 1 7 0 00 00
mypy extensions-1.0.0-py3-none-any.whl (4.7
Downloading pandas-2.2.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.1 MB)
13 1 13 1 MB 28 4 0 00 00
pytz-2025.2-py2.py3-none-any.whl (509
509 2 509 2 kB 7 0 0 00 00
portalocker-3.1.1-py3-none-any.whl (19
pyarrow hotfix-0.6-py3-none-any.whl (7.9
pyDeprecate-0.3.2-py3-none-any.whl (10
pyparsing-3.2.3-py3-none-any.whl (111
111 1 111 1 kB 1 4 0 00 00
requirements parser-0.11.0-py3-none-any.whl (14
scikit build-0.18.1-py3-none-any.whl (85
85 6 85 6 kB 1 1 0 00 00
tabulate-0.9.0-py3-none-any.whl (35
torchmetrics-1.7.1-py3-none-any.whl (961
961 5 961 5 kB 13 3 0 00 00
typing inspect-0.9.0-py3-none-any.whl (8.8
usort-1.0.8.post1-py3-none-any.whl (37
websocket client-1.8.0-py3-none-any.whl (58
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 58.8/58.8 kB 682.5 kB/s eta 0:00:00
Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)
194 1 194 1 kB 2 6 0 00 00
aiohappyeyeballs-2.6.1-py3-none-any.whl (15
async timeout-5.0.1-py3-none-any.whl (6.2
Downloading libcst-1.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.3 MB)
2 3 2 3 MB 16 2 0 00 00
lightning utilities-0.14.3-py3-none-any.whl (28
moreorless-0.4.0-py2.py3-none-any.whl (9.3
Downloading multidict-6.4.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (219 kB)
219 8 219 8 kB 3 0 0 00 00
Downloading propcache-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (206 kB)
206 6 206 6 kB 2 8 0 00 00
stdlibs-2025.4.4-py3-none-any.whl (57
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 57.2/57.2 kB 657.8 kB/s eta 0:00:00
toml-0.10.2-py2.py3-none-any.whl (16
trailrunner-1.4.0-py3-none-any.whl (11
Downloading types_python_dateutil-2.9.0.20241206-py3-none-any.whl (14 kB)
types setuptools-78.1.0.20250329-py3-none-any.whl (66
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 67.0/67.0 kB 798.2 kB/s eta 0:00:00
tzdata-2025.2-py2.py3-none-any.whl (347
347 8 347 8 kB 4 7 0 00 00
Downloading yarl-1.19.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (334 kB)
334 0 334 0 kB 4 5 0 00 00
pathspec-0.12.1-py3-none-any.whl (31
Building wheels for collected packages: docstring-parser, pytest-testmon, titans, flash_attn, iopath
Building wheel for docstring-parser (pyproject.toml): started
Building wheel for docstring-parser (pyproject.toml): finished with status 'done'
Created wheel for docstring-parser: filename=docstring_parser-0.8.1-py3-none-any.whl size=19697 sha256=e200f555702049d4b6e66e10ffef729c660ad58ac03df5cc4d3e50a4739c3989
Stored in directory: /tmp/pip-ephem-wheel-cache-61bbd2f1/wheels/26/e4/54/64439f1d0c5d3721041ddc0f001e4b57756a394880a2af8981
Building wheel for pytest-testmon (pyproject.toml): started
Building wheel for pytest-testmon (pyproject.toml): finished with status 'done'
Created wheel for pytest-testmon: filename=pytest_testmon-2.0.7b1-py3-none-any.whl size=35044 sha256=3ec1c2943987eee4e37f71553f8a3ec3f4b057719fd1287d03993a8cfa2d02d1
Stored in directory: /tmp/pip-ephem-wheel-cache-61bbd2f1/wheels/cb/f6/db/609602674f7b7c7ecbfd91b3d67b1ea34fe598f7e344495c44
Building wheel for titans (setup.py): started
Building wheel for titans (setup.py): finished with status 'done'
Created wheel for titans: filename=titans-0.0.7-py3-none-any.whl size=63319 sha256=51438bdb24a0ed111e84a5cf71094315c38422b0a7d97c45adedf6277b3086a7
Stored in directory: /tmp/pip-ephem-wheel-cache-61bbd2f1/wheels/65/21/1b/3dfb7cdd10cdc650f08fbb72b6f28dd75e1e9b7b42f6695a16
Building wheel for flash_attn (setup.py): started
Building wheel for flash_attn (setup.py): finished with status 'done'
Created wheel for flash_attn: filename=flash_attn-2.7.4.post1-cp310-cp310-linux_x86_64.whl size=187696268 sha256=1776769f7ae3a8be3b31ec3a4c875ad1764da74be2d9b1751e5c01162ad0096f
Stored in directory: /tmp/pip-ephem-wheel-cache-61bbd2f1/wheels/59/ce/d5/08ea07bfc16ba218dc65a3a7ef9b6a270530bcbd2cea2ee1ca
Building wheel for iopath (setup.py): started
Building wheel for iopath (setup.py): finished with status 'done'
Created wheel for iopath: filename=iopath-0.1.10-py3-none-any.whl size=31532 sha256=5176f3f57e1853d850a4cae931e4621391ba12acb872d984ae182fd093813c96
Stored in directory: /tmp/pip-ephem-wheel-cache-61bbd2f1/wheels/9a/a3/b6/ac0fcd1b4ed5cfeb3db92e6a0e476cfd48ed0df92b91080c1d
Successfully built docstring-parser pytest-testmon titans flash_attn iopath
Installing collected packages: sortedcontainers, pytz, xxhash, websocket-client, urllib3, tzdata, types-setuptools, types-python-dateutil, tomli, toml, tabulate, stdlibs, six, pyparsing, pyDeprecate, pyarrow-hotfix, pyarrow, propcache, portalocker, pluggy, pathspec, mypy-extensions, multidict, moreorless, lightning-utilities, LibCST, iniconfig, hypothesis, fsspec, fbgemm-gpu, docstring-parser, distro, dill, Cython, coverage, cmake, async-timeout, aiohappyeyeballs, yarl, typing-inspect, trailrunner, scikit-build, requirements-parser, requests, python-dateutil, pytest, multiprocess, iopath, usort, torchmetrics, pytest-testmon, pyre-extensions, pandas, flash_attn, docker, arrow, aiohttp, torchx-nightly, timm, torchrec, datasets, titans
Attempting uninstall: urllib3
Found existing installation: urllib3 2.2.2
Uninstalling urllib3-2.2.2:
Successfully uninstalled urllib3-2.2.2
Attempting uninstall: fsspec
Found existing installation: fsspec 2024.6.1
Uninstalling fsspec-2024.6.1:
Successfully uninstalled fsspec-2024.6.1
Attempting uninstall: requests
Found existing installation: requests 2.32.2
Uninstalling requests-2.32.2:
Successfully uninstalled requests-2.32.2
Successfully installed Cython-3.0.12 LibCST-1.7.0 aiohappyeyeballs-2.6.1 aiohttp-3.11.16 arrow-1.3.0 async-timeout-5.0.1 cmake-4.0.0 coverage-7.2.3 datasets-2.19.1 dill-0.3.8 distro-1.9.0 docker-7.1.0 docstring-parser-0.8.1 fbgemm-gpu-1.1.0 flash_attn-2.7.4.post1 fsspec-2024.3.1 hypothesis-6.131.0 iniconfig-2.1.0 iopath-0.1.10 lightning-utilities-0.14.3 moreorless-0.4.0 multidict-6.4.3 multiprocess-0.70.16 mypy-extensions-1.0.0 pandas-2.2.3 pathspec-0.12.1 pluggy-1.5.0 portalocker-3.1.1 propcache-0.3.1 pyDeprecate-0.3.2 pyarrow-19.0.1 pyarrow-hotfix-0.6 pyparsing-3.2.3 pyre-extensions-0.0.27 pytest-7.4.4 pytest-testmon-2.0.7b1 python-dateutil-2.9.0.post0 pytz-2025.2 requests-2.27.1 requirements-parser-0.11.0 scikit-build-0.18.1 six-1.17.0 sortedcontainers-2.4.0 stdlibs-2025.4.4 tabulate-0.9.0 timm-1.0.15 titans-0.0.7 toml-0.10.2 tomli-2.2.1 torchmetrics-1.7.1 torchrec-0.2.0 torchx-nightly-2022.6.29 trailrunner-1.4.0 types-python-dateutil-2.9.0.20241206 types-setuptools-78.1.0.20250329 typing-inspect-0.9.0 tzdata-2025.2 urllib3-1.26.20 usort-1.0.8.post1 websocket-client-1.8.0 xxhash-3.5.0 yarl-1.19.0
WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv
##[group]Run # -p flag is required to preserve the file timestamp to avoid ninja rebuild
[36;1m# -p flag is required to preserve the file timestamp to avoid ninja rebuild[0m
[36;1mcp -p -r /__w/ColossalAI/ColossalAI/build /github/home/cuda_ext_cache/[0m
shell: bash --noprofile --norc -e -o pipefail {0}
##[endgroup]
##[group]Run CURL_CA_BUNDLE="" PYTHONPATH=$PWD FAST_TEST=1 pytest \
[36;1mCURL_CA_BUNDLE="" PYTHONPATH=$PWD FAST_TEST=1 pytest \[0m
[36;1m-m "not largedist" \[0m
[36;1m--durations=0 \[0m
[36;1m--ignore tests/test_analyzer \[0m
[36;1m--ignore tests/test_auto_parallel \[0m
[36;1m--ignore tests/test_fx \[0m
[36;1m--ignore tests/test_autochunk \[0m
[36;1m--ignore tests/test_gptq \[0m
[36;1m--ignore tests/test_infer_ops \[0m
[36;1m--ignore tests/test_legacy \[0m
[36;1m--ignore tests/test_smoothquant \[0m
[36;1mtests/[0m
shell: bash --noprofile --norc -e -o pipefail {0}

LD_LIBRARY_PATH: /github/home/.tensornvme/lib:/usr/local/nvidia/lib:/usr/local/nvidia/lib64
LLAMA_PATH: /data/scratch/llama-tiny
MOE_TENSOR_PATH: /data/scratch/moe_tensors
HF_ENDPOINT: https://hf-mirror.com
##[endgroup]
============================= test session starts ==============================
platform linux -- Python 3.10.14, pytest-7.4.4, pluggy-1.5.0
rootdir: /__w/ColossalAI/ColossalAI
configfile: pytest.ini
plugins: hypothesis-6.131.0, anyio-4.9.0, testmon-2.0.7b1
collected 865 items / 23 deselected / 842 selected
2025-04-11T04:12:24.7055579Z
tests/test_booster/test_accelerator.py F                                 [  0%]
tests/test_booster/test_mixed_precision/test_fp16_torch.py s             [  0%]
tests/test_booster/test_plugin/test_3d_plugin.py F                       [  0%]
tests/test_booster/test_plugin/test_dp_plugin_base.py F                  [  0%]
tests/test_booster/test_plugin/test_gemini_plugin.py F                   [  0%]
tests/test_booster/test_plugin/test_low_level_zero_plugin.py F           [  0%]
tests/test_booster/test_plugin/test_torch_ddp_plugin.py F                [  0%]
tests/test_booster/test_plugin/test_torch_fsdp_plugin.py F               [  0%]
tests/test_checkpoint_io/test_gemini_checkpoint_io.py F                  [  1%]
tests/test_checkpoint_io/test_gemini_torch_compability.py F              [  1%]
tests/test_checkpoint_io/test_general_checkpoint_io.py F..FFFFFF         [  2%]
tests/test_checkpoint_io/test_hybrid_parallel_plugin_checkpoint_io.py F  [  2%]
tests/test_checkpoint_io/test_low_level_zero_checkpoint_io.py F          [  2%]
tests/test_checkpoint_io/test_plugins_huggingface_compatibility.py F     [  2%]
tests/test_checkpoint_io/test_safetensors_async_io.py FFFFF              [  3%]
tests/test_checkpoint_io/test_torch_ddp_checkpoint_io.py F               [  3%]
tests/test_checkpoint_io/test_torch_fsdp_checkpoint_io.py F              [  3%]
tests/test_cluster/test_device_mesh_manager.py .                         [  3%]
tests/test_cluster/test_process_group_mesh.py .                          [  3%]
tests/test_config/test_load_config.py .                                  [  3%]
tests/test_device/test_alpha_beta.py s                                   [  3%]
tests/test_device/test_device_mesh.py ..                                 [  4%]
tests/test_device/test_extract_alpha_beta.py s                           [  4%]
tests/test_device/test_init_logical_pg.py F                              [  4%]
tests/test_device/test_search_logical_device_mesh.py s                   [  4%]
tests/test_fp8/test_all_to_all_single.py F                               [  4%]
tests/test_fp8/test_fp8_all_to_all.py F                                  [  4%]
tests/test_fp8/test_fp8_all_to_all_single.py F                           [  4%]
tests/test_fp8/test_fp8_allgather.py F                                   [  4%]
tests/test_fp8/test_fp8_allreduce.py F                                   [  5%]
tests/test_fp8/test_fp8_cast.py F                                        [  5%]
tests/test_fp8/test_fp8_fsdp_comm_hook.py F                              [  5%]
tests/test_fp8/test_fp8_hook.py F                                        [  5%]
tests/test_fp8/test_fp8_linear.py FFFF                                   [  5%]
tests/test_fp8/test_fp8_reduce_scatter.py F                              [  6%]
tests/test_infer/test_batch_bucket.py F                                  [  6%]
tests/test_infer/test_config_and_struct.py .                             [  6%]
tests/test_infer/test_continuous_batching.py F                           [  6%]
tests/test_infer/test_drafter.py FF                                      [  6%]
tests/test_infer/test_kvcache_manager.py .F                              [  6%]
tests/test_infer/test_request_handler.py F                               [  7%]
tests/test_infer/test_streamingllm.py F                                  [  7%]
tests/test_infer/test_async_engine/test_async_engine.py s                [  7%]
tests/test_infer/test_async_engine/test_request_tracer.py .              [  7%]
tests/test_infer/test_kernels/cuda/test_convert_fp8.py sssssssssssssssss [  9%]
ssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssss [ 17%]
sssssssssssssssssss                                                      [ 20%]
tests/test_infer/test_kernels/cuda/test_flash_decoding_attention.py FF   [ 20%]
tests/test_infer/test_kernels/cuda/test_get_cos_and_sin.py FF            [ 20%]
tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py FFFFFFFFFFFFF [ 22%]
FFFFFFFFFFFFFFFFFFFFFFF                                                  [ 24%]
tests/test_infer/test_kernels/cuda/test_rms_layernorm.py FFFFFFFFFFFFFFF [ 26%]
F                                                                        [ 26%]
tests/test_infer/test_kernels/cuda/test_rotary_embdding_unpad.py FFFF    [ 27%]
tests/test_infer/test_kernels/cuda/test_silu_and_mul.py FF               [ 27%]
tests/test_infer/test_kernels/triton/test_context_attn_unpad.py ........ [ 28%]
........................FFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFF [ 37%]
FFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFF                         [ 42%]
tests/test_infer/test_kernels/triton/test_decoding_attn.py sssssssssssss [ 44%]
sssssssssssssssssssssssssssssssssssssssssssssssssssFFFFFFFFFFFFFFFFFFFFF [ 52%]
FFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFF [ 61%]
FFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFF [ 69%]
FFFFFFFFFFFFFFFFFFFFFFFFFFF                                              [ 73%]
tests/test_infer/test_kernels/triton/test_fused_rotary_embedding.py s    [ 73%]
tests/test_infer/test_kernels/triton/test_kvcache_copy.py FFFFFFFFFFFFFF [ 74%]
FFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFF                                       [ 78%]
tests/test_infer/test_kernels/triton/test_rmsnorm_triton.py F            [ 79%]
tests/test_infer/test_kernels/triton/test_rotary_embdding_unpad.py FF    [ 79%]
tests/test_infer/test_kernels/triton/test_xine_copy.py F                 [ 79%]
tests/test_infer/test_models/test_attention.py ssss                      [ 79%]
tests/test_infer/test_models/test_custom_model.py s                      [ 80%]
tests/test_lazy/test_from_pretrained.py .                                [ 80%]
tests/test_lazy/test_models.py .F                                        [ 80%]
tests/test_lazy/test_ops.py F                                            [ 80%]
tests/test_lora/test_lora.py F                                           [ 80%]
tests/test_moe/test_deepseek_layer.py s                                  [ 80%]
tests/test_moe/test_kernel.py FF                                         [ 80%]
tests/test_moe/test_mixtral_layer.py s                                   [ 81%]
tests/test_moe/test_moe_checkpoint.py F                                  [ 81%]
tests/test_moe/test_moe_ep_tp.py s                                       [ 81%]
tests/test_moe/test_moe_ep_zero.py s                                     [ 81%]
tests/test_optimizer/test_adam_kernel.py FFFFFFFFFFFFFFFFFFFF........... [ 85%]
.                                                                        [ 85%]
tests/test_optimizer/test_adam_optim.py F.FFFF.FFFF.FFFF.FFFF.FFFF.FFF   [ 88%]
tests/test_optimizer/test_dist_adafactor.py F                            [ 88%]
tests/test_optimizer/test_dist_came.py F                                 [ 89%]
tests/test_optimizer/test_dist_galore.py F                               [ 89%]
tests/test_optimizer/test_dist_lamb.py F                                 [ 89%]
tests/test_optimizer/test_lr_scheduler.py .                              [ 89%]
tests/test_optimizer/test_nvme.py s                                      [ 89%]
tests/test_pipeline/test_p2p_communication.py F                          [ 89%]
tests/test_pipeline/test_stage_manager.py F                              [ 89%]
tests/test_pipeline/test_pipeline_utils/test_t5_pipeline_utils.py ..     [ 90%]
tests/test_pipeline/test_pipeline_utils/test_whisper_pipeline_utils.py . [ 90%]
.                                                                        [ 90%]
tests/test_pipeline/test_schedule/test_interleaved.py FFFF               [ 90%]
tests/test_pipeline/test_schedule/test_oneF_oneB.py FFFF                 [ 91%]
tests/test_pipeline/test_schedule/test_pipeline_schedule_utils.py ...    [ 91%]
tests/test_pipeline/test_schedule/test_zerobubble_pp.py F                [ 91%]
tests/test_shardformer/test_flash_attention.py F                         [ 91%]
tests/test_shardformer/test_shard_utils.py F                             [ 91%]
tests/test_shardformer/test_with_torch_ddp.py F                          [ 92%]
tests/test_shardformer/test_hybrid_parallel_grad_clip_norm/test_amp_optimizer.py F [ 92%]
[ 92%]
tests/test_shardformer/test_hybrid_parallel_grad_clip_norm/test_naive_optimizer.py F [ 92%]
[ 92%]
tests/test_shardformer/test_hybrid_parallel_grad_clip_norm/test_zero_optimizer.py F [ 92%]
[ 92%]
tests/test_shardformer/test_layer/test_dist_crossentropy.py F            [ 92%]
tests/test_shardformer/test_layer/test_dropout.py F                      [ 92%]
tests/test_shardformer/test_layer/test_embedding.py F                    [ 92%]
tests/test_shardformer/test_layer/test_gpt2_qkv_fused_linear_1d.py F     [ 92%]
tests/test_shardformer/test_layer/test_layernorm.py F                    [ 92%]
tests/test_shardformer/test_layer/test_linear_1d.py F                    [ 93%]
tests/test_shardformer/test_layer/test_qkv_fused_linear_1d.py F          [ 93%]
tests/test_shardformer/test_layer/test_ring_attn.py FF                   [ 93%]
tests/test_shardformer/test_layer/test_sequence_parallel.py F            [ 93%]
tests/test_shardformer/test_layer/test_vocab_parallel_embedding_1d.py F  [ 93%]
tests/test_shardformer/test_model/test_shard_bert.py F                   [ 93%]
tests/test_shardformer/test_model/test_shard_blip2.py F                  [ 93%]
tests/test_shardformer/test_model/test_shard_bloom.py F                  [ 94%]
tests/test_shardformer/test_model/test_shard_chatglm2.py F               [ 94%]
tests/test_shardformer/test_model/test_shard_command.py F                [ 94%]
tests/test_shardformer/test_model/test_shard_deepseek.py F               [ 94%]
tests/test_shardformer/test_model/test_shard_deepseek_v3.py F            [ 94%]
tests/test_shardformer/test_model/test_shard_falcon.py F                 [ 94%]
tests/test_shardformer/test_model/test_shard_gpt2.py F                   [ 94%]
tests/test_shardformer/test_model/test_shard_gptj.py s                   [ 94%]
tests/test_shardformer/test_model/test_shard_llama.py F                  [ 95%]
tests/test_shardformer/test_model/test_shard_mistral.py F                [ 95%]
tests/test_shardformer/test_model/test_shard_mixtral.py F                [ 95%]
tests/test_shardformer/test_model/test_shard_opt.py F                    [ 95%]
tests/test_shardformer/test_model/test_shard_qwen2.py F                  [ 95%]
tests/test_shardformer/test_model/test_shard_sam.py F                    [ 95%]
tests/test_shardformer/test_model/test_shard_t5.py F                     [ 95%]
tests/test_shardformer/test_model/test_shard_vit.py F                    [ 95%]
tests/test_shardformer/test_model/test_shard_whisper.py F                [ 95%]
tests/test_tensor/test_comm_spec_apply.py F                              [ 96%]
tests/test_tensor/test_mix_gather.py s                                   [ 96%]
tests/test_tensor/test_padded_tensor.py F                                [ 96%]
tests/test_tensor/test_shape_consistency.py ..                           [ 96%]
tests/test_tensor/test_shape_consistency_apply.py F                      [ 96%]
tests/test_tensor/test_sharding_spec.py .                                [ 96%]
tests/test_tensor/test_dtensor/test_comm_spec.py F                       [ 96%]
tests/test_tensor/test_dtensor/test_dtensor.py F                         [ 97%]
tests/test_tensor/test_dtensor/test_dtensor_sharding_spec.py .           [ 97%]
tests/test_tensor/test_dtensor/test_layout_converter.py F                [ 97%]
tests/test_zero/test_gemini/test_chunk_mgrv2.py F                        [ 97%]
tests/test_zero/test_gemini/test_chunkv2.py FFF                          [ 97%]
tests/test_zero/test_gemini/test_gemini_use_rmt.py ss                    [ 97%]
tests/test_zero/test_gemini/test_grad_accum.py F                         [ 98%]
tests/test_zero/test_gemini/test_grad_clip.py FF                         [ 98%]
tests/test_zero/test_gemini/test_inference.py FF                         [ 98%]
tests/test_zero/test_gemini/test_optim.py F                              [ 98%]
tests/test_zero/test_gemini/test_runtime_mem_tracer.py s                 [ 98%]
tests/test_zero/test_gemini/test_search.py FF                            [ 99%]
tests/test_zero/test_gemini/test_zeroddp_state_dict.py F                 [ 99%]
tests/test_zero/test_gemini/test_zerooptim_state_dict.py ss              [ 99%]
tests/test_zero/test_low_level/test_coll_nd.py F                         [ 99%]
tests/test_zero/test_low_level/test_grad_acc.py F                        [ 99%]
tests/test_zero/test_low_level/test_mem_leak.py F                        [ 99%]
tests/test_zero/test_low_level/test_zero1_2.py F                         [ 99%]
tests/test_zero/test_low_level/test_zero_ckpt.py F                       [100%]
2025-04-11T04:23:18.3942503Z
=================================== FAILURES ===================================
_______________________________ test_accelerator _______________________________
2025-04-11T04:23:18.3944292Z
args = (), kwargs = {}
2025-04-11T04:23:18.3944549Z
def _clear_cache(*args, **kwargs):
get_accelerator().empty_cache()
get_accelerator().reset_peak_memory_stats()
get_accelerator().reset_max_memory_allocated()
get_accelerator().reset_max_memory_cached()
>       get_accelerator().synchronize()
2025-04-11T04:23:18.3946731Z
colossalai/testing/utils.py:271:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
colossalai/accelerator/cuda_accelerator.py:62: in synchronize
torch.cuda.synchronize(device)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.3948314Z
device = None
2025-04-11T04:23:18.3948562Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.3949375Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.3953705Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
________________________________ test_3d_plugin ________________________________
2025-04-11T04:23:18.3954650Z
args = (), kwargs = {}, try_count = 1
error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T04:23:18.3956236Z
def _run_until_success(*args, **kwargs):
try_count = 0
assert max_try is None or isinstance(
max_try, int
), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T04:23:18.3957736Z
while max_try is None or try_count < max_try:
try:
try_count += 1
ret = func(*args, **kwargs)
return ret
except exception_type as e:
error_lines = str(e).split("\n")
if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
print("Exception is caught, retrying...")
# when pattern is not specified, we always skip the exception
# when pattern is specified, we only skip when pattern is matched
continue
else:
print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
>                   raise e
2025-04-11T04:23:18.3962463Z
colossalai/testing/utils.py:144:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
colossalai/testing/utils.py:133: in _run_until_success
ret = func(*args, **kwargs)
tests/test_booster/test_plugin/test_3d_plugin.py:277: in test_3d_plugin
spawn(run_dist, 4, early_stop=early_stop)
colossalai/testing/utils.py:252: in spawn
mp.spawn(wrapped_func, nprocs=nprocs)
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
while not context.join():
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.3967161Z
self = <torch.multiprocessing.spawn.ProcessContext object at 0x7fb5b8493580>
timeout = None
2025-04-11T04:23:18.3967833Z
def join(self, timeout=None):
r"""Join one or more processes within spawn context.
2025-04-11T04:23:18.3968479Z
Attempt to join one or more processes in this spawn context.
If one of them exited with a non-zero exit status, this function
kills the remaining processes and raises an exception with the cause
of the first process exiting.
2025-04-11T04:23:18.3970308Z
Returns ``True`` if all processes have been joined successfully,
``False`` if there are more processes that need to be joined.
2025-04-11T04:23:18.3971212Z
Args:
timeout (float): Wait this long before giving up on waiting.
"""
# Ensure this function can be called even when we're done.
if len(self.sentinels) == 0:
return True
2025-04-11T04:23:18.3972926Z
# Wait for any process to fail or all of them to succeed.
ready = multiprocessing.connection.wait(
self.sentinels.keys(),
timeout=timeout,
)
2025-04-11T04:23:18.3974450Z
error_index = None
for sentinel in ready:
index = self.sentinels.pop(sentinel)
process = self.processes[index]
process.join()
if process.exitcode != 0:
error_index = index
break
2025-04-11T04:23:18.3976544Z
# Return if there was no error.
if error_index is None:
# Return whether or not all processes have been joined.
return len(self.sentinels) == 0
2025-04-11T04:23:18.3977805Z
# Assume failure. Terminate processes that are still alive.
for process in self.processes:
if process.is_alive():
process.terminate()
process.join()
2025-04-11T04:23:18.3979434Z
# There won't be an error on the queue if the process crashed.
failed_process = self.processes[error_index]
if self.error_queues[error_index].empty():
exitcode = self.processes[error_index].exitcode
if exitcode < 0:
name = signal.Signals(-exitcode).name
raise ProcessExitedException(
"process %d terminated with signal %s" % (error_index, name),
error_index=error_index,
error_pid=failed_process.pid,
exit_code=exitcode,
signal_name=name,
)
else:
raise ProcessExitedException(
"process %d terminated with exit code %d" % (error_index, exitcode),
error_index=error_index,
error_pid=failed_process.pid,
exit_code=exitcode,
)
2025-04-11T04:23:18.3985307Z
original_trace = self.error_queues[error_index].get()
msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
msg += original_trace
>       raise ProcessRaisedException(msg, error_index, failed_process.pid)
E       torch.multiprocessing.spawn.ProcessRaisedException:
E
E       -- Process 0 terminated with the following error:
E       Traceback (most recent call last):
E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
E           fn(i, *args)
E         File "/__w/ColossalAI/ColossalAI/tests/test_booster/test_plugin/test_3d_plugin.py", line 271, in run_dist
E           check_3d_plugin(early_stop=early_stop)
E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
E           partial_func(**kwargs)
E         File "/__w/ColossalAI/ColossalAI/tests/test_booster/test_plugin/test_3d_plugin.py", line 104, in check_3d_plugin
E           err = run_fn(init_method, model_fn, data_gen_fn, output_transform_fn)
E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 271, in _clear_cache
E           get_accelerator().synchronize()
E         File "/__w/ColossalAI/ColossalAI/colossalai/accelerator/cuda_accelerator.py", line 62, in synchronize
E           torch.cuda.synchronize(device)
E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py", line 801, in synchronize
E           return torch._C._cuda_synchronize()
E       RuntimeError: CUDA error: out of memory
E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.3996144Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
----------------------------- Captured stdout call -----------------------------
[04/11/25 04:12:32] INFO     colossalai - colossalai - INFO:
/__w/ColossalAI/ColossalAI/colossalai/initialize.py
:75 launch
INFO     colossalai - colossalai - INFO: Distributed
environment is initialized, world size: 4
Maximum number of attempts is reached or pattern is not matched, no more retrying...
----------------------------- Captured stderr call -----------------------------
/opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
/opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
/opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
/opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
/opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:356: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:356: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:356: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:356: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:356: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
warnings.warn(
__________________________ test_dp_plugin_dataloader ___________________________
2025-04-11T04:23:18.4053850Z
args = (), kwargs = {}, try_count = 1
error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T04:23:18.4055384Z
def _run_until_success(*args, **kwargs):
try_count = 0
assert max_try is None or isinstance(
max_try, int
), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T04:23:18.4056806Z
while max_try is None or try_count < max_try:
try:
try_count += 1
ret = func(*args, **kwargs)
return ret
except exception_type as e:
error_lines = str(e).split("\n")
if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
print("Exception is caught, retrying...")
# when pattern is not specified, we always skip the exception
# when pattern is specified, we only skip when pattern is matched
continue
else:
print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
>                   raise e
2025-04-11T04:23:18.4061558Z
colossalai/testing/utils.py:144:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
colossalai/testing/utils.py:133: in _run_until_success
ret = func(*args, **kwargs)
tests/test_booster/test_plugin/test_dp_plugin_base.py:94: in test_dp_plugin_dataloader
spawn(run_dist, 2)
colossalai/testing/utils.py:252: in spawn
mp.spawn(wrapped_func, nprocs=nprocs)
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
while not context.join():
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.4066240Z
self = <torch.multiprocessing.spawn.ProcessContext object at 0x7fb83b3179a0>
timeout = None
2025-04-11T04:23:18.4066930Z
def join(self, timeout=None):
r"""Join one or more processes within spawn context.
2025-04-11T04:23:18.4067591Z
Attempt to join one or more processes in this spawn context.
If one of them exited with a non-zero exit status, this function
kills the remaining processes and raises an exception with the cause
of the first process exiting.
2025-04-11T04:23:18.4069329Z
Returns ``True`` if all processes have been joined successfully,
``False`` if there are more processes that need to be joined.
2025-04-11T04:23:18.4070257Z
Args:
timeout (float): Wait this long before giving up on waiting.
"""
# Ensure this function can be called even when we're done.
if len(self.sentinels) == 0:
return True
2025-04-11T04:23:18.4072014Z
# Wait for any process to fail or all of them to succeed.
ready = multiprocessing.connection.wait(
self.sentinels.keys(),
timeout=timeout,
)
2025-04-11T04:23:18.4073618Z
error_index = None
for sentinel in ready:
index = self.sentinels.pop(sentinel)
process = self.processes[index]
process.join()
if process.exitcode != 0:
error_index = index
break
2025-04-11T04:23:18.4075835Z
# Return if there was no error.
if error_index is None:
# Return whether or not all processes have been joined.
return len(self.sentinels) == 0
2025-04-11T04:23:18.4077163Z
# Assume failure. Terminate processes that are still alive.
for process in self.processes:
if process.is_alive():
process.terminate()
process.join()
2025-04-11T04:23:18.4078748Z
# There won't be an error on the queue if the process crashed.
failed_process = self.processes[error_index]
if self.error_queues[error_index].empty():
exitcode = self.processes[error_index].exitcode
if exitcode < 0:
name = signal.Signals(-exitcode).name
raise ProcessExitedException(
"process %d terminated with signal %s" % (error_index, name),
error_index=error_index,
error_pid=failed_process.pid,
exit_code=exitcode,
signal_name=name,
)
else:
raise ProcessExitedException(
"process %d terminated with exit code %d" % (error_index, exitcode),
error_index=error_index,
error_pid=failed_process.pid,
exit_code=exitcode,
)
2025-04-11T04:23:18.4084720Z
original_trace = self.error_queues[error_index].get()
msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
msg += original_trace
>       raise ProcessRaisedException(msg, error_index, failed_process.pid)
E       torch.multiprocessing.spawn.ProcessRaisedException:
E
E       -- Process 0 terminated with the following error:
E       Traceback (most recent call last):
E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
E           fn(i, *args)
E         File "/__w/ColossalAI/ColossalAI/tests/test_booster/test_plugin/test_dp_plugin_base.py", line 89, in run_dist
E           check_dataloader_sharding()
E         File "/__w/ColossalAI/ColossalAI/tests/test_booster/test_plugin/test_dp_plugin_base.py", line 69, in check_dataloader_sharding
E           batch = next(iter(train_dataloader))[0].cuda()
E       RuntimeError: CUDA error: out of memory
E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.4092032Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
----------------------------- Captured stdout call -----------------------------
[04/11/25 04:12:37] INFO     colossalai - colossalai - INFO:
/__w/ColossalAI/ColossalAI/colossalai/initialize.py
:75 launch
INFO     colossalai - colossalai - INFO: Distributed
environment is initialized, world size: 2
Maximum number of attempts is reached or pattern is not matched, no more retrying...
______________________________ test_gemini_plugin ______________________________
2025-04-11T04:23:18.4095710Z
args = (), kwargs = {}, try_count = 1
error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T04:23:18.4097275Z
def _run_until_success(*args, **kwargs):
try_count = 0
assert max_try is None or isinstance(
max_try, int
), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T04:23:18.4098703Z
while max_try is None or try_count < max_try:
try:
try_count += 1
ret = func(*args, **kwargs)
return ret
except exception_type as e:
error_lines = str(e).split("\n")
if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
print("Exception is caught, retrying...")
# when pattern is not specified, we always skip the exception
# when pattern is specified, we only skip when pattern is matched
continue
else:
print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
>                   raise e
2025-04-11T04:23:18.4103373Z
colossalai/testing/utils.py:144:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
colossalai/testing/utils.py:133: in _run_until_success
ret = func(*args, **kwargs)
tests/test_booster/test_plugin/test_gemini_plugin.py:172: in test_gemini_plugin
spawn(run_dist, 4, early_stop=early_stop)
colossalai/testing/utils.py:252: in spawn
mp.spawn(wrapped_func, nprocs=nprocs)
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
while not context.join():
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.4108191Z
self = <torch.multiprocessing.spawn.ProcessContext object at 0x7fb5b84917e0>
timeout = None
2025-04-11T04:23:18.4108951Z
def join(self, timeout=None):
r"""Join one or more processes within spawn context.
2025-04-11T04:23:18.4109617Z
Attempt to join one or more processes in this spawn context.
If one of them exited with a non-zero exit status, this function
kills the remaining processes and raises an exception with the cause
of the first process exiting.
2025-04-11T04:23:18.4111233Z
Returns ``True`` if all processes have been joined successfully,
``False`` if there are more processes that need to be joined.
2025-04-11T04:23:18.4112128Z
Args:
timeout (float): Wait this long before giving up on waiting.
"""
# Ensure this function can be called even when we're done.
if len(self.sentinels) == 0:
return True
2025-04-11T04:23:18.4113856Z
# Wait for any process to fail or all of them to succeed.
ready = multiprocessing.connection.wait(
self.sentinels.keys(),
timeout=timeout,
)
2025-04-11T04:23:18.4115334Z
error_index = None
for sentinel in ready:
index = self.sentinels.pop(sentinel)
process = self.processes[index]
process.join()
if process.exitcode != 0:
error_index = index
break
2025-04-11T04:23:18.4117590Z
# Return if there was no error.
if error_index is None:
# Return whether or not all processes have been joined.
return len(self.sentinels) == 0
2025-04-11T04:23:18.4118901Z
# Assume failure. Terminate processes that are still alive.
for process in self.processes:
if process.is_alive():
process.terminate()
process.join()
2025-04-11T04:23:18.4120421Z
# There won't be an error on the queue if the process crashed.
failed_process = self.processes[error_index]
if self.error_queues[error_index].empty():
exitcode = self.processes[error_index].exitcode
if exitcode < 0:
name = signal.Signals(-exitcode).name
raise ProcessExitedException(
"process %d terminated with signal %s" % (error_index, name),
error_index=error_index,
error_pid=failed_process.pid,
exit_code=exitcode,
signal_name=name,
)
else:
raise ProcessExitedException(
"process %d terminated with exit code %d" % (error_index, exitcode),
error_index=error_index,
error_pid=failed_process.pid,
exit_code=exitcode,
)
2025-04-11T04:23:18.4126286Z
original_trace = self.error_queues[error_index].get()
msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
msg += original_trace
>       raise ProcessRaisedException(msg, error_index, failed_process.pid)
E       torch.multiprocessing.spawn.ProcessRaisedException:
E
E       -- Process 0 terminated with the following error:
E       Traceback (most recent call last):
E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
E           fn(i, *args)
E         File "/__w/ColossalAI/ColossalAI/tests/test_booster/test_plugin/test_gemini_plugin.py", line 167, in run_dist
E           check_gemini_plugin(early_stop=early_stop)
E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
E           partial_func(**kwargs)
E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
E           partial_func(**kwargs)
E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
E           partial_func(**kwargs)
E         [Previous line repeated 1 more time]
E         File "/__w/ColossalAI/ColossalAI/tests/test_booster/test_plugin/test_gemini_plugin.py", line 149, in check_gemini_plugin
E           err = run_fn(init_method, model_fn, data_gen_fn, output_transform_fn, zero_size, tp_size)
E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 271, in _clear_cache
E           get_accelerator().synchronize()
E         File "/__w/ColossalAI/ColossalAI/colossalai/accelerator/cuda_accelerator.py", line 62, in synchronize
E           torch.cuda.synchronize(device)
E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py", line 801, in synchronize
E           return torch._C._cuda_synchronize()
E       RuntimeError: CUDA error: out of memory
E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.4139338Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
----------------------------- Captured stdout call -----------------------------
[04/11/25 04:12:45] INFO     colossalai - colossalai - INFO:
/__w/ColossalAI/ColossalAI/colossalai/initialize.py
:75 launch
INFO     colossalai - colossalai - INFO: Distributed
environment is initialized, world size: 4
Maximum number of attempts is reached or pattern is not matched, no more retrying...
----------------------------- Captured stderr call -----------------------------
/opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
/opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
/opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
/opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
/opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
warnings.warn(
[W socket.cpp:697] [c10d] The client socket has failed to connect to [localhost]:25212 (errno: 99 - Cannot assign requested address).
[W socket.cpp:697] [c10d] The client socket has failed to connect to [localhost]:25212 (errno: 99 - Cannot assign requested address).
[W socket.cpp:697] [c10d] The client socket has failed to connect to [localhost]:25212 (errno: 99 - Cannot assign requested address).
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:356: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:356: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:356: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:356: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:356: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:356: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:356: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
warnings.warn(
__________________________ test_low_level_zero_plugin __________________________
2025-04-11T04:23:18.4204857Z
args = (), kwargs = {}, try_count = 1
error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T04:23:18.4206392Z
def _run_until_success(*args, **kwargs):
try_count = 0
assert max_try is None or isinstance(
max_try, int
), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T04:23:18.4207956Z
while max_try is None or try_count < max_try:
try:
try_count += 1
ret = func(*args, **kwargs)
return ret
except exception_type as e:
error_lines = str(e).split("\n")
if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
print("Exception is caught, retrying...")
# when pattern is not specified, we always skip the exception
# when pattern is specified, we only skip when pattern is matched
continue
else:
print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
>                   raise e
2025-04-11T04:23:18.4212546Z
colossalai/testing/utils.py:144:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
colossalai/testing/utils.py:133: in _run_until_success
ret = func(*args, **kwargs)
tests/test_booster/test_plugin/test_low_level_zero_plugin.py:141: in test_low_level_zero_plugin
spawn(run_dist, 2, early_stop=early_stop)
colossalai/testing/utils.py:252: in spawn
mp.spawn(wrapped_func, nprocs=nprocs)
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
while not context.join():
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.4217228Z
self = <torch.multiprocessing.spawn.ProcessContext object at 0x7fb83b36d6c0>
timeout = None
2025-04-11T04:23:18.4217889Z
def join(self, timeout=None):
r"""Join one or more processes within spawn context.
2025-04-11T04:23:18.4218530Z
Attempt to join one or more processes in this spawn context.
If one of them exited with a non-zero exit status, this function
kills the remaining processes and raises an exception with the cause
of the first process exiting.
2025-04-11T04:23:18.4220095Z
Returns ``True`` if all processes have been joined successfully,
``False`` if there are more processes that need to be joined.
2025-04-11T04:23:18.4220985Z
Args:
timeout (float): Wait this long before giving up on waiting.
"""
# Ensure this function can be called even when we're done.
if len(self.sentinels) == 0:
return True
2025-04-11T04:23:18.4222647Z
# Wait for any process to fail or all of them to succeed.
ready = multiprocessing.connection.wait(
self.sentinels.keys(),
timeout=timeout,
)
2025-04-11T04:23:18.4224206Z
error_index = None
for sentinel in ready:
index = self.sentinels.pop(sentinel)
process = self.processes[index]
process.join()
if process.exitcode != 0:
error_index = index
break
2025-04-11T04:23:18.4226302Z
# Return if there was no error.
if error_index is None:
# Return whether or not all processes have been joined.
return len(self.sentinels) == 0
2025-04-11T04:23:18.4227554Z
# Assume failure. Terminate processes that are still alive.
for process in self.processes:
if process.is_alive():
process.terminate()
process.join()
2025-04-11T04:23:18.4229191Z
# There won't be an error on the queue if the process crashed.
failed_process = self.processes[error_index]
if self.error_queues[error_index].empty():
exitcode = self.processes[error_index].exitcode
if exitcode < 0:
name = signal.Signals(-exitcode).name
raise ProcessExitedException(
"process %d terminated with signal %s" % (error_index, name),
error_index=error_index,
error_pid=failed_process.pid,
exit_code=exitcode,
signal_name=name,
)
else:
raise ProcessExitedException(
"process %d terminated with exit code %d" % (error_index, exitcode),
error_index=error_index,
error_pid=failed_process.pid,
exit_code=exitcode,
)
2025-04-11T04:23:18.4234823Z
original_trace = self.error_queues[error_index].get()
msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
msg += original_trace
>       raise ProcessRaisedException(msg, error_index, failed_process.pid)
E       torch.multiprocessing.spawn.ProcessRaisedException:
E
E       -- Process 0 terminated with the following error:
E       Traceback (most recent call last):
E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
E           fn(i, *args)
E         File "/__w/ColossalAI/ColossalAI/tests/test_booster/test_plugin/test_low_level_zero_plugin.py", line 135, in run_dist
E           check_low_level_zero_plugin(early_stop=early_stop)
E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
E           partial_func(**kwargs)
E         File "/__w/ColossalAI/ColossalAI/tests/test_booster/test_plugin/test_low_level_zero_plugin.py", line 84, in check_low_level_zero_plugin
E           err = run_fn(stage, model_fn, data_gen_fn, output_transform_fn)
E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 271, in _clear_cache
E           get_accelerator().synchronize()
E         File "/__w/ColossalAI/ColossalAI/colossalai/accelerator/cuda_accelerator.py", line 62, in synchronize
E           torch.cuda.synchronize(device)
E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py", line 801, in synchronize
E           return torch._C._cuda_synchronize()
E       RuntimeError: CUDA error: out of memory
E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.4245850Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
----------------------------- Captured stdout call -----------------------------
[04/11/25 04:12:52] INFO     colossalai - colossalai - INFO:
/__w/ColossalAI/ColossalAI/colossalai/initialize.py
:75 launch
INFO     colossalai - colossalai - INFO: Distributed
environment is initialized, world size: 2
Maximum number of attempts is reached or pattern is not matched, no more retrying...
----------------------------- Captured stderr call -----------------------------
/opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
/opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
/opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:356: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:356: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
warnings.warn(
____________________________ test_torch_ddp_plugin _____________________________
2025-04-11T04:23:18.4275351Z
args = (), kwargs = {}, try_count = 1
error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T04:23:18.4276960Z
def _run_until_success(*args, **kwargs):
try_count = 0
assert max_try is None or isinstance(
max_try, int
), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T04:23:18.4278375Z
while max_try is None or try_count < max_try:
try:
try_count += 1
ret = func(*args, **kwargs)
return ret
except exception_type as e:
error_lines = str(e).split("\n")
if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
print("Exception is caught, retrying...")
# when pattern is not specified, we always skip the exception
# when pattern is specified, we only skip when pattern is matched
continue
else:
print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
>                   raise e
2025-04-11T04:23:18.4325521Z
colossalai/testing/utils.py:144:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
colossalai/testing/utils.py:133: in _run_until_success
ret = func(*args, **kwargs)
tests/test_booster/test_plugin/test_torch_ddp_plugin.py:119: in test_torch_ddp_plugin
spawn(run_dist, 2)
colossalai/testing/utils.py:252: in spawn
mp.spawn(wrapped_func, nprocs=nprocs)
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
while not context.join():
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.4330369Z
self = <torch.multiprocessing.spawn.ProcessContext object at 0x7fb5b84938e0>
timeout = None
2025-04-11T04:23:18.4331097Z
def join(self, timeout=None):
r"""Join one or more processes within spawn context.
2025-04-11T04:23:18.4331762Z
Attempt to join one or more processes in this spawn context.
If one of them exited with a non-zero exit status, this function
kills the remaining processes and raises an exception with the cause
of the first process exiting.
2025-04-11T04:23:18.4333334Z
Returns ``True`` if all processes have been joined successfully,
``False`` if there are more processes that need to be joined.
2025-04-11T04:23:18.4334438Z
Args:
timeout (float): Wait this long before giving up on waiting.
"""
# Ensure this function can be called even when we're done.
if len(self.sentinels) == 0:
return True
2025-04-11T04:23:18.4336173Z
# Wait for any process to fail or all of them to succeed.
ready = multiprocessing.connection.wait(
self.sentinels.keys(),
timeout=timeout,
)
2025-04-11T04:23:18.4337652Z
error_index = None
for sentinel in ready:
index = self.sentinels.pop(sentinel)
process = self.processes[index]
process.join()
if process.exitcode != 0:
error_index = index
break
2025-04-11T04:23:18.4339947Z
# Return if there was no error.
if error_index is None:
# Return whether or not all processes have been joined.
return len(self.sentinels) == 0
2025-04-11T04:23:18.4341212Z
# Assume failure. Terminate processes that are still alive.
for process in self.processes:
if process.is_alive():
process.terminate()
process.join()
2025-04-11T04:23:18.4342705Z
# There won't be an error on the queue if the process crashed.
failed_process = self.processes[error_index]
if self.error_queues[error_index].empty():
exitcode = self.processes[error_index].exitcode
if exitcode < 0:
name = signal.Signals(-exitcode).name
raise ProcessExitedException(
"process %d terminated with signal %s" % (error_index, name),
error_index=error_index,
error_pid=failed_process.pid,
exit_code=exitcode,
signal_name=name,
)
else:
raise ProcessExitedException(
"process %d terminated with exit code %d" % (error_index, exitcode),
error_index=error_index,
error_pid=failed_process.pid,
exit_code=exitcode,
)
2025-04-11T04:23:18.4348339Z
original_trace = self.error_queues[error_index].get()
msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
msg += original_trace
>       raise ProcessRaisedException(msg, error_index, failed_process.pid)
E       torch.multiprocessing.spawn.ProcessRaisedException:
E
E       -- Process 0 terminated with the following error:
E       Traceback (most recent call last):
E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
E           fn(i, *args)
E         File "/__w/ColossalAI/ColossalAI/tests/test_booster/test_plugin/test_torch_ddp_plugin.py", line 113, in run_dist
E           check_torch_ddp_plugin()
E         File "/__w/ColossalAI/ColossalAI/tests/test_booster/test_plugin/test_torch_ddp_plugin.py", line 52, in check_torch_ddp_plugin
E           run_fn(model_fn, data_gen_fn, output_transform_fn)
E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 271, in _clear_cache
E           get_accelerator().synchronize()
E         File "/__w/ColossalAI/ColossalAI/colossalai/accelerator/cuda_accelerator.py", line 62, in synchronize
E           torch.cuda.synchronize(device)
E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py", line 801, in synchronize
E           return torch._C._cuda_synchronize()
E       RuntimeError: CUDA error: out of memory
E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.4358417Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
----------------------------- Captured stdout call -----------------------------
[04/11/25 04:12:59] INFO     colossalai - colossalai - INFO:
/__w/ColossalAI/ColossalAI/colossalai/initialize.py
:75 launch
INFO     colossalai - colossalai - INFO: Distributed
environment is initialized, world size: 2
Maximum number of attempts is reached or pattern is not matched, no more retrying...
----------------------------- Captured stderr call -----------------------------
/opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
/opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
/opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:356: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:356: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
warnings.warn(
____________________________ test_torch_fsdp_plugin ____________________________
2025-04-11T04:23:18.4387771Z
args = (), kwargs = {}, try_count = 1
error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T04:23:18.4389336Z
def _run_until_success(*args, **kwargs):
try_count = 0
assert max_try is None or isinstance(
max_try, int
), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T04:23:18.4390783Z
while max_try is None or try_count < max_try:
try:
try_count += 1
ret = func(*args, **kwargs)
return ret
except exception_type as e:
error_lines = str(e).split("\n")
if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
print("Exception is caught, retrying...")
# when pattern is not specified, we always skip the exception
# when pattern is specified, we only skip when pattern is matched
continue
else:
print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
>                   raise e
2025-04-11T04:23:18.4395366Z
colossalai/testing/utils.py:144:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
colossalai/testing/utils.py:133: in _run_until_success
ret = func(*args, **kwargs)
tests/test_booster/test_plugin/test_torch_fsdp_plugin.py:83: in test_torch_fsdp_plugin
spawn(run_dist, 2)
colossalai/testing/utils.py:252: in spawn
mp.spawn(wrapped_func, nprocs=nprocs)
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
while not context.join():
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.4399824Z
self = <torch.multiprocessing.spawn.ProcessContext object at 0x7fb5b838ccd0>
timeout = None
2025-04-11T04:23:18.4400471Z
def join(self, timeout=None):
r"""Join one or more processes within spawn context.
2025-04-11T04:23:18.4401097Z
Attempt to join one or more processes in this spawn context.
If one of them exited with a non-zero exit status, this function
kills the remaining processes and raises an exception with the cause
of the first process exiting.
2025-04-11T04:23:18.4402746Z
Returns ``True`` if all processes have been joined successfully,
``False`` if there are more processes that need to be joined.
2025-04-11T04:23:18.4403613Z
Args:
timeout (float): Wait this long before giving up on waiting.
"""
# Ensure this function can be called even when we're done.
if len(self.sentinels) == 0:
return True
2025-04-11T04:23:18.4405247Z
# Wait for any process to fail or all of them to succeed.
ready = multiprocessing.connection.wait(
self.sentinels.keys(),
timeout=timeout,
)
2025-04-11T04:23:18.4406655Z
error_index = None
for sentinel in ready:
index = self.sentinels.pop(sentinel)
process = self.processes[index]
process.join()
if process.exitcode != 0:
error_index = index
break
2025-04-11T04:23:18.4408792Z
# Return if there was no error.
if error_index is None:
# Return whether or not all processes have been joined.
return len(self.sentinels) == 0
2025-04-11T04:23:18.4411451Z
# Assume failure. Terminate processes that are still alive.
for process in self.processes:
if process.is_alive():
process.terminate()
process.join()
2025-04-11T04:23:18.4412934Z
# There won't be an error on the queue if the process crashed.
failed_process = self.processes[error_index]
if self.error_queues[error_index].empty():
exitcode = self.processes[error_index].exitcode
if exitcode < 0:
name = signal.Signals(-exitcode).name
raise ProcessExitedException(
"process %d terminated with signal %s" % (error_index, name),
error_index=error_index,
error_pid=failed_process.pid,
exit_code=exitcode,
signal_name=name,
)
else:
raise ProcessExitedException(
"process %d terminated with exit code %d" % (error_index, exitcode),
error_index=error_index,
error_pid=failed_process.pid,
exit_code=exitcode,
)
2025-04-11T04:23:18.4418382Z
original_trace = self.error_queues[error_index].get()
msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
msg += original_trace
>       raise ProcessRaisedException(msg, error_index, failed_process.pid)
E       torch.multiprocessing.spawn.ProcessRaisedException:
E
E       -- Process 0 terminated with the following error:
E       Traceback (most recent call last):
E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
E           fn(i, *args)
E         File "/__w/ColossalAI/ColossalAI/tests/test_booster/test_plugin/test_torch_fsdp_plugin.py", line 77, in run_dist
E           check_torch_fsdp_plugin()
E         File "/__w/ColossalAI/ColossalAI/tests/test_booster/test_plugin/test_torch_fsdp_plugin.py", line 70, in check_torch_fsdp_plugin
E           run_fn(model_fn, data_gen_fn, output_transform_fn)
E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 271, in _clear_cache
E           get_accelerator().synchronize()
E         File "/__w/ColossalAI/ColossalAI/colossalai/accelerator/cuda_accelerator.py", line 62, in synchronize
E           torch.cuda.synchronize(device)
E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py", line 801, in synchronize
E           return torch._C._cuda_synchronize()
E       RuntimeError: CUDA error: out of memory
E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.4425195Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
----------------------------- Captured stdout call -----------------------------
[04/11/25 04:13:05] INFO     colossalai - colossalai - INFO:
/__w/ColossalAI/ColossalAI/colossalai/initialize.py
:75 launch
INFO     colossalai - colossalai - INFO: Distributed
environment is initialized, world size: 2
custom_hanging_param_model
custom_hanging_param_model
Maximum number of attempts is reached or pattern is not matched, no more retrying...
----------------------------- Captured stderr call -----------------------------
/opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
/opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
/opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:356: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:356: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
warnings.warn(
______________________________ test_gemini_ckpIO _______________________________
2025-04-11T04:23:18.4439118Z
args = (), kwargs = {}, try_count = 1
error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T04:23:18.4439820Z
def _run_until_success(*args, **kwargs):
try_count = 0
assert max_try is None or isinstance(
max_try, int
), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T04:23:18.4440498Z
while max_try is None or try_count < max_try:
try:
try_count += 1
ret = func(*args, **kwargs)
return ret
except exception_type as e:
error_lines = str(e).split("\n")
if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
print("Exception is caught, retrying...")
# when pattern is not specified, we always skip the exception
# when pattern is specified, we only skip when pattern is matched
continue
else:
print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
>                   raise e
2025-04-11T04:23:18.4442365Z
colossalai/testing/utils.py:144:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
colossalai/testing/utils.py:133: in _run_until_success
ret = func(*args, **kwargs)
tests/test_checkpoint_io/test_gemini_checkpoint_io.py:220: in test_gemini_ckpIO
spawn(run_dist, 4)
colossalai/testing/utils.py:252: in spawn
mp.spawn(wrapped_func, nprocs=nprocs)
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
while not context.join():
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.4444210Z
self = <torch.multiprocessing.spawn.ProcessContext object at 0x7fb5b8493940>
timeout = None
2025-04-11T04:23:18.4444581Z
def join(self, timeout=None):
r"""Join one or more processes within spawn context.
2025-04-11T04:23:18.4444877Z
Attempt to join one or more processes in this spawn context.
If one of them exited with a non-zero exit status, this function
kills the remaining processes and raises an exception with the cause
of the first process exiting.
2025-04-11T04:23:18.4445512Z
Returns ``True`` if all processes have been joined successfully,
``False`` if there are more processes that need to be joined.
2025-04-11T04:23:18.4445873Z
Args:
timeout (float): Wait this long before giving up on waiting.
"""
# Ensure this function can be called even when we're done.
if len(self.sentinels) == 0:
return True
2025-04-11T04:23:18.4446642Z
# Wait for any process to fail or all of them to succeed.
ready = multiprocessing.connection.wait(
self.sentinels.keys(),
timeout=timeout,
)
2025-04-11T04:23:18.4447216Z
error_index = None
for sentinel in ready:
index = self.sentinels.pop(sentinel)
process = self.processes[index]
process.join()
if process.exitcode != 0:
error_index = index
break
2025-04-11T04:23:18.4448016Z
# Return if there was no error.
if error_index is None:
# Return whether or not all processes have been joined.
return len(self.sentinels) == 0
2025-04-11T04:23:18.4448533Z
# Assume failure. Terminate processes that are still alive.
for process in self.processes:
if process.is_alive():
process.terminate()
process.join()
2025-04-11T04:23:18.4449118Z
# There won't be an error on the queue if the process crashed.
failed_process = self.processes[error_index]
if self.error_queues[error_index].empty():
exitcode = self.processes[error_index].exitcode
if exitcode < 0:
name = signal.Signals(-exitcode).name
raise ProcessExitedException(
"process %d terminated with signal %s" % (error_index, name),
error_index=error_index,
error_pid=failed_process.pid,
exit_code=exitcode,
signal_name=name,
)
else:
raise ProcessExitedException(
"process %d terminated with exit code %d" % (error_index, exitcode),
error_index=error_index,
error_pid=failed_process.pid,
exit_code=exitcode,
)
2025-04-11T04:23:18.4451410Z
original_trace = self.error_queues[error_index].get()
msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
msg += original_trace
>       raise ProcessRaisedException(msg, error_index, failed_process.pid)
E       torch.multiprocessing.spawn.ProcessRaisedException:
E
E       -- Process 0 terminated with the following error:
E       Traceback (most recent call last):
E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
E           fn(i, *args)
E         File "/__w/ColossalAI/ColossalAI/tests/test_checkpoint_io/test_gemini_checkpoint_io.py", line 212, in run_dist
E           exam_state_dict()
E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 271, in _clear_cache
E           get_accelerator().synchronize()
E         File "/__w/ColossalAI/ColossalAI/colossalai/accelerator/cuda_accelerator.py", line 62, in synchronize
E           torch.cuda.synchronize(device)
E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py", line 801, in synchronize
E           return torch._C._cuda_synchronize()
E       RuntimeError: CUDA error: out of memory
E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.4455062Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
----------------------------- Captured stdout call -----------------------------
[04/11/25 04:13:14] INFO     colossalai - colossalai - INFO:
/__w/ColossalAI/ColossalAI/colossalai/initialize.py
:75 launch
INFO     colossalai - colossalai - INFO: Distributed
environment is initialized, world size: 4
Maximum number of attempts is reached or pattern is not matched, no more retrying...
----------------------------- Captured stderr call -----------------------------
/opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
/opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
/opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
/opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
/opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
warnings.warn(
[W socket.cpp:697] [c10d] The client socket has failed to connect to [localhost]:55733 (errno: 99 - Cannot assign requested address).
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:356: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:356: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:356: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:356: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
warnings.warn(
Exception ignored in: <function GeminiDDP.__del__ at 0x7fc3b46f5f30>
Traceback (most recent call last):
File "/__w/ColossalAI/ColossalAI/colossalai/zero/gemini/gemini_ddp.py", line 222, in __del__
self.remove_hooks()
File "/__w/ColossalAI/ColossalAI/colossalai/zero/gemini/gemini_ddp.py", line 213, in remove_hooks
for p in self.module.parameters():
File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1688, in __getattr__
raise AttributeError(f"'{type(self).__name__}' object has no attribute '{name}'")
AttributeError: 'GeminiDDP' object has no attribute 'module'
Exception ignored in: <function GeminiDDP.__del__ at 0x7fec640edf30>
Traceback (most recent call last):
File "/__w/ColossalAI/ColossalAI/colossalai/zero/gemini/gemini_ddp.py", line 222, in __del__
self.remove_hooks()
File "/__w/ColossalAI/ColossalAI/colossalai/zero/gemini/gemini_ddp.py", line 213, in remove_hooks
for p in self.module.parameters():
File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1688, in __getattr__
raise AttributeError(f"'{type(self).__name__}' object has no attribute '{name}'")
AttributeError: 'GeminiDDP' object has no attribute 'module'
Exception ignored in: <function GeminiDDP.__del__ at 0x7fd220b15ea0>
Traceback (most recent call last):
File "/__w/ColossalAI/ColossalAI/colossalai/zero/gemini/gemini_ddp.py", line 222, in __del__
self.remove_hooks()
File "/__w/ColossalAI/ColossalAI/colossalai/zero/gemini/gemini_ddp.py", line 213, in remove_hooks
for p in self.module.parameters():
File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1688, in __getattr__
raise AttributeError(f"'{type(self).__name__}' object has no attribute '{name}'")
AttributeError: 'GeminiDDP' object has no attribute 'module'
_____________________________ test_gemini_ckpIO[2] _____________________________
2025-04-11T04:23:18.4486321Z
args = (), kwargs = {'world_size': 2}, try_count = 1
error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T04:23:18.4487063Z
def _run_until_success(*args, **kwargs):
try_count = 0
assert max_try is None or isinstance(
max_try, int
), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T04:23:18.4487749Z
while max_try is None or try_count < max_try:
try:
try_count += 1
ret = func(*args, **kwargs)
return ret
except exception_type as e:
error_lines = str(e).split("\n")
if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
print("Exception is caught, retrying...")
# when pattern is not specified, we always skip the exception
# when pattern is specified, we only skip when pattern is matched
continue
else:
print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
>                   raise e
2025-04-11T04:23:18.4489509Z
colossalai/testing/utils.py:144:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
colossalai/testing/utils.py:133: in _run_until_success
ret = func(*args, **kwargs)
tests/test_checkpoint_io/test_gemini_torch_compability.py:175: in test_gemini_ckpIO
spawn(run_dist, world_size)
colossalai/testing/utils.py:252: in spawn
mp.spawn(wrapped_func, nprocs=nprocs)
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
while not context.join():
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.4491372Z
self = <torch.multiprocessing.spawn.ProcessContext object at 0x7fb83b36d570>
timeout = None
2025-04-11T04:23:18.4491753Z
def join(self, timeout=None):
r"""Join one or more processes within spawn context.
2025-04-11T04:23:18.4492045Z
Attempt to join one or more processes in this spawn context.
If one of them exited with a non-zero exit status, this function
kills the remaining processes and raises an exception with the cause
of the first process exiting.
2025-04-11T04:23:18.4492670Z
Returns ``True`` if all processes have been joined successfully,
``False`` if there are more processes that need to be joined.
2025-04-11T04:23:18.4493032Z
Args:
timeout (float): Wait this long before giving up on waiting.
"""
# Ensure this function can be called even when we're done.
if len(self.sentinels) == 0:
return True
2025-04-11T04:23:18.4493795Z
# Wait for any process to fail or all of them to succeed.
ready = multiprocessing.connection.wait(
self.sentinels.keys(),
timeout=timeout,
)
2025-04-11T04:23:18.4494368Z
error_index = None
for sentinel in ready:
index = self.sentinels.pop(sentinel)
process = self.processes[index]
process.join()
if process.exitcode != 0:
error_index = index
break
2025-04-11T04:23:18.4495182Z
# Return if there was no error.
if error_index is None:
# Return whether or not all processes have been joined.
return len(self.sentinels) == 0
2025-04-11T04:23:18.4495684Z
# Assume failure. Terminate processes that are still alive.
for process in self.processes:
if process.is_alive():
process.terminate()
process.join()
2025-04-11T04:23:18.4496265Z
# There won't be an error on the queue if the process crashed.
failed_process = self.processes[error_index]
if self.error_queues[error_index].empty():
exitcode = self.processes[error_index].exitcode
if exitcode < 0:
name = signal.Signals(-exitcode).name
raise ProcessExitedException(
"process %d terminated with signal %s" % (error_index, name),
error_index=error_index,
error_pid=failed_process.pid,
exit_code=exitcode,
signal_name=name,
)
else:
raise ProcessExitedException(
"process %d terminated with exit code %d" % (error_index, exitcode),
error_index=error_index,
error_pid=failed_process.pid,
exit_code=exitcode,
)
2025-04-11T04:23:18.4498557Z
original_trace = self.error_queues[error_index].get()
msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
msg += original_trace
>       raise ProcessRaisedException(msg, error_index, failed_process.pid)
E       torch.multiprocessing.spawn.ProcessRaisedException:
E
E       -- Process 0 terminated with the following error:
E       Traceback (most recent call last):
E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
E           fn(i, *args)
E         File "/__w/ColossalAI/ColossalAI/tests/test_checkpoint_io/test_gemini_torch_compability.py", line 167, in run_dist
E           exam_torch_load_from_gemini()
E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 271, in _clear_cache
E           get_accelerator().synchronize()
E         File "/__w/ColossalAI/ColossalAI/colossalai/accelerator/cuda_accelerator.py", line 62, in synchronize
E           torch.cuda.synchronize(device)
E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py", line 801, in synchronize
E           return torch._C._cuda_synchronize()
E       RuntimeError: CUDA error: out of memory
E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.4502231Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
----------------------------- Captured stdout call -----------------------------
[04/11/25 04:13:21] INFO     colossalai - colossalai - INFO:
/__w/ColossalAI/ColossalAI/colossalai/initialize.py
:75 launch
INFO     colossalai - colossalai - INFO: Distributed
environment is initialized, world size: 2
Maximum number of attempts is reached or pattern is not matched, no more retrying...
----------------------------- Captured stderr call -----------------------------
/opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
/opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
/opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:356: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:356: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
warnings.warn(
Exception ignored in: <function GeminiDDP.__del__ at 0x7f4b31341750>
Traceback (most recent call last):
File "/__w/ColossalAI/ColossalAI/colossalai/zero/gemini/gemini_ddp.py", line 222, in __del__
self.remove_hooks()
File "/__w/ColossalAI/ColossalAI/colossalai/zero/gemini/gemini_ddp.py", line 213, in remove_hooks
for p in self.module.parameters():
File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1688, in __getattr__
raise AttributeError(f"'{type(self).__name__}' object has no attribute '{name}'")
AttributeError: 'GeminiDDP' object has no attribute 'module'
__________________________ test_unsharded_checkpoint ___________________________
2025-04-11T04:23:18.4517786Z
args = (), kwargs = {}
2025-04-11T04:23:18.4517875Z
def _clear_cache(*args, **kwargs):
get_accelerator().empty_cache()
get_accelerator().reset_peak_memory_stats()
get_accelerator().reset_max_memory_allocated()
get_accelerator().reset_max_memory_cached()
>       get_accelerator().synchronize()
2025-04-11T04:23:18.4518506Z
colossalai/testing/utils.py:271:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
colossalai/accelerator/cuda_accelerator.py:62: in synchronize
torch.cuda.synchronize(device)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.4519086Z
device = None
2025-04-11T04:23:18.4519173Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.4519540Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.4521227Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
___________________ test_sharded_model_checkpoint[True-True] ___________________
2025-04-11T04:23:18.4521654Z
use_safetensors = True, use_async = True
2025-04-11T04:23:18.4521758Z
@pytest.mark.parametrize("use_safetensors", [True, False])
@pytest.mark.parametrize("use_async", [False, True])
def test_sharded_model_checkpoint(use_safetensors: bool, use_async: bool):
# create a model and optimizer
model = resnet18()
optimizer = Adam(model.parameters(), lr=0.001)
# create test data sample
x = torch.randn(1, 3, 224, 224)
2025-04-11T04:23:18.4522774Z
# run fwd and bwd
y = model(x)
loss = y.sum()
loss.backward()
optimizer.step()
2025-04-11T04:23:18.4523350Z
model_ckpt_dir = tempfile.TemporaryDirectory()
optimizer_ckpt_tempfile = tempfile.NamedTemporaryFile()
2025-04-11T04:23:18.4523696Z
# save the model and optimizer
ckpt_io = GeneralCheckpointIO()
2025-04-11T04:23:18.4523953Z
>       ckpt_io.save_model(
model, model_ckpt_dir.name, True, True, "", 10, use_safetensors=use_safetensors, use_async=use_async
)
2025-04-11T04:23:18.4524357Z
tests/test_checkpoint_io/test_general_checkpoint_io.py:96:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
colossalai/checkpoint_io/checkpoint_io_base.py:190: in save_model
self.save_sharded_model(
colossalai/checkpoint_io/general_checkpoint_io.py:238: in save_sharded_model
total_size, new_pinned_state_dict, writers = async_move_save_state_dict_shards(
colossalai/checkpoint_io/utils.py:390: in async_move_save_state_dict_shards
sub_pinned_state_dict = create_pinned_state_dict(state_dict)
colossalai/checkpoint_io/utils.py:977: in create_pinned_state_dict
return tree_map(lambda x: _pin_tensor(x, empty=empty) if isinstance(x, torch.Tensor) else x, state_dict)
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/utils/_pytree.py:602: in tree_map
return tree_unflatten([func(i) for i in flat_args], spec)
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/utils/_pytree.py:602: in <listcomp>
return tree_unflatten([func(i) for i in flat_args], spec)
colossalai/checkpoint_io/utils.py:977: in <lambda>
return tree_map(lambda x: _pin_tensor(x, empty=empty) if isinstance(x, torch.Tensor) else x, state_dict)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.4527212Z
tensor = tensor([[[[-7.9533e-03,  2.4479e-03,  2.2101e-02,  ...,  2.2613e-02,
-7.4241e-03,  4.8188e-02],
[...594e-02],
[-3.3366e-02, -1.8918e-02, -3.1868e-02,  ..., -2.9370e-02,
-1.6090e-02, -4.3804e-02]]]])
empty = True
2025-04-11T04:23:18.4527948Z
def _pin_tensor(tensor: torch.Tensor, empty: bool = True) -> torch.Tensor:
if empty:
>           return torch.empty_like(tensor, pin_memory=True, device="cpu")
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.4529078Z
colossalai/checkpoint_io/utils.py:967: RuntimeError
__________________ test_sharded_model_checkpoint[True-False] ___________________
2025-04-11T04:23:18.4529365Z
use_safetensors = False, use_async = True
2025-04-11T04:23:18.4529561Z
@pytest.mark.parametrize("use_safetensors", [True, False])
@pytest.mark.parametrize("use_async", [False, True])
def test_sharded_model_checkpoint(use_safetensors: bool, use_async: bool):
# create a model and optimizer
model = resnet18()
optimizer = Adam(model.parameters(), lr=0.001)
# create test data sample
x = torch.randn(1, 3, 224, 224)
2025-04-11T04:23:18.4530562Z
# run fwd and bwd
y = model(x)
loss = y.sum()
loss.backward()
optimizer.step()
2025-04-11T04:23:18.4531057Z
model_ckpt_dir = tempfile.TemporaryDirectory()
optimizer_ckpt_tempfile = tempfile.NamedTemporaryFile()
2025-04-11T04:23:18.4531400Z
# save the model and optimizer
ckpt_io = GeneralCheckpointIO()
2025-04-11T04:23:18.4531667Z
>       ckpt_io.save_model(
model, model_ckpt_dir.name, True, True, "", 10, use_safetensors=use_safetensors, use_async=use_async
)
2025-04-11T04:23:18.4532052Z
tests/test_checkpoint_io/test_general_checkpoint_io.py:96:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
colossalai/checkpoint_io/checkpoint_io_base.py:190: in save_model
self.save_sharded_model(
colossalai/checkpoint_io/general_checkpoint_io.py:238: in save_sharded_model
total_size, new_pinned_state_dict, writers = async_move_save_state_dict_shards(
colossalai/checkpoint_io/utils.py:390: in async_move_save_state_dict_shards
sub_pinned_state_dict = create_pinned_state_dict(state_dict)
colossalai/checkpoint_io/utils.py:977: in create_pinned_state_dict
return tree_map(lambda x: _pin_tensor(x, empty=empty) if isinstance(x, torch.Tensor) else x, state_dict)
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/utils/_pytree.py:602: in tree_map
return tree_unflatten([func(i) for i in flat_args], spec)
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/utils/_pytree.py:602: in <listcomp>
return tree_unflatten([func(i) for i in flat_args], spec)
colossalai/checkpoint_io/utils.py:977: in <lambda>
return tree_map(lambda x: _pin_tensor(x, empty=empty) if isinstance(x, torch.Tensor) else x, state_dict)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.4534983Z
tensor = tensor([[[[ 1.0525e-02,  3.3759e-02, -3.8922e-03,  ...,  1.3324e-02,
-1.5804e-03, -4.7921e-03],
[...171e-03],
[-3.2414e-03, -5.0440e-02,  3.3628e-02,  ..., -8.8520e-03,
4.6587e-03,  4.6623e-02]]]])
empty = True
2025-04-11T04:23:18.4535610Z
def _pin_tensor(tensor: torch.Tensor, empty: bool = True) -> torch.Tensor:
if empty:
>           return torch.empty_like(tensor, pin_memory=True, device="cpu")
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.4536805Z
colossalai/checkpoint_io/utils.py:967: RuntimeError
----------------------------- Captured stdout call -----------------------------
[04/11/25 04:13:24] WARNING  colossalai - colossalai - WARNING:
/__w/ColossalAI/ColossalAI/colossalai/checkpoint_io
/checkpoint_io_base.py:183 save_model
WARNING  colossalai - colossalai - WARNING: Async save is
only supported when use_safetensors is set to True.
Setting use_safetensors to True for async save.
___________________ test_sharded_optimizer_checkpoint[False] ___________________
2025-04-11T04:23:18.4538063Z
use_async = False
2025-04-11T04:23:18.4538145Z
@pytest.mark.parametrize("use_async", [False, True])
def test_sharded_optimizer_checkpoint(use_async: bool):
# create a model and optimizer
model = resnet18()
optimizer = Adam(model.parameters(), lr=0.001)
2025-04-11T04:23:18.4538792Z
# create test data sample
x = torch.randn(1, 3, 224, 224)
2025-04-11T04:23:18.4539046Z
# run fwd and bwd
y = model(x)
loss = y.sum()
loss.backward()
optimizer.step()
2025-04-11T04:23:18.4539526Z
# create temp directories for checkpoint
model_ckpt_dir = tempfile.TemporaryDirectory()
optimizer_ckpt_dir = tempfile.TemporaryDirectory()
2025-04-11T04:23:18.4539964Z
# save the model and optimizer
ckpt_io = GeneralCheckpointIO()
2025-04-11T04:23:18.4540328Z
ckpt_io.save_model(model, model_ckpt_dir.name, True, True, "", 10, use_safetensors=False)
ckpt_io.save_optimizer(optimizer, optimizer_ckpt_dir.name, shard=True, size_per_shard=10, use_async=use_async)
2025-04-11T04:23:18.4540886Z
ckpt_io._sync_d2h()
ckpt_io._sync_io()
2025-04-11T04:23:18.4541131Z
# create new model
new_model = resnet18()
new_optimizer = Adam(new_model.parameters(), lr=0.001)
2025-04-11T04:23:18.4541526Z
ckpt_io.load_model(new_model, str(model_ckpt_dir.name), strict=True)
>       ckpt_io.load_optimizer(new_optimizer, str(optimizer_ckpt_dir.name))
2025-04-11T04:23:18.4541845Z
tests/test_checkpoint_io/test_general_checkpoint_io.py:149:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
colossalai/checkpoint_io/checkpoint_io_base.py:224: in load_optimizer
self.load_sharded_optimizer(
colossalai/checkpoint_io/general_checkpoint_io.py:100: in load_sharded_optimizer
load_states_into_optimizer(optimizer, state_dict, id_map)
colossalai/checkpoint_io/utils.py:830: in load_states_into_optimizer
get_accelerator().synchronize()
colossalai/accelerator/cuda_accelerator.py:62: in synchronize
torch.cuda.synchronize(device)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.4543447Z
device = None
2025-04-11T04:23:18.4543534Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.4543884Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.4545472Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
___________________ test_sharded_optimizer_checkpoint[True] ____________________
2025-04-11T04:23:18.4545880Z
use_async = True
2025-04-11T04:23:18.4545963Z
@pytest.mark.parametrize("use_async", [False, True])
def test_sharded_optimizer_checkpoint(use_async: bool):
# create a model and optimizer
model = resnet18()
optimizer = Adam(model.parameters(), lr=0.001)
2025-04-11T04:23:18.4546689Z
# create test data sample
x = torch.randn(1, 3, 224, 224)
2025-04-11T04:23:18.4546945Z
# run fwd and bwd
y = model(x)
loss = y.sum()
loss.backward()
optimizer.step()
2025-04-11T04:23:18.4547420Z
# create temp directories for checkpoint
model_ckpt_dir = tempfile.TemporaryDirectory()
optimizer_ckpt_dir = tempfile.TemporaryDirectory()
2025-04-11T04:23:18.4547845Z
# save the model and optimizer
ckpt_io = GeneralCheckpointIO()
2025-04-11T04:23:18.4548107Z
ckpt_io.save_model(model, model_ckpt_dir.name, True, True, "", 10, use_safetensors=False)
>       ckpt_io.save_optimizer(optimizer, optimizer_ckpt_dir.name, shard=True, size_per_shard=10, use_async=use_async)
2025-04-11T04:23:18.4548717Z
tests/test_checkpoint_io/test_general_checkpoint_io.py:139:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
colossalai/checkpoint_io/checkpoint_io_base.py:258: in save_optimizer
self.save_sharded_optimizer(
colossalai/checkpoint_io/general_checkpoint_io.py:144: in save_sharded_optimizer
total_size, new_pinned_state_dict, writers = async_move_save_state_dict_shards(
colossalai/checkpoint_io/utils.py:390: in async_move_save_state_dict_shards
sub_pinned_state_dict = create_pinned_state_dict(state_dict)
colossalai/checkpoint_io/utils.py:977: in create_pinned_state_dict
return tree_map(lambda x: _pin_tensor(x, empty=empty) if isinstance(x, torch.Tensor) else x, state_dict)
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/utils/_pytree.py:602: in tree_map
return tree_unflatten([func(i) for i in flat_args], spec)
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/utils/_pytree.py:602: in <listcomp>
return tree_unflatten([func(i) for i in flat_args], spec)
colossalai/checkpoint_io/utils.py:977: in <lambda>
return tree_map(lambda x: _pin_tensor(x, empty=empty) if isinstance(x, torch.Tensor) else x, state_dict)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.4551621Z
tensor = tensor(1.), empty = True
2025-04-11T04:23:18.4551718Z
def _pin_tensor(tensor: torch.Tensor, empty: bool = True) -> torch.Tensor:
if empty:
>           return torch.empty_like(tensor, pin_memory=True, device="cpu")
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.4552849Z
colossalai/checkpoint_io/utils.py:967: RuntimeError
_____________ test_sharded_optimizer_multiple_param_groups[False] ______________
2025-04-11T04:23:18.4553240Z
use_async = False
2025-04-11T04:23:18.4553329Z
@pytest.mark.parametrize("use_async", [False, True])
def test_sharded_optimizer_multiple_param_groups(use_async: bool):
# create a model and optimizer
model = resnet18()
optimizer = Adam(
[{"params": model.layer1.parameters()}, {"params": model.layer2.parameters(), "lr": 0.002}], lr=0.001
)
2025-04-11T04:23:18.4554282Z
# create test data sample
x = torch.randn(1, 3, 224, 224)
2025-04-11T04:23:18.4554532Z
# run fwd and bwd
y = model(x)
loss = y.sum()
loss.backward()
optimizer.step()
2025-04-11T04:23:18.4555030Z
# create temp directories for checkpoint
model_ckpt_dir = tempfile.TemporaryDirectory()
optimizer_ckpt_dir = tempfile.TemporaryDirectory()
2025-04-11T04:23:18.4555552Z
# save the model and optimizer
ckpt_io = GeneralCheckpointIO()
2025-04-11T04:23:18.4555816Z
ckpt_io.save_model(model, model_ckpt_dir.name, True, True, "", 10, use_safetensors=False)
ckpt_io.save_optimizer(optimizer, optimizer_ckpt_dir.name, shard=True, size_per_shard=10, use_async=use_async)
2025-04-11T04:23:18.4556352Z
ckpt_io._sync_d2h()
ckpt_io._sync_io()
2025-04-11T04:23:18.4556596Z
# create new model
new_model = resnet18()
new_optimizer = Adam(
[{"params": new_model.layer1.parameters()}, {"params": new_model.layer2.parameters(), "lr": 0.002}], lr=0.001
)
2025-04-11T04:23:18.4557255Z
ckpt_io.load_model(new_model, str(model_ckpt_dir.name), strict=True)
>       ckpt_io.load_optimizer(new_optimizer, str(optimizer_ckpt_dir.name))
2025-04-11T04:23:18.4557578Z
tests/test_checkpoint_io/test_general_checkpoint_io.py:222:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
colossalai/checkpoint_io/checkpoint_io_base.py:224: in load_optimizer
self.load_sharded_optimizer(
colossalai/checkpoint_io/general_checkpoint_io.py:100: in load_sharded_optimizer
load_states_into_optimizer(optimizer, state_dict, id_map)
colossalai/checkpoint_io/utils.py:830: in load_states_into_optimizer
get_accelerator().synchronize()
colossalai/accelerator/cuda_accelerator.py:62: in synchronize
torch.cuda.synchronize(device)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.4559071Z
device = None
2025-04-11T04:23:18.4559162Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.4559522Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.4561233Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
______________ test_sharded_optimizer_multiple_param_groups[True] ______________
2025-04-11T04:23:18.4561736Z
use_async = True
2025-04-11T04:23:18.4561822Z
@pytest.mark.parametrize("use_async", [False, True])
def test_sharded_optimizer_multiple_param_groups(use_async: bool):
# create a model and optimizer
model = resnet18()
optimizer = Adam(
[{"params": model.layer1.parameters()}, {"params": model.layer2.parameters(), "lr": 0.002}], lr=0.001
)
2025-04-11T04:23:18.4562748Z
# create test data sample
x = torch.randn(1, 3, 224, 224)
2025-04-11T04:23:18.4563001Z
# run fwd and bwd
y = model(x)
loss = y.sum()
loss.backward()
optimizer.step()
2025-04-11T04:23:18.4563485Z
# create temp directories for checkpoint
model_ckpt_dir = tempfile.TemporaryDirectory()
optimizer_ckpt_dir = tempfile.TemporaryDirectory()
2025-04-11T04:23:18.4563927Z
# save the model and optimizer
ckpt_io = GeneralCheckpointIO()
2025-04-11T04:23:18.4564185Z
ckpt_io.save_model(model, model_ckpt_dir.name, True, True, "", 10, use_safetensors=False)
>       ckpt_io.save_optimizer(optimizer, optimizer_ckpt_dir.name, shard=True, size_per_shard=10, use_async=use_async)
2025-04-11T04:23:18.4564650Z
tests/test_checkpoint_io/test_general_checkpoint_io.py:210:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
colossalai/checkpoint_io/checkpoint_io_base.py:258: in save_optimizer
self.save_sharded_optimizer(
colossalai/checkpoint_io/general_checkpoint_io.py:144: in save_sharded_optimizer
total_size, new_pinned_state_dict, writers = async_move_save_state_dict_shards(
colossalai/checkpoint_io/utils.py:390: in async_move_save_state_dict_shards
sub_pinned_state_dict = create_pinned_state_dict(state_dict)
colossalai/checkpoint_io/utils.py:977: in create_pinned_state_dict
return tree_map(lambda x: _pin_tensor(x, empty=empty) if isinstance(x, torch.Tensor) else x, state_dict)
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/utils/_pytree.py:602: in tree_map
return tree_unflatten([func(i) for i in flat_args], spec)
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/utils/_pytree.py:602: in <listcomp>
return tree_unflatten([func(i) for i in flat_args], spec)
colossalai/checkpoint_io/utils.py:977: in <lambda>
return tree_map(lambda x: _pin_tensor(x, empty=empty) if isinstance(x, torch.Tensor) else x, state_dict)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.4567599Z
tensor = tensor(1.), empty = True
2025-04-11T04:23:18.4567695Z
def _pin_tensor(tensor: torch.Tensor, empty: bool = True) -> torch.Tensor:
if empty:
>           return torch.empty_like(tensor, pin_memory=True, device="cpu")
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.4568905Z
colossalai/checkpoint_io/utils.py:967: RuntimeError
_____________________________ test_hybrid_ckpIO[4] _____________________________
2025-04-11T04:23:18.4569166Z
args = (), kwargs = {'world_size': 4}, try_count = 1
error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T04:23:18.4569910Z
def _run_until_success(*args, **kwargs):
try_count = 0
assert max_try is None or isinstance(
max_try, int
), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T04:23:18.4570513Z
while max_try is None or try_count < max_try:
try:
try_count += 1
ret = func(*args, **kwargs)
return ret
except exception_type as e:
error_lines = str(e).split("\n")
if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
print("Exception is caught, retrying...")
# when pattern is not specified, we always skip the exception
# when pattern is specified, we only skip when pattern is matched
continue
else:
print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
>                   raise e
2025-04-11T04:23:18.4572243Z
colossalai/testing/utils.py:144:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
colossalai/testing/utils.py:133: in _run_until_success
ret = func(*args, **kwargs)
tests/test_checkpoint_io/test_hybrid_parallel_plugin_checkpoint_io.py:155: in test_hybrid_ckpIO
spawn(run_dist, world_size)
colossalai/testing/utils.py:252: in spawn
mp.spawn(wrapped_func, nprocs=nprocs)
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
while not context.join():
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.4574226Z
self = <torch.multiprocessing.spawn.ProcessContext object at 0x7fb5b840d6f0>
timeout = None
2025-04-11T04:23:18.4574593Z
def join(self, timeout=None):
r"""Join one or more processes within spawn context.
2025-04-11T04:23:18.4574894Z
Attempt to join one or more processes in this spawn context.
If one of them exited with a non-zero exit status, this function
kills the remaining processes and raises an exception with the cause
of the first process exiting.
2025-04-11T04:23:18.4575516Z
Returns ``True`` if all processes have been joined successfully,
``False`` if there are more processes that need to be joined.
2025-04-11T04:23:18.4575876Z
Args:
timeout (float): Wait this long before giving up on waiting.
"""
# Ensure this function can be called even when we're done.
if len(self.sentinels) == 0:
return True
2025-04-11T04:23:18.4576546Z
# Wait for any process to fail or all of them to succeed.
ready = multiprocessing.connection.wait(
self.sentinels.keys(),
timeout=timeout,
)
2025-04-11T04:23:18.4577113Z
error_index = None
for sentinel in ready:
index = self.sentinels.pop(sentinel)
process = self.processes[index]
process.join()
if process.exitcode != 0:
error_index = index
break
2025-04-11T04:23:18.4577932Z
# Return if there was no error.
if error_index is None:
# Return whether or not all processes have been joined.
return len(self.sentinels) == 0
2025-04-11T04:23:18.4578418Z
# Assume failure. Terminate processes that are still alive.
for process in self.processes:
if process.is_alive():
process.terminate()
process.join()
2025-04-11T04:23:18.4579092Z
# There won't be an error on the queue if the process crashed.
failed_process = self.processes[error_index]
if self.error_queues[error_index].empty():
exitcode = self.processes[error_index].exitcode
if exitcode < 0:
name = signal.Signals(-exitcode).name
raise ProcessExitedException(
"process %d terminated with signal %s" % (error_index, name),
error_index=error_index,
error_pid=failed_process.pid,
exit_code=exitcode,
signal_name=name,
)
else:
raise ProcessExitedException(
"process %d terminated with exit code %d" % (error_index, exitcode),
error_index=error_index,
error_pid=failed_process.pid,
exit_code=exitcode,
)
2025-04-11T04:23:18.4581357Z
original_trace = self.error_queues[error_index].get()
msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
msg += original_trace
>       raise ProcessRaisedException(msg, error_index, failed_process.pid)
E       torch.multiprocessing.spawn.ProcessRaisedException:
E
E       -- Process 0 terminated with the following error:
E       Traceback (most recent call last):
E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
E           fn(i, *args)
E         File "/__w/ColossalAI/ColossalAI/tests/test_checkpoint_io/test_hybrid_parallel_plugin_checkpoint_io.py", line 148, in run_dist
E           exam_state_dict()
E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
E           partial_func(**kwargs)
E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
E           partial_func(**kwargs)
E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
E           partial_func(**kwargs)
E         [Previous line repeated 3 more times]
E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 271, in _clear_cache
E           get_accelerator().synchronize()
E         File "/__w/ColossalAI/ColossalAI/colossalai/accelerator/cuda_accelerator.py", line 62, in synchronize
E           torch.cuda.synchronize(device)
E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py", line 801, in synchronize
E           return torch._C._cuda_synchronize()
E       RuntimeError: CUDA error: out of memory
E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.4586195Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
----------------------------- Captured stdout call -----------------------------
[04/11/25 04:13:35] INFO     colossalai - colossalai - INFO:
/__w/ColossalAI/ColossalAI/colossalai/initialize.py
:75 launch
INFO     colossalai - colossalai - INFO: Distributed
environment is initialized, world size: 4
[04/11/25 04:13:35] WARNING  colossalai - colossalai.shardformer.modeling.llama
- WARNING: `use_cache=True` is incompatible with
pipeline parallelism. Setting `use_cache=False`...
Maximum number of attempts is reached or pattern is not matched, no more retrying...
----------------------------- Captured stderr call -----------------------------
/opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
/opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
/opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
/opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
/opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
warnings.warn(
[W socket.cpp:697] [c10d] The client socket has failed to connect to [localhost]:57270 (errno: 99 - Cannot assign requested address).
[W socket.cpp:697] [c10d] The client socket has failed to connect to [localhost]:57270 (errno: 99 - Cannot assign requested address).
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:356: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:356: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:356: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:356: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
warnings.warn(
_______________________ test_low_level_zero_checkpointIO _______________________
2025-04-11T04:23:18.4613439Z
args = (), kwargs = {}, try_count = 1
2025-04-11T04:23:18.4613540Z
def _run_until_success(*args, **kwargs):
try_count = 0
assert max_try is None or isinstance(
max_try, int
), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T04:23:18.4614128Z
while max_try is None or try_count < max_try:
try:
try_count += 1
>               ret = func(*args, **kwargs)
2025-04-11T04:23:18.4614695Z
colossalai/testing/utils.py:133:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
colossalai/testing/utils.py:271: in _clear_cache
get_accelerator().synchronize()
colossalai/accelerator/cuda_accelerator.py:62: in synchronize
torch.cuda.synchronize(device)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.4615521Z
device = None
2025-04-11T04:23:18.4615603Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.4615969Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.4617595Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
______________________ test_huggingface_compatibility[2] _______________________
2025-04-11T04:23:18.4617995Z
args = (), kwargs = {'world_size': 2}, try_count = 1
error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T04:23:18.4618725Z
def _run_until_success(*args, **kwargs):
try_count = 0
assert max_try is None or isinstance(
max_try, int
), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T04:23:18.4619401Z
while max_try is None or try_count < max_try:
try:
try_count += 1
ret = func(*args, **kwargs)
return ret
except exception_type as e:
error_lines = str(e).split("\n")
if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
print("Exception is caught, retrying...")
# when pattern is not specified, we always skip the exception
# when pattern is specified, we only skip when pattern is matched
continue
else:
print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
>                   raise e
2025-04-11T04:23:18.4621214Z
colossalai/testing/utils.py:144:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
colossalai/testing/utils.py:133: in _run_until_success
ret = func(*args, **kwargs)
tests/test_checkpoint_io/test_plugins_huggingface_compatibility.py:79: in test_huggingface_compatibility
spawn(run_dist, world_size)
colossalai/testing/utils.py:252: in spawn
mp.spawn(wrapped_func, nprocs=nprocs)
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
while not context.join():
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.4623145Z
self = <torch.multiprocessing.spawn.ProcessContext object at 0x7fb5b840ea10>
timeout = None
2025-04-11T04:23:18.4623433Z
def join(self, timeout=None):
r"""Join one or more processes within spawn context.
2025-04-11T04:23:18.4623725Z
Attempt to join one or more processes in this spawn context.
If one of them exited with a non-zero exit status, this function
kills the remaining processes and raises an exception with the cause
of the first process exiting.
2025-04-11T04:23:18.4624354Z
Returns ``True`` if all processes have been joined successfully,
``False`` if there are more processes that need to be joined.
2025-04-11T04:23:18.4624719Z
Args:
timeout (float): Wait this long before giving up on waiting.
"""
# Ensure this function can be called even when we're done.
if len(self.sentinels) == 0:
return True
2025-04-11T04:23:18.4625511Z
# Wait for any process to fail or all of them to succeed.
ready = multiprocessing.connection.wait(
self.sentinels.keys(),
timeout=timeout,
)
2025-04-11T04:23:18.4626095Z
error_index = None
for sentinel in ready:
index = self.sentinels.pop(sentinel)
process = self.processes[index]
process.join()
if process.exitcode != 0:
error_index = index
break
2025-04-11T04:23:18.4626903Z
# Return if there was no error.
if error_index is None:
# Return whether or not all processes have been joined.
return len(self.sentinels) == 0
2025-04-11T04:23:18.4627500Z
# Assume failure. Terminate processes that are still alive.
for process in self.processes:
if process.is_alive():
process.terminate()
process.join()
2025-04-11T04:23:18.4628104Z
# There won't be an error on the queue if the process crashed.
failed_process = self.processes[error_index]
if self.error_queues[error_index].empty():
exitcode = self.processes[error_index].exitcode
if exitcode < 0:
name = signal.Signals(-exitcode).name
raise ProcessExitedException(
"process %d terminated with signal %s" % (error_index, name),
error_index=error_index,
error_pid=failed_process.pid,
exit_code=exitcode,
signal_name=name,
)
else:
raise ProcessExitedException(
"process %d terminated with exit code %d" % (error_index, exitcode),
error_index=error_index,
error_pid=failed_process.pid,
exit_code=exitcode,
)
2025-04-11T04:23:18.4630429Z
original_trace = self.error_queues[error_index].get()
msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
msg += original_trace
>       raise ProcessRaisedException(msg, error_index, failed_process.pid)
E       torch.multiprocessing.spawn.ProcessRaisedException:
E
E       -- Process 0 terminated with the following error:
E       Traceback (most recent call last):
E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
E           fn(i, *args)
E         File "/__w/ColossalAI/ColossalAI/tests/test_checkpoint_io/test_plugins_huggingface_compatibility.py", line 72, in run_dist
E           exam_from_pretrained()
E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 271, in _clear_cache
E           get_accelerator().synchronize()
E         File "/__w/ColossalAI/ColossalAI/colossalai/accelerator/cuda_accelerator.py", line 62, in synchronize
E           torch.cuda.synchronize(device)
E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py", line 801, in synchronize
E           return torch._C._cuda_synchronize()
E       RuntimeError: CUDA error: out of memory
E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.4634340Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
----------------------------- Captured stdout call -----------------------------
[04/11/25 04:13:43] INFO     colossalai - colossalai - INFO:
/__w/ColossalAI/ColossalAI/colossalai/initialize.py
:75 launch
INFO     colossalai - colossalai - INFO: Distributed
environment is initialized, world size: 2
Maximum number of attempts is reached or pattern is not matched, no more retrying...
----------------------------- Captured stderr call -----------------------------
/opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
/opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
/opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:356: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:356: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
warnings.warn(
___________________________ test_create_pin[1-True] ____________________________
2025-04-11T04:23:18.4648235Z
empty = True, num_threads = 1
2025-04-11T04:23:18.4648338Z
@pytest.mark.parametrize("empty", [True, False])
@pytest.mark.parametrize("num_threads", [1, 4])
def test_create_pin(empty: bool, num_threads: int):
model_state_dict = gen_model_state_dict()
>       model_state_dict_pinned = create_pinned_state_dict(model_state_dict, empty=empty, num_threads=num_threads)
2025-04-11T04:23:18.4649189Z
tests/test_checkpoint_io/test_safetensors_async_io.py:120:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
colossalai/checkpoint_io/utils.py:977: in create_pinned_state_dict
return tree_map(lambda x: _pin_tensor(x, empty=empty) if isinstance(x, torch.Tensor) else x, state_dict)
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/utils/_pytree.py:602: in tree_map
return tree_unflatten([func(i) for i in flat_args], spec)
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/utils/_pytree.py:602: in <listcomp>
return tree_unflatten([func(i) for i in flat_args], spec)
colossalai/checkpoint_io/utils.py:977: in <lambda>
return tree_map(lambda x: _pin_tensor(x, empty=empty) if isinstance(x, torch.Tensor) else x, state_dict)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.4651225Z
tensor = tensor([[8.6847e-01, 8.1748e-04, 4.0391e-02,  ..., 7.9576e-01, 7.4807e-01,
1.9106e-01],
[8.6654e-02, ...         9.2385e-01],
[3.8017e-01, 2.4513e-01, 9.6436e-01,  ..., 3.2863e-01, 2.9029e-01,
5.3874e-01]])
empty = True
2025-04-11T04:23:18.4651904Z
def _pin_tensor(tensor: torch.Tensor, empty: bool = True) -> torch.Tensor:
if empty:
>           return torch.empty_like(tensor, pin_memory=True, device="cpu")
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.4653061Z
colossalai/checkpoint_io/utils.py:967: RuntimeError
___________________________ test_create_pin[1-False] ___________________________
2025-04-11T04:23:18.4653341Z
empty = False, num_threads = 1
2025-04-11T04:23:18.4653446Z
@pytest.mark.parametrize("empty", [True, False])
@pytest.mark.parametrize("num_threads", [1, 4])
def test_create_pin(empty: bool, num_threads: int):
model_state_dict = gen_model_state_dict()
>       model_state_dict_pinned = create_pinned_state_dict(model_state_dict, empty=empty, num_threads=num_threads)
2025-04-11T04:23:18.4654186Z
tests/test_checkpoint_io/test_safetensors_async_io.py:120:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
colossalai/checkpoint_io/utils.py:977: in create_pinned_state_dict
return tree_map(lambda x: _pin_tensor(x, empty=empty) if isinstance(x, torch.Tensor) else x, state_dict)
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/utils/_pytree.py:602: in tree_map
return tree_unflatten([func(i) for i in flat_args], spec)
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/utils/_pytree.py:602: in <listcomp>
return tree_unflatten([func(i) for i in flat_args], spec)
colossalai/checkpoint_io/utils.py:977: in <lambda>
return tree_map(lambda x: _pin_tensor(x, empty=empty) if isinstance(x, torch.Tensor) else x, state_dict)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.4656175Z
tensor = tensor([[0.3194, 0.1471, 0.8299,  ..., 0.0654, 0.6019, 0.1725],
[0.6552, 0.0854, 0.7144,  ..., 0.4938, 0.4092,...0.2009, 0.3128, 0.3861,  ..., 0.5467, 0.1594, 0.9723],
[0.6342, 0.6712, 0.4807,  ..., 0.1087, 0.7685, 0.4644]])
empty = False
2025-04-11T04:23:18.4656697Z
def _pin_tensor(tensor: torch.Tensor, empty: bool = True) -> torch.Tensor:
if empty:
return torch.empty_like(tensor, pin_memory=True, device="cpu")
>       return tensor.pin_memory()
E       RuntimeError: CUDA error: out of memory
E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.4658032Z
colossalai/checkpoint_io/utils.py:968: RuntimeError
___________________________ test_create_pin[4-True] ____________________________
2025-04-11T04:23:18.4658305Z
empty = True, num_threads = 4
2025-04-11T04:23:18.4658400Z
@pytest.mark.parametrize("empty", [True, False])
@pytest.mark.parametrize("num_threads", [1, 4])
def test_create_pin(empty: bool, num_threads: int):
model_state_dict = gen_model_state_dict()
>       model_state_dict_pinned = create_pinned_state_dict(model_state_dict, empty=empty, num_threads=num_threads)
2025-04-11T04:23:18.4659127Z
tests/test_checkpoint_io/test_safetensors_async_io.py:120:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
colossalai/checkpoint_io/utils.py:987: in create_pinned_state_dict
elems[idx] = future.result()
/opt/conda/envs/pytorch/lib/python3.10/concurrent/futures/_base.py:451: in result
return self.__get_result()
/opt/conda/envs/pytorch/lib/python3.10/concurrent/futures/_base.py:403: in __get_result
raise self._exception
/opt/conda/envs/pytorch/lib/python3.10/concurrent/futures/thread.py:58: in run
result = self.fn(*self.args, **self.kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.4660663Z
tensor = tensor([[0.5821, 0.7269, 0.9384,  ..., 0.4998, 0.7969, 0.4132],
[0.1225, 0.3994, 0.1804,  ..., 0.1278, 0.2117,...0.5724, 0.2939, 0.7177,  ..., 0.9228, 0.0104, 0.5268],
[0.9206, 0.8477, 0.4199,  ..., 0.2250, 0.8432, 0.0800]])
empty = True
2025-04-11T04:23:18.4661193Z
def _pin_tensor(tensor: torch.Tensor, empty: bool = True) -> torch.Tensor:
if empty:
>           return torch.empty_like(tensor, pin_memory=True, device="cpu")
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.4662395Z
colossalai/checkpoint_io/utils.py:967: RuntimeError
___________________________ test_create_pin[4-False] ___________________________
2025-04-11T04:23:18.4662656Z
empty = False, num_threads = 4
2025-04-11T04:23:18.4662748Z
@pytest.mark.parametrize("empty", [True, False])
@pytest.mark.parametrize("num_threads", [1, 4])
def test_create_pin(empty: bool, num_threads: int):
model_state_dict = gen_model_state_dict()
>       model_state_dict_pinned = create_pinned_state_dict(model_state_dict, empty=empty, num_threads=num_threads)
2025-04-11T04:23:18.4663566Z
tests/test_checkpoint_io/test_safetensors_async_io.py:120:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
colossalai/checkpoint_io/utils.py:987: in create_pinned_state_dict
elems[idx] = future.result()
/opt/conda/envs/pytorch/lib/python3.10/concurrent/futures/_base.py:451: in result
return self.__get_result()
/opt/conda/envs/pytorch/lib/python3.10/concurrent/futures/_base.py:403: in __get_result
raise self._exception
/opt/conda/envs/pytorch/lib/python3.10/concurrent/futures/thread.py:58: in run
result = self.fn(*self.args, **self.kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.4665088Z
tensor = tensor([[0.6302, 0.2935, 0.7903,  ..., 0.0028, 0.9982, 0.4133],
[0.3542, 0.4418, 0.9797,  ..., 0.7308, 0.7955,...0.9666, 0.5375, 0.3321,  ..., 0.7164, 0.7701, 0.3787],
[0.0016, 0.1229, 0.2857,  ..., 0.3524, 0.9990, 0.6592]])
empty = False
2025-04-11T04:23:18.4665588Z
def _pin_tensor(tensor: torch.Tensor, empty: bool = True) -> torch.Tensor:
if empty:
return torch.empty_like(tensor, pin_memory=True, device="cpu")
>       return tensor.pin_memory()
E       RuntimeError: CUDA error: out of memory
E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.4666777Z
colossalai/checkpoint_io/utils.py:968: RuntimeError
________________________________ test_save_load ________________________________
2025-04-11T04:23:18.4667038Z
args = (), kwargs = {}
2025-04-11T04:23:18.4667131Z
def _clear_cache(*args, **kwargs):
get_accelerator().empty_cache()
get_accelerator().reset_peak_memory_stats()
get_accelerator().reset_max_memory_allocated()
get_accelerator().reset_max_memory_cached()
>       get_accelerator().synchronize()
2025-04-11T04:23:18.4667854Z
colossalai/testing/utils.py:271:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
colossalai/accelerator/cuda_accelerator.py:62: in synchronize
torch.cuda.synchronize(device)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.4668480Z
device = None
2025-04-11T04:23:18.4668568Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.4668926Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.4670609Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
_________________________ test_torch_ddp_checkpointIO __________________________
2025-04-11T04:23:18.4671067Z
args = (), kwargs = {}, try_count = 1
error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T04:23:18.4671785Z
def _run_until_success(*args, **kwargs):
try_count = 0
assert max_try is None or isinstance(
max_try, int
), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T04:23:18.4672390Z
while max_try is None or try_count < max_try:
try:
try_count += 1
ret = func(*args, **kwargs)
return ret
except exception_type as e:
error_lines = str(e).split("\n")
if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
print("Exception is caught, retrying...")
# when pattern is not specified, we always skip the exception
# when pattern is specified, we only skip when pattern is matched
continue
else:
print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
>                   raise e
2025-04-11T04:23:18.4674217Z
colossalai/testing/utils.py:144:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
colossalai/testing/utils.py:133: in _run_until_success
ret = func(*args, **kwargs)
tests/test_checkpoint_io/test_torch_ddp_checkpoint_io.py:81: in test_torch_ddp_checkpointIO
spawn(run_dist, 2)
colossalai/testing/utils.py:252: in spawn
mp.spawn(wrapped_func, nprocs=nprocs)
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
while not context.join():
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.4676176Z
self = <torch.multiprocessing.spawn.ProcessContext object at 0x7fb5b7870280>
timeout = None
2025-04-11T04:23:18.4676461Z
def join(self, timeout=None):
r"""Join one or more processes within spawn context.
2025-04-11T04:23:18.4676753Z
Attempt to join one or more processes in this spawn context.
If one of them exited with a non-zero exit status, this function
kills the remaining processes and raises an exception with the cause
of the first process exiting.
2025-04-11T04:23:18.4677385Z
Returns ``True`` if all processes have been joined successfully,
``False`` if there are more processes that need to be joined.
2025-04-11T04:23:18.4677743Z
Args:
timeout (float): Wait this long before giving up on waiting.
"""
# Ensure this function can be called even when we're done.
if len(self.sentinels) == 0:
return True
2025-04-11T04:23:18.4678416Z
# Wait for any process to fail or all of them to succeed.
ready = multiprocessing.connection.wait(
self.sentinels.keys(),
timeout=timeout,
)
2025-04-11T04:23:18.4678993Z
error_index = None
for sentinel in ready:
index = self.sentinels.pop(sentinel)
process = self.processes[index]
process.join()
if process.exitcode != 0:
error_index = index
break
2025-04-11T04:23:18.4679793Z
# Return if there was no error.
if error_index is None:
# Return whether or not all processes have been joined.
return len(self.sentinels) == 0
2025-04-11T04:23:18.4680382Z
# Assume failure. Terminate processes that are still alive.
for process in self.processes:
if process.is_alive():
process.terminate()
process.join()
2025-04-11T04:23:18.4680976Z
# There won't be an error on the queue if the process crashed.
failed_process = self.processes[error_index]
if self.error_queues[error_index].empty():
exitcode = self.processes[error_index].exitcode
if exitcode < 0:
name = signal.Signals(-exitcode).name
raise ProcessExitedException(
"process %d terminated with signal %s" % (error_index, name),
error_index=error_index,
error_pid=failed_process.pid,
exit_code=exitcode,
signal_name=name,
)
else:
raise ProcessExitedException(
"process %d terminated with exit code %d" % (error_index, exitcode),
error_index=error_index,
error_pid=failed_process.pid,
exit_code=exitcode,
)
2025-04-11T04:23:18.4683317Z
original_trace = self.error_queues[error_index].get()
msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
msg += original_trace
>       raise ProcessRaisedException(msg, error_index, failed_process.pid)
E       torch.multiprocessing.spawn.ProcessRaisedException:
E
E       -- Process 0 terminated with the following error:
E       Traceback (most recent call last):
E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
E           fn(i, *args)
E         File "/__w/ColossalAI/ColossalAI/tests/test_checkpoint_io/test_torch_ddp_checkpoint_io.py", line 76, in run_dist
E           check_torch_ddp_checkpointIO()
E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
E           partial_func(**kwargs)
E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
E           partial_func(**kwargs)
E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
E           partial_func(**kwargs)
E         [Previous line repeated 1 more time]
E         File "/__w/ColossalAI/ColossalAI/tests/test_checkpoint_io/test_torch_ddp_checkpoint_io.py", line 26, in check_torch_ddp_checkpointIO
E           model, optimizer, criterion, _, _ = booster.boost(model, optimizer, criterion, lr_scheduler=scheduler)
E         File "/__w/ColossalAI/ColossalAI/colossalai/booster/booster.py", line 154, in boost
E           model, optimizer, criterion, dataloader, lr_scheduler = self.plugin.configure(
E         File "/__w/ColossalAI/ColossalAI/colossalai/booster/plugin/torch_ddp_plugin.py", line 283, in configure
E           model = model.to(get_current_device())
E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1152, in to
E           return self._apply(convert)
E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
E           module._apply(fn)
E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
E           param_applied = fn(param)
E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1150, in convert
E           return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
E       RuntimeError: CUDA error: out of memory
E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.4690326Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
----------------------------- Captured stdout call -----------------------------
[04/11/25 04:13:50] INFO     colossalai - colossalai - INFO:
/__w/ColossalAI/ColossalAI/colossalai/initialize.py
:75 launch
INFO     colossalai - colossalai - INFO: Distributed
environment is initialized, world size: 2
Maximum number of attempts is reached or pattern is not matched, no more retrying...
----------------------------- Captured stderr call -----------------------------
[W socket.cpp:697] [c10d] The client socket has failed to connect to [localhost]:28471 (errno: 99 - Cannot assign requested address).
_____________________________ test_torch_fsdp_ckpt _____________________________
2025-04-11T04:23:18.4692298Z
args = (), kwargs = {}, try_count = 1
error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T04:23:18.4693021Z
def _run_until_success(*args, **kwargs):
try_count = 0
assert max_try is None or isinstance(
max_try, int
), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T04:23:18.4693642Z
while max_try is None or try_count < max_try:
try:
try_count += 1
ret = func(*args, **kwargs)
return ret
except exception_type as e:
error_lines = str(e).split("\n")
if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
print("Exception is caught, retrying...")
# when pattern is not specified, we always skip the exception
# when pattern is specified, we only skip when pattern is matched
continue
else:
print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
>                   raise e
2025-04-11T04:23:18.4695509Z
colossalai/testing/utils.py:144:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
colossalai/testing/utils.py:133: in _run_until_success
ret = func(*args, **kwargs)
tests/test_checkpoint_io/test_torch_fsdp_checkpoint_io.py:162: in test_torch_fsdp_ckpt
spawn(run_dist, 2)
colossalai/testing/utils.py:252: in spawn
mp.spawn(wrapped_func, nprocs=nprocs)
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
while not context.join():
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.4697524Z
self = <torch.multiprocessing.spawn.ProcessContext object at 0x7fb5e6115c90>
timeout = None
2025-04-11T04:23:18.4697843Z
def join(self, timeout=None):
r"""Join one or more processes within spawn context.
2025-04-11T04:23:18.4698146Z
Attempt to join one or more processes in this spawn context.
If one of them exited with a non-zero exit status, this function
kills the remaining processes and raises an exception with the cause
of the first process exiting.
2025-04-11T04:23:18.4698797Z
Returns ``True`` if all processes have been joined successfully,
``False`` if there are more processes that need to be joined.
2025-04-11T04:23:18.4699168Z
Args:
timeout (float): Wait this long before giving up on waiting.
"""
# Ensure this function can be called even when we're done.
if len(self.sentinels) == 0:
return True
2025-04-11T04:23:18.4699862Z
# Wait for any process to fail or all of them to succeed.
ready = multiprocessing.connection.wait(
self.sentinels.keys(),
timeout=timeout,
)
2025-04-11T04:23:18.4700568Z
error_index = None
for sentinel in ready:
index = self.sentinels.pop(sentinel)
process = self.processes[index]
process.join()
if process.exitcode != 0:
error_index = index
break
2025-04-11T04:23:18.4701385Z
# Return if there was no error.
if error_index is None:
# Return whether or not all processes have been joined.
return len(self.sentinels) == 0
2025-04-11T04:23:18.4701888Z
# Assume failure. Terminate processes that are still alive.
for process in self.processes:
if process.is_alive():
process.terminate()
process.join()
2025-04-11T04:23:18.4702576Z
# There won't be an error on the queue if the process crashed.
failed_process = self.processes[error_index]
if self.error_queues[error_index].empty():
exitcode = self.processes[error_index].exitcode
if exitcode < 0:
name = signal.Signals(-exitcode).name
raise ProcessExitedException(
"process %d terminated with signal %s" % (error_index, name),
error_index=error_index,
error_pid=failed_process.pid,
exit_code=exitcode,
signal_name=name,
)
else:
raise ProcessExitedException(
"process %d terminated with exit code %d" % (error_index, exitcode),
error_index=error_index,
error_pid=failed_process.pid,
exit_code=exitcode,
)
2025-04-11T04:23:18.4704841Z
original_trace = self.error_queues[error_index].get()
msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
msg += original_trace
>       raise ProcessRaisedException(msg, error_index, failed_process.pid)
E       torch.multiprocessing.spawn.ProcessRaisedException:
E
E       -- Process 0 terminated with the following error:
E       Traceback (most recent call last):
E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
E           fn(i, *args)
E         File "/__w/ColossalAI/ColossalAI/tests/test_checkpoint_io/test_torch_fsdp_checkpoint_io.py", line 156, in run_dist
E           check_torch_fsdp_ckpt()
E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
E           partial_func(**kwargs)
E         File "/__w/ColossalAI/ColossalAI/tests/test_checkpoint_io/test_torch_fsdp_checkpoint_io.py", line 53, in check_torch_fsdp_ckpt
E           fsdp_model, optimizer, criterion, _, _ = booster.boost(model, optimizer, criterion)
E         File "/__w/ColossalAI/ColossalAI/colossalai/booster/booster.py", line 154, in boost
E           model, optimizer, criterion, dataloader, lr_scheduler = self.plugin.configure(
E         File "/__w/ColossalAI/ColossalAI/colossalai/booster/plugin/torch_fsdp_plugin.py", line 533, in configure
E           fsdp_model = TorchFSDPModel(model, device_id=torch.cuda.current_device(), **self.fsdp_kwargs)
E         File "/__w/ColossalAI/ColossalAI/colossalai/booster/plugin/torch_fsdp_plugin.py", line 438, in __init__
E           self.module = FSDP(module, *args, **kwargs)
E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py", line 503, in __init__
E           _init_param_handle_from_module(
E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/distributed/fsdp/_init_utils.py", line 568, in _init_param_handle_from_module
E           _move_module_to_device(
E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/distributed/fsdp/_init_utils.py", line 956, in _move_module_to_device
E           _move_states_to_device(params_to_move, bufs_to_move, device_from_device_id)
E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/distributed/fsdp/_init_utils.py", line 986, in _move_states_to_device
E           param.data = param.to(device_from_device_id)
E       RuntimeError: CUDA error: out of memory
E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.4711907Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
----------------------------- Captured stdout call -----------------------------
[04/11/25 04:13:55] INFO     colossalai - colossalai - INFO:
/__w/ColossalAI/ColossalAI/colossalai/initialize.py
:75 launch
INFO     colossalai - colossalai - INFO: Distributed
environment is initialized, world size: 2
Maximum number of attempts is reached or pattern is not matched, no more retrying...
_______________________________ test_logical_pg ________________________________
2025-04-11T04:23:18.4713400Z
args = (), kwargs = {}, try_count = 1
error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T04:23:18.4714212Z
def _run_until_success(*args, **kwargs):
try_count = 0
assert max_try is None or isinstance(
max_try, int
), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T04:23:18.4714827Z
while max_try is None or try_count < max_try:
try:
try_count += 1
ret = func(*args, **kwargs)
return ret
except exception_type as e:
error_lines = str(e).split("\n")
if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
print("Exception is caught, retrying...")
# when pattern is not specified, we always skip the exception
# when pattern is specified, we only skip when pattern is matched
continue
else:
print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
>                   raise e
2025-04-11T04:23:18.4716715Z
colossalai/testing/utils.py:144:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
colossalai/testing/utils.py:133: in _run_until_success
ret = func(*args, **kwargs)
tests/test_device/test_init_logical_pg.py:33: in test_logical_pg
spawn(check_layer, 4)
colossalai/testing/utils.py:252: in spawn
mp.spawn(wrapped_func, nprocs=nprocs)
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
while not context.join():
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.4718568Z
self = <torch.multiprocessing.spawn.ProcessContext object at 0x7fb5b841ba00>
timeout = None
2025-04-11T04:23:18.4718864Z
def join(self, timeout=None):
r"""Join one or more processes within spawn context.
2025-04-11T04:23:18.4719168Z
Attempt to join one or more processes in this spawn context.
If one of them exited with a non-zero exit status, this function
kills the remaining processes and raises an exception with the cause
of the first process exiting.
2025-04-11T04:23:18.4719804Z
Returns ``True`` if all processes have been joined successfully,
``False`` if there are more processes that need to be joined.
2025-04-11T04:23:18.4720173Z
Args:
timeout (float): Wait this long before giving up on waiting.
"""
# Ensure this function can be called even when we're done.
if len(self.sentinels) == 0:
return True
2025-04-11T04:23:18.4720959Z
# Wait for any process to fail or all of them to succeed.
ready = multiprocessing.connection.wait(
self.sentinels.keys(),
timeout=timeout,
)
2025-04-11T04:23:18.4721566Z
error_index = None
for sentinel in ready:
index = self.sentinels.pop(sentinel)
process = self.processes[index]
process.join()
if process.exitcode != 0:
error_index = index
break
2025-04-11T04:23:18.4722390Z
# Return if there was no error.
if error_index is None:
# Return whether or not all processes have been joined.
return len(self.sentinels) == 0
2025-04-11T04:23:18.4723082Z
# Assume failure. Terminate processes that are still alive.
for process in self.processes:
if process.is_alive():
process.terminate()
process.join()
2025-04-11T04:23:18.4723679Z
# There won't be an error on the queue if the process crashed.
failed_process = self.processes[error_index]
if self.error_queues[error_index].empty():
exitcode = self.processes[error_index].exitcode
if exitcode < 0:
name = signal.Signals(-exitcode).name
raise ProcessExitedException(
"process %d terminated with signal %s" % (error_index, name),
error_index=error_index,
error_pid=failed_process.pid,
exit_code=exitcode,
signal_name=name,
)
else:
raise ProcessExitedException(
"process %d terminated with exit code %d" % (error_index, exitcode),
error_index=error_index,
error_pid=failed_process.pid,
exit_code=exitcode,
)
2025-04-11T04:23:18.4725944Z
original_trace = self.error_queues[error_index].get()
msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
msg += original_trace
>       raise ProcessRaisedException(msg, error_index, failed_process.pid)
E       torch.multiprocessing.spawn.ProcessRaisedException:
E
E       -- Process 0 terminated with the following error:
E       Traceback (most recent call last):
E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
E           fn(i, *args)
E         File "/__w/ColossalAI/ColossalAI/tests/test_device/test_init_logical_pg.py", line 17, in check_layer
E           tensor_to_check = torch.tensor([2, 2, 2, 2]).cuda()
E       RuntimeError: CUDA error: out of memory
E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.4728567Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
----------------------------- Captured stdout call -----------------------------
[04/11/25 04:14:14] INFO     colossalai - colossalai - INFO:
/__w/ColossalAI/ColossalAI/colossalai/initialize.py
:75 launch
INFO     colossalai - colossalai - INFO: Distributed
environment is initialized, world size: 4
Maximum number of attempts is reached or pattern is not matched, no more retrying...
----------------------------- Captured stderr call -----------------------------
[W socket.cpp:697] [c10d] The client socket has failed to connect to [localhost]:64432 (errno: 99 - Cannot assign requested address).
[W socket.cpp:697] [c10d] The client socket has failed to connect to [localhost]:64432 (errno: 99 - Cannot assign requested address).
____________________________ test_all_to_all_single ____________________________
2025-04-11T04:23:18.4730895Z
args = (), kwargs = {}, try_count = 1
error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T04:23:18.4731603Z
def _run_until_success(*args, **kwargs):
try_count = 0
assert max_try is None or isinstance(
max_try, int
), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T04:23:18.4732224Z
while max_try is None or try_count < max_try:
try:
try_count += 1
ret = func(*args, **kwargs)
return ret
except exception_type as e:
error_lines = str(e).split("\n")
if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
print("Exception is caught, retrying...")
# when pattern is not specified, we always skip the exception
# when pattern is specified, we only skip when pattern is matched
continue
else:
print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
>                   raise e
2025-04-11T04:23:18.4734100Z
colossalai/testing/utils.py:144:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
colossalai/testing/utils.py:133: in _run_until_success
ret = func(*args, **kwargs)
tests/test_fp8/test_all_to_all_single.py:73: in test_all_to_all_single
spawn(run_dist, 4)
colossalai/testing/utils.py:252: in spawn
mp.spawn(wrapped_func, nprocs=nprocs)
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
while not context.join():
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.4736077Z
self = <torch.multiprocessing.spawn.ProcessContext object at 0x7fb5b8236230>
timeout = None
2025-04-11T04:23:18.4736373Z
def join(self, timeout=None):
r"""Join one or more processes within spawn context.
2025-04-11T04:23:18.4736668Z
Attempt to join one or more processes in this spawn context.
If one of them exited with a non-zero exit status, this function
kills the remaining processes and raises an exception with the cause
of the first process exiting.
2025-04-11T04:23:18.4737308Z
Returns ``True`` if all processes have been joined successfully,
``False`` if there are more processes that need to be joined.
2025-04-11T04:23:18.4737679Z
Args:
timeout (float): Wait this long before giving up on waiting.
"""
# Ensure this function can be called even when we're done.
if len(self.sentinels) == 0:
return True
2025-04-11T04:23:18.4738389Z
# Wait for any process to fail or all of them to succeed.
ready = multiprocessing.connection.wait(
self.sentinels.keys(),
timeout=timeout,
)
2025-04-11T04:23:18.4738987Z
error_index = None
for sentinel in ready:
index = self.sentinels.pop(sentinel)
process = self.processes[index]
process.join()
if process.exitcode != 0:
error_index = index
break
2025-04-11T04:23:18.4739815Z
# Return if there was no error.
if error_index is None:
# Return whether or not all processes have been joined.
return len(self.sentinels) == 0
2025-04-11T04:23:18.4740416Z
# Assume failure. Terminate processes that are still alive.
for process in self.processes:
if process.is_alive():
process.terminate()
process.join()
2025-04-11T04:23:18.4741036Z
# There won't be an error on the queue if the process crashed.
failed_process = self.processes[error_index]
if self.error_queues[error_index].empty():
exitcode = self.processes[error_index].exitcode
if exitcode < 0:
name = signal.Signals(-exitcode).name
raise ProcessExitedException(
"process %d terminated with signal %s" % (error_index, name),
error_index=error_index,
error_pid=failed_process.pid,
exit_code=exitcode,
signal_name=name,
)
else:
raise ProcessExitedException(
"process %d terminated with exit code %d" % (error_index, exitcode),
error_index=error_index,
error_pid=failed_process.pid,
exit_code=exitcode,
)
2025-04-11T04:23:18.4743391Z
original_trace = self.error_queues[error_index].get()
msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
msg += original_trace
>       raise ProcessRaisedException(msg, error_index, failed_process.pid)
E       torch.multiprocessing.spawn.ProcessRaisedException:
E
E       -- Process 0 terminated with the following error:
E       Traceback (most recent call last):
E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
E           fn(i, *args)
E         File "/__w/ColossalAI/ColossalAI/tests/test_fp8/test_all_to_all_single.py", line 67, in run_dist
E           check_all2all()
E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 271, in _clear_cache
E           get_accelerator().synchronize()
E         File "/__w/ColossalAI/ColossalAI/colossalai/accelerator/cuda_accelerator.py", line 62, in synchronize
E           torch.cuda.synchronize(device)
E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py", line 801, in synchronize
E           return torch._C._cuda_synchronize()
E       RuntimeError: CUDA error: out of memory
E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.4747082Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
----------------------------- Captured stdout call -----------------------------
[04/11/25 04:14:20] INFO     colossalai - colossalai - INFO:
/__w/ColossalAI/ColossalAI/colossalai/initialize.py
:75 launch
INFO     colossalai - colossalai - INFO: Distributed
environment is initialized, world size: 4
Maximum number of attempts is reached or pattern is not matched, no more retrying...
----------------------------- Captured stderr call -----------------------------
[W socket.cpp:697] [c10d] The client socket has failed to connect to [localhost]:62642 (errno: 99 - Cannot assign requested address).
[W socket.cpp:697] [c10d] The client socket has failed to connect to [localhost]:62642 (errno: 99 - Cannot assign requested address).
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:356: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:356: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:356: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:356: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
warnings.warn(
_______________________________ test_all_to_all ________________________________
2025-04-11T04:23:18.4754610Z
args = (), kwargs = {}, try_count = 1
error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T04:23:18.4755422Z
def _run_until_success(*args, **kwargs):
try_count = 0
assert max_try is None or isinstance(
max_try, int
), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T04:23:18.4756061Z
while max_try is None or try_count < max_try:
try:
try_count += 1
ret = func(*args, **kwargs)
return ret
except exception_type as e:
error_lines = str(e).split("\n")
if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
print("Exception is caught, retrying...")
# when pattern is not specified, we always skip the exception
# when pattern is specified, we only skip when pattern is matched
continue
else:
print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
>                   raise e
2025-04-11T04:23:18.4757972Z
colossalai/testing/utils.py:144:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
colossalai/testing/utils.py:133: in _run_until_success
ret = func(*args, **kwargs)
tests/test_fp8/test_fp8_all_to_all.py:36: in test_all_to_all
spawn(run_dist, 4)
colossalai/testing/utils.py:252: in spawn
mp.spawn(wrapped_func, nprocs=nprocs)
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
while not context.join():
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.4759822Z
self = <torch.multiprocessing.spawn.ProcessContext object at 0x7fb5b841a410>
timeout = None
2025-04-11T04:23:18.4760117Z
def join(self, timeout=None):
r"""Join one or more processes within spawn context.
2025-04-11T04:23:18.4760414Z
Attempt to join one or more processes in this spawn context.
If one of them exited with a non-zero exit status, this function
kills the remaining processes and raises an exception with the cause
of the first process exiting.
2025-04-11T04:23:18.4761047Z
Returns ``True`` if all processes have been joined successfully,
``False`` if there are more processes that need to be joined.
2025-04-11T04:23:18.4761416Z
Args:
timeout (float): Wait this long before giving up on waiting.
"""
# Ensure this function can be called even when we're done.
if len(self.sentinels) == 0:
return True
2025-04-11T04:23:18.4762223Z
# Wait for any process to fail or all of them to succeed.
ready = multiprocessing.connection.wait(
self.sentinels.keys(),
timeout=timeout,
)
2025-04-11T04:23:18.4762818Z
error_index = None
for sentinel in ready:
index = self.sentinels.pop(sentinel)
process = self.processes[index]
process.join()
if process.exitcode != 0:
error_index = index
break
2025-04-11T04:23:18.4763649Z
# Return if there was no error.
if error_index is None:
# Return whether or not all processes have been joined.
return len(self.sentinels) == 0
2025-04-11T04:23:18.4764234Z
# Assume failure. Terminate processes that are still alive.
for process in self.processes:
if process.is_alive():
process.terminate()
process.join()
2025-04-11T04:23:18.4764826Z
# There won't be an error on the queue if the process crashed.
failed_process = self.processes[error_index]
if self.error_queues[error_index].empty():
exitcode = self.processes[error_index].exitcode
if exitcode < 0:
name = signal.Signals(-exitcode).name
raise ProcessExitedException(
"process %d terminated with signal %s" % (error_index, name),
error_index=error_index,
error_pid=failed_process.pid,
exit_code=exitcode,
signal_name=name,
)
else:
raise ProcessExitedException(
"process %d terminated with exit code %d" % (error_index, exitcode),
error_index=error_index,
error_pid=failed_process.pid,
exit_code=exitcode,
)
2025-04-11T04:23:18.4767067Z
original_trace = self.error_queues[error_index].get()
msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
msg += original_trace
>       raise ProcessRaisedException(msg, error_index, failed_process.pid)
E       torch.multiprocessing.spawn.ProcessRaisedException:
E
E       -- Process 0 terminated with the following error:
E       Traceback (most recent call last):
E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
E           fn(i, *args)
E         File "/__w/ColossalAI/ColossalAI/tests/test_fp8/test_fp8_all_to_all.py", line 31, in run_dist
E           check_4gpu()
E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 271, in _clear_cache
E           get_accelerator().synchronize()
E         File "/__w/ColossalAI/ColossalAI/colossalai/accelerator/cuda_accelerator.py", line 62, in synchronize
E           torch.cuda.synchronize(device)
E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py", line 801, in synchronize
E           return torch._C._cuda_synchronize()
E       RuntimeError: CUDA error: out of memory
E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.4770855Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
----------------------------- Captured stdout call -----------------------------
[04/11/25 04:14:24] INFO     colossalai - colossalai - INFO:
/__w/ColossalAI/ColossalAI/colossalai/initialize.py
:75 launch
INFO     colossalai - colossalai - INFO: Distributed
environment is initialized, world size: 4
Maximum number of attempts is reached or pattern is not matched, no more retrying...
----------------------------- Captured stderr call -----------------------------
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:356: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:356: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:356: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:356: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
warnings.warn(
____________________________ test_all_to_all_single ____________________________
2025-04-11T04:23:18.4777597Z
args = (), kwargs = {}, try_count = 1
error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T04:23:18.4778384Z
def _run_until_success(*args, **kwargs):
try_count = 0
assert max_try is None or isinstance(
max_try, int
), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T04:23:18.4778998Z
while max_try is None or try_count < max_try:
try:
try_count += 1
ret = func(*args, **kwargs)
return ret
except exception_type as e:
error_lines = str(e).split("\n")
if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
print("Exception is caught, retrying...")
# when pattern is not specified, we always skip the exception
# when pattern is specified, we only skip when pattern is matched
continue
else:
print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
>                   raise e
2025-04-11T04:23:18.4780783Z
colossalai/testing/utils.py:144:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
colossalai/testing/utils.py:133: in _run_until_success
ret = func(*args, **kwargs)
tests/test_fp8/test_fp8_all_to_all_single.py:34: in test_all_to_all_single
spawn(run_dist, 4)
colossalai/testing/utils.py:252: in spawn
mp.spawn(wrapped_func, nprocs=nprocs)
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
while not context.join():
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.4782648Z
self = <torch.multiprocessing.spawn.ProcessContext object at 0x7fb5b835abf0>
timeout = None
2025-04-11T04:23:18.4783033Z
def join(self, timeout=None):
r"""Join one or more processes within spawn context.
2025-04-11T04:23:18.4783339Z
Attempt to join one or more processes in this spawn context.
If one of them exited with a non-zero exit status, this function
kills the remaining processes and raises an exception with the cause
of the first process exiting.
2025-04-11T04:23:18.4783982Z
Returns ``True`` if all processes have been joined successfully,
``False`` if there are more processes that need to be joined.
2025-04-11T04:23:18.4784354Z
Args:
timeout (float): Wait this long before giving up on waiting.
"""
# Ensure this function can be called even when we're done.
if len(self.sentinels) == 0:
return True
2025-04-11T04:23:18.4785163Z
# Wait for any process to fail or all of them to succeed.
ready = multiprocessing.connection.wait(
self.sentinels.keys(),
timeout=timeout,
)
2025-04-11T04:23:18.4785750Z
error_index = None
for sentinel in ready:
index = self.sentinels.pop(sentinel)
process = self.processes[index]
process.join()
if process.exitcode != 0:
error_index = index
break
2025-04-11T04:23:18.4786574Z
# Return if there was no error.
if error_index is None:
# Return whether or not all processes have been joined.
return len(self.sentinels) == 0
2025-04-11T04:23:18.4787071Z
# Assume failure. Terminate processes that are still alive.
for process in self.processes:
if process.is_alive():
process.terminate()
process.join()
2025-04-11T04:23:18.4787662Z
# There won't be an error on the queue if the process crashed.
failed_process = self.processes[error_index]
if self.error_queues[error_index].empty():
exitcode = self.processes[error_index].exitcode
if exitcode < 0:
name = signal.Signals(-exitcode).name
raise ProcessExitedException(
"process %d terminated with signal %s" % (error_index, name),
error_index=error_index,
error_pid=failed_process.pid,
exit_code=exitcode,
signal_name=name,
)
else:
raise ProcessExitedException(
"process %d terminated with exit code %d" % (error_index, exitcode),
error_index=error_index,
error_pid=failed_process.pid,
exit_code=exitcode,
)
2025-04-11T04:23:18.4790045Z
original_trace = self.error_queues[error_index].get()
msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
msg += original_trace
>       raise ProcessRaisedException(msg, error_index, failed_process.pid)
E       torch.multiprocessing.spawn.ProcessRaisedException:
E
E       -- Process 0 terminated with the following error:
E       Traceback (most recent call last):
E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
E           fn(i, *args)
E         File "/__w/ColossalAI/ColossalAI/tests/test_fp8/test_fp8_all_to_all_single.py", line 29, in run_dist
E           check_4gpu()
E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 271, in _clear_cache
E           get_accelerator().synchronize()
E         File "/__w/ColossalAI/ColossalAI/colossalai/accelerator/cuda_accelerator.py", line 62, in synchronize
E           torch.cuda.synchronize(device)
E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py", line 801, in synchronize
E           return torch._C._cuda_synchronize()
E       RuntimeError: CUDA error: out of memory
E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.4793829Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
----------------------------- Captured stdout call -----------------------------
[04/11/25 04:14:29] INFO     colossalai - colossalai - INFO:
/__w/ColossalAI/ColossalAI/colossalai/initialize.py
:75 launch
INFO     colossalai - colossalai - INFO: Distributed
environment is initialized, world size: 4
Maximum number of attempts is reached or pattern is not matched, no more retrying...
----------------------------- Captured stderr call -----------------------------
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:356: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:356: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:356: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:356: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
warnings.warn(
_______________________________ test_all_gather ________________________________
2025-04-11T04:23:18.4800833Z
args = (), kwargs = {}, try_count = 1
error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T04:23:18.4801538Z
def _run_until_success(*args, **kwargs):
try_count = 0
assert max_try is None or isinstance(
max_try, int
), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T04:23:18.4802147Z
while max_try is None or try_count < max_try:
try:
try_count += 1
ret = func(*args, **kwargs)
return ret
except exception_type as e:
error_lines = str(e).split("\n")
if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
print("Exception is caught, retrying...")
# when pattern is not specified, we always skip the exception
# when pattern is specified, we only skip when pattern is matched
continue
else:
print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
>                   raise e
2025-04-11T04:23:18.4803918Z
colossalai/testing/utils.py:144:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
colossalai/testing/utils.py:133: in _run_until_success
ret = func(*args, **kwargs)
tests/test_fp8/test_fp8_allgather.py:42: in test_all_gather
spawn(run_dist, 4)
colossalai/testing/utils.py:252: in spawn
mp.spawn(wrapped_func, nprocs=nprocs)
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
while not context.join():
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.4805857Z
self = <torch.multiprocessing.spawn.ProcessContext object at 0x7fb5b8571db0>
timeout = None
2025-04-11T04:23:18.4806234Z
def join(self, timeout=None):
r"""Join one or more processes within spawn context.
2025-04-11T04:23:18.4806533Z
Attempt to join one or more processes in this spawn context.
If one of them exited with a non-zero exit status, this function
kills the remaining processes and raises an exception with the cause
of the first process exiting.
2025-04-11T04:23:18.4807176Z
Returns ``True`` if all processes have been joined successfully,
``False`` if there are more processes that need to be joined.
2025-04-11T04:23:18.4807545Z
Args:
timeout (float): Wait this long before giving up on waiting.
"""
# Ensure this function can be called even when we're done.
if len(self.sentinels) == 0:
return True
2025-04-11T04:23:18.4808252Z
# Wait for any process to fail or all of them to succeed.
ready = multiprocessing.connection.wait(
self.sentinels.keys(),
timeout=timeout,
)
2025-04-11T04:23:18.4808848Z
error_index = None
for sentinel in ready:
index = self.sentinels.pop(sentinel)
process = self.processes[index]
process.join()
if process.exitcode != 0:
error_index = index
break
2025-04-11T04:23:18.4809693Z
# Return if there was no error.
if error_index is None:
# Return whether or not all processes have been joined.
return len(self.sentinels) == 0
2025-04-11T04:23:18.4810208Z
# Assume failure. Terminate processes that are still alive.
for process in self.processes:
if process.is_alive():
process.terminate()
process.join()
2025-04-11T04:23:18.4810910Z
# There won't be an error on the queue if the process crashed.
failed_process = self.processes[error_index]
if self.error_queues[error_index].empty():
exitcode = self.processes[error_index].exitcode
if exitcode < 0:
name = signal.Signals(-exitcode).name
raise ProcessExitedException(
"process %d terminated with signal %s" % (error_index, name),
error_index=error_index,
error_pid=failed_process.pid,
exit_code=exitcode,
signal_name=name,
)
else:
raise ProcessExitedException(
"process %d terminated with exit code %d" % (error_index, exitcode),
error_index=error_index,
error_pid=failed_process.pid,
exit_code=exitcode,
)
2025-04-11T04:23:18.4813228Z
original_trace = self.error_queues[error_index].get()
msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
msg += original_trace
>       raise ProcessRaisedException(msg, error_index, failed_process.pid)
E       torch.multiprocessing.spawn.ProcessRaisedException:
E
E       -- Process 0 terminated with the following error:
E       Traceback (most recent call last):
E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
E           fn(i, *args)
E         File "/__w/ColossalAI/ColossalAI/tests/test_fp8/test_fp8_allgather.py", line 37, in run_dist
E           check_4gpu()
E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 271, in _clear_cache
E           get_accelerator().synchronize()
E         File "/__w/ColossalAI/ColossalAI/colossalai/accelerator/cuda_accelerator.py", line 62, in synchronize
E           torch.cuda.synchronize(device)
E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py", line 801, in synchronize
E           return torch._C._cuda_synchronize()
E       RuntimeError: CUDA error: out of memory
E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.4816823Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
----------------------------- Captured stdout call -----------------------------
[04/11/25 04:14:34] INFO     colossalai - colossalai - INFO:
/__w/ColossalAI/ColossalAI/colossalai/initialize.py
:75 launch
INFO     colossalai - colossalai - INFO: Distributed
environment is initialized, world size: 4
Maximum number of attempts is reached or pattern is not matched, no more retrying...
----------------------------- Captured stderr call -----------------------------
[W socket.cpp:697] [c10d] The client socket has failed to connect to [localhost]:60836 (errno: 99 - Cannot assign requested address).
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:356: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:356: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:356: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:356: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
warnings.warn(
_______________________________ test_all_reduce ________________________________
2025-04-11T04:23:18.4823967Z
args = (), kwargs = {}, try_count = 1
error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T04:23:18.4824665Z
def _run_until_success(*args, **kwargs):
try_count = 0
assert max_try is None or isinstance(
max_try, int
), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T04:23:18.4825376Z
while max_try is None or try_count < max_try:
try:
try_count += 1
ret = func(*args, **kwargs)
return ret
except exception_type as e:
error_lines = str(e).split("\n")
if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
print("Exception is caught, retrying...")
# when pattern is not specified, we always skip the exception
# when pattern is specified, we only skip when pattern is matched
continue
else:
print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
>                   raise e
2025-04-11T04:23:18.4827256Z
colossalai/testing/utils.py:144:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
colossalai/testing/utils.py:133: in _run_until_success
ret = func(*args, **kwargs)
tests/test_fp8/test_fp8_allreduce.py:52: in test_all_reduce
spawn(run_dist, 4)
colossalai/testing/utils.py:252: in spawn
mp.spawn(wrapped_func, nprocs=nprocs)
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
while not context.join():
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.4829121Z
self = <torch.multiprocessing.spawn.ProcessContext object at 0x7fb5b84e1fc0>
timeout = None
2025-04-11T04:23:18.4829414Z
def join(self, timeout=None):
r"""Join one or more processes within spawn context.
2025-04-11T04:23:18.4829709Z
Attempt to join one or more processes in this spawn context.
If one of them exited with a non-zero exit status, this function
kills the remaining processes and raises an exception with the cause
of the first process exiting.
2025-04-11T04:23:18.4830349Z
Returns ``True`` if all processes have been joined successfully,
``False`` if there are more processes that need to be joined.
2025-04-11T04:23:18.4830720Z
Args:
timeout (float): Wait this long before giving up on waiting.
"""
# Ensure this function can be called even when we're done.
if len(self.sentinels) == 0:
return True
2025-04-11T04:23:18.4831422Z
# Wait for any process to fail or all of them to succeed.
ready = multiprocessing.connection.wait(
self.sentinels.keys(),
timeout=timeout,
)
2025-04-11T04:23:18.4832214Z
error_index = None
for sentinel in ready:
index = self.sentinels.pop(sentinel)
process = self.processes[index]
process.join()
if process.exitcode != 0:
error_index = index
break
2025-04-11T04:23:18.4833054Z
# Return if there was no error.
if error_index is None:
# Return whether or not all processes have been joined.
return len(self.sentinels) == 0
2025-04-11T04:23:18.4833566Z
# Assume failure. Terminate processes that are still alive.
for process in self.processes:
if process.is_alive():
process.terminate()
process.join()
2025-04-11T04:23:18.4834265Z
# There won't be an error on the queue if the process crashed.
failed_process = self.processes[error_index]
if self.error_queues[error_index].empty():
exitcode = self.processes[error_index].exitcode
if exitcode < 0:
name = signal.Signals(-exitcode).name
raise ProcessExitedException(
"process %d terminated with signal %s" % (error_index, name),
error_index=error_index,
error_pid=failed_process.pid,
exit_code=exitcode,
signal_name=name,
)
else:
raise ProcessExitedException(
"process %d terminated with exit code %d" % (error_index, exitcode),
error_index=error_index,
error_pid=failed_process.pid,
exit_code=exitcode,
)
2025-04-11T04:23:18.4836617Z
original_trace = self.error_queues[error_index].get()
msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
msg += original_trace
>       raise ProcessRaisedException(msg, error_index, failed_process.pid)
E       torch.multiprocessing.spawn.ProcessRaisedException:
E
E       -- Process 0 terminated with the following error:
E       Traceback (most recent call last):
E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
E           fn(i, *args)
E         File "/__w/ColossalAI/ColossalAI/tests/test_fp8/test_fp8_allreduce.py", line 47, in run_dist
E           check_4gpu()
E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
E           partial_func(**kwargs)
E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 271, in _clear_cache
E           get_accelerator().synchronize()
E         File "/__w/ColossalAI/ColossalAI/colossalai/accelerator/cuda_accelerator.py", line 62, in synchronize
E           torch.cuda.synchronize(device)
E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py", line 801, in synchronize
E           return torch._C._cuda_synchronize()
E       RuntimeError: CUDA error: out of memory
E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.4840737Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
----------------------------- Captured stdout call -----------------------------
[04/11/25 04:14:40] INFO     colossalai - colossalai - INFO:
/__w/ColossalAI/ColossalAI/colossalai/initialize.py
:75 launch
INFO     colossalai - colossalai - INFO: Distributed
environment is initialized, world size: 4
Maximum number of attempts is reached or pattern is not matched, no more retrying...
----------------------------- Captured stderr call -----------------------------
[W socket.cpp:697] [c10d] The client socket has failed to connect to [localhost]:26964 (errno: 99 - Cannot assign requested address).
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:356: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:356: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:356: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:356: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
warnings.warn(
________________________________ test_fp8_cast _________________________________
2025-04-11T04:23:18.4847882Z
args = (), kwargs = {}
2025-04-11T04:23:18.4847981Z
def _clear_cache(*args, **kwargs):
get_accelerator().empty_cache()
get_accelerator().reset_peak_memory_stats()
get_accelerator().reset_max_memory_allocated()
get_accelerator().reset_max_memory_cached()
>       get_accelerator().synchronize()
2025-04-11T04:23:18.4848743Z
colossalai/testing/utils.py:271:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
colossalai/accelerator/cuda_accelerator.py:62: in synchronize
torch.cuda.synchronize(device)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.4849352Z
device = None
2025-04-11T04:23:18.4849451Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.4849820Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.4851539Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
__________________________________ test_fsdp ___________________________________
2025-04-11T04:23:18.4851931Z
args = (), kwargs = {}, try_count = 1
error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T04:23:18.4852640Z
def _run_until_success(*args, **kwargs):
try_count = 0
assert max_try is None or isinstance(
max_try, int
), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T04:23:18.4853345Z
while max_try is None or try_count < max_try:
try:
try_count += 1
ret = func(*args, **kwargs)
return ret
except exception_type as e:
error_lines = str(e).split("\n")
if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
print("Exception is caught, retrying...")
# when pattern is not specified, we always skip the exception
# when pattern is specified, we only skip when pattern is matched
continue
else:
print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
>                   raise e
2025-04-11T04:23:18.4855271Z
colossalai/testing/utils.py:144:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
colossalai/testing/utils.py:133: in _run_until_success
ret = func(*args, **kwargs)
tests/test_fp8/test_fp8_fsdp_comm_hook.py:104: in test_fsdp
spawn(demo_basic, n_gpus)
colossalai/testing/utils.py:252: in spawn
mp.spawn(wrapped_func, nprocs=nprocs)
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
while not context.join():
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.4857144Z
self = <torch.multiprocessing.spawn.ProcessContext object at 0x7fb5b84e3ee0>
timeout = None
2025-04-11T04:23:18.4857430Z
def join(self, timeout=None):
r"""Join one or more processes within spawn context.
2025-04-11T04:23:18.4857749Z
Attempt to join one or more processes in this spawn context.
If one of them exited with a non-zero exit status, this function
kills the remaining processes and raises an exception with the cause
of the first process exiting.
2025-04-11T04:23:18.4858406Z
Returns ``True`` if all processes have been joined successfully,
``False`` if there are more processes that need to be joined.
2025-04-11T04:23:18.4858777Z
Args:
timeout (float): Wait this long before giving up on waiting.
"""
# Ensure this function can be called even when we're done.
if len(self.sentinels) == 0:
return True
2025-04-11T04:23:18.4859489Z
# Wait for any process to fail or all of them to succeed.
ready = multiprocessing.connection.wait(
self.sentinels.keys(),
timeout=timeout,
)
2025-04-11T04:23:18.4860198Z
error_index = None
for sentinel in ready:
index = self.sentinels.pop(sentinel)
process = self.processes[index]
process.join()
if process.exitcode != 0:
error_index = index
break
2025-04-11T04:23:18.4861054Z
# Return if there was no error.
if error_index is None:
# Return whether or not all processes have been joined.
return len(self.sentinels) == 0
2025-04-11T04:23:18.4861587Z
# Assume failure. Terminate processes that are still alive.
for process in self.processes:
if process.is_alive():
process.terminate()
process.join()
2025-04-11T04:23:18.4862312Z
# There won't be an error on the queue if the process crashed.
failed_process = self.processes[error_index]
if self.error_queues[error_index].empty():
exitcode = self.processes[error_index].exitcode
if exitcode < 0:
name = signal.Signals(-exitcode).name
raise ProcessExitedException(
"process %d terminated with signal %s" % (error_index, name),
error_index=error_index,
error_pid=failed_process.pid,
exit_code=exitcode,
signal_name=name,
)
else:
raise ProcessExitedException(
"process %d terminated with exit code %d" % (error_index, exitcode),
error_index=error_index,
error_pid=failed_process.pid,
exit_code=exitcode,
)
2025-04-11T04:23:18.4864639Z
original_trace = self.error_queues[error_index].get()
msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
msg += original_trace
>       raise ProcessRaisedException(msg, error_index, failed_process.pid)
E       torch.multiprocessing.spawn.ProcessRaisedException:
E
E       -- Process 0 terminated with the following error:
E       Traceback (most recent call last):
E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
E           fn(i, *args)
E         File "/__w/ColossalAI/ColossalAI/tests/test_fp8/test_fp8_fsdp_comm_hook.py", line 95, in demo_basic
E           run_model()
E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 271, in _clear_cache
E           get_accelerator().synchronize()
E         File "/__w/ColossalAI/ColossalAI/colossalai/accelerator/cuda_accelerator.py", line 62, in synchronize
E           torch.cuda.synchronize(device)
E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py", line 801, in synchronize
E           return torch._C._cuda_synchronize()
E       RuntimeError: CUDA error: out of memory
E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.4868374Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
----------------------------- Captured stdout call -----------------------------
Running basic FSDP example on rank 1.
Running basic FSDP example on rank 5.
Running basic FSDP example on rank 0.
[04/11/25 04:14:47] INFO     colossalai - colossalai - INFO:
/__w/ColossalAI/ColossalAI/colossalai/initialize.py
:75 launch
INFO     colossalai - colossalai - INFO: Distributed
environment is initialized, world size: 8
Running basic FSDP example on rank 6.
Running basic FSDP example on rank 4.
Running basic FSDP example on rank 7.
Running basic FSDP example on rank 2.
Running basic FSDP example on rank 3.
Maximum number of attempts is reached or pattern is not matched, no more retrying...
----------------------------- Captured stderr call -----------------------------
[W socket.cpp:697] [c10d] The client socket has failed to connect to [localhost]:34448 (errno: 99 - Cannot assign requested address).
[W socket.cpp:697] [c10d] The client socket has failed to connect to [localhost]:34448 (errno: 99 - Cannot assign requested address).
[W socket.cpp:697] [c10d] The client socket has failed to connect to [localhost]:34448 (errno: 99 - Cannot assign requested address).
[W socket.cpp:697] [c10d] The client socket has failed to connect to [localhost]:34448 (errno: 99 - Cannot assign requested address).
[W socket.cpp:697] [c10d] The client socket has failed to connect to [localhost]:34448 (errno: 99 - Cannot assign requested address).
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:356: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:356: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:356: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:356: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:356: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:356: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:356: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:356: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
warnings.warn(
[rank5]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank1]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank6]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
[rank7]:[W PyInterpreter.cpp:232] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)
________________________________ test_fp8_hook _________________________________
2025-04-11T04:23:18.4886129Z
@pytest.mark.skipif(get_accelerator().get_device_capability()[0] < 9, reason="Test requires device capability >= 9.0")
def test_fp8_hook():
# create tensors
>       w = nn.Parameter(torch.rand(D_OUT, D_IN, device=get_current_device(), dtype=DTYPE))
E       RuntimeError: CUDA error: out of memory
E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.4887509Z
tests/test_fp8/test_fp8_hook.py:41: RuntimeError
__________________________ test_fp8_linear[True-True] __________________________
2025-04-11T04:23:18.4887780Z
use_bias = True, use_batch = True
2025-04-11T04:23:18.4887885Z
@pytest.mark.skipif(get_accelerator().get_device_capability()[0] < 9, reason="Test requires device capability >= 9.0")
@pytest.mark.parametrize("use_bias", [True, False])
@pytest.mark.parametrize("use_batch", [True, False])
def test_fp8_linear(use_bias: bool, use_batch: bool):
# create tensors
>       w = torch.rand(D_OUT, D_IN, device=get_current_device(), dtype=DTYPE, requires_grad=True)
E       RuntimeError: CUDA error: out of memory
E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.4889537Z
tests/test_fp8/test_fp8_linear.py:20: RuntimeError
_________________________ test_fp8_linear[True-False] __________________________
2025-04-11T04:23:18.4889813Z
use_bias = False, use_batch = True
2025-04-11T04:23:18.4889913Z
@pytest.mark.skipif(get_accelerator().get_device_capability()[0] < 9, reason="Test requires device capability >= 9.0")
@pytest.mark.parametrize("use_bias", [True, False])
@pytest.mark.parametrize("use_batch", [True, False])
def test_fp8_linear(use_bias: bool, use_batch: bool):
# create tensors
>       w = torch.rand(D_OUT, D_IN, device=get_current_device(), dtype=DTYPE, requires_grad=True)
E       RuntimeError: CUDA error: out of memory
E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.4891661Z
tests/test_fp8/test_fp8_linear.py:20: RuntimeError
_________________________ test_fp8_linear[False-True] __________________________
2025-04-11T04:23:18.4891940Z
use_bias = True, use_batch = False
2025-04-11T04:23:18.4892125Z
@pytest.mark.skipif(get_accelerator().get_device_capability()[0] < 9, reason="Test requires device capability >= 9.0")
@pytest.mark.parametrize("use_bias", [True, False])
@pytest.mark.parametrize("use_batch", [True, False])
def test_fp8_linear(use_bias: bool, use_batch: bool):
# create tensors
>       w = torch.rand(D_OUT, D_IN, device=get_current_device(), dtype=DTYPE, requires_grad=True)
E       RuntimeError: CUDA error: out of memory
E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.4893758Z
tests/test_fp8/test_fp8_linear.py:20: RuntimeError
_________________________ test_fp8_linear[False-False] _________________________
2025-04-11T04:23:18.4894031Z
use_bias = False, use_batch = False
2025-04-11T04:23:18.4894141Z
@pytest.mark.skipif(get_accelerator().get_device_capability()[0] < 9, reason="Test requires device capability >= 9.0")
@pytest.mark.parametrize("use_bias", [True, False])
@pytest.mark.parametrize("use_batch", [True, False])
def test_fp8_linear(use_bias: bool, use_batch: bool):
# create tensors
>       w = torch.rand(D_OUT, D_IN, device=get_current_device(), dtype=DTYPE, requires_grad=True)
E       RuntimeError: CUDA error: out of memory
E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.4895781Z
tests/test_fp8/test_fp8_linear.py:20: RuntimeError
_____________________________ test_reduce_scatter ______________________________
2025-04-11T04:23:18.4896048Z
args = (), kwargs = {}, try_count = 1
error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T04:23:18.4896855Z
def _run_until_success(*args, **kwargs):
try_count = 0
assert max_try is None or isinstance(
max_try, int
), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T04:23:18.4897462Z
while max_try is None or try_count < max_try:
try:
try_count += 1
ret = func(*args, **kwargs)
return ret
except exception_type as e:
error_lines = str(e).split("\n")
if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
print("Exception is caught, retrying...")
# when pattern is not specified, we always skip the exception
# when pattern is specified, we only skip when pattern is matched
continue
else:
print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
>                   raise e
2025-04-11T04:23:18.4899343Z
colossalai/testing/utils.py:144:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
colossalai/testing/utils.py:133: in _run_until_success
ret = func(*args, **kwargs)
tests/test_fp8/test_fp8_reduce_scatter.py:41: in test_reduce_scatter
spawn(run_dist, 4)
colossalai/testing/utils.py:252: in spawn
mp.spawn(wrapped_func, nprocs=nprocs)
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
while not context.join():
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.4901247Z
self = <torch.multiprocessing.spawn.ProcessContext object at 0x7fb5b84e06d0>
timeout = None
2025-04-11T04:23:18.4901546Z
def join(self, timeout=None):
r"""Join one or more processes within spawn context.
2025-04-11T04:23:18.4901847Z
Attempt to join one or more processes in this spawn context.
If one of them exited with a non-zero exit status, this function
kills the remaining processes and raises an exception with the cause
of the first process exiting.
2025-04-11T04:23:18.4902496Z
Returns ``True`` if all processes have been joined successfully,
``False`` if there are more processes that need to be joined.
2025-04-11T04:23:18.4902863Z
Args:
timeout (float): Wait this long before giving up on waiting.
"""
# Ensure this function can be called even when we're done.
if len(self.sentinels) == 0:
return True
2025-04-11T04:23:18.4903660Z
# Wait for any process to fail or all of them to succeed.
ready = multiprocessing.connection.wait(
self.sentinels.keys(),
timeout=timeout,
)
2025-04-11T04:23:18.4904254Z
error_index = None
for sentinel in ready:
index = self.sentinels.pop(sentinel)
process = self.processes[index]
process.join()
if process.exitcode != 0:
error_index = index
break
2025-04-11T04:23:18.4905184Z
# Return if there was no error.
if error_index is None:
# Return whether or not all processes have been joined.
return len(self.sentinels) == 0
2025-04-11T04:23:18.4905682Z
# Assume failure. Terminate processes that are still alive.
for process in self.processes:
if process.is_alive():
process.terminate()
process.join()
2025-04-11T04:23:18.4906273Z
# There won't be an error on the queue if the process crashed.
failed_process = self.processes[error_index]
if self.error_queues[error_index].empty():
exitcode = self.processes[error_index].exitcode
if exitcode < 0:
name = signal.Signals(-exitcode).name
raise ProcessExitedException(
"process %d terminated with signal %s" % (error_index, name),
error_index=error_index,
error_pid=failed_process.pid,
exit_code=exitcode,
signal_name=name,
)
else:
raise ProcessExitedException(
"process %d terminated with exit code %d" % (error_index, exitcode),
error_index=error_index,
error_pid=failed_process.pid,
exit_code=exitcode,
)
2025-04-11T04:23:18.4908601Z
original_trace = self.error_queues[error_index].get()
msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
msg += original_trace
>       raise ProcessRaisedException(msg, error_index, failed_process.pid)
E       torch.multiprocessing.spawn.ProcessRaisedException:
E
E       -- Process 0 terminated with the following error:
E       Traceback (most recent call last):
E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
E           fn(i, *args)
E         File "/__w/ColossalAI/ColossalAI/tests/test_fp8/test_fp8_reduce_scatter.py", line 36, in run_dist
E           check_4gpu()
E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 271, in _clear_cache
E           get_accelerator().synchronize()
E         File "/__w/ColossalAI/ColossalAI/colossalai/accelerator/cuda_accelerator.py", line 62, in synchronize
E           torch.cuda.synchronize(device)
E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py", line 801, in synchronize
E           return torch._C._cuda_synchronize()
E       RuntimeError: CUDA error: out of memory
E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.4912402Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
----------------------------- Captured stdout call -----------------------------
[04/11/25 04:14:54] INFO     colossalai - colossalai - INFO:
/__w/ColossalAI/ColossalAI/colossalai/initialize.py
:75 launch
INFO     colossalai - colossalai - INFO: Distributed
environment is initialized, world size: 4
Maximum number of attempts is reached or pattern is not matched, no more retrying...
----------------------------- Captured stderr call -----------------------------
[W socket.cpp:697] [c10d] The client socket has failed to connect to [localhost]:24751 (errno: 99 - Cannot assign requested address).
[W socket.cpp:697] [c10d] The client socket has failed to connect to [localhost]:24751 (errno: 99 - Cannot assign requested address).
[W socket.cpp:697] [c10d] The client socket has failed to connect to [localhost]:24751 (errno: 99 - Cannot assign requested address).
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:356: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:356: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:356: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:356: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
warnings.warn(
_________________________________ test_bucket __________________________________
2025-04-11T04:23:18.4928976Z
kwargs = {}
val = {'block_size': 4, 'dtype': torch.float16, 'max_batch_size': 4, 'max_input_len': 32, ...}
arg_map = {'test_config': {'block_size': 4, 'dtype': torch.float16, 'max_batch_size': 4, 'max_input_len': 32, ...}}
partial_func = functools.partial(<function test_bucket at 0x7fb5e55d57e0>, test_config={'block_size': 4, 'max_batch_size': 4, 'max_input_len': 32, 'max_output_len': 8, 'dtype': torch.float16, 'tp_size': 1})
2025-04-11T04:23:18.4931142Z
def _execute_function_by_param(**kwargs):
for val in values:
arg_map = {argument: val}
partial_func = partial(func, **arg_map)
>           partial_func(**kwargs)
2025-04-11T04:23:18.4932458Z
colossalai/testing/utils.py:64:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
tests/test_infer/test_batch_bucket.py:42: in test_bucket
cache_manager = KVCacheManager(inference_config, model_config)
colossalai/inference/kv_cache/kvcache_manager.py:105: in __init__
self._kv_caches = self._init_device_caches(alloc_shape, alloc_shape)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.4934833Z
self = <colossalai.inference.kv_cache.kvcache_manager.KVCacheManager object at 0x7fb5b8491750>
kalloc_shape = (40, 4, 4, 32), valloc_shape = (40, 4, 4, 32)
2025-04-11T04:23:18.4935714Z
def _init_device_caches(
self, kalloc_shape: Tuple[int, ...], valloc_shape: Tuple[int, ...]
) -> Tuple[torch.Tensor, torch.Tensor]:
"""Initialize the physical cache on the device.
2025-04-11T04:23:18.4936998Z
For each layer of the model, we allocate two tensors for key and value respectively,
with shape of [num_blocks, num_kv_heads, block_size, head_size]
"""
k_cache: List[torch.Tensor] = []
v_cache: List[torch.Tensor] = []
for _ in range(self.num_layers):
>           k_cache.append(torch.zeros(kalloc_shape, dtype=self.kv_cache_dtype, device=self.device))
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.4941145Z
colossalai/inference/kv_cache/kvcache_manager.py:519: RuntimeError
----------------------------- Captured stdout call -----------------------------
[04/11/25 04:14:55] INFO     colossalai -
colossalai.inference.kv_cache.kvcache_manager -
INFO:
/__w/ColossalAI/ColossalAI/colossalai/inference/kv_
cache/kvcache_manager.py:104 __init__
INFO     colossalai -
colossalai.inference.kv_cache.kvcache_manager -
INFO: Allocating KV cache with shape: (40, 4, 4,
32) consisting of 40 blocks.
___________________________ test_continuous_batching ___________________________
2025-04-11T04:23:18.4945252Z
args = (), kwargs = {}, try_count = 1
error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T04:23:18.4946800Z
def _run_until_success(*args, **kwargs):
try_count = 0
assert max_try is None or isinstance(
max_try, int
), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T04:23:18.4948248Z
while max_try is None or try_count < max_try:
try:
try_count += 1
ret = func(*args, **kwargs)
return ret
except exception_type as e:
error_lines = str(e).split("\n")
if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
print("Exception is caught, retrying...")
# when pattern is not specified, we always skip the exception
# when pattern is specified, we only skip when pattern is matched
continue
else:
print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
>                   raise e
2025-04-11T04:23:18.4953124Z
colossalai/testing/utils.py:144:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
colossalai/testing/utils.py:133: in _run_until_success
ret = func(*args, **kwargs)
tests/test_infer/test_continuous_batching.py:67: in test_continuous_batching
spawn(run_dist, 1)
colossalai/testing/utils.py:252: in spawn
mp.spawn(wrapped_func, nprocs=nprocs)
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
while not context.join():
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.4957797Z
self = <torch.multiprocessing.spawn.ProcessContext object at 0x7fb5e5782c50>
timeout = None
2025-04-11T04:23:18.4958585Z
def join(self, timeout=None):
r"""Join one or more processes within spawn context.
2025-04-11T04:23:18.4959225Z
Attempt to join one or more processes in this spawn context.
If one of them exited with a non-zero exit status, this function
kills the remaining processes and raises an exception with the cause
of the first process exiting.
2025-04-11T04:23:18.4960775Z
Returns ``True`` if all processes have been joined successfully,
``False`` if there are more processes that need to be joined.
2025-04-11T04:23:18.4961676Z
Args:
timeout (float): Wait this long before giving up on waiting.
"""
# Ensure this function can be called even when we're done.
if len(self.sentinels) == 0:
return True
2025-04-11T04:23:18.4963386Z
# Wait for any process to fail or all of them to succeed.
ready = multiprocessing.connection.wait(
self.sentinels.keys(),
timeout=timeout,
)
2025-04-11T04:23:18.4964835Z
error_index = None
for sentinel in ready:
index = self.sentinels.pop(sentinel)
process = self.processes[index]
process.join()
if process.exitcode != 0:
error_index = index
break
2025-04-11T04:23:18.4966968Z
# Return if there was no error.
if error_index is None:
# Return whether or not all processes have been joined.
return len(self.sentinels) == 0
2025-04-11T04:23:18.4968229Z
# Assume failure. Terminate processes that are still alive.
for process in self.processes:
if process.is_alive():
process.terminate()
process.join()
2025-04-11T04:23:18.4969816Z
# There won't be an error on the queue if the process crashed.
failed_process = self.processes[error_index]
if self.error_queues[error_index].empty():
exitcode = self.processes[error_index].exitcode
if exitcode < 0:
name = signal.Signals(-exitcode).name
raise ProcessExitedException(
"process %d terminated with signal %s" % (error_index, name),
error_index=error_index,
error_pid=failed_process.pid,
exit_code=exitcode,
signal_name=name,
)
else:
raise ProcessExitedException(
"process %d terminated with exit code %d" % (error_index, exitcode),
error_index=error_index,
error_pid=failed_process.pid,
exit_code=exitcode,
)
2025-04-11T04:23:18.4975602Z
original_trace = self.error_queues[error_index].get()
msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
msg += original_trace
>       raise ProcessRaisedException(msg, error_index, failed_process.pid)
E       torch.multiprocessing.spawn.ProcessRaisedException:
E
E       -- Process 0 terminated with the following error:
E       Traceback (most recent call last):
E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
E           fn(i, *args)
E         File "/__w/ColossalAI/ColossalAI/tests/test_infer/test_continuous_batching.py", line 61, in run_dist
E           check_inference_engine()
E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
E           partial_func(**kwargs)
E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
E           partial_func(**kwargs)
E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
E           partial_func(**kwargs)
E         [Previous line repeated 1 more time]
E         File "/__w/ColossalAI/ColossalAI/tests/test_infer/test_continuous_batching.py", line 39, in check_inference_engine
E           model = LlamaForCausalLM(LlamaConfig(num_hidden_layers=2)).cuda()
E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2548, in cuda
E           return super().cuda(*args, **kwargs)
E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in cuda
E           return self._apply(lambda t: t.cuda(device))
E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
E           module._apply(fn)
E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
E           module._apply(fn)
E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
E           param_applied = fn(param)
E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in <lambda>
E           return self._apply(lambda t: t.cuda(device))
E       RuntimeError: CUDA error: out of memory
E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.4991102Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
----------------------------- Captured stdout call -----------------------------
[04/11/25 04:15:02] INFO     colossalai - colossalai - INFO:
/__w/ColossalAI/ColossalAI/colossalai/initialize.py
:75 launch
INFO     colossalai - colossalai - INFO: Distributed
environment is initialized, world size: 1
Maximum number of attempts is reached or pattern is not matched, no more retrying...
----------------------------- Captured stderr call -----------------------------
/opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
/opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
warnings.warn(
_______________________________ test_drafter[5] ________________________________
2025-04-11T04:23:18.5016383Z
tokenizer = LlamaTokenizerFast(name_or_path='hf-internal-testing/llama-tokenizer', vocab_size=32000, model_max_length=2048, is_fas... special=True),
2: AddedToken("</s>", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),
}
spec_num = 5
2025-04-11T04:23:18.5018104Z
@pytest.mark.parametrize("spec_num", [SPEC_NUM])
def test_drafter(tokenizer, spec_num: int):
torch.manual_seed(123)
2025-04-11T04:23:18.5019042Z
device = get_current_device()
toy_config = LlamaConfig(num_hidden_layers=NUM_LAYERS)
toy_config.pad_token_id = tokenizer.eos_token_id
drafter_model = LlamaForCausalLM(toy_config)
>       drafter_model = drafter_model.eval().cuda()
2025-04-11T04:23:18.5020637Z
tests/test_infer/test_drafter.py:27:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
/opt/conda/envs/pytorch/lib/python3.10/site-packages/transformers/modeling_utils.py:2548: in cuda
return super().cuda(*args, **kwargs)
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:911: in cuda
return self._apply(lambda t: t.cuda(device))
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:802: in _apply
module._apply(fn)
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:802: in _apply
module._apply(fn)
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:825: in _apply
param_applied = fn(param)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.5025630Z
t = Parameter containing:
tensor([[-0.0259,  0.0026,  0.0006,  ...,  0.0104,  0.0194,  0.0062],
[-0.0076,  0.0020,...5,  0.0329,  0.0046],
[-0.0124,  0.0230, -0.0264,  ..., -0.0224, -0.0274, -0.0157]],
requires_grad=True)
2025-04-11T04:23:18.5027085Z
>   return self._apply(lambda t: t.cuda(device))
E   RuntimeError: CUDA error: out of memory
E   CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E   For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E   Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5029067Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:911: RuntimeError
________________________________ test_spec_dec _________________________________
2025-04-11T04:23:18.5029995Z
tokenizer = LlamaTokenizerFast(name_or_path='hf-internal-testing/llama-tokenizer', vocab_size=32000, model_max_length=2048, is_fas... special=True),
2: AddedToken("</s>", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),
}
2025-04-11T04:23:18.5031494Z
def test_spec_dec(tokenizer):
spec_num = SPEC_NUM
device = get_current_device()
tokenizer.pad_token = tokenizer.eos_token
2025-04-11T04:23:18.5032603Z
# Dummy config for Glide Model
glide_config = GlideLlamaConfig(
intermediate_size=8192,
large_hidden_size=4096,
large_num_attention_heads=32,
num_hidden_layers=NUM_LAYERS,
)
drafter_model = GlideLlamaForCausalLM(glide_config)
2025-04-11T04:23:18.5034815Z
assert hasattr(drafter_model, "model")
assert hasattr(drafter_model.model, "layers")
for _, layer in enumerate(drafter_model.model.layers):
assert hasattr(layer, "cross_attn")
2025-04-11T04:23:18.5036158Z
# Init the Drafter by providing the sharded drafter model
>       drafter = Drafter(drafter_model, tokenizer, device=device, dtype=torch.float16)
2025-04-11T04:23:18.5037150Z
tests/test_infer/test_drafter.py:65:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
colossalai/inference/spec/drafter.py:31: in __init__
self._drafter_model = model.to(self._device)
/opt/conda/envs/pytorch/lib/python3.10/site-packages/transformers/modeling_utils.py:2576: in to
return super().to(*args, **kwargs)
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:1152: in to
return self._apply(convert)
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:802: in _apply
module._apply(fn)
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:802: in _apply
module._apply(fn)
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:825: in _apply
param_applied = fn(param)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.5042587Z
t = Parameter containing:
tensor([[-0.0389,  0.0039, -0.0004,  ...,  0.0133,  0.0029, -0.0177],
[-0.0144,  0.0054,...4,  0.0227,  0.0264],
[ 0.0320, -0.0080,  0.0294,  ...,  0.0173,  0.0005, -0.0045]],
requires_grad=True)
2025-04-11T04:23:18.5043984Z
def convert(t):
if convert_to_format is not None and t.dim() in (4, 5):
return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None,
non_blocking, memory_format=convert_to_format)
>       return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
E       RuntimeError: CUDA error: out of memory
E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5047469Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:1150: RuntimeError
______________________________ test_cache_manager ______________________________
2025-04-11T04:23:18.5048419Z
args = (), kwargs = {}, try_count = 1
error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T04:23:18.5050001Z
def _run_until_success(*args, **kwargs):
try_count = 0
assert max_try is None or isinstance(
max_try, int
), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T04:23:18.5051466Z
while max_try is None or try_count < max_try:
try:
try_count += 1
ret = func(*args, **kwargs)
return ret
except exception_type as e:
error_lines = str(e).split("\n")
if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
print("Exception is caught, retrying...")
# when pattern is not specified, we always skip the exception
# when pattern is specified, we only skip when pattern is matched
continue
else:
print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
>                   raise e
2025-04-11T04:23:18.5056031Z
colossalai/testing/utils.py:144:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
colossalai/testing/utils.py:133: in _run_until_success
ret = func(*args, **kwargs)
tests/test_infer/test_kvcache_manager.py:174: in test_cache_manager
spawn(run_dist, 1)
colossalai/testing/utils.py:252: in spawn
mp.spawn(wrapped_func, nprocs=nprocs)
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
while not context.join():
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.5060578Z
self = <torch.multiprocessing.spawn.ProcessContext object at 0x7fb5b840c1f0>
timeout = None
2025-04-11T04:23:18.5061244Z
def join(self, timeout=None):
r"""Join one or more processes within spawn context.
2025-04-11T04:23:18.5061882Z
Attempt to join one or more processes in this spawn context.
If one of them exited with a non-zero exit status, this function
kills the remaining processes and raises an exception with the cause
of the first process exiting.
2025-04-11T04:23:18.5063416Z
Returns ``True`` if all processes have been joined successfully,
``False`` if there are more processes that need to be joined.
2025-04-11T04:23:18.5064299Z
Args:
timeout (float): Wait this long before giving up on waiting.
"""
# Ensure this function can be called even when we're done.
if len(self.sentinels) == 0:
return True
2025-04-11T04:23:18.5065974Z
# Wait for any process to fail or all of them to succeed.
ready = multiprocessing.connection.wait(
self.sentinels.keys(),
timeout=timeout,
)
2025-04-11T04:23:18.5067435Z
error_index = None
for sentinel in ready:
index = self.sentinels.pop(sentinel)
process = self.processes[index]
process.join()
if process.exitcode != 0:
error_index = index
break
2025-04-11T04:23:18.5069681Z
# Return if there was no error.
if error_index is None:
# Return whether or not all processes have been joined.
return len(self.sentinels) == 0
2025-04-11T04:23:18.5070933Z
# Assume failure. Terminate processes that are still alive.
for process in self.processes:
if process.is_alive():
process.terminate()
process.join()
2025-04-11T04:23:18.5072413Z
# There won't be an error on the queue if the process crashed.
failed_process = self.processes[error_index]
if self.error_queues[error_index].empty():
exitcode = self.processes[error_index].exitcode
if exitcode < 0:
name = signal.Signals(-exitcode).name
raise ProcessExitedException(
"process %d terminated with signal %s" % (error_index, name),
error_index=error_index,
error_pid=failed_process.pid,
exit_code=exitcode,
signal_name=name,
)
else:
raise ProcessExitedException(
"process %d terminated with exit code %d" % (error_index, exitcode),
error_index=error_index,
error_pid=failed_process.pid,
exit_code=exitcode,
)
2025-04-11T04:23:18.5078163Z
original_trace = self.error_queues[error_index].get()
msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
msg += original_trace
>       raise ProcessRaisedException(msg, error_index, failed_process.pid)
E       torch.multiprocessing.spawn.ProcessRaisedException:
E
E       -- Process 0 terminated with the following error:
E       Traceback (most recent call last):
E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
E           fn(i, *args)
E         File "/__w/ColossalAI/ColossalAI/tests/test_infer/test_kvcache_manager.py", line 168, in run_dist
E           check_cache_manager()
E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
E           partial_func(**kwargs)
E         File "/__w/ColossalAI/ColossalAI/tests/test_infer/test_kvcache_manager.py", line 89, in check_cache_manager
E           cache_manager = KVCacheManager(inference_config, model_config)
E         File "/__w/ColossalAI/ColossalAI/colossalai/inference/kv_cache/kvcache_manager.py", line 105, in __init__
E           self._kv_caches = self._init_device_caches(alloc_shape, alloc_shape)
E         File "/__w/ColossalAI/ColossalAI/colossalai/inference/kv_cache/kvcache_manager.py", line 519, in _init_device_caches
E           k_cache.append(torch.zeros(kalloc_shape, dtype=self.kv_cache_dtype, device=self.device))
E       RuntimeError: CUDA error: out of memory
E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5122750Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
----------------------------- Captured stdout call -----------------------------
[04/11/25 04:15:28] INFO     colossalai - colossalai - INFO:
/__w/ColossalAI/ColossalAI/colossalai/initialize.py
:75 launch
INFO     colossalai - colossalai - INFO: Distributed
environment is initialized, world size: 1
[04/11/25 04:15:28] INFO     colossalai -
colossalai.inference.kv_cache.kvcache_manager -
INFO:
/__w/ColossalAI/ColossalAI/colossalai/inference/kv_
cache/kvcache_manager.py:104 __init__
INFO     colossalai -
colossalai.inference.kv_cache.kvcache_manager -
INFO: Allocating KV cache with shape: (80, 16, 8,
32) consisting of 80 blocks.
Maximum number of attempts is reached or pattern is not matched, no more retrying...
----------------------------- Captured stderr call -----------------------------
/opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
____________________ test_running_list_and_request_handler _____________________
2025-04-11T04:23:18.5132432Z
args = (), kwargs = {}, try_count = 1
error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T04:23:18.5134068Z
def _run_until_success(*args, **kwargs):
try_count = 0
assert max_try is None or isinstance(
max_try, int
), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T04:23:18.5135479Z
while max_try is None or try_count < max_try:
try:
try_count += 1
ret = func(*args, **kwargs)
return ret
except exception_type as e:
error_lines = str(e).split("\n")
if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
print("Exception is caught, retrying...")
# when pattern is not specified, we always skip the exception
# when pattern is specified, we only skip when pattern is matched
continue
else:
print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
>                   raise e
2025-04-11T04:23:18.5138491Z
colossalai/testing/utils.py:144:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
colossalai/testing/utils.py:133: in _run_until_success
ret = func(*args, **kwargs)
tests/test_infer/test_request_handler.py:101: in test_running_list_and_request_handler
spawn(run_dist, 1)
colossalai/testing/utils.py:252: in spawn
mp.spawn(wrapped_func, nprocs=nprocs)
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
while not context.join():
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.5140402Z
self = <torch.multiprocessing.spawn.ProcessContext object at 0x7fb5b84e17b0>
timeout = None
2025-04-11T04:23:18.5140700Z
def join(self, timeout=None):
r"""Join one or more processes within spawn context.
2025-04-11T04:23:18.5140995Z
Attempt to join one or more processes in this spawn context.
If one of them exited with a non-zero exit status, this function
kills the remaining processes and raises an exception with the cause
of the first process exiting.
2025-04-11T04:23:18.5141621Z
Returns ``True`` if all processes have been joined successfully,
``False`` if there are more processes that need to be joined.
2025-04-11T04:23:18.5141981Z
Args:
timeout (float): Wait this long before giving up on waiting.
"""
# Ensure this function can be called even when we're done.
if len(self.sentinels) == 0:
return True
2025-04-11T04:23:18.5142768Z
# Wait for any process to fail or all of them to succeed.
ready = multiprocessing.connection.wait(
self.sentinels.keys(),
timeout=timeout,
)
2025-04-11T04:23:18.5143346Z
error_index = None
for sentinel in ready:
index = self.sentinels.pop(sentinel)
process = self.processes[index]
process.join()
if process.exitcode != 0:
error_index = index
break
2025-04-11T04:23:18.5144266Z
# Return if there was no error.
if error_index is None:
# Return whether or not all processes have been joined.
return len(self.sentinels) == 0
2025-04-11T04:23:18.5144762Z
# Assume failure. Terminate processes that are still alive.
for process in self.processes:
if process.is_alive():
process.terminate()
process.join()
2025-04-11T04:23:18.5145344Z
# There won't be an error on the queue if the process crashed.
failed_process = self.processes[error_index]
if self.error_queues[error_index].empty():
exitcode = self.processes[error_index].exitcode
if exitcode < 0:
name = signal.Signals(-exitcode).name
raise ProcessExitedException(
"process %d terminated with signal %s" % (error_index, name),
error_index=error_index,
error_pid=failed_process.pid,
exit_code=exitcode,
signal_name=name,
)
else:
raise ProcessExitedException(
"process %d terminated with exit code %d" % (error_index, exitcode),
error_index=error_index,
error_pid=failed_process.pid,
exit_code=exitcode,
)
2025-04-11T04:23:18.5147560Z
original_trace = self.error_queues[error_index].get()
msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
msg += original_trace
>       raise ProcessRaisedException(msg, error_index, failed_process.pid)
E       torch.multiprocessing.spawn.ProcessRaisedException:
E
E       -- Process 0 terminated with the following error:
E       Traceback (most recent call last):
E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
E           fn(i, *args)
E         File "/__w/ColossalAI/ColossalAI/tests/test_infer/test_request_handler.py", line 95, in run_dist
E           check_request_handler()
E         File "/__w/ColossalAI/ColossalAI/tests/test_infer/test_request_handler.py", line 70, in check_request_handler
E           request_handler = RequestHandler(inference_config, model_config)
E         File "/__w/ColossalAI/ColossalAI/colossalai/inference/core/request_handler.py", line 160, in __init__
E           self._init_cache(model_config)
E         File "/__w/ColossalAI/ColossalAI/colossalai/inference/core/request_handler.py", line 222, in _init_cache
E           self.cache_manager = KVCacheManager(self.inference_config, model_config)
E         File "/__w/ColossalAI/ColossalAI/colossalai/inference/kv_cache/kvcache_manager.py", line 105, in __init__
E           self._kv_caches = self._init_device_caches(alloc_shape, alloc_shape)
E         File "/__w/ColossalAI/ColossalAI/colossalai/inference/kv_cache/kvcache_manager.py", line 519, in _init_device_caches
E           k_cache.append(torch.zeros(kalloc_shape, dtype=self.kv_cache_dtype, device=self.device))
E       RuntimeError: CUDA error: out of memory
E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5152480Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
----------------------------- Captured stdout call -----------------------------
[04/11/25 04:15:32] INFO     colossalai - colossalai - INFO:
/__w/ColossalAI/ColossalAI/colossalai/initialize.py
:75 launch
INFO     colossalai - colossalai - INFO: Distributed
environment is initialized, world size: 1
[04/11/25 04:15:32] INFO     colossalai -
colossalai.inference.kv_cache.kvcache_manager -
INFO:
/__w/ColossalAI/ColossalAI/colossalai/inference/kv_
cache/kvcache_manager.py:104 __init__
INFO     colossalai -
colossalai.inference.kv_cache.kvcache_manager -
INFO: Allocating KV cache with shape: (24, 4, 8, 8)
consisting of 24 blocks.
Maximum number of attempts is reached or pattern is not matched, no more retrying...
----------------------------- Captured stderr call -----------------------------
/opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
_________________________________ test_engine __________________________________
2025-04-11T04:23:18.5156590Z
args = (), kwargs = {}, try_count = 1
error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T04:23:18.5157377Z
def _run_until_success(*args, **kwargs):
try_count = 0
assert max_try is None or isinstance(
max_try, int
), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T04:23:18.5157965Z
while max_try is None or try_count < max_try:
try:
try_count += 1
ret = func(*args, **kwargs)
return ret
except exception_type as e:
error_lines = str(e).split("\n")
if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
print("Exception is caught, retrying...")
# when pattern is not specified, we always skip the exception
# when pattern is specified, we only skip when pattern is matched
continue
else:
print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
>                   raise e
2025-04-11T04:23:18.5159719Z
colossalai/testing/utils.py:144:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
colossalai/testing/utils.py:133: in _run_until_success
ret = func(*args, **kwargs)
tests/test_infer/test_streamingllm.py:117: in test_engine
spawn(run_dist, 1, func_to_run=check_streamingllm, ret=result_list)
colossalai/testing/utils.py:252: in spawn
mp.spawn(wrapped_func, nprocs=nprocs)
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
while not context.join():
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.5161567Z
self = <torch.multiprocessing.spawn.ProcessContext object at 0x7fb5b8477e50>
timeout = None
2025-04-11T04:23:18.5161951Z
def join(self, timeout=None):
r"""Join one or more processes within spawn context.
2025-04-11T04:23:18.5162249Z
Attempt to join one or more processes in this spawn context.
If one of them exited with a non-zero exit status, this function
kills the remaining processes and raises an exception with the cause
of the first process exiting.
2025-04-11T04:23:18.5162872Z
Returns ``True`` if all processes have been joined successfully,
``False`` if there are more processes that need to be joined.
2025-04-11T04:23:18.5163229Z
Args:
timeout (float): Wait this long before giving up on waiting.
"""
# Ensure this function can be called even when we're done.
if len(self.sentinels) == 0:
return True
2025-04-11T04:23:18.5163999Z
# Wait for any process to fail or all of them to succeed.
ready = multiprocessing.connection.wait(
self.sentinels.keys(),
timeout=timeout,
)
2025-04-11T04:23:18.5164578Z
error_index = None
for sentinel in ready:
index = self.sentinels.pop(sentinel)
process = self.processes[index]
process.join()
if process.exitcode != 0:
error_index = index
break
2025-04-11T04:23:18.5165377Z
# Return if there was no error.
if error_index is None:
# Return whether or not all processes have been joined.
return len(self.sentinels) == 0
2025-04-11T04:23:18.5165862Z
# Assume failure. Terminate processes that are still alive.
for process in self.processes:
if process.is_alive():
process.terminate()
process.join()
2025-04-11T04:23:18.5166442Z
# There won't be an error on the queue if the process crashed.
failed_process = self.processes[error_index]
if self.error_queues[error_index].empty():
exitcode = self.processes[error_index].exitcode
if exitcode < 0:
name = signal.Signals(-exitcode).name
raise ProcessExitedException(
"process %d terminated with signal %s" % (error_index, name),
error_index=error_index,
error_pid=failed_process.pid,
exit_code=exitcode,
signal_name=name,
)
else:
raise ProcessExitedException(
"process %d terminated with exit code %d" % (error_index, exitcode),
error_index=error_index,
error_pid=failed_process.pid,
exit_code=exitcode,
)
2025-04-11T04:23:18.5168693Z
original_trace = self.error_queues[error_index].get()
msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
msg += original_trace
>       raise ProcessRaisedException(msg, error_index, failed_process.pid)
E       torch.multiprocessing.spawn.ProcessRaisedException:
E
E       -- Process 0 terminated with the following error:
E       Traceback (most recent call last):
E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
E           fn(i, *args)
E         File "/__w/ColossalAI/ColossalAI/tests/test_infer/test_streamingllm.py", line 107, in run_dist
E           ret[rank] = func_to_run(**kwargs)
E         File "/__w/ColossalAI/ColossalAI/tests/test_infer/test_streamingllm.py", line 39, in check_streamingllm
E           ).cuda()
E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2548, in cuda
E           return super().cuda(*args, **kwargs)
E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in cuda
E           return self._apply(lambda t: t.cuda(device))
E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
E           module._apply(fn)
E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
E           module._apply(fn)
E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
E           param_applied = fn(param)
E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in <lambda>
E           return self._apply(lambda t: t.cuda(device))
E       RuntimeError: CUDA error: out of memory
E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5173913Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
----------------------------- Captured stdout call -----------------------------
[04/11/25 04:15:36] INFO     colossalai - colossalai - INFO:
/__w/ColossalAI/ColossalAI/colossalai/initialize.py
:75 launch
INFO     colossalai - colossalai - INFO: Distributed
environment is initialized, world size: 1
Maximum number of attempts is reached or pattern is not matched, no more retrying...
----------------------------- Captured stderr call -----------------------------
/opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
/opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
warnings.warn(
________________________ test_flash_decoding_attention _________________________
2025-04-11T04:23:18.5178756Z
args = (), kwargs = {}
2025-04-11T04:23:18.5178843Z
def _clear_cache(*args, **kwargs):
get_accelerator().empty_cache()
get_accelerator().reset_peak_memory_stats()
get_accelerator().reset_max_memory_allocated()
get_accelerator().reset_max_memory_cached()
>       get_accelerator().synchronize()
2025-04-11T04:23:18.5179488Z
colossalai/testing/utils.py:271:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
colossalai/accelerator/cuda_accelerator.py:62: in synchronize
torch.cuda.synchronize(device)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.5180075Z
device = None
2025-04-11T04:23:18.5180158Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.5180523Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5182240Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
______________________ test_vllm_flash_decoding_attention ______________________
2025-04-11T04:23:18.5182633Z
args = (), kwargs = {}
2025-04-11T04:23:18.5182725Z
def _clear_cache(*args, **kwargs):
get_accelerator().empty_cache()
get_accelerator().reset_peak_memory_stats()
get_accelerator().reset_max_memory_allocated()
get_accelerator().reset_max_memory_cached()
>       get_accelerator().synchronize()
2025-04-11T04:23:18.5183336Z
colossalai/testing/utils.py:271:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
colossalai/accelerator/cuda_accelerator.py:62: in synchronize
torch.cuda.synchronize(device)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.5184002Z
device = None
2025-04-11T04:23:18.5184085Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.5184430Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5186081Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
_____________________ test_get_cos_and_sin[dtype0-64-64-4] _____________________
2025-04-11T04:23:18.5186491Z
BATCH_SIZE = 4, MAX_SEQ_LEN = 64, HEAD_DIM = 64, dtype = torch.float16
2025-04-11T04:23:18.5186659Z
@pytest.mark.parametrize("BATCH_SIZE", [4])
@pytest.mark.parametrize("MAX_SEQ_LEN", [64])
@pytest.mark.parametrize("HEAD_DIM", [64])
@pytest.mark.parametrize("dtype", [torch.float16, torch.float32])
def test_get_cos_and_sin(BATCH_SIZE, MAX_SEQ_LEN, HEAD_DIM, dtype):
MAX_TOTAL_TOKENS = BATCH_SIZE * MAX_SEQ_LEN
>       cos_cache = torch.randn((MAX_TOTAL_TOKENS, HEAD_DIM), dtype=dtype, device="cuda")
E       RuntimeError: CUDA error: out of memory
E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5188402Z
tests/test_infer/test_kernels/cuda/test_get_cos_and_sin.py:24: RuntimeError
_____________________ test_get_cos_and_sin[dtype1-64-64-4] _____________________
2025-04-11T04:23:18.5188791Z
BATCH_SIZE = 4, MAX_SEQ_LEN = 64, HEAD_DIM = 64, dtype = torch.float32
2025-04-11T04:23:18.5188939Z
@pytest.mark.parametrize("BATCH_SIZE", [4])
@pytest.mark.parametrize("MAX_SEQ_LEN", [64])
@pytest.mark.parametrize("HEAD_DIM", [64])
@pytest.mark.parametrize("dtype", [torch.float16, torch.float32])
def test_get_cos_and_sin(BATCH_SIZE, MAX_SEQ_LEN, HEAD_DIM, dtype):
MAX_TOTAL_TOKENS = BATCH_SIZE * MAX_SEQ_LEN
>       cos_cache = torch.randn((MAX_TOTAL_TOKENS, HEAD_DIM), dtype=dtype, device="cuda")
E       RuntimeError: CUDA error: out of memory
E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5190784Z
tests/test_infer/test_kernels/cuda/test_get_cos_and_sin.py:24: RuntimeError
____________________ test_kv_cache_memcopy[True-16-8-16-4] _____________________
2025-04-11T04:23:18.5191114Z
bsz = 4, block_size = 16, max_num_blocks_per_seq = 8, num_kv_heads = 16
same_context_len = True
2025-04-11T04:23:18.5191362Z
@pytest.mark.parametrize("bsz", [4, 7, 32])
@pytest.mark.parametrize("block_size", [16, 32, 64])
@pytest.mark.parametrize("max_num_blocks_per_seq", [8, 32])
@pytest.mark.parametrize("num_kv_heads", [16])
@pytest.mark.parametrize("same_context_len", [True, False])
def test_kv_cache_memcopy(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_kv_heads: int,
same_context_len: bool,
):
>       run_context_copy_kv_to_cache(bsz, block_size, max_num_blocks_per_seq, num_kv_heads, same_context_len)
2025-04-11T04:23:18.5192844Z
tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:152:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.5193123Z
bsz = 4, block_size = 16, max_num_blocks_per_seq = 8, num_kv_heads = 16
same_context_len = True
2025-04-11T04:23:18.5193363Z
def run_context_copy_kv_to_cache(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_kv_heads: int,
same_context_len: bool,
):
torch.manual_seed(123)
2025-04-11T04:23:18.5194121Z
assert isinstance(num_kv_heads, int) and num_kv_heads > 0, "Invalid number of kv heads."
max_seq_len = max_num_blocks_per_seq * block_size
dtype = torch.float16
device = get_current_device()
2025-04-11T04:23:18.5194825Z
if same_context_len:
>           context_lengths = torch.tensor([max_seq_len for _ in range(bsz)], dtype=torch.int32, device=device)
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5195865Z
tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:116: RuntimeError
____________________ test_kv_cache_memcopy[True-16-8-16-7] _____________________
2025-04-11T04:23:18.5196303Z
bsz = 7, block_size = 16, max_num_blocks_per_seq = 8, num_kv_heads = 16
same_context_len = True
2025-04-11T04:23:18.5196544Z
@pytest.mark.parametrize("bsz", [4, 7, 32])
@pytest.mark.parametrize("block_size", [16, 32, 64])
@pytest.mark.parametrize("max_num_blocks_per_seq", [8, 32])
@pytest.mark.parametrize("num_kv_heads", [16])
@pytest.mark.parametrize("same_context_len", [True, False])
def test_kv_cache_memcopy(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_kv_heads: int,
same_context_len: bool,
):
>       run_context_copy_kv_to_cache(bsz, block_size, max_num_blocks_per_seq, num_kv_heads, same_context_len)
2025-04-11T04:23:18.5198005Z
tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:152:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.5198269Z
bsz = 7, block_size = 16, max_num_blocks_per_seq = 8, num_kv_heads = 16
same_context_len = True
2025-04-11T04:23:18.5198505Z
def run_context_copy_kv_to_cache(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_kv_heads: int,
same_context_len: bool,
):
torch.manual_seed(123)
2025-04-11T04:23:18.5199259Z
assert isinstance(num_kv_heads, int) and num_kv_heads > 0, "Invalid number of kv heads."
max_seq_len = max_num_blocks_per_seq * block_size
dtype = torch.float16
device = get_current_device()
2025-04-11T04:23:18.5199841Z
if same_context_len:
>           context_lengths = torch.tensor([max_seq_len for _ in range(bsz)], dtype=torch.int32, device=device)
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5200948Z
tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:116: RuntimeError
____________________ test_kv_cache_memcopy[True-16-8-16-32] ____________________
2025-04-11T04:23:18.5201280Z
bsz = 32, block_size = 16, max_num_blocks_per_seq = 8, num_kv_heads = 16
same_context_len = True
2025-04-11T04:23:18.5201521Z
@pytest.mark.parametrize("bsz", [4, 7, 32])
@pytest.mark.parametrize("block_size", [16, 32, 64])
@pytest.mark.parametrize("max_num_blocks_per_seq", [8, 32])
@pytest.mark.parametrize("num_kv_heads", [16])
@pytest.mark.parametrize("same_context_len", [True, False])
def test_kv_cache_memcopy(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_kv_heads: int,
same_context_len: bool,
):
>       run_context_copy_kv_to_cache(bsz, block_size, max_num_blocks_per_seq, num_kv_heads, same_context_len)
2025-04-11T04:23:18.5203055Z
tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:152:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.5203322Z
bsz = 32, block_size = 16, max_num_blocks_per_seq = 8, num_kv_heads = 16
same_context_len = True
2025-04-11T04:23:18.5203559Z
def run_context_copy_kv_to_cache(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_kv_heads: int,
same_context_len: bool,
):
torch.manual_seed(123)
2025-04-11T04:23:18.5204310Z
assert isinstance(num_kv_heads, int) and num_kv_heads > 0, "Invalid number of kv heads."
max_seq_len = max_num_blocks_per_seq * block_size
dtype = torch.float16
device = get_current_device()
2025-04-11T04:23:18.5204894Z
if same_context_len:
>           context_lengths = torch.tensor([max_seq_len for _ in range(bsz)], dtype=torch.int32, device=device)
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5205910Z
tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:116: RuntimeError
____________________ test_kv_cache_memcopy[True-16-8-32-4] _____________________
2025-04-11T04:23:18.5206243Z
bsz = 4, block_size = 32, max_num_blocks_per_seq = 8, num_kv_heads = 16
same_context_len = True
2025-04-11T04:23:18.5206481Z
@pytest.mark.parametrize("bsz", [4, 7, 32])
@pytest.mark.parametrize("block_size", [16, 32, 64])
@pytest.mark.parametrize("max_num_blocks_per_seq", [8, 32])
@pytest.mark.parametrize("num_kv_heads", [16])
@pytest.mark.parametrize("same_context_len", [True, False])
def test_kv_cache_memcopy(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_kv_heads: int,
same_context_len: bool,
):
>       run_context_copy_kv_to_cache(bsz, block_size, max_num_blocks_per_seq, num_kv_heads, same_context_len)
2025-04-11T04:23:18.5208014Z
tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:152:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.5208289Z
bsz = 4, block_size = 32, max_num_blocks_per_seq = 8, num_kv_heads = 16
same_context_len = True
2025-04-11T04:23:18.5208604Z
def run_context_copy_kv_to_cache(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_kv_heads: int,
same_context_len: bool,
):
torch.manual_seed(123)
2025-04-11T04:23:18.5209357Z
assert isinstance(num_kv_heads, int) and num_kv_heads > 0, "Invalid number of kv heads."
max_seq_len = max_num_blocks_per_seq * block_size
dtype = torch.float16
device = get_current_device()
2025-04-11T04:23:18.5209937Z
if same_context_len:
>           context_lengths = torch.tensor([max_seq_len for _ in range(bsz)], dtype=torch.int32, device=device)
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5210953Z
tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:116: RuntimeError
____________________ test_kv_cache_memcopy[True-16-8-32-7] _____________________
2025-04-11T04:23:18.5211285Z
bsz = 7, block_size = 32, max_num_blocks_per_seq = 8, num_kv_heads = 16
same_context_len = True
2025-04-11T04:23:18.5211515Z
@pytest.mark.parametrize("bsz", [4, 7, 32])
@pytest.mark.parametrize("block_size", [16, 32, 64])
@pytest.mark.parametrize("max_num_blocks_per_seq", [8, 32])
@pytest.mark.parametrize("num_kv_heads", [16])
@pytest.mark.parametrize("same_context_len", [True, False])
def test_kv_cache_memcopy(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_kv_heads: int,
same_context_len: bool,
):
>       run_context_copy_kv_to_cache(bsz, block_size, max_num_blocks_per_seq, num_kv_heads, same_context_len)
2025-04-11T04:23:18.5213052Z
tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:152:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.5213321Z
bsz = 7, block_size = 32, max_num_blocks_per_seq = 8, num_kv_heads = 16
same_context_len = True
2025-04-11T04:23:18.5213552Z
def run_context_copy_kv_to_cache(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_kv_heads: int,
same_context_len: bool,
):
torch.manual_seed(123)
2025-04-11T04:23:18.5214293Z
assert isinstance(num_kv_heads, int) and num_kv_heads > 0, "Invalid number of kv heads."
max_seq_len = max_num_blocks_per_seq * block_size
dtype = torch.float16
device = get_current_device()
2025-04-11T04:23:18.5214962Z
if same_context_len:
>           context_lengths = torch.tensor([max_seq_len for _ in range(bsz)], dtype=torch.int32, device=device)
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5215984Z
tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:116: RuntimeError
____________________ test_kv_cache_memcopy[True-16-8-32-32] ____________________
2025-04-11T04:23:18.5216318Z
bsz = 32, block_size = 32, max_num_blocks_per_seq = 8, num_kv_heads = 16
same_context_len = True
2025-04-11T04:23:18.5216560Z
@pytest.mark.parametrize("bsz", [4, 7, 32])
@pytest.mark.parametrize("block_size", [16, 32, 64])
@pytest.mark.parametrize("max_num_blocks_per_seq", [8, 32])
@pytest.mark.parametrize("num_kv_heads", [16])
@pytest.mark.parametrize("same_context_len", [True, False])
def test_kv_cache_memcopy(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_kv_heads: int,
same_context_len: bool,
):
>       run_context_copy_kv_to_cache(bsz, block_size, max_num_blocks_per_seq, num_kv_heads, same_context_len)
2025-04-11T04:23:18.5218036Z
tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:152:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.5218313Z
bsz = 32, block_size = 32, max_num_blocks_per_seq = 8, num_kv_heads = 16
same_context_len = True
2025-04-11T04:23:18.5218546Z
def run_context_copy_kv_to_cache(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_kv_heads: int,
same_context_len: bool,
):
torch.manual_seed(123)
2025-04-11T04:23:18.5219427Z
assert isinstance(num_kv_heads, int) and num_kv_heads > 0, "Invalid number of kv heads."
max_seq_len = max_num_blocks_per_seq * block_size
dtype = torch.float16
device = get_current_device()
2025-04-11T04:23:18.5220007Z
if same_context_len:
>           context_lengths = torch.tensor([max_seq_len for _ in range(bsz)], dtype=torch.int32, device=device)
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5221125Z
tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:116: RuntimeError
____________________ test_kv_cache_memcopy[True-16-8-64-4] _____________________
2025-04-11T04:23:18.5221458Z
bsz = 4, block_size = 64, max_num_blocks_per_seq = 8, num_kv_heads = 16
same_context_len = True
2025-04-11T04:23:18.5221688Z
@pytest.mark.parametrize("bsz", [4, 7, 32])
@pytest.mark.parametrize("block_size", [16, 32, 64])
@pytest.mark.parametrize("max_num_blocks_per_seq", [8, 32])
@pytest.mark.parametrize("num_kv_heads", [16])
@pytest.mark.parametrize("same_context_len", [True, False])
def test_kv_cache_memcopy(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_kv_heads: int,
same_context_len: bool,
):
>       run_context_copy_kv_to_cache(bsz, block_size, max_num_blocks_per_seq, num_kv_heads, same_context_len)
2025-04-11T04:23:18.5223142Z
tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:152:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.5223414Z
bsz = 4, block_size = 64, max_num_blocks_per_seq = 8, num_kv_heads = 16
same_context_len = True
2025-04-11T04:23:18.5223650Z
def run_context_copy_kv_to_cache(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_kv_heads: int,
same_context_len: bool,
):
torch.manual_seed(123)
2025-04-11T04:23:18.5224426Z
assert isinstance(num_kv_heads, int) and num_kv_heads > 0, "Invalid number of kv heads."
max_seq_len = max_num_blocks_per_seq * block_size
dtype = torch.float16
device = get_current_device()
2025-04-11T04:23:18.5225087Z
if same_context_len:
>           context_lengths = torch.tensor([max_seq_len for _ in range(bsz)], dtype=torch.int32, device=device)
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5226111Z
tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:116: RuntimeError
____________________ test_kv_cache_memcopy[True-16-8-64-7] _____________________
2025-04-11T04:23:18.5226441Z
bsz = 7, block_size = 64, max_num_blocks_per_seq = 8, num_kv_heads = 16
same_context_len = True
2025-04-11T04:23:18.5226680Z
@pytest.mark.parametrize("bsz", [4, 7, 32])
@pytest.mark.parametrize("block_size", [16, 32, 64])
@pytest.mark.parametrize("max_num_blocks_per_seq", [8, 32])
@pytest.mark.parametrize("num_kv_heads", [16])
@pytest.mark.parametrize("same_context_len", [True, False])
def test_kv_cache_memcopy(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_kv_heads: int,
same_context_len: bool,
):
>       run_context_copy_kv_to_cache(bsz, block_size, max_num_blocks_per_seq, num_kv_heads, same_context_len)
2025-04-11T04:23:18.5228196Z
tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:152:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.5228513Z
bsz = 7, block_size = 64, max_num_blocks_per_seq = 8, num_kv_heads = 16
same_context_len = True
2025-04-11T04:23:18.5228750Z
def run_context_copy_kv_to_cache(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_kv_heads: int,
same_context_len: bool,
):
torch.manual_seed(123)
2025-04-11T04:23:18.5229514Z
assert isinstance(num_kv_heads, int) and num_kv_heads > 0, "Invalid number of kv heads."
max_seq_len = max_num_blocks_per_seq * block_size
dtype = torch.float16
device = get_current_device()
2025-04-11T04:23:18.5230108Z
if same_context_len:
>           context_lengths = torch.tensor([max_seq_len for _ in range(bsz)], dtype=torch.int32, device=device)
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5231121Z
tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:116: RuntimeError
____________________ test_kv_cache_memcopy[True-16-8-64-32] ____________________
2025-04-11T04:23:18.5231547Z
bsz = 32, block_size = 64, max_num_blocks_per_seq = 8, num_kv_heads = 16
same_context_len = True
2025-04-11T04:23:18.5231796Z
@pytest.mark.parametrize("bsz", [4, 7, 32])
@pytest.mark.parametrize("block_size", [16, 32, 64])
@pytest.mark.parametrize("max_num_blocks_per_seq", [8, 32])
@pytest.mark.parametrize("num_kv_heads", [16])
@pytest.mark.parametrize("same_context_len", [True, False])
def test_kv_cache_memcopy(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_kv_heads: int,
same_context_len: bool,
):
>       run_context_copy_kv_to_cache(bsz, block_size, max_num_blocks_per_seq, num_kv_heads, same_context_len)
2025-04-11T04:23:18.5233355Z
tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:152:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.5233632Z
bsz = 32, block_size = 64, max_num_blocks_per_seq = 8, num_kv_heads = 16
same_context_len = True
2025-04-11T04:23:18.5233897Z
def run_context_copy_kv_to_cache(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_kv_heads: int,
same_context_len: bool,
):
torch.manual_seed(123)
2025-04-11T04:23:18.5234683Z
assert isinstance(num_kv_heads, int) and num_kv_heads > 0, "Invalid number of kv heads."
max_seq_len = max_num_blocks_per_seq * block_size
dtype = torch.float16
device = get_current_device()
2025-04-11T04:23:18.5235261Z
if same_context_len:
>           context_lengths = torch.tensor([max_seq_len for _ in range(bsz)], dtype=torch.int32, device=device)
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5236280Z
tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:116: RuntimeError
____________________ test_kv_cache_memcopy[True-16-32-16-4] ____________________
2025-04-11T04:23:18.5236611Z
bsz = 4, block_size = 16, max_num_blocks_per_seq = 32, num_kv_heads = 16
same_context_len = True
2025-04-11T04:23:18.5236850Z
@pytest.mark.parametrize("bsz", [4, 7, 32])
@pytest.mark.parametrize("block_size", [16, 32, 64])
@pytest.mark.parametrize("max_num_blocks_per_seq", [8, 32])
@pytest.mark.parametrize("num_kv_heads", [16])
@pytest.mark.parametrize("same_context_len", [True, False])
def test_kv_cache_memcopy(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_kv_heads: int,
same_context_len: bool,
):
>       run_context_copy_kv_to_cache(bsz, block_size, max_num_blocks_per_seq, num_kv_heads, same_context_len)
2025-04-11T04:23:18.5238380Z
tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:152:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.5238646Z
bsz = 4, block_size = 16, max_num_blocks_per_seq = 32, num_kv_heads = 16
same_context_len = True
2025-04-11T04:23:18.5238885Z
def run_context_copy_kv_to_cache(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_kv_heads: int,
same_context_len: bool,
):
torch.manual_seed(123)
2025-04-11T04:23:18.5239756Z
assert isinstance(num_kv_heads, int) and num_kv_heads > 0, "Invalid number of kv heads."
max_seq_len = max_num_blocks_per_seq * block_size
dtype = torch.float16
device = get_current_device()
2025-04-11T04:23:18.5240327Z
if same_context_len:
>           context_lengths = torch.tensor([max_seq_len for _ in range(bsz)], dtype=torch.int32, device=device)
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5241335Z
tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:116: RuntimeError
____________________ test_kv_cache_memcopy[True-16-32-16-7] ____________________
2025-04-11T04:23:18.5241670Z
bsz = 7, block_size = 16, max_num_blocks_per_seq = 32, num_kv_heads = 16
same_context_len = True
2025-04-11T04:23:18.5241911Z
@pytest.mark.parametrize("bsz", [4, 7, 32])
@pytest.mark.parametrize("block_size", [16, 32, 64])
@pytest.mark.parametrize("max_num_blocks_per_seq", [8, 32])
@pytest.mark.parametrize("num_kv_heads", [16])
@pytest.mark.parametrize("same_context_len", [True, False])
def test_kv_cache_memcopy(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_kv_heads: int,
same_context_len: bool,
):
>       run_context_copy_kv_to_cache(bsz, block_size, max_num_blocks_per_seq, num_kv_heads, same_context_len)
2025-04-11T04:23:18.5243384Z
tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:152:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.5243808Z
bsz = 7, block_size = 16, max_num_blocks_per_seq = 32, num_kv_heads = 16
same_context_len = True
2025-04-11T04:23:18.5244047Z
def run_context_copy_kv_to_cache(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_kv_heads: int,
same_context_len: bool,
):
torch.manual_seed(123)
2025-04-11T04:23:18.5244796Z
assert isinstance(num_kv_heads, int) and num_kv_heads > 0, "Invalid number of kv heads."
max_seq_len = max_num_blocks_per_seq * block_size
dtype = torch.float16
device = get_current_device()
2025-04-11T04:23:18.5245382Z
if same_context_len:
>           context_lengths = torch.tensor([max_seq_len for _ in range(bsz)], dtype=torch.int32, device=device)
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5246543Z
tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:116: RuntimeError
___________________ test_kv_cache_memcopy[True-16-32-16-32] ____________________
2025-04-11T04:23:18.5246892Z
bsz = 32, block_size = 16, max_num_blocks_per_seq = 32, num_kv_heads = 16
same_context_len = True
2025-04-11T04:23:18.5247143Z
@pytest.mark.parametrize("bsz", [4, 7, 32])
@pytest.mark.parametrize("block_size", [16, 32, 64])
@pytest.mark.parametrize("max_num_blocks_per_seq", [8, 32])
@pytest.mark.parametrize("num_kv_heads", [16])
@pytest.mark.parametrize("same_context_len", [True, False])
def test_kv_cache_memcopy(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_kv_heads: int,
same_context_len: bool,
):
>       run_context_copy_kv_to_cache(bsz, block_size, max_num_blocks_per_seq, num_kv_heads, same_context_len)
2025-04-11T04:23:18.5248644Z
tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:152:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.5248924Z
bsz = 32, block_size = 16, max_num_blocks_per_seq = 32, num_kv_heads = 16
same_context_len = True
2025-04-11T04:23:18.5249169Z
def run_context_copy_kv_to_cache(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_kv_heads: int,
same_context_len: bool,
):
torch.manual_seed(123)
2025-04-11T04:23:18.5250043Z
assert isinstance(num_kv_heads, int) and num_kv_heads > 0, "Invalid number of kv heads."
max_seq_len = max_num_blocks_per_seq * block_size
dtype = torch.float16
device = get_current_device()
2025-04-11T04:23:18.5250625Z
if same_context_len:
>           context_lengths = torch.tensor([max_seq_len for _ in range(bsz)], dtype=torch.int32, device=device)
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5251625Z
tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:116: RuntimeError
____________________ test_kv_cache_memcopy[True-16-32-32-4] ____________________
2025-04-11T04:23:18.5252042Z
bsz = 4, block_size = 32, max_num_blocks_per_seq = 32, num_kv_heads = 16
same_context_len = True
2025-04-11T04:23:18.5252280Z
@pytest.mark.parametrize("bsz", [4, 7, 32])
@pytest.mark.parametrize("block_size", [16, 32, 64])
@pytest.mark.parametrize("max_num_blocks_per_seq", [8, 32])
@pytest.mark.parametrize("num_kv_heads", [16])
@pytest.mark.parametrize("same_context_len", [True, False])
def test_kv_cache_memcopy(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_kv_heads: int,
same_context_len: bool,
):
>       run_context_copy_kv_to_cache(bsz, block_size, max_num_blocks_per_seq, num_kv_heads, same_context_len)
2025-04-11T04:23:18.5253729Z
tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:152:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.5254000Z
bsz = 4, block_size = 32, max_num_blocks_per_seq = 32, num_kv_heads = 16
same_context_len = True
2025-04-11T04:23:18.5254226Z
def run_context_copy_kv_to_cache(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_kv_heads: int,
same_context_len: bool,
):
torch.manual_seed(123)
2025-04-11T04:23:18.5254979Z
assert isinstance(num_kv_heads, int) and num_kv_heads > 0, "Invalid number of kv heads."
max_seq_len = max_num_blocks_per_seq * block_size
dtype = torch.float16
device = get_current_device()
2025-04-11T04:23:18.5255553Z
if same_context_len:
>           context_lengths = torch.tensor([max_seq_len for _ in range(bsz)], dtype=torch.int32, device=device)
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5256651Z
tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:116: RuntimeError
____________________ test_kv_cache_memcopy[True-16-32-32-7] ____________________
2025-04-11T04:23:18.5256995Z
bsz = 7, block_size = 32, max_num_blocks_per_seq = 32, num_kv_heads = 16
same_context_len = True
2025-04-11T04:23:18.5257243Z
@pytest.mark.parametrize("bsz", [4, 7, 32])
@pytest.mark.parametrize("block_size", [16, 32, 64])
@pytest.mark.parametrize("max_num_blocks_per_seq", [8, 32])
@pytest.mark.parametrize("num_kv_heads", [16])
@pytest.mark.parametrize("same_context_len", [True, False])
def test_kv_cache_memcopy(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_kv_heads: int,
same_context_len: bool,
):
>       run_context_copy_kv_to_cache(bsz, block_size, max_num_blocks_per_seq, num_kv_heads, same_context_len)
2025-04-11T04:23:18.5258789Z
tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:152:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.5259055Z
bsz = 7, block_size = 32, max_num_blocks_per_seq = 32, num_kv_heads = 16
same_context_len = True
2025-04-11T04:23:18.5259291Z
def run_context_copy_kv_to_cache(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_kv_heads: int,
same_context_len: bool,
):
torch.manual_seed(123)
2025-04-11T04:23:18.5260070Z
assert isinstance(num_kv_heads, int) and num_kv_heads > 0, "Invalid number of kv heads."
max_seq_len = max_num_blocks_per_seq * block_size
dtype = torch.float16
device = get_current_device()
2025-04-11T04:23:18.5260641Z
if same_context_len:
>           context_lengths = torch.tensor([max_seq_len for _ in range(bsz)], dtype=torch.int32, device=device)
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5261650Z
tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:116: RuntimeError
___________________ test_kv_cache_memcopy[True-16-32-32-32] ____________________
2025-04-11T04:23:18.5261984Z
bsz = 32, block_size = 32, max_num_blocks_per_seq = 32, num_kv_heads = 16
same_context_len = True
2025-04-11T04:23:18.5262315Z
@pytest.mark.parametrize("bsz", [4, 7, 32])
@pytest.mark.parametrize("block_size", [16, 32, 64])
@pytest.mark.parametrize("max_num_blocks_per_seq", [8, 32])
@pytest.mark.parametrize("num_kv_heads", [16])
@pytest.mark.parametrize("same_context_len", [True, False])
def test_kv_cache_memcopy(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_kv_heads: int,
same_context_len: bool,
):
>       run_context_copy_kv_to_cache(bsz, block_size, max_num_blocks_per_seq, num_kv_heads, same_context_len)
2025-04-11T04:23:18.5263770Z
tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:152:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.5264142Z
bsz = 32, block_size = 32, max_num_blocks_per_seq = 32, num_kv_heads = 16
same_context_len = True
2025-04-11T04:23:18.5264379Z
def run_context_copy_kv_to_cache(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_kv_heads: int,
same_context_len: bool,
):
torch.manual_seed(123)
2025-04-11T04:23:18.5265130Z
assert isinstance(num_kv_heads, int) and num_kv_heads > 0, "Invalid number of kv heads."
max_seq_len = max_num_blocks_per_seq * block_size
dtype = torch.float16
device = get_current_device()
2025-04-11T04:23:18.5265717Z
if same_context_len:
>           context_lengths = torch.tensor([max_seq_len for _ in range(bsz)], dtype=torch.int32, device=device)
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5266733Z
tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:116: RuntimeError
____________________ test_kv_cache_memcopy[True-16-32-64-4] ____________________
2025-04-11T04:23:18.5267067Z
bsz = 4, block_size = 64, max_num_blocks_per_seq = 32, num_kv_heads = 16
same_context_len = True
2025-04-11T04:23:18.5267304Z
@pytest.mark.parametrize("bsz", [4, 7, 32])
@pytest.mark.parametrize("block_size", [16, 32, 64])
@pytest.mark.parametrize("max_num_blocks_per_seq", [8, 32])
@pytest.mark.parametrize("num_kv_heads", [16])
@pytest.mark.parametrize("same_context_len", [True, False])
def test_kv_cache_memcopy(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_kv_heads: int,
same_context_len: bool,
):
>       run_context_copy_kv_to_cache(bsz, block_size, max_num_blocks_per_seq, num_kv_heads, same_context_len)
2025-04-11T04:23:18.5268892Z
tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:152:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.5269165Z
bsz = 4, block_size = 64, max_num_blocks_per_seq = 32, num_kv_heads = 16
same_context_len = True
2025-04-11T04:23:18.5269404Z
def run_context_copy_kv_to_cache(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_kv_heads: int,
same_context_len: bool,
):
torch.manual_seed(123)
2025-04-11T04:23:18.5270158Z
assert isinstance(num_kv_heads, int) and num_kv_heads > 0, "Invalid number of kv heads."
max_seq_len = max_num_blocks_per_seq * block_size
dtype = torch.float16
device = get_current_device()
2025-04-11T04:23:18.5270837Z
if same_context_len:
>           context_lengths = torch.tensor([max_seq_len for _ in range(bsz)], dtype=torch.int32, device=device)
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5271842Z
tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:116: RuntimeError
____________________ test_kv_cache_memcopy[True-16-32-64-7] ____________________
2025-04-11T04:23:18.5272175Z
bsz = 7, block_size = 64, max_num_blocks_per_seq = 32, num_kv_heads = 16
same_context_len = True
2025-04-11T04:23:18.5272408Z
@pytest.mark.parametrize("bsz", [4, 7, 32])
@pytest.mark.parametrize("block_size", [16, 32, 64])
@pytest.mark.parametrize("max_num_blocks_per_seq", [8, 32])
@pytest.mark.parametrize("num_kv_heads", [16])
@pytest.mark.parametrize("same_context_len", [True, False])
def test_kv_cache_memcopy(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_kv_heads: int,
same_context_len: bool,
):
>       run_context_copy_kv_to_cache(bsz, block_size, max_num_blocks_per_seq, num_kv_heads, same_context_len)
2025-04-11T04:23:18.5273862Z
tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:152:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.5274134Z
bsz = 7, block_size = 64, max_num_blocks_per_seq = 32, num_kv_heads = 16
same_context_len = True
2025-04-11T04:23:18.5274359Z
def run_context_copy_kv_to_cache(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_kv_heads: int,
same_context_len: bool,
):
torch.manual_seed(123)
2025-04-11T04:23:18.5275217Z
assert isinstance(num_kv_heads, int) and num_kv_heads > 0, "Invalid number of kv heads."
max_seq_len = max_num_blocks_per_seq * block_size
dtype = torch.float16
device = get_current_device()
2025-04-11T04:23:18.5275790Z
if same_context_len:
>           context_lengths = torch.tensor([max_seq_len for _ in range(bsz)], dtype=torch.int32, device=device)
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5276893Z
tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:116: RuntimeError
___________________ test_kv_cache_memcopy[True-16-32-64-32] ____________________
2025-04-11T04:23:18.5277224Z
bsz = 32, block_size = 64, max_num_blocks_per_seq = 32, num_kv_heads = 16
same_context_len = True
2025-04-11T04:23:18.5277460Z
@pytest.mark.parametrize("bsz", [4, 7, 32])
@pytest.mark.parametrize("block_size", [16, 32, 64])
@pytest.mark.parametrize("max_num_blocks_per_seq", [8, 32])
@pytest.mark.parametrize("num_kv_heads", [16])
@pytest.mark.parametrize("same_context_len", [True, False])
def test_kv_cache_memcopy(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_kv_heads: int,
same_context_len: bool,
):
>       run_context_copy_kv_to_cache(bsz, block_size, max_num_blocks_per_seq, num_kv_heads, same_context_len)
2025-04-11T04:23:18.5278965Z
tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:152:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.5279242Z
bsz = 32, block_size = 64, max_num_blocks_per_seq = 32, num_kv_heads = 16
same_context_len = True
2025-04-11T04:23:18.5279482Z
def run_context_copy_kv_to_cache(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_kv_heads: int,
same_context_len: bool,
):
torch.manual_seed(123)
2025-04-11T04:23:18.5280228Z
assert isinstance(num_kv_heads, int) and num_kv_heads > 0, "Invalid number of kv heads."
max_seq_len = max_num_blocks_per_seq * block_size
dtype = torch.float16
device = get_current_device()
2025-04-11T04:23:18.5280882Z
if same_context_len:
>           context_lengths = torch.tensor([max_seq_len for _ in range(bsz)], dtype=torch.int32, device=device)
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5281885Z
tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:116: RuntimeError
____________________ test_kv_cache_memcopy[False-16-8-16-4] ____________________
2025-04-11T04:23:18.5282215Z
bsz = 4, block_size = 16, max_num_blocks_per_seq = 8, num_kv_heads = 16
same_context_len = False
2025-04-11T04:23:18.5282459Z
@pytest.mark.parametrize("bsz", [4, 7, 32])
@pytest.mark.parametrize("block_size", [16, 32, 64])
@pytest.mark.parametrize("max_num_blocks_per_seq", [8, 32])
@pytest.mark.parametrize("num_kv_heads", [16])
@pytest.mark.parametrize("same_context_len", [True, False])
def test_kv_cache_memcopy(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_kv_heads: int,
same_context_len: bool,
):
>       run_context_copy_kv_to_cache(bsz, block_size, max_num_blocks_per_seq, num_kv_heads, same_context_len)
2025-04-11T04:23:18.5283992Z
tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:152:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.5284258Z
bsz = 4, block_size = 16, max_num_blocks_per_seq = 8, num_kv_heads = 16
same_context_len = False
2025-04-11T04:23:18.5284494Z
def run_context_copy_kv_to_cache(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_kv_heads: int,
same_context_len: bool,
):
torch.manual_seed(123)
2025-04-11T04:23:18.5285252Z
assert isinstance(num_kv_heads, int) and num_kv_heads > 0, "Invalid number of kv heads."
max_seq_len = max_num_blocks_per_seq * block_size
dtype = torch.float16
device = get_current_device()
2025-04-11T04:23:18.5285829Z
if same_context_len:
context_lengths = torch.tensor([max_seq_len for _ in range(bsz)], dtype=torch.int32, device=device)
else:
>           context_lengths = torch.randint(low=1, high=max_seq_len, size=(bsz,), dtype=torch.int32, device=device)
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5287259Z
tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:118: RuntimeError
____________________ test_kv_cache_memcopy[False-16-8-16-7] ____________________
2025-04-11T04:23:18.5287592Z
bsz = 7, block_size = 16, max_num_blocks_per_seq = 8, num_kv_heads = 16
same_context_len = False
2025-04-11T04:23:18.5287824Z
@pytest.mark.parametrize("bsz", [4, 7, 32])
@pytest.mark.parametrize("block_size", [16, 32, 64])
@pytest.mark.parametrize("max_num_blocks_per_seq", [8, 32])
@pytest.mark.parametrize("num_kv_heads", [16])
@pytest.mark.parametrize("same_context_len", [True, False])
def test_kv_cache_memcopy(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_kv_heads: int,
same_context_len: bool,
):
>       run_context_copy_kv_to_cache(bsz, block_size, max_num_blocks_per_seq, num_kv_heads, same_context_len)
2025-04-11T04:23:18.5289465Z
tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:152:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.5289742Z
bsz = 7, block_size = 16, max_num_blocks_per_seq = 8, num_kv_heads = 16
same_context_len = False
2025-04-11T04:23:18.5289987Z
def run_context_copy_kv_to_cache(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_kv_heads: int,
same_context_len: bool,
):
torch.manual_seed(123)
2025-04-11T04:23:18.5290767Z
assert isinstance(num_kv_heads, int) and num_kv_heads > 0, "Invalid number of kv heads."
max_seq_len = max_num_blocks_per_seq * block_size
dtype = torch.float16
device = get_current_device()
2025-04-11T04:23:18.5291340Z
if same_context_len:
context_lengths = torch.tensor([max_seq_len for _ in range(bsz)], dtype=torch.int32, device=device)
else:
>           context_lengths = torch.randint(low=1, high=max_seq_len, size=(bsz,), dtype=torch.int32, device=device)
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5292678Z
tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:118: RuntimeError
___________________ test_kv_cache_memcopy[False-16-8-16-32] ____________________
2025-04-11T04:23:18.5293010Z
bsz = 32, block_size = 16, max_num_blocks_per_seq = 8, num_kv_heads = 16
same_context_len = False
2025-04-11T04:23:18.5293343Z
@pytest.mark.parametrize("bsz", [4, 7, 32])
@pytest.mark.parametrize("block_size", [16, 32, 64])
@pytest.mark.parametrize("max_num_blocks_per_seq", [8, 32])
@pytest.mark.parametrize("num_kv_heads", [16])
@pytest.mark.parametrize("same_context_len", [True, False])
def test_kv_cache_memcopy(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_kv_heads: int,
same_context_len: bool,
):
>       run_context_copy_kv_to_cache(bsz, block_size, max_num_blocks_per_seq, num_kv_heads, same_context_len)
2025-04-11T04:23:18.5294794Z
tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:152:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.5295151Z
bsz = 32, block_size = 16, max_num_blocks_per_seq = 8, num_kv_heads = 16
same_context_len = False
2025-04-11T04:23:18.5295391Z
def run_context_copy_kv_to_cache(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_kv_heads: int,
same_context_len: bool,
):
torch.manual_seed(123)
2025-04-11T04:23:18.5296138Z
assert isinstance(num_kv_heads, int) and num_kv_heads > 0, "Invalid number of kv heads."
max_seq_len = max_num_blocks_per_seq * block_size
dtype = torch.float16
device = get_current_device()
2025-04-11T04:23:18.5296717Z
if same_context_len:
context_lengths = torch.tensor([max_seq_len for _ in range(bsz)], dtype=torch.int32, device=device)
else:
>           context_lengths = torch.randint(low=1, high=max_seq_len, size=(bsz,), dtype=torch.int32, device=device)
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5298028Z
tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:118: RuntimeError
____________________ test_kv_cache_memcopy[False-16-8-32-4] ____________________
2025-04-11T04:23:18.5298363Z
bsz = 4, block_size = 32, max_num_blocks_per_seq = 8, num_kv_heads = 16
same_context_len = False
2025-04-11T04:23:18.5298596Z
@pytest.mark.parametrize("bsz", [4, 7, 32])
@pytest.mark.parametrize("block_size", [16, 32, 64])
@pytest.mark.parametrize("max_num_blocks_per_seq", [8, 32])
@pytest.mark.parametrize("num_kv_heads", [16])
@pytest.mark.parametrize("same_context_len", [True, False])
def test_kv_cache_memcopy(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_kv_heads: int,
same_context_len: bool,
):
>       run_context_copy_kv_to_cache(bsz, block_size, max_num_blocks_per_seq, num_kv_heads, same_context_len)
2025-04-11T04:23:18.5300124Z
tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:152:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.5300386Z
bsz = 4, block_size = 32, max_num_blocks_per_seq = 8, num_kv_heads = 16
same_context_len = False
2025-04-11T04:23:18.5300623Z
def run_context_copy_kv_to_cache(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_kv_heads: int,
same_context_len: bool,
):
torch.manual_seed(123)
2025-04-11T04:23:18.5301517Z
assert isinstance(num_kv_heads, int) and num_kv_heads > 0, "Invalid number of kv heads."
max_seq_len = max_num_blocks_per_seq * block_size
dtype = torch.float16
device = get_current_device()
2025-04-11T04:23:18.5302095Z
if same_context_len:
context_lengths = torch.tensor([max_seq_len for _ in range(bsz)], dtype=torch.int32, device=device)
else:
>           context_lengths = torch.randint(low=1, high=max_seq_len, size=(bsz,), dtype=torch.int32, device=device)
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5303457Z
tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:118: RuntimeError
____________________ test_kv_cache_memcopy[False-16-8-32-7] ____________________
2025-04-11T04:23:18.5303793Z
bsz = 7, block_size = 32, max_num_blocks_per_seq = 8, num_kv_heads = 16
same_context_len = False
2025-04-11T04:23:18.5304031Z
@pytest.mark.parametrize("bsz", [4, 7, 32])
@pytest.mark.parametrize("block_size", [16, 32, 64])
@pytest.mark.parametrize("max_num_blocks_per_seq", [8, 32])
@pytest.mark.parametrize("num_kv_heads", [16])
@pytest.mark.parametrize("same_context_len", [True, False])
def test_kv_cache_memcopy(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_kv_heads: int,
same_context_len: bool,
):
>       run_context_copy_kv_to_cache(bsz, block_size, max_num_blocks_per_seq, num_kv_heads, same_context_len)
2025-04-11T04:23:18.5305502Z
tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:152:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.5305892Z
bsz = 7, block_size = 32, max_num_blocks_per_seq = 8, num_kv_heads = 16
same_context_len = False
2025-04-11T04:23:18.5306133Z
def run_context_copy_kv_to_cache(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_kv_heads: int,
same_context_len: bool,
):
torch.manual_seed(123)
2025-04-11T04:23:18.5306906Z
assert isinstance(num_kv_heads, int) and num_kv_heads > 0, "Invalid number of kv heads."
max_seq_len = max_num_blocks_per_seq * block_size
dtype = torch.float16
device = get_current_device()
2025-04-11T04:23:18.5307580Z
if same_context_len:
context_lengths = torch.tensor([max_seq_len for _ in range(bsz)], dtype=torch.int32, device=device)
else:
>           context_lengths = torch.randint(low=1, high=max_seq_len, size=(bsz,), dtype=torch.int32, device=device)
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5308991Z
tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:118: RuntimeError
___________________ test_kv_cache_memcopy[False-16-8-32-32] ____________________
2025-04-11T04:23:18.5309332Z
bsz = 32, block_size = 32, max_num_blocks_per_seq = 8, num_kv_heads = 16
same_context_len = False
2025-04-11T04:23:18.5309567Z
@pytest.mark.parametrize("bsz", [4, 7, 32])
@pytest.mark.parametrize("block_size", [16, 32, 64])
@pytest.mark.parametrize("max_num_blocks_per_seq", [8, 32])
@pytest.mark.parametrize("num_kv_heads", [16])
@pytest.mark.parametrize("same_context_len", [True, False])
def test_kv_cache_memcopy(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_kv_heads: int,
same_context_len: bool,
):
>       run_context_copy_kv_to_cache(bsz, block_size, max_num_blocks_per_seq, num_kv_heads, same_context_len)
2025-04-11T04:23:18.5311045Z
tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:152:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.5311317Z
bsz = 32, block_size = 32, max_num_blocks_per_seq = 8, num_kv_heads = 16
same_context_len = False
2025-04-11T04:23:18.5311554Z
def run_context_copy_kv_to_cache(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_kv_heads: int,
same_context_len: bool,
):
torch.manual_seed(123)
2025-04-11T04:23:18.5312444Z
assert isinstance(num_kv_heads, int) and num_kv_heads > 0, "Invalid number of kv heads."
max_seq_len = max_num_blocks_per_seq * block_size
dtype = torch.float16
device = get_current_device()
2025-04-11T04:23:18.5313063Z
if same_context_len:
context_lengths = torch.tensor([max_seq_len for _ in range(bsz)], dtype=torch.int32, device=device)
else:
>           context_lengths = torch.randint(low=1, high=max_seq_len, size=(bsz,), dtype=torch.int32, device=device)
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5314502Z
tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:118: RuntimeError
____________________ test_kv_cache_memcopy[False-16-8-64-4] ____________________
2025-04-11T04:23:18.5314833Z
bsz = 4, block_size = 64, max_num_blocks_per_seq = 8, num_kv_heads = 16
same_context_len = False
2025-04-11T04:23:18.5315072Z
@pytest.mark.parametrize("bsz", [4, 7, 32])
@pytest.mark.parametrize("block_size", [16, 32, 64])
@pytest.mark.parametrize("max_num_blocks_per_seq", [8, 32])
@pytest.mark.parametrize("num_kv_heads", [16])
@pytest.mark.parametrize("same_context_len", [True, False])
def test_kv_cache_memcopy(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_kv_heads: int,
same_context_len: bool,
):
>       run_context_copy_kv_to_cache(bsz, block_size, max_num_blocks_per_seq, num_kv_heads, same_context_len)
2025-04-11T04:23:18.5316549Z
tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:152:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.5316819Z
bsz = 4, block_size = 64, max_num_blocks_per_seq = 8, num_kv_heads = 16
same_context_len = False
2025-04-11T04:23:18.5317065Z
def run_context_copy_kv_to_cache(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_kv_heads: int,
same_context_len: bool,
):
torch.manual_seed(123)
2025-04-11T04:23:18.5317830Z
assert isinstance(num_kv_heads, int) and num_kv_heads > 0, "Invalid number of kv heads."
max_seq_len = max_num_blocks_per_seq * block_size
dtype = torch.float16
device = get_current_device()
2025-04-11T04:23:18.5318499Z
if same_context_len:
context_lengths = torch.tensor([max_seq_len for _ in range(bsz)], dtype=torch.int32, device=device)
else:
>           context_lengths = torch.randint(low=1, high=max_seq_len, size=(bsz,), dtype=torch.int32, device=device)
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5319830Z
tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:118: RuntimeError
____________________ test_kv_cache_memcopy[False-16-8-64-7] ____________________
2025-04-11T04:23:18.5320266Z
bsz = 7, block_size = 64, max_num_blocks_per_seq = 8, num_kv_heads = 16
same_context_len = False
2025-04-11T04:23:18.5320499Z
@pytest.mark.parametrize("bsz", [4, 7, 32])
@pytest.mark.parametrize("block_size", [16, 32, 64])
@pytest.mark.parametrize("max_num_blocks_per_seq", [8, 32])
@pytest.mark.parametrize("num_kv_heads", [16])
@pytest.mark.parametrize("same_context_len", [True, False])
def test_kv_cache_memcopy(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_kv_heads: int,
same_context_len: bool,
):
>       run_context_copy_kv_to_cache(bsz, block_size, max_num_blocks_per_seq, num_kv_heads, same_context_len)
2025-04-11T04:23:18.5321975Z
tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:152:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.5322251Z
bsz = 7, block_size = 64, max_num_blocks_per_seq = 8, num_kv_heads = 16
same_context_len = False
2025-04-11T04:23:18.5322486Z
def run_context_copy_kv_to_cache(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_kv_heads: int,
same_context_len: bool,
):
torch.manual_seed(123)
2025-04-11T04:23:18.5323268Z
assert isinstance(num_kv_heads, int) and num_kv_heads > 0, "Invalid number of kv heads."
max_seq_len = max_num_blocks_per_seq * block_size
dtype = torch.float16
device = get_current_device()
2025-04-11T04:23:18.5323859Z
if same_context_len:
context_lengths = torch.tensor([max_seq_len for _ in range(bsz)], dtype=torch.int32, device=device)
else:
>           context_lengths = torch.randint(low=1, high=max_seq_len, size=(bsz,), dtype=torch.int32, device=device)
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5325323Z
tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:118: RuntimeError
___________________ test_kv_cache_memcopy[False-16-8-64-32] ____________________
2025-04-11T04:23:18.5325661Z
bsz = 32, block_size = 64, max_num_blocks_per_seq = 8, num_kv_heads = 16
same_context_len = False
2025-04-11T04:23:18.5325899Z
@pytest.mark.parametrize("bsz", [4, 7, 32])
@pytest.mark.parametrize("block_size", [16, 32, 64])
@pytest.mark.parametrize("max_num_blocks_per_seq", [8, 32])
@pytest.mark.parametrize("num_kv_heads", [16])
@pytest.mark.parametrize("same_context_len", [True, False])
def test_kv_cache_memcopy(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_kv_heads: int,
same_context_len: bool,
):
>       run_context_copy_kv_to_cache(bsz, block_size, max_num_blocks_per_seq, num_kv_heads, same_context_len)
2025-04-11T04:23:18.5327459Z
tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:152:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.5327735Z
bsz = 32, block_size = 64, max_num_blocks_per_seq = 8, num_kv_heads = 16
same_context_len = False
2025-04-11T04:23:18.5327976Z
def run_context_copy_kv_to_cache(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_kv_heads: int,
same_context_len: bool,
):
torch.manual_seed(123)
2025-04-11T04:23:18.5328747Z
assert isinstance(num_kv_heads, int) and num_kv_heads > 0, "Invalid number of kv heads."
max_seq_len = max_num_blocks_per_seq * block_size
dtype = torch.float16
device = get_current_device()
2025-04-11T04:23:18.5329329Z
if same_context_len:
context_lengths = torch.tensor([max_seq_len for _ in range(bsz)], dtype=torch.int32, device=device)
else:
>           context_lengths = torch.randint(low=1, high=max_seq_len, size=(bsz,), dtype=torch.int32, device=device)
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5330656Z
tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:118: RuntimeError
___________________ test_kv_cache_memcopy[False-16-32-16-4] ____________________
2025-04-11T04:23:18.5331076Z
bsz = 4, block_size = 16, max_num_blocks_per_seq = 32, num_kv_heads = 16
same_context_len = False
2025-04-11T04:23:18.5331324Z
@pytest.mark.parametrize("bsz", [4, 7, 32])
@pytest.mark.parametrize("block_size", [16, 32, 64])
@pytest.mark.parametrize("max_num_blocks_per_seq", [8, 32])
@pytest.mark.parametrize("num_kv_heads", [16])
@pytest.mark.parametrize("same_context_len", [True, False])
def test_kv_cache_memcopy(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_kv_heads: int,
same_context_len: bool,
):
>       run_context_copy_kv_to_cache(bsz, block_size, max_num_blocks_per_seq, num_kv_heads, same_context_len)
2025-04-11T04:23:18.5332896Z
tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:152:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.5333167Z
bsz = 4, block_size = 16, max_num_blocks_per_seq = 32, num_kv_heads = 16
same_context_len = False
2025-04-11T04:23:18.5333407Z
def run_context_copy_kv_to_cache(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_kv_heads: int,
same_context_len: bool,
):
torch.manual_seed(123)
2025-04-11T04:23:18.5334184Z
assert isinstance(num_kv_heads, int) and num_kv_heads > 0, "Invalid number of kv heads."
max_seq_len = max_num_blocks_per_seq * block_size
dtype = torch.float16
device = get_current_device()
2025-04-11T04:23:18.5334771Z
if same_context_len:
context_lengths = torch.tensor([max_seq_len for _ in range(bsz)], dtype=torch.int32, device=device)
else:
>           context_lengths = torch.randint(low=1, high=max_seq_len, size=(bsz,), dtype=torch.int32, device=device)
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5336133Z
tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:118: RuntimeError
___________________ test_kv_cache_memcopy[False-16-32-16-7] ____________________
2025-04-11T04:23:18.5336463Z
bsz = 7, block_size = 16, max_num_blocks_per_seq = 32, num_kv_heads = 16
same_context_len = False
2025-04-11T04:23:18.5336706Z
@pytest.mark.parametrize("bsz", [4, 7, 32])
@pytest.mark.parametrize("block_size", [16, 32, 64])
@pytest.mark.parametrize("max_num_blocks_per_seq", [8, 32])
@pytest.mark.parametrize("num_kv_heads", [16])
@pytest.mark.parametrize("same_context_len", [True, False])
def test_kv_cache_memcopy(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_kv_heads: int,
same_context_len: bool,
):
>       run_context_copy_kv_to_cache(bsz, block_size, max_num_blocks_per_seq, num_kv_heads, same_context_len)
2025-04-11T04:23:18.5338251Z
tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:152:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.5338519Z
bsz = 7, block_size = 16, max_num_blocks_per_seq = 32, num_kv_heads = 16
same_context_len = False
2025-04-11T04:23:18.5338754Z
def run_context_copy_kv_to_cache(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_kv_heads: int,
same_context_len: bool,
):
torch.manual_seed(123)
2025-04-11T04:23:18.5339589Z
assert isinstance(num_kv_heads, int) and num_kv_heads > 0, "Invalid number of kv heads."
max_seq_len = max_num_blocks_per_seq * block_size
dtype = torch.float16
device = get_current_device()
2025-04-11T04:23:18.5340156Z
if same_context_len:
context_lengths = torch.tensor([max_seq_len for _ in range(bsz)], dtype=torch.int32, device=device)
else:
>           context_lengths = torch.randint(low=1, high=max_seq_len, size=(bsz,), dtype=torch.int32, device=device)
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5341472Z
tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:118: RuntimeError
___________________ test_kv_cache_memcopy[False-16-32-16-32] ___________________
2025-04-11T04:23:18.5341808Z
bsz = 32, block_size = 16, max_num_blocks_per_seq = 32, num_kv_heads = 16
same_context_len = False
2025-04-11T04:23:18.5342047Z
@pytest.mark.parametrize("bsz", [4, 7, 32])
@pytest.mark.parametrize("block_size", [16, 32, 64])
@pytest.mark.parametrize("max_num_blocks_per_seq", [8, 32])
@pytest.mark.parametrize("num_kv_heads", [16])
@pytest.mark.parametrize("same_context_len", [True, False])
def test_kv_cache_memcopy(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_kv_heads: int,
same_context_len: bool,
):
>       run_context_copy_kv_to_cache(bsz, block_size, max_num_blocks_per_seq, num_kv_heads, same_context_len)
2025-04-11T04:23:18.5343574Z
tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:152:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.5343848Z
bsz = 32, block_size = 16, max_num_blocks_per_seq = 32, num_kv_heads = 16
same_context_len = False
2025-04-11T04:23:18.5344085Z
def run_context_copy_kv_to_cache(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_kv_heads: int,
same_context_len: bool,
):
torch.manual_seed(123)
2025-04-11T04:23:18.5344842Z
assert isinstance(num_kv_heads, int) and num_kv_heads > 0, "Invalid number of kv heads."
max_seq_len = max_num_blocks_per_seq * block_size
dtype = torch.float16
device = get_current_device()
2025-04-11T04:23:18.5345506Z
if same_context_len:
context_lengths = torch.tensor([max_seq_len for _ in range(bsz)], dtype=torch.int32, device=device)
else:
>           context_lengths = torch.randint(low=1, high=max_seq_len, size=(bsz,), dtype=torch.int32, device=device)
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5346852Z
tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:118: RuntimeError
___________________ test_kv_cache_memcopy[False-16-32-32-4] ____________________
2025-04-11T04:23:18.5347187Z
bsz = 4, block_size = 32, max_num_blocks_per_seq = 32, num_kv_heads = 16
same_context_len = False
2025-04-11T04:23:18.5347420Z
@pytest.mark.parametrize("bsz", [4, 7, 32])
@pytest.mark.parametrize("block_size", [16, 32, 64])
@pytest.mark.parametrize("max_num_blocks_per_seq", [8, 32])
@pytest.mark.parametrize("num_kv_heads", [16])
@pytest.mark.parametrize("same_context_len", [True, False])
def test_kv_cache_memcopy(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_kv_heads: int,
same_context_len: bool,
):
>       run_context_copy_kv_to_cache(bsz, block_size, max_num_blocks_per_seq, num_kv_heads, same_context_len)
2025-04-11T04:23:18.5348914Z
tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:152:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.5349182Z
bsz = 4, block_size = 32, max_num_blocks_per_seq = 32, num_kv_heads = 16
same_context_len = False
2025-04-11T04:23:18.5349516Z
def run_context_copy_kv_to_cache(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_kv_heads: int,
same_context_len: bool,
):
torch.manual_seed(123)
2025-04-11T04:23:18.5350260Z
assert isinstance(num_kv_heads, int) and num_kv_heads > 0, "Invalid number of kv heads."
max_seq_len = max_num_blocks_per_seq * block_size
dtype = torch.float16
device = get_current_device()
2025-04-11T04:23:18.5350829Z
if same_context_len:
context_lengths = torch.tensor([max_seq_len for _ in range(bsz)], dtype=torch.int32, device=device)
else:
>           context_lengths = torch.randint(low=1, high=max_seq_len, size=(bsz,), dtype=torch.int32, device=device)
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5352247Z
tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:118: RuntimeError
___________________ test_kv_cache_memcopy[False-16-32-32-7] ____________________
2025-04-11T04:23:18.5352573Z
bsz = 7, block_size = 32, max_num_blocks_per_seq = 32, num_kv_heads = 16
same_context_len = False
2025-04-11T04:23:18.5352809Z
@pytest.mark.parametrize("bsz", [4, 7, 32])
@pytest.mark.parametrize("block_size", [16, 32, 64])
@pytest.mark.parametrize("max_num_blocks_per_seq", [8, 32])
@pytest.mark.parametrize("num_kv_heads", [16])
@pytest.mark.parametrize("same_context_len", [True, False])
def test_kv_cache_memcopy(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_kv_heads: int,
same_context_len: bool,
):
>       run_context_copy_kv_to_cache(bsz, block_size, max_num_blocks_per_seq, num_kv_heads, same_context_len)
2025-04-11T04:23:18.5354250Z
tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:152:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.5354523Z
bsz = 7, block_size = 32, max_num_blocks_per_seq = 32, num_kv_heads = 16
same_context_len = False
2025-04-11T04:23:18.5354757Z
def run_context_copy_kv_to_cache(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_kv_heads: int,
same_context_len: bool,
):
torch.manual_seed(123)
2025-04-11T04:23:18.5355580Z
assert isinstance(num_kv_heads, int) and num_kv_heads > 0, "Invalid number of kv heads."
max_seq_len = max_num_blocks_per_seq * block_size
dtype = torch.float16
device = get_current_device()
2025-04-11T04:23:18.5356155Z
if same_context_len:
context_lengths = torch.tensor([max_seq_len for _ in range(bsz)], dtype=torch.int32, device=device)
else:
>           context_lengths = torch.randint(low=1, high=max_seq_len, size=(bsz,), dtype=torch.int32, device=device)
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5357577Z
tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:118: RuntimeError
___________________ test_kv_cache_memcopy[False-16-32-32-32] ___________________
2025-04-11T04:23:18.5357928Z
bsz = 32, block_size = 32, max_num_blocks_per_seq = 32, num_kv_heads = 16
same_context_len = False
2025-04-11T04:23:18.5358168Z
@pytest.mark.parametrize("bsz", [4, 7, 32])
@pytest.mark.parametrize("block_size", [16, 32, 64])
@pytest.mark.parametrize("max_num_blocks_per_seq", [8, 32])
@pytest.mark.parametrize("num_kv_heads", [16])
@pytest.mark.parametrize("same_context_len", [True, False])
def test_kv_cache_memcopy(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_kv_heads: int,
same_context_len: bool,
):
>       run_context_copy_kv_to_cache(bsz, block_size, max_num_blocks_per_seq, num_kv_heads, same_context_len)
2025-04-11T04:23:18.5359609Z
tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:152:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.5359877Z
bsz = 32, block_size = 32, max_num_blocks_per_seq = 32, num_kv_heads = 16
same_context_len = False
2025-04-11T04:23:18.5360114Z
def run_context_copy_kv_to_cache(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_kv_heads: int,
same_context_len: bool,
):
torch.manual_seed(123)
2025-04-11T04:23:18.5360860Z
assert isinstance(num_kv_heads, int) and num_kv_heads > 0, "Invalid number of kv heads."
max_seq_len = max_num_blocks_per_seq * block_size
dtype = torch.float16
device = get_current_device()
2025-04-11T04:23:18.5361423Z
if same_context_len:
context_lengths = torch.tensor([max_seq_len for _ in range(bsz)], dtype=torch.int32, device=device)
else:
>           context_lengths = torch.randint(low=1, high=max_seq_len, size=(bsz,), dtype=torch.int32, device=device)
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5362809Z
tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:118: RuntimeError
___________________ test_kv_cache_memcopy[False-16-32-64-4] ____________________
2025-04-11T04:23:18.5363141Z
bsz = 4, block_size = 64, max_num_blocks_per_seq = 32, num_kv_heads = 16
same_context_len = False
2025-04-11T04:23:18.5363376Z
@pytest.mark.parametrize("bsz", [4, 7, 32])
@pytest.mark.parametrize("block_size", [16, 32, 64])
@pytest.mark.parametrize("max_num_blocks_per_seq", [8, 32])
@pytest.mark.parametrize("num_kv_heads", [16])
@pytest.mark.parametrize("same_context_len", [True, False])
def test_kv_cache_memcopy(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_kv_heads: int,
same_context_len: bool,
):
>       run_context_copy_kv_to_cache(bsz, block_size, max_num_blocks_per_seq, num_kv_heads, same_context_len)
2025-04-11T04:23:18.5364911Z
tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:152:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.5365189Z
bsz = 4, block_size = 64, max_num_blocks_per_seq = 32, num_kv_heads = 16
same_context_len = False
2025-04-11T04:23:18.5365423Z
def run_context_copy_kv_to_cache(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_kv_heads: int,
same_context_len: bool,
):
torch.manual_seed(123)
2025-04-11T04:23:18.5366171Z
assert isinstance(num_kv_heads, int) and num_kv_heads > 0, "Invalid number of kv heads."
max_seq_len = max_num_blocks_per_seq * block_size
dtype = torch.float16
device = get_current_device()
2025-04-11T04:23:18.5366750Z
if same_context_len:
context_lengths = torch.tensor([max_seq_len for _ in range(bsz)], dtype=torch.int32, device=device)
else:
>           context_lengths = torch.randint(low=1, high=max_seq_len, size=(bsz,), dtype=torch.int32, device=device)
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5368151Z
tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:118: RuntimeError
___________________ test_kv_cache_memcopy[False-16-32-64-7] ____________________
2025-04-11T04:23:18.5368480Z
bsz = 7, block_size = 64, max_num_blocks_per_seq = 32, num_kv_heads = 16
same_context_len = False
2025-04-11T04:23:18.5368735Z
@pytest.mark.parametrize("bsz", [4, 7, 32])
@pytest.mark.parametrize("block_size", [16, 32, 64])
@pytest.mark.parametrize("max_num_blocks_per_seq", [8, 32])
@pytest.mark.parametrize("num_kv_heads", [16])
@pytest.mark.parametrize("same_context_len", [True, False])
def test_kv_cache_memcopy(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_kv_heads: int,
same_context_len: bool,
):
>       run_context_copy_kv_to_cache(bsz, block_size, max_num_blocks_per_seq, num_kv_heads, same_context_len)
2025-04-11T04:23:18.5370290Z
tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:152:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.5370559Z
bsz = 7, block_size = 64, max_num_blocks_per_seq = 32, num_kv_heads = 16
same_context_len = False
2025-04-11T04:23:18.5370791Z
def run_context_copy_kv_to_cache(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_kv_heads: int,
same_context_len: bool,
):
torch.manual_seed(123)
2025-04-11T04:23:18.5371543Z
assert isinstance(num_kv_heads, int) and num_kv_heads > 0, "Invalid number of kv heads."
max_seq_len = max_num_blocks_per_seq * block_size
dtype = torch.float16
device = get_current_device()
2025-04-11T04:23:18.5372111Z
if same_context_len:
context_lengths = torch.tensor([max_seq_len for _ in range(bsz)], dtype=torch.int32, device=device)
else:
>           context_lengths = torch.randint(low=1, high=max_seq_len, size=(bsz,), dtype=torch.int32, device=device)
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5373426Z
tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:118: RuntimeError
___________________ test_kv_cache_memcopy[False-16-32-64-32] ___________________
2025-04-11T04:23:18.5373758Z
bsz = 32, block_size = 64, max_num_blocks_per_seq = 32, num_kv_heads = 16
same_context_len = False
2025-04-11T04:23:18.5374085Z
@pytest.mark.parametrize("bsz", [4, 7, 32])
@pytest.mark.parametrize("block_size", [16, 32, 64])
@pytest.mark.parametrize("max_num_blocks_per_seq", [8, 32])
@pytest.mark.parametrize("num_kv_heads", [16])
@pytest.mark.parametrize("same_context_len", [True, False])
def test_kv_cache_memcopy(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_kv_heads: int,
same_context_len: bool,
):
>       run_context_copy_kv_to_cache(bsz, block_size, max_num_blocks_per_seq, num_kv_heads, same_context_len)
2025-04-11T04:23:18.5375523Z
tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:152:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.5375878Z
bsz = 32, block_size = 64, max_num_blocks_per_seq = 32, num_kv_heads = 16
same_context_len = False
2025-04-11T04:23:18.5376115Z
def run_context_copy_kv_to_cache(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_kv_heads: int,
same_context_len: bool,
):
torch.manual_seed(123)
2025-04-11T04:23:18.5376861Z
assert isinstance(num_kv_heads, int) and num_kv_heads > 0, "Invalid number of kv heads."
max_seq_len = max_num_blocks_per_seq * block_size
dtype = torch.float16
device = get_current_device()
2025-04-11T04:23:18.5377433Z
if same_context_len:
context_lengths = torch.tensor([max_seq_len for _ in range(bsz)], dtype=torch.int32, device=device)
else:
>           context_lengths = torch.randint(low=1, high=max_seq_len, size=(bsz,), dtype=torch.int32, device=device)
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5378743Z
tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py:118: RuntimeError
___________________________ test_rms_layernorm[64-2] ___________________________
2025-04-11T04:23:18.5379070Z
M = 2, N = 64
2025-04-11T04:23:18.5379156Z
@pytest.mark.parametrize("M", [2, 4, 8, 16])
@pytest.mark.parametrize("N", [64, 128, 512, 5120])
def test_rms_layernorm(M: int, N: int):
torch.manual_seed(123)
torch.cuda.empty_cache()
>       torch.cuda.synchronize()
2025-04-11T04:23:18.5379784Z
tests/test_infer/test_kernels/cuda/test_rms_layernorm.py:16:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:800: in synchronize
with torch.cuda.device(device):
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.5380640Z
self = <torch.cuda.device object at 0x7fb5b8706890>
2025-04-11T04:23:18.5380763Z
def __enter__(self):
>       self.prev_idx = torch.cuda._exchange_device(self.idx)
E       RuntimeError: CUDA error: out of memory
E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5381699Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:374: RuntimeError
___________________________ test_rms_layernorm[64-4] ___________________________
2025-04-11T04:23:18.5382180Z
M = 4, N = 64
2025-04-11T04:23:18.5382266Z
@pytest.mark.parametrize("M", [2, 4, 8, 16])
@pytest.mark.parametrize("N", [64, 128, 512, 5120])
def test_rms_layernorm(M: int, N: int):
torch.manual_seed(123)
torch.cuda.empty_cache()
>       torch.cuda.synchronize()
2025-04-11T04:23:18.5382878Z
tests/test_infer/test_kernels/cuda/test_rms_layernorm.py:16:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.5383144Z
device = None
2025-04-11T04:23:18.5383231Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.5383586Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5385176Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
___________________________ test_rms_layernorm[64-8] ___________________________
2025-04-11T04:23:18.5385561Z
M = 8, N = 64
2025-04-11T04:23:18.5385639Z
@pytest.mark.parametrize("M", [2, 4, 8, 16])
@pytest.mark.parametrize("N", [64, 128, 512, 5120])
def test_rms_layernorm(M: int, N: int):
torch.manual_seed(123)
torch.cuda.empty_cache()
>       torch.cuda.synchronize()
2025-04-11T04:23:18.5386423Z
tests/test_infer/test_kernels/cuda/test_rms_layernorm.py:16:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.5386700Z
device = None
2025-04-11T04:23:18.5386783Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.5387139Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5388903Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
__________________________ test_rms_layernorm[64-16] ___________________________
2025-04-11T04:23:18.5389295Z
M = 16, N = 64
2025-04-11T04:23:18.5389376Z
@pytest.mark.parametrize("M", [2, 4, 8, 16])
@pytest.mark.parametrize("N", [64, 128, 512, 5120])
def test_rms_layernorm(M: int, N: int):
torch.manual_seed(123)
torch.cuda.empty_cache()
>       torch.cuda.synchronize()
2025-04-11T04:23:18.5389997Z
tests/test_infer/test_kernels/cuda/test_rms_layernorm.py:16:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.5390275Z
device = None
2025-04-11T04:23:18.5390359Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.5390711Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5392385Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
__________________________ test_rms_layernorm[128-2] ___________________________
2025-04-11T04:23:18.5392777Z
M = 2, N = 128
2025-04-11T04:23:18.5392860Z
@pytest.mark.parametrize("M", [2, 4, 8, 16])
@pytest.mark.parametrize("N", [64, 128, 512, 5120])
def test_rms_layernorm(M: int, N: int):
torch.manual_seed(123)
torch.cuda.empty_cache()
>       torch.cuda.synchronize()
2025-04-11T04:23:18.5393532Z
tests/test_infer/test_kernels/cuda/test_rms_layernorm.py:16:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.5393811Z
device = None
2025-04-11T04:23:18.5393898Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.5394339Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5395909Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
__________________________ test_rms_layernorm[128-4] ___________________________
2025-04-11T04:23:18.5396293Z
M = 4, N = 128
2025-04-11T04:23:18.5396372Z
@pytest.mark.parametrize("M", [2, 4, 8, 16])
@pytest.mark.parametrize("N", [64, 128, 512, 5120])
def test_rms_layernorm(M: int, N: int):
torch.manual_seed(123)
torch.cuda.empty_cache()
>       torch.cuda.synchronize()
2025-04-11T04:23:18.5396979Z
tests/test_infer/test_kernels/cuda/test_rms_layernorm.py:16:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.5397248Z
device = None
2025-04-11T04:23:18.5397330Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.5397670Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5399328Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
__________________________ test_rms_layernorm[128-8] ___________________________
2025-04-11T04:23:18.5399715Z
M = 8, N = 128
2025-04-11T04:23:18.5399794Z
@pytest.mark.parametrize("M", [2, 4, 8, 16])
@pytest.mark.parametrize("N", [64, 128, 512, 5120])
def test_rms_layernorm(M: int, N: int):
torch.manual_seed(123)
torch.cuda.empty_cache()
>       torch.cuda.synchronize()
2025-04-11T04:23:18.5400505Z
tests/test_infer/test_kernels/cuda/test_rms_layernorm.py:16:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.5400766Z
device = None
2025-04-11T04:23:18.5400854Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.5401200Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5402780Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
__________________________ test_rms_layernorm[128-16] __________________________
2025-04-11T04:23:18.5403172Z
M = 16, N = 128
2025-04-11T04:23:18.5403255Z
@pytest.mark.parametrize("M", [2, 4, 8, 16])
@pytest.mark.parametrize("N", [64, 128, 512, 5120])
def test_rms_layernorm(M: int, N: int):
torch.manual_seed(123)
torch.cuda.empty_cache()
>       torch.cuda.synchronize()
2025-04-11T04:23:18.5403853Z
tests/test_infer/test_kernels/cuda/test_rms_layernorm.py:16:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.5404224Z
device = None
2025-04-11T04:23:18.5404313Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.5404663Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5406303Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
__________________________ test_rms_layernorm[512-2] ___________________________
2025-04-11T04:23:18.5406683Z
M = 2, N = 512
2025-04-11T04:23:18.5406763Z
@pytest.mark.parametrize("M", [2, 4, 8, 16])
@pytest.mark.parametrize("N", [64, 128, 512, 5120])
def test_rms_layernorm(M: int, N: int):
torch.manual_seed(123)
torch.cuda.empty_cache()
>       torch.cuda.synchronize()
2025-04-11T04:23:18.5407363Z
tests/test_infer/test_kernels/cuda/test_rms_layernorm.py:16:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.5407632Z
device = None
2025-04-11T04:23:18.5407715Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.5408058Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5409635Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
__________________________ test_rms_layernorm[512-4] ___________________________
2025-04-11T04:23:18.5410096Z
M = 4, N = 512
2025-04-11T04:23:18.5410180Z
@pytest.mark.parametrize("M", [2, 4, 8, 16])
@pytest.mark.parametrize("N", [64, 128, 512, 5120])
def test_rms_layernorm(M: int, N: int):
torch.manual_seed(123)
torch.cuda.empty_cache()
>       torch.cuda.synchronize()
2025-04-11T04:23:18.5410788Z
tests/test_infer/test_kernels/cuda/test_rms_layernorm.py:16:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.5411046Z
device = None
2025-04-11T04:23:18.5411131Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.5411475Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5413150Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
__________________________ test_rms_layernorm[512-8] ___________________________
2025-04-11T04:23:18.5413532Z
M = 8, N = 512
2025-04-11T04:23:18.5413615Z
@pytest.mark.parametrize("M", [2, 4, 8, 16])
@pytest.mark.parametrize("N", [64, 128, 512, 5120])
def test_rms_layernorm(M: int, N: int):
torch.manual_seed(123)
torch.cuda.empty_cache()
>       torch.cuda.synchronize()
2025-04-11T04:23:18.5414218Z
tests/test_infer/test_kernels/cuda/test_rms_layernorm.py:16:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.5414484Z
device = None
2025-04-11T04:23:18.5414572Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.5414918Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5416570Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
__________________________ test_rms_layernorm[512-16] __________________________
2025-04-11T04:23:18.5416948Z
M = 16, N = 512
2025-04-11T04:23:18.5417027Z
@pytest.mark.parametrize("M", [2, 4, 8, 16])
@pytest.mark.parametrize("N", [64, 128, 512, 5120])
def test_rms_layernorm(M: int, N: int):
torch.manual_seed(123)
torch.cuda.empty_cache()
>       torch.cuda.synchronize()
2025-04-11T04:23:18.5417705Z
tests/test_infer/test_kernels/cuda/test_rms_layernorm.py:16:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.5417975Z
device = None
2025-04-11T04:23:18.5418056Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.5418393Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5419953Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
__________________________ test_rms_layernorm[5120-2] __________________________
2025-04-11T04:23:18.5420331Z
M = 2, N = 5120
2025-04-11T04:23:18.5420418Z
@pytest.mark.parametrize("M", [2, 4, 8, 16])
@pytest.mark.parametrize("N", [64, 128, 512, 5120])
def test_rms_layernorm(M: int, N: int):
torch.manual_seed(123)
torch.cuda.empty_cache()
>       torch.cuda.synchronize()
2025-04-11T04:23:18.5421016Z
tests/test_infer/test_kernels/cuda/test_rms_layernorm.py:16:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.5421284Z
device = None
2025-04-11T04:23:18.5421368Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.5421796Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5423452Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
__________________________ test_rms_layernorm[5120-4] __________________________
2025-04-11T04:23:18.5423957Z
M = 4, N = 5120
2025-04-11T04:23:18.5424044Z
@pytest.mark.parametrize("M", [2, 4, 8, 16])
@pytest.mark.parametrize("N", [64, 128, 512, 5120])
def test_rms_layernorm(M: int, N: int):
torch.manual_seed(123)
torch.cuda.empty_cache()
>       torch.cuda.synchronize()
2025-04-11T04:23:18.5424648Z
tests/test_infer/test_kernels/cuda/test_rms_layernorm.py:16:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.5424917Z
device = None
2025-04-11T04:23:18.5425000Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.5425357Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5426921Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
__________________________ test_rms_layernorm[5120-8] __________________________
2025-04-11T04:23:18.5427298Z
M = 8, N = 5120
2025-04-11T04:23:18.5427377Z
@pytest.mark.parametrize("M", [2, 4, 8, 16])
@pytest.mark.parametrize("N", [64, 128, 512, 5120])
def test_rms_layernorm(M: int, N: int):
torch.manual_seed(123)
torch.cuda.empty_cache()
>       torch.cuda.synchronize()
2025-04-11T04:23:18.5428075Z
tests/test_infer/test_kernels/cuda/test_rms_layernorm.py:16:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.5428338Z
device = None
2025-04-11T04:23:18.5428445Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.5428793Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5430445Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
_________________________ test_rms_layernorm[5120-16] __________________________
2025-04-11T04:23:18.5430824Z
M = 16, N = 5120
2025-04-11T04:23:18.5430910Z
@pytest.mark.parametrize("M", [2, 4, 8, 16])
@pytest.mark.parametrize("N", [64, 128, 512, 5120])
def test_rms_layernorm(M: int, N: int):
torch.manual_seed(123)
torch.cuda.empty_cache()
>       torch.cuda.synchronize()
2025-04-11T04:23:18.5431509Z
tests/test_infer/test_kernels/cuda/test_rms_layernorm.py:16:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.5431773Z
device = None
2025-04-11T04:23:18.5431858Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.5432203Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5433858Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
____________________ test_rotary_emb[dtype0-64-16-32-64-4] _____________________
2025-04-11T04:23:18.5434254Z
BATCH_SIZE = 4, SEQ_LEN = 64, H = 32, K_H = 16, D = 64, dtype = torch.float16
2025-04-11T04:23:18.5434425Z
@pytest.mark.parametrize("BATCH_SIZE", [4])
@pytest.mark.parametrize("SEQ_LEN", [64])
@pytest.mark.parametrize("H", [32])
@pytest.mark.parametrize("K_H", [16, 32])
@pytest.mark.parametrize("D", [64])
@pytest.mark.parametrize("dtype", [torch.float16, torch.float32])
def test_rotary_emb(BATCH_SIZE, SEQ_LEN, H, K_H, D, dtype):
torch.manual_seed(10)
TOTAL_TOKENS = BATCH_SIZE * SEQ_LEN
# our crafted op equals to Transformers
x0 = torch.randn(BATCH_SIZE, H, SEQ_LEN, D, dtype=dtype)
x1 = torch.randn(BATCH_SIZE, H, SEQ_LEN, D, dtype=dtype)
2025-04-11T04:23:18.5435991Z
position_ids = torch.arange(TOTAL_TOKENS).reshape((BATCH_SIZE, SEQ_LEN))
2025-04-11T04:23:18.5436236Z
emb = LlamaRotaryEmbedding(D)
2025-04-11T04:23:18.5436409Z
cos, sin = emb(x0, position_ids)
embd_x0, _ = apply_rotary_pos_emb(x0, x1, cos, sin)
cos = cos.reshape((TOTAL_TOKENS, -1))
sin = sin.reshape((TOTAL_TOKENS, -1))
cos_2 = cos[:, : D // 2]
sin_2 = sin[:, : D // 2]
x2 = x0.transpose(1, 2).reshape(TOTAL_TOKENS, H, D)
embd_stimulated_x = torch_rotary_emb(x2, cos_2, sin_2)
embd_stimulated_x = embd_stimulated_x.reshape((BATCH_SIZE, SEQ_LEN, H, D)).transpose(1, 2)
assert torch.allclose(embd_x0, embd_stimulated_x)
2025-04-11T04:23:18.5437673Z
# create data
block_size = 32
max_blocks_per_sequence = (TOTAL_TOKENS + block_size - 1) // block_size
q_shape = (TOTAL_TOKENS, H, D)
>       q = -2.3 + 0.5 * torch.randn(q_shape, dtype=dtype, device="cuda")
E       RuntimeError: CUDA error: out of memory
E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5438953Z
tests/test_infer/test_kernels/cuda/test_rotary_embdding_unpad.py:53: RuntimeError
____________________ test_rotary_emb[dtype0-64-32-32-64-4] _____________________
2025-04-11T04:23:18.5439304Z
BATCH_SIZE = 4, SEQ_LEN = 64, H = 32, K_H = 32, D = 64, dtype = torch.float16
2025-04-11T04:23:18.5439462Z
@pytest.mark.parametrize("BATCH_SIZE", [4])
@pytest.mark.parametrize("SEQ_LEN", [64])
@pytest.mark.parametrize("H", [32])
@pytest.mark.parametrize("K_H", [16, 32])
@pytest.mark.parametrize("D", [64])
@pytest.mark.parametrize("dtype", [torch.float16, torch.float32])
def test_rotary_emb(BATCH_SIZE, SEQ_LEN, H, K_H, D, dtype):
torch.manual_seed(10)
TOTAL_TOKENS = BATCH_SIZE * SEQ_LEN
# our crafted op equals to Transformers
x0 = torch.randn(BATCH_SIZE, H, SEQ_LEN, D, dtype=dtype)
x1 = torch.randn(BATCH_SIZE, H, SEQ_LEN, D, dtype=dtype)
2025-04-11T04:23:18.5440988Z
position_ids = torch.arange(TOTAL_TOKENS).reshape((BATCH_SIZE, SEQ_LEN))
2025-04-11T04:23:18.5441230Z
emb = LlamaRotaryEmbedding(D)
2025-04-11T04:23:18.5441401Z
cos, sin = emb(x0, position_ids)
embd_x0, _ = apply_rotary_pos_emb(x0, x1, cos, sin)
cos = cos.reshape((TOTAL_TOKENS, -1))
sin = sin.reshape((TOTAL_TOKENS, -1))
cos_2 = cos[:, : D // 2]
sin_2 = sin[:, : D // 2]
x2 = x0.transpose(1, 2).reshape(TOTAL_TOKENS, H, D)
embd_stimulated_x = torch_rotary_emb(x2, cos_2, sin_2)
embd_stimulated_x = embd_stimulated_x.reshape((BATCH_SIZE, SEQ_LEN, H, D)).transpose(1, 2)
assert torch.allclose(embd_x0, embd_stimulated_x)
2025-04-11T04:23:18.5442747Z
# create data
block_size = 32
max_blocks_per_sequence = (TOTAL_TOKENS + block_size - 1) // block_size
q_shape = (TOTAL_TOKENS, H, D)
>       q = -2.3 + 0.5 * torch.randn(q_shape, dtype=dtype, device="cuda")
E       RuntimeError: CUDA error: out of memory
E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5444017Z
tests/test_infer/test_kernels/cuda/test_rotary_embdding_unpad.py:53: RuntimeError
____________________ test_rotary_emb[dtype1-64-16-32-64-4] _____________________
2025-04-11T04:23:18.5444365Z
BATCH_SIZE = 4, SEQ_LEN = 64, H = 32, K_H = 16, D = 64, dtype = torch.float32
2025-04-11T04:23:18.5444525Z
@pytest.mark.parametrize("BATCH_SIZE", [4])
@pytest.mark.parametrize("SEQ_LEN", [64])
@pytest.mark.parametrize("H", [32])
@pytest.mark.parametrize("K_H", [16, 32])
@pytest.mark.parametrize("D", [64])
@pytest.mark.parametrize("dtype", [torch.float16, torch.float32])
def test_rotary_emb(BATCH_SIZE, SEQ_LEN, H, K_H, D, dtype):
torch.manual_seed(10)
TOTAL_TOKENS = BATCH_SIZE * SEQ_LEN
# our crafted op equals to Transformers
x0 = torch.randn(BATCH_SIZE, H, SEQ_LEN, D, dtype=dtype)
x1 = torch.randn(BATCH_SIZE, H, SEQ_LEN, D, dtype=dtype)
2025-04-11T04:23:18.5445982Z
position_ids = torch.arange(TOTAL_TOKENS).reshape((BATCH_SIZE, SEQ_LEN))
2025-04-11T04:23:18.5446333Z
emb = LlamaRotaryEmbedding(D)
2025-04-11T04:23:18.5446511Z
cos, sin = emb(x0, position_ids)
embd_x0, _ = apply_rotary_pos_emb(x0, x1, cos, sin)
cos = cos.reshape((TOTAL_TOKENS, -1))
sin = sin.reshape((TOTAL_TOKENS, -1))
cos_2 = cos[:, : D // 2]
sin_2 = sin[:, : D // 2]
x2 = x0.transpose(1, 2).reshape(TOTAL_TOKENS, H, D)
embd_stimulated_x = torch_rotary_emb(x2, cos_2, sin_2)
embd_stimulated_x = embd_stimulated_x.reshape((BATCH_SIZE, SEQ_LEN, H, D)).transpose(1, 2)
assert torch.allclose(embd_x0, embd_stimulated_x)
2025-04-11T04:23:18.5447746Z
# create data
block_size = 32
max_blocks_per_sequence = (TOTAL_TOKENS + block_size - 1) // block_size
q_shape = (TOTAL_TOKENS, H, D)
>       q = -2.3 + 0.5 * torch.randn(q_shape, dtype=dtype, device="cuda")
E       RuntimeError: CUDA error: out of memory
E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5449101Z
tests/test_infer/test_kernels/cuda/test_rotary_embdding_unpad.py:53: RuntimeError
____________________ test_rotary_emb[dtype1-64-32-32-64-4] _____________________
2025-04-11T04:23:18.5449451Z
BATCH_SIZE = 4, SEQ_LEN = 64, H = 32, K_H = 32, D = 64, dtype = torch.float32
2025-04-11T04:23:18.5449612Z
@pytest.mark.parametrize("BATCH_SIZE", [4])
@pytest.mark.parametrize("SEQ_LEN", [64])
@pytest.mark.parametrize("H", [32])
@pytest.mark.parametrize("K_H", [16, 32])
@pytest.mark.parametrize("D", [64])
@pytest.mark.parametrize("dtype", [torch.float16, torch.float32])
def test_rotary_emb(BATCH_SIZE, SEQ_LEN, H, K_H, D, dtype):
torch.manual_seed(10)
TOTAL_TOKENS = BATCH_SIZE * SEQ_LEN
# our crafted op equals to Transformers
x0 = torch.randn(BATCH_SIZE, H, SEQ_LEN, D, dtype=dtype)
x1 = torch.randn(BATCH_SIZE, H, SEQ_LEN, D, dtype=dtype)
2025-04-11T04:23:18.5451031Z
position_ids = torch.arange(TOTAL_TOKENS).reshape((BATCH_SIZE, SEQ_LEN))
2025-04-11T04:23:18.5451276Z
emb = LlamaRotaryEmbedding(D)
2025-04-11T04:23:18.5451442Z
cos, sin = emb(x0, position_ids)
embd_x0, _ = apply_rotary_pos_emb(x0, x1, cos, sin)
cos = cos.reshape((TOTAL_TOKENS, -1))
sin = sin.reshape((TOTAL_TOKENS, -1))
cos_2 = cos[:, : D // 2]
sin_2 = sin[:, : D // 2]
x2 = x0.transpose(1, 2).reshape(TOTAL_TOKENS, H, D)
embd_stimulated_x = torch_rotary_emb(x2, cos_2, sin_2)
embd_stimulated_x = embd_stimulated_x.reshape((BATCH_SIZE, SEQ_LEN, H, D)).transpose(1, 2)
assert torch.allclose(embd_x0, embd_stimulated_x)
2025-04-11T04:23:18.5452756Z
# create data
block_size = 32
max_blocks_per_sequence = (TOTAL_TOKENS + block_size - 1) // block_size
q_shape = (TOTAL_TOKENS, H, D)
>       q = -2.3 + 0.5 * torch.randn(q_shape, dtype=dtype, device="cuda")
E       RuntimeError: CUDA error: out of memory
E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5454008Z
tests/test_infer/test_kernels/cuda/test_rotary_embdding_unpad.py:53: RuntimeError
_____________________ test_silu_and_mul[dtype0-11008-64-2] _____________________
2025-04-11T04:23:18.5454437Z
SHAPE_X = 2, SHAPE_Y = 64, SHAPE_Z = 11008, dtype = torch.float32
2025-04-11T04:23:18.5454590Z
@pytest.mark.parametrize("SHAPE_X", [2])
@pytest.mark.parametrize("SHAPE_Y", [64])
@pytest.mark.parametrize("SHAPE_Z", [11008])
@pytest.mark.parametrize("dtype", [torch.float32, torch.float16])
def test_silu_and_mul(SHAPE_X, SHAPE_Y, SHAPE_Z, dtype):
torch.manual_seed(5)
device = get_current_device()
>       ref_input = torch.randn(SHAPE_X, SHAPE_Y, SHAPE_Z, dtype=dtype, device=device)
E       RuntimeError: CUDA error: out of memory
E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5456253Z
tests/test_infer/test_kernels/cuda/test_silu_and_mul.py:17: RuntimeError
_____________________ test_silu_and_mul[dtype1-11008-64-2] _____________________
2025-04-11T04:23:18.5456581Z
SHAPE_X = 2, SHAPE_Y = 64, SHAPE_Z = 11008, dtype = torch.float16
2025-04-11T04:23:18.5456722Z
@pytest.mark.parametrize("SHAPE_X", [2])
@pytest.mark.parametrize("SHAPE_Y", [64])
@pytest.mark.parametrize("SHAPE_Z", [11008])
@pytest.mark.parametrize("dtype", [torch.float32, torch.float16])
def test_silu_and_mul(SHAPE_X, SHAPE_Y, SHAPE_Z, dtype):
torch.manual_seed(5)
device = get_current_device()
>       ref_input = torch.randn(SHAPE_X, SHAPE_Y, SHAPE_Z, dtype=dtype, device=device)
E       RuntimeError: CUDA error: out of memory
E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5458508Z
tests/test_infer/test_kernels/cuda/test_silu_and_mul.py:17: RuntimeError
_____________ test_context_attention[True-False-True-1-16-8-16-7] ______________
2025-04-11T04:23:18.5458861Z
bsz = 7, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
kv_group_num = 1, same_context_len = True, use_alibi_slopes = False
use_new_kcache_layout = True
2025-04-11T04:23:18.5459262Z
@pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
@pytest.mark.parametrize("bsz", [7, 32])
@pytest.mark.parametrize("block_size", [16, 32])
@pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
@pytest.mark.parametrize("num_attn_heads", [16])
@pytest.mark.parametrize("kv_group_num", [1, 4])
@pytest.mark.parametrize("same_context_len", [True, False])
@pytest.mark.parametrize("use_alibi_slopes", [True, False])
@pytest.mark.parametrize("use_new_kcache_layout", [True, False])
def test_context_attention(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_attn_heads: int,
kv_group_num: int,
same_context_len: bool,
use_alibi_slopes: bool,
use_new_kcache_layout: bool,
):
if use_new_kcache_layout and use_alibi_slopes:
# TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
# the code (alibi kernel) will be refactored later to avoid code duplication, when
# the whole triton flow with new k cache layout has been supported and tested.
# And tests for the alibi kernel using new kcache layout will be added then.
return
2025-04-11T04:23:18.5462481Z
torch.manual_seed(123)
# It's necessary to clear cache here.
torch.cuda.empty_cache()
>       torch.cuda.synchronize()
2025-04-11T04:23:18.5462861Z
tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:800: in synchronize
with torch.cuda.device(device):
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.5463615Z
self = <torch.cuda.device object at 0x7fb5b84e9bd0>
2025-04-11T04:23:18.5463741Z
def __enter__(self):
>       self.prev_idx = torch.cuda._exchange_device(self.idx)
E       RuntimeError: CUDA error: out of memory
E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5464729Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:374: RuntimeError
_____________ test_context_attention[True-False-True-1-16-8-16-32] _____________
2025-04-11T04:23:18.5465148Z
bsz = 32, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
kv_group_num = 1, same_context_len = True, use_alibi_slopes = False
use_new_kcache_layout = True
2025-04-11T04:23:18.5465549Z
@pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
@pytest.mark.parametrize("bsz", [7, 32])
@pytest.mark.parametrize("block_size", [16, 32])
@pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
@pytest.mark.parametrize("num_attn_heads", [16])
@pytest.mark.parametrize("kv_group_num", [1, 4])
@pytest.mark.parametrize("same_context_len", [True, False])
@pytest.mark.parametrize("use_alibi_slopes", [True, False])
@pytest.mark.parametrize("use_new_kcache_layout", [True, False])
def test_context_attention(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_attn_heads: int,
kv_group_num: int,
same_context_len: bool,
use_alibi_slopes: bool,
use_new_kcache_layout: bool,
):
if use_new_kcache_layout and use_alibi_slopes:
# TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
# the code (alibi kernel) will be refactored later to avoid code duplication, when
# the whole triton flow with new k cache layout has been supported and tested.
# And tests for the alibi kernel using new kcache layout will be added then.
return
2025-04-11T04:23:18.5468818Z
torch.manual_seed(123)
# It's necessary to clear cache here.
torch.cuda.empty_cache()
>       torch.cuda.synchronize()
2025-04-11T04:23:18.5469198Z
tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.5469493Z
device = None
2025-04-11T04:23:18.5469582Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.5469940Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5471661Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
_____________ test_context_attention[True-False-True-1-16-8-32-7] ______________
2025-04-11T04:23:18.5472072Z
bsz = 7, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
kv_group_num = 1, same_context_len = True, use_alibi_slopes = False
use_new_kcache_layout = True
2025-04-11T04:23:18.5472472Z
@pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
@pytest.mark.parametrize("bsz", [7, 32])
@pytest.mark.parametrize("block_size", [16, 32])
@pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
@pytest.mark.parametrize("num_attn_heads", [16])
@pytest.mark.parametrize("kv_group_num", [1, 4])
@pytest.mark.parametrize("same_context_len", [True, False])
@pytest.mark.parametrize("use_alibi_slopes", [True, False])
@pytest.mark.parametrize("use_new_kcache_layout", [True, False])
def test_context_attention(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_attn_heads: int,
kv_group_num: int,
same_context_len: bool,
use_alibi_slopes: bool,
use_new_kcache_layout: bool,
):
if use_new_kcache_layout and use_alibi_slopes:
# TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
# the code (alibi kernel) will be refactored later to avoid code duplication, when
# the whole triton flow with new k cache layout has been supported and tested.
# And tests for the alibi kernel using new kcache layout will be added then.
return
2025-04-11T04:23:18.5475685Z
torch.manual_seed(123)
# It's necessary to clear cache here.
torch.cuda.empty_cache()
>       torch.cuda.synchronize()
2025-04-11T04:23:18.5476086Z
tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.5476381Z
device = None
2025-04-11T04:23:18.5476465Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.5476811Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5478494Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
_____________ test_context_attention[True-False-True-1-16-8-32-32] _____________
2025-04-11T04:23:18.5478915Z
bsz = 32, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
kv_group_num = 1, same_context_len = True, use_alibi_slopes = False
use_new_kcache_layout = True
2025-04-11T04:23:18.5479413Z
@pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
@pytest.mark.parametrize("bsz", [7, 32])
@pytest.mark.parametrize("block_size", [16, 32])
@pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
@pytest.mark.parametrize("num_attn_heads", [16])
@pytest.mark.parametrize("kv_group_num", [1, 4])
@pytest.mark.parametrize("same_context_len", [True, False])
@pytest.mark.parametrize("use_alibi_slopes", [True, False])
@pytest.mark.parametrize("use_new_kcache_layout", [True, False])
def test_context_attention(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_attn_heads: int,
kv_group_num: int,
same_context_len: bool,
use_alibi_slopes: bool,
use_new_kcache_layout: bool,
):
if use_new_kcache_layout and use_alibi_slopes:
# TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
# the code (alibi kernel) will be refactored later to avoid code duplication, when
# the whole triton flow with new k cache layout has been supported and tested.
# And tests for the alibi kernel using new kcache layout will be added then.
return
2025-04-11T04:23:18.5482508Z
torch.manual_seed(123)
# It's necessary to clear cache here.
torch.cuda.empty_cache()
>       torch.cuda.synchronize()
2025-04-11T04:23:18.5482880Z
tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.5483166Z
device = None
2025-04-11T04:23:18.5483249Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.5483802Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5485479Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
_____________ test_context_attention[True-False-True-1-16-16-16-7] _____________
2025-04-11T04:23:18.5485916Z
bsz = 7, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
kv_group_num = 1, same_context_len = True, use_alibi_slopes = False
use_new_kcache_layout = True
2025-04-11T04:23:18.5486319Z
@pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
@pytest.mark.parametrize("bsz", [7, 32])
@pytest.mark.parametrize("block_size", [16, 32])
@pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
@pytest.mark.parametrize("num_attn_heads", [16])
@pytest.mark.parametrize("kv_group_num", [1, 4])
@pytest.mark.parametrize("same_context_len", [True, False])
@pytest.mark.parametrize("use_alibi_slopes", [True, False])
@pytest.mark.parametrize("use_new_kcache_layout", [True, False])
def test_context_attention(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_attn_heads: int,
kv_group_num: int,
same_context_len: bool,
use_alibi_slopes: bool,
use_new_kcache_layout: bool,
):
if use_new_kcache_layout and use_alibi_slopes:
# TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
# the code (alibi kernel) will be refactored later to avoid code duplication, when
# the whole triton flow with new k cache layout has been supported and tested.
# And tests for the alibi kernel using new kcache layout will be added then.
return
2025-04-11T04:23:18.5489398Z
torch.manual_seed(123)
# It's necessary to clear cache here.
torch.cuda.empty_cache()
>       torch.cuda.synchronize()
2025-04-11T04:23:18.5489858Z
tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.5490152Z
device = None
2025-04-11T04:23:18.5490234Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.5490582Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5492287Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
____________ test_context_attention[True-False-True-1-16-16-16-32] _____________
2025-04-11T04:23:18.5492703Z
bsz = 32, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
kv_group_num = 1, same_context_len = True, use_alibi_slopes = False
use_new_kcache_layout = True
2025-04-11T04:23:18.5493102Z
@pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
@pytest.mark.parametrize("bsz", [7, 32])
@pytest.mark.parametrize("block_size", [16, 32])
@pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
@pytest.mark.parametrize("num_attn_heads", [16])
@pytest.mark.parametrize("kv_group_num", [1, 4])
@pytest.mark.parametrize("same_context_len", [True, False])
@pytest.mark.parametrize("use_alibi_slopes", [True, False])
@pytest.mark.parametrize("use_new_kcache_layout", [True, False])
def test_context_attention(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_attn_heads: int,
kv_group_num: int,
same_context_len: bool,
use_alibi_slopes: bool,
use_new_kcache_layout: bool,
):
if use_new_kcache_layout and use_alibi_slopes:
# TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
# the code (alibi kernel) will be refactored later to avoid code duplication, when
# the whole triton flow with new k cache layout has been supported and tested.
# And tests for the alibi kernel using new kcache layout will be added then.
return
2025-04-11T04:23:18.5496321Z
torch.manual_seed(123)
# It's necessary to clear cache here.
torch.cuda.empty_cache()
>       torch.cuda.synchronize()
2025-04-11T04:23:18.5496703Z
tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.5496991Z
device = None
2025-04-11T04:23:18.5497072Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.5497419Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5499083Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
_____________ test_context_attention[True-False-True-1-16-16-32-7] _____________
2025-04-11T04:23:18.5499506Z
bsz = 7, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
kv_group_num = 1, same_context_len = True, use_alibi_slopes = False
use_new_kcache_layout = True
2025-04-11T04:23:18.5499899Z
@pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
@pytest.mark.parametrize("bsz", [7, 32])
@pytest.mark.parametrize("block_size", [16, 32])
@pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
@pytest.mark.parametrize("num_attn_heads", [16])
@pytest.mark.parametrize("kv_group_num", [1, 4])
@pytest.mark.parametrize("same_context_len", [True, False])
@pytest.mark.parametrize("use_alibi_slopes", [True, False])
@pytest.mark.parametrize("use_new_kcache_layout", [True, False])
def test_context_attention(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_attn_heads: int,
kv_group_num: int,
same_context_len: bool,
use_alibi_slopes: bool,
use_new_kcache_layout: bool,
):
if use_new_kcache_layout and use_alibi_slopes:
# TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
# the code (alibi kernel) will be refactored later to avoid code duplication, when
# the whole triton flow with new k cache layout has been supported and tested.
# And tests for the alibi kernel using new kcache layout will be added then.
return
2025-04-11T04:23:18.5503009Z
torch.manual_seed(123)
# It's necessary to clear cache here.
torch.cuda.empty_cache()
>       torch.cuda.synchronize()
2025-04-11T04:23:18.5503375Z
tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.5503660Z
device = None
2025-04-11T04:23:18.5503743Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.5504167Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5505764Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
____________ test_context_attention[True-False-True-1-16-16-32-32] _____________
2025-04-11T04:23:18.5506181Z
bsz = 32, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
kv_group_num = 1, same_context_len = True, use_alibi_slopes = False
use_new_kcache_layout = True
2025-04-11T04:23:18.5506571Z
@pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
@pytest.mark.parametrize("bsz", [7, 32])
@pytest.mark.parametrize("block_size", [16, 32])
@pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
@pytest.mark.parametrize("num_attn_heads", [16])
@pytest.mark.parametrize("kv_group_num", [1, 4])
@pytest.mark.parametrize("same_context_len", [True, False])
@pytest.mark.parametrize("use_alibi_slopes", [True, False])
@pytest.mark.parametrize("use_new_kcache_layout", [True, False])
def test_context_attention(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_attn_heads: int,
kv_group_num: int,
same_context_len: bool,
use_alibi_slopes: bool,
use_new_kcache_layout: bool,
):
if use_new_kcache_layout and use_alibi_slopes:
# TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
# the code (alibi kernel) will be refactored later to avoid code duplication, when
# the whole triton flow with new k cache layout has been supported and tested.
# And tests for the alibi kernel using new kcache layout will be added then.
return
2025-04-11T04:23:18.5509713Z
torch.manual_seed(123)
# It's necessary to clear cache here.
torch.cuda.empty_cache()
>       torch.cuda.synchronize()
2025-04-11T04:23:18.5510088Z
tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.5510480Z
device = None
2025-04-11T04:23:18.5510563Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.5510902Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5512478Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
_____________ test_context_attention[True-False-True-4-16-8-16-7] ______________
2025-04-11T04:23:18.5512904Z
bsz = 7, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
kv_group_num = 4, same_context_len = True, use_alibi_slopes = False
use_new_kcache_layout = True
2025-04-11T04:23:18.5513303Z
@pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
@pytest.mark.parametrize("bsz", [7, 32])
@pytest.mark.parametrize("block_size", [16, 32])
@pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
@pytest.mark.parametrize("num_attn_heads", [16])
@pytest.mark.parametrize("kv_group_num", [1, 4])
@pytest.mark.parametrize("same_context_len", [True, False])
@pytest.mark.parametrize("use_alibi_slopes", [True, False])
@pytest.mark.parametrize("use_new_kcache_layout", [True, False])
def test_context_attention(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_attn_heads: int,
kv_group_num: int,
same_context_len: bool,
use_alibi_slopes: bool,
use_new_kcache_layout: bool,
):
if use_new_kcache_layout and use_alibi_slopes:
# TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
# the code (alibi kernel) will be refactored later to avoid code duplication, when
# the whole triton flow with new k cache layout has been supported and tested.
# And tests for the alibi kernel using new kcache layout will be added then.
return
2025-04-11T04:23:18.5516542Z
torch.manual_seed(123)
# It's necessary to clear cache here.
torch.cuda.empty_cache()
>       torch.cuda.synchronize()
2025-04-11T04:23:18.5516905Z
tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.5517193Z
device = None
2025-04-11T04:23:18.5517277Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.5517620Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5519173Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
_____________ test_context_attention[True-False-True-4-16-8-16-32] _____________
2025-04-11T04:23:18.5519594Z
bsz = 32, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
kv_group_num = 4, same_context_len = True, use_alibi_slopes = False
use_new_kcache_layout = True
2025-04-11T04:23:18.5519985Z
@pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
@pytest.mark.parametrize("bsz", [7, 32])
@pytest.mark.parametrize("block_size", [16, 32])
@pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
@pytest.mark.parametrize("num_attn_heads", [16])
@pytest.mark.parametrize("kv_group_num", [1, 4])
@pytest.mark.parametrize("same_context_len", [True, False])
@pytest.mark.parametrize("use_alibi_slopes", [True, False])
@pytest.mark.parametrize("use_new_kcache_layout", [True, False])
def test_context_attention(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_attn_heads: int,
kv_group_num: int,
same_context_len: bool,
use_alibi_slopes: bool,
use_new_kcache_layout: bool,
):
if use_new_kcache_layout and use_alibi_slopes:
# TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
# the code (alibi kernel) will be refactored later to avoid code duplication, when
# the whole triton flow with new k cache layout has been supported and tested.
# And tests for the alibi kernel using new kcache layout will be added then.
return
2025-04-11T04:23:18.5523250Z
torch.manual_seed(123)
# It's necessary to clear cache here.
torch.cuda.empty_cache()
>       torch.cuda.synchronize()
2025-04-11T04:23:18.5523633Z
tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.5523919Z
device = None
2025-04-11T04:23:18.5524004Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.5524341Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5525898Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
_____________ test_context_attention[True-False-True-4-16-8-32-7] ______________
2025-04-11T04:23:18.5526310Z
bsz = 7, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
kv_group_num = 4, same_context_len = True, use_alibi_slopes = False
use_new_kcache_layout = True
2025-04-11T04:23:18.5526785Z
@pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
@pytest.mark.parametrize("bsz", [7, 32])
@pytest.mark.parametrize("block_size", [16, 32])
@pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
@pytest.mark.parametrize("num_attn_heads", [16])
@pytest.mark.parametrize("kv_group_num", [1, 4])
@pytest.mark.parametrize("same_context_len", [True, False])
@pytest.mark.parametrize("use_alibi_slopes", [True, False])
@pytest.mark.parametrize("use_new_kcache_layout", [True, False])
def test_context_attention(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_attn_heads: int,
kv_group_num: int,
same_context_len: bool,
use_alibi_slopes: bool,
use_new_kcache_layout: bool,
):
if use_new_kcache_layout and use_alibi_slopes:
# TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
# the code (alibi kernel) will be refactored later to avoid code duplication, when
# the whole triton flow with new k cache layout has been supported and tested.
# And tests for the alibi kernel using new kcache layout will be added then.
return
2025-04-11T04:23:18.5529891Z
torch.manual_seed(123)
# It's necessary to clear cache here.
torch.cuda.empty_cache()
>       torch.cuda.synchronize()
2025-04-11T04:23:18.5530262Z
tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.5530545Z
device = None
2025-04-11T04:23:18.5530624Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.5530958Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5532519Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
_____________ test_context_attention[True-False-True-4-16-8-32-32] _____________
2025-04-11T04:23:18.5533042Z
bsz = 32, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
kv_group_num = 4, same_context_len = True, use_alibi_slopes = False
use_new_kcache_layout = True
2025-04-11T04:23:18.5533454Z
@pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
@pytest.mark.parametrize("bsz", [7, 32])
@pytest.mark.parametrize("block_size", [16, 32])
@pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
@pytest.mark.parametrize("num_attn_heads", [16])
@pytest.mark.parametrize("kv_group_num", [1, 4])
@pytest.mark.parametrize("same_context_len", [True, False])
@pytest.mark.parametrize("use_alibi_slopes", [True, False])
@pytest.mark.parametrize("use_new_kcache_layout", [True, False])
def test_context_attention(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_attn_heads: int,
kv_group_num: int,
same_context_len: bool,
use_alibi_slopes: bool,
use_new_kcache_layout: bool,
):
if use_new_kcache_layout and use_alibi_slopes:
# TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
# the code (alibi kernel) will be refactored later to avoid code duplication, when
# the whole triton flow with new k cache layout has been supported and tested.
# And tests for the alibi kernel using new kcache layout will be added then.
return
2025-04-11T04:23:18.5536569Z
torch.manual_seed(123)
# It's necessary to clear cache here.
torch.cuda.empty_cache()
>       torch.cuda.synchronize()
2025-04-11T04:23:18.5536929Z
tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.5537211Z
device = None
2025-04-11T04:23:18.5537297Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.5537635Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5539265Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
_____________ test_context_attention[True-False-True-4-16-16-16-7] _____________
2025-04-11T04:23:18.5539675Z
bsz = 7, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
kv_group_num = 4, same_context_len = True, use_alibi_slopes = False
use_new_kcache_layout = True
2025-04-11T04:23:18.5540064Z
@pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
@pytest.mark.parametrize("bsz", [7, 32])
@pytest.mark.parametrize("block_size", [16, 32])
@pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
@pytest.mark.parametrize("num_attn_heads", [16])
@pytest.mark.parametrize("kv_group_num", [1, 4])
@pytest.mark.parametrize("same_context_len", [True, False])
@pytest.mark.parametrize("use_alibi_slopes", [True, False])
@pytest.mark.parametrize("use_new_kcache_layout", [True, False])
def test_context_attention(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_attn_heads: int,
kv_group_num: int,
same_context_len: bool,
use_alibi_slopes: bool,
use_new_kcache_layout: bool,
):
if use_new_kcache_layout and use_alibi_slopes:
# TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
# the code (alibi kernel) will be refactored later to avoid code duplication, when
# the whole triton flow with new k cache layout has been supported and tested.
# And tests for the alibi kernel using new kcache layout will be added then.
return
2025-04-11T04:23:18.5543214Z
torch.manual_seed(123)
# It's necessary to clear cache here.
torch.cuda.empty_cache()
>       torch.cuda.synchronize()
2025-04-11T04:23:18.5543587Z
tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.5543879Z
device = None
2025-04-11T04:23:18.5543958Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.5544298Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5546001Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
____________ test_context_attention[True-False-True-4-16-16-16-32] _____________
2025-04-11T04:23:18.5546413Z
bsz = 32, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
kv_group_num = 4, same_context_len = True, use_alibi_slopes = False
use_new_kcache_layout = True
2025-04-11T04:23:18.5546891Z
@pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
@pytest.mark.parametrize("bsz", [7, 32])
@pytest.mark.parametrize("block_size", [16, 32])
@pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
@pytest.mark.parametrize("num_attn_heads", [16])
@pytest.mark.parametrize("kv_group_num", [1, 4])
@pytest.mark.parametrize("same_context_len", [True, False])
@pytest.mark.parametrize("use_alibi_slopes", [True, False])
@pytest.mark.parametrize("use_new_kcache_layout", [True, False])
def test_context_attention(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_attn_heads: int,
kv_group_num: int,
same_context_len: bool,
use_alibi_slopes: bool,
use_new_kcache_layout: bool,
):
if use_new_kcache_layout and use_alibi_slopes:
# TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
# the code (alibi kernel) will be refactored later to avoid code duplication, when
# the whole triton flow with new k cache layout has been supported and tested.
# And tests for the alibi kernel using new kcache layout will be added then.
return
2025-04-11T04:23:18.5549988Z
torch.manual_seed(123)
# It's necessary to clear cache here.
torch.cuda.empty_cache()
>       torch.cuda.synchronize()
2025-04-11T04:23:18.5550364Z
tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.5550653Z
device = None
2025-04-11T04:23:18.5550737Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.5551178Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5552746Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
_____________ test_context_attention[True-False-True-4-16-16-32-7] _____________
2025-04-11T04:23:18.5553253Z
bsz = 7, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
kv_group_num = 4, same_context_len = True, use_alibi_slopes = False
use_new_kcache_layout = True
2025-04-11T04:23:18.5553648Z
@pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
@pytest.mark.parametrize("bsz", [7, 32])
@pytest.mark.parametrize("block_size", [16, 32])
@pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
@pytest.mark.parametrize("num_attn_heads", [16])
@pytest.mark.parametrize("kv_group_num", [1, 4])
@pytest.mark.parametrize("same_context_len", [True, False])
@pytest.mark.parametrize("use_alibi_slopes", [True, False])
@pytest.mark.parametrize("use_new_kcache_layout", [True, False])
def test_context_attention(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_attn_heads: int,
kv_group_num: int,
same_context_len: bool,
use_alibi_slopes: bool,
use_new_kcache_layout: bool,
):
if use_new_kcache_layout and use_alibi_slopes:
# TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
# the code (alibi kernel) will be refactored later to avoid code duplication, when
# the whole triton flow with new k cache layout has been supported and tested.
# And tests for the alibi kernel using new kcache layout will be added then.
return
2025-04-11T04:23:18.5556734Z
torch.manual_seed(123)
# It's necessary to clear cache here.
torch.cuda.empty_cache()
>       torch.cuda.synchronize()
2025-04-11T04:23:18.5557112Z
tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.5557495Z
device = None
2025-04-11T04:23:18.5557583Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.5557943Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5559607Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
____________ test_context_attention[True-False-True-4-16-16-32-32] _____________
2025-04-11T04:23:18.5560017Z
bsz = 32, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
kv_group_num = 4, same_context_len = True, use_alibi_slopes = False
use_new_kcache_layout = True
2025-04-11T04:23:18.5560421Z
@pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
@pytest.mark.parametrize("bsz", [7, 32])
@pytest.mark.parametrize("block_size", [16, 32])
@pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
@pytest.mark.parametrize("num_attn_heads", [16])
@pytest.mark.parametrize("kv_group_num", [1, 4])
@pytest.mark.parametrize("same_context_len", [True, False])
@pytest.mark.parametrize("use_alibi_slopes", [True, False])
@pytest.mark.parametrize("use_new_kcache_layout", [True, False])
def test_context_attention(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_attn_heads: int,
kv_group_num: int,
same_context_len: bool,
use_alibi_slopes: bool,
use_new_kcache_layout: bool,
):
if use_new_kcache_layout and use_alibi_slopes:
# TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
# the code (alibi kernel) will be refactored later to avoid code duplication, when
# the whole triton flow with new k cache layout has been supported and tested.
# And tests for the alibi kernel using new kcache layout will be added then.
return
2025-04-11T04:23:18.5563448Z
torch.manual_seed(123)
# It's necessary to clear cache here.
torch.cuda.empty_cache()
>       torch.cuda.synchronize()
2025-04-11T04:23:18.5563931Z
tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.5564215Z
device = None
2025-04-11T04:23:18.5564297Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.5564637Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5566295Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
_____________ test_context_attention[True-False-False-1-16-8-16-7] _____________
2025-04-11T04:23:18.5566709Z
bsz = 7, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
kv_group_num = 1, same_context_len = False, use_alibi_slopes = False
use_new_kcache_layout = True
2025-04-11T04:23:18.5567109Z
@pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
@pytest.mark.parametrize("bsz", [7, 32])
@pytest.mark.parametrize("block_size", [16, 32])
@pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
@pytest.mark.parametrize("num_attn_heads", [16])
@pytest.mark.parametrize("kv_group_num", [1, 4])
@pytest.mark.parametrize("same_context_len", [True, False])
@pytest.mark.parametrize("use_alibi_slopes", [True, False])
@pytest.mark.parametrize("use_new_kcache_layout", [True, False])
def test_context_attention(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_attn_heads: int,
kv_group_num: int,
same_context_len: bool,
use_alibi_slopes: bool,
use_new_kcache_layout: bool,
):
if use_new_kcache_layout and use_alibi_slopes:
# TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
# the code (alibi kernel) will be refactored later to avoid code duplication, when
# the whole triton flow with new k cache layout has been supported and tested.
# And tests for the alibi kernel using new kcache layout will be added then.
return
2025-04-11T04:23:18.5570253Z
torch.manual_seed(123)
# It's necessary to clear cache here.
torch.cuda.empty_cache()
>       torch.cuda.synchronize()
2025-04-11T04:23:18.5570620Z
tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.5570902Z
device = None
2025-04-11T04:23:18.5570985Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.5571324Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5572992Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
____________ test_context_attention[True-False-False-1-16-8-16-32] _____________
2025-04-11T04:23:18.5573412Z
bsz = 32, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
kv_group_num = 1, same_context_len = False, use_alibi_slopes = False
use_new_kcache_layout = True
2025-04-11T04:23:18.5573805Z
@pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
@pytest.mark.parametrize("bsz", [7, 32])
@pytest.mark.parametrize("block_size", [16, 32])
@pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
@pytest.mark.parametrize("num_attn_heads", [16])
@pytest.mark.parametrize("kv_group_num", [1, 4])
@pytest.mark.parametrize("same_context_len", [True, False])
@pytest.mark.parametrize("use_alibi_slopes", [True, False])
@pytest.mark.parametrize("use_new_kcache_layout", [True, False])
def test_context_attention(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_attn_heads: int,
kv_group_num: int,
same_context_len: bool,
use_alibi_slopes: bool,
use_new_kcache_layout: bool,
):
if use_new_kcache_layout and use_alibi_slopes:
# TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
# the code (alibi kernel) will be refactored later to avoid code duplication, when
# the whole triton flow with new k cache layout has been supported and tested.
# And tests for the alibi kernel using new kcache layout will be added then.
return
2025-04-11T04:23:18.5576942Z
torch.manual_seed(123)
# It's necessary to clear cache here.
torch.cuda.empty_cache()
>       torch.cuda.synchronize()
2025-04-11T04:23:18.5577314Z
tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.5577771Z
device = None
2025-04-11T04:23:18.5577854Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.5578209Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5579776Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
_____________ test_context_attention[True-False-False-1-16-8-32-7] _____________
2025-04-11T04:23:18.5580182Z
bsz = 7, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
kv_group_num = 1, same_context_len = False, use_alibi_slopes = False
use_new_kcache_layout = True
2025-04-11T04:23:18.5580574Z
@pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
@pytest.mark.parametrize("bsz", [7, 32])
@pytest.mark.parametrize("block_size", [16, 32])
@pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
@pytest.mark.parametrize("num_attn_heads", [16])
@pytest.mark.parametrize("kv_group_num", [1, 4])
@pytest.mark.parametrize("same_context_len", [True, False])
@pytest.mark.parametrize("use_alibi_slopes", [True, False])
@pytest.mark.parametrize("use_new_kcache_layout", [True, False])
def test_context_attention(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_attn_heads: int,
kv_group_num: int,
same_context_len: bool,
use_alibi_slopes: bool,
use_new_kcache_layout: bool,
):
if use_new_kcache_layout and use_alibi_slopes:
# TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
# the code (alibi kernel) will be refactored later to avoid code duplication, when
# the whole triton flow with new k cache layout has been supported and tested.
# And tests for the alibi kernel using new kcache layout will be added then.
return
2025-04-11T04:23:18.5583712Z
torch.manual_seed(123)
# It's necessary to clear cache here.
torch.cuda.empty_cache()
>       torch.cuda.synchronize()
2025-04-11T04:23:18.5584179Z
tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.5584463Z
device = None
2025-04-11T04:23:18.5584543Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.5584880Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5586438Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
____________ test_context_attention[True-False-False-1-16-8-32-32] _____________
2025-04-11T04:23:18.5586857Z
bsz = 32, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
kv_group_num = 1, same_context_len = False, use_alibi_slopes = False
use_new_kcache_layout = True
2025-04-11T04:23:18.5587248Z
@pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
@pytest.mark.parametrize("bsz", [7, 32])
@pytest.mark.parametrize("block_size", [16, 32])
@pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
@pytest.mark.parametrize("num_attn_heads", [16])
@pytest.mark.parametrize("kv_group_num", [1, 4])
@pytest.mark.parametrize("same_context_len", [True, False])
@pytest.mark.parametrize("use_alibi_slopes", [True, False])
@pytest.mark.parametrize("use_new_kcache_layout", [True, False])
def test_context_attention(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_attn_heads: int,
kv_group_num: int,
same_context_len: bool,
use_alibi_slopes: bool,
use_new_kcache_layout: bool,
):
if use_new_kcache_layout and use_alibi_slopes:
# TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
# the code (alibi kernel) will be refactored later to avoid code duplication, when
# the whole triton flow with new k cache layout has been supported and tested.
# And tests for the alibi kernel using new kcache layout will be added then.
return
2025-04-11T04:23:18.5590559Z
torch.manual_seed(123)
# It's necessary to clear cache here.
torch.cuda.empty_cache()
>       torch.cuda.synchronize()
2025-04-11T04:23:18.5590924Z
tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.5591211Z
device = None
2025-04-11T04:23:18.5591294Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.5591634Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5593222Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
____________ test_context_attention[True-False-False-1-16-16-16-7] _____________
2025-04-11T04:23:18.5593644Z
bsz = 7, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
kv_group_num = 1, same_context_len = False, use_alibi_slopes = False
use_new_kcache_layout = True
2025-04-11T04:23:18.5594041Z
@pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
@pytest.mark.parametrize("bsz", [7, 32])
@pytest.mark.parametrize("block_size", [16, 32])
@pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
@pytest.mark.parametrize("num_attn_heads", [16])
@pytest.mark.parametrize("kv_group_num", [1, 4])
@pytest.mark.parametrize("same_context_len", [True, False])
@pytest.mark.parametrize("use_alibi_slopes", [True, False])
@pytest.mark.parametrize("use_new_kcache_layout", [True, False])
def test_context_attention(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_attn_heads: int,
kv_group_num: int,
same_context_len: bool,
use_alibi_slopes: bool,
use_new_kcache_layout: bool,
):
if use_new_kcache_layout and use_alibi_slopes:
# TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
# the code (alibi kernel) will be refactored later to avoid code duplication, when
# the whole triton flow with new k cache layout has been supported and tested.
# And tests for the alibi kernel using new kcache layout will be added then.
return
2025-04-11T04:23:18.5597270Z
torch.manual_seed(123)
# It's necessary to clear cache here.
torch.cuda.empty_cache()
>       torch.cuda.synchronize()
2025-04-11T04:23:18.5597638Z
tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.5597938Z
device = None
2025-04-11T04:23:18.5598019Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.5598362Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5599912Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
____________ test_context_attention[True-False-False-1-16-16-16-32] ____________
2025-04-11T04:23:18.5600409Z
bsz = 32, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
kv_group_num = 1, same_context_len = False, use_alibi_slopes = False
use_new_kcache_layout = True
2025-04-11T04:23:18.5600809Z
@pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
@pytest.mark.parametrize("bsz", [7, 32])
@pytest.mark.parametrize("block_size", [16, 32])
@pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
@pytest.mark.parametrize("num_attn_heads", [16])
@pytest.mark.parametrize("kv_group_num", [1, 4])
@pytest.mark.parametrize("same_context_len", [True, False])
@pytest.mark.parametrize("use_alibi_slopes", [True, False])
@pytest.mark.parametrize("use_new_kcache_layout", [True, False])
def test_context_attention(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_attn_heads: int,
kv_group_num: int,
same_context_len: bool,
use_alibi_slopes: bool,
use_new_kcache_layout: bool,
):
if use_new_kcache_layout and use_alibi_slopes:
# TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
# the code (alibi kernel) will be refactored later to avoid code duplication, when
# the whole triton flow with new k cache layout has been supported and tested.
# And tests for the alibi kernel using new kcache layout will be added then.
return
2025-04-11T04:23:18.5603969Z
torch.manual_seed(123)
# It's necessary to clear cache here.
torch.cuda.empty_cache()
>       torch.cuda.synchronize()
2025-04-11T04:23:18.5604330Z
tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.5604614Z
device = None
2025-04-11T04:23:18.5604697Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.5605037Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5606675Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
____________ test_context_attention[True-False-False-1-16-16-32-7] _____________
2025-04-11T04:23:18.5607089Z
bsz = 7, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
kv_group_num = 1, same_context_len = False, use_alibi_slopes = False
use_new_kcache_layout = True
2025-04-11T04:23:18.5607480Z
@pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
@pytest.mark.parametrize("bsz", [7, 32])
@pytest.mark.parametrize("block_size", [16, 32])
@pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
@pytest.mark.parametrize("num_attn_heads", [16])
@pytest.mark.parametrize("kv_group_num", [1, 4])
@pytest.mark.parametrize("same_context_len", [True, False])
@pytest.mark.parametrize("use_alibi_slopes", [True, False])
@pytest.mark.parametrize("use_new_kcache_layout", [True, False])
def test_context_attention(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_attn_heads: int,
kv_group_num: int,
same_context_len: bool,
use_alibi_slopes: bool,
use_new_kcache_layout: bool,
):
if use_new_kcache_layout and use_alibi_slopes:
# TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
# the code (alibi kernel) will be refactored later to avoid code duplication, when
# the whole triton flow with new k cache layout has been supported and tested.
# And tests for the alibi kernel using new kcache layout will be added then.
return
2025-04-11T04:23:18.5610617Z
torch.manual_seed(123)
# It's necessary to clear cache here.
torch.cuda.empty_cache()
>       torch.cuda.synchronize()
2025-04-11T04:23:18.5610980Z
tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.5611272Z
device = None
2025-04-11T04:23:18.5611352Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.5611692Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5613359Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
____________ test_context_attention[True-False-False-1-16-16-32-32] ____________
2025-04-11T04:23:18.5613775Z
bsz = 32, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
kv_group_num = 1, same_context_len = False, use_alibi_slopes = False
use_new_kcache_layout = True
2025-04-11T04:23:18.5614168Z
@pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
@pytest.mark.parametrize("bsz", [7, 32])
@pytest.mark.parametrize("block_size", [16, 32])
@pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
@pytest.mark.parametrize("num_attn_heads", [16])
@pytest.mark.parametrize("kv_group_num", [1, 4])
@pytest.mark.parametrize("same_context_len", [True, False])
@pytest.mark.parametrize("use_alibi_slopes", [True, False])
@pytest.mark.parametrize("use_new_kcache_layout", [True, False])
def test_context_attention(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_attn_heads: int,
kv_group_num: int,
same_context_len: bool,
use_alibi_slopes: bool,
use_new_kcache_layout: bool,
):
if use_new_kcache_layout and use_alibi_slopes:
# TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
# the code (alibi kernel) will be refactored later to avoid code duplication, when
# the whole triton flow with new k cache layout has been supported and tested.
# And tests for the alibi kernel using new kcache layout will be added then.
return
2025-04-11T04:23:18.5617285Z
torch.manual_seed(123)
# It's necessary to clear cache here.
torch.cuda.empty_cache()
>       torch.cuda.synchronize()
2025-04-11T04:23:18.5617663Z
tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.5617948Z
device = None
2025-04-11T04:23:18.5618030Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.5618366Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5620011Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
_____________ test_context_attention[True-False-False-4-16-8-16-7] _____________
2025-04-11T04:23:18.5620426Z
bsz = 7, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
kv_group_num = 4, same_context_len = False, use_alibi_slopes = False
use_new_kcache_layout = True
2025-04-11T04:23:18.5620908Z
@pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
@pytest.mark.parametrize("bsz", [7, 32])
@pytest.mark.parametrize("block_size", [16, 32])
@pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
@pytest.mark.parametrize("num_attn_heads", [16])
@pytest.mark.parametrize("kv_group_num", [1, 4])
@pytest.mark.parametrize("same_context_len", [True, False])
@pytest.mark.parametrize("use_alibi_slopes", [True, False])
@pytest.mark.parametrize("use_new_kcache_layout", [True, False])
def test_context_attention(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_attn_heads: int,
kv_group_num: int,
same_context_len: bool,
use_alibi_slopes: bool,
use_new_kcache_layout: bool,
):
if use_new_kcache_layout and use_alibi_slopes:
# TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
# the code (alibi kernel) will be refactored later to avoid code duplication, when
# the whole triton flow with new k cache layout has been supported and tested.
# And tests for the alibi kernel using new kcache layout will be added then.
return
2025-04-11T04:23:18.5623945Z
torch.manual_seed(123)
# It's necessary to clear cache here.
torch.cuda.empty_cache()
>       torch.cuda.synchronize()
2025-04-11T04:23:18.5624307Z
tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.5624592Z
device = None
2025-04-11T04:23:18.5624758Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.5625109Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5626766Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
____________ test_context_attention[True-False-False-4-16-8-16-32] _____________
2025-04-11T04:23:18.5627181Z
bsz = 32, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
kv_group_num = 4, same_context_len = False, use_alibi_slopes = False
use_new_kcache_layout = True
2025-04-11T04:23:18.5627567Z
@pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
@pytest.mark.parametrize("bsz", [7, 32])
@pytest.mark.parametrize("block_size", [16, 32])
@pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
@pytest.mark.parametrize("num_attn_heads", [16])
@pytest.mark.parametrize("kv_group_num", [1, 4])
@pytest.mark.parametrize("same_context_len", [True, False])
@pytest.mark.parametrize("use_alibi_slopes", [True, False])
@pytest.mark.parametrize("use_new_kcache_layout", [True, False])
def test_context_attention(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_attn_heads: int,
kv_group_num: int,
same_context_len: bool,
use_alibi_slopes: bool,
use_new_kcache_layout: bool,
):
if use_new_kcache_layout and use_alibi_slopes:
# TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
# the code (alibi kernel) will be refactored later to avoid code duplication, when
# the whole triton flow with new k cache layout has been supported and tested.
# And tests for the alibi kernel using new kcache layout will be added then.
return
2025-04-11T04:23:18.5630640Z
torch.manual_seed(123)
# It's necessary to clear cache here.
torch.cuda.empty_cache()
>       torch.cuda.synchronize()
2025-04-11T04:23:18.5631114Z
tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.5631406Z
device = None
2025-04-11T04:23:18.5631486Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.5631831Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5633542Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
_____________ test_context_attention[True-False-False-4-16-8-32-7] _____________
2025-04-11T04:23:18.5633994Z
bsz = 7, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
kv_group_num = 4, same_context_len = False, use_alibi_slopes = False
use_new_kcache_layout = True
2025-04-11T04:23:18.5634390Z
@pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
@pytest.mark.parametrize("bsz", [7, 32])
@pytest.mark.parametrize("block_size", [16, 32])
@pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
@pytest.mark.parametrize("num_attn_heads", [16])
@pytest.mark.parametrize("kv_group_num", [1, 4])
@pytest.mark.parametrize("same_context_len", [True, False])
@pytest.mark.parametrize("use_alibi_slopes", [True, False])
@pytest.mark.parametrize("use_new_kcache_layout", [True, False])
def test_context_attention(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_attn_heads: int,
kv_group_num: int,
same_context_len: bool,
use_alibi_slopes: bool,
use_new_kcache_layout: bool,
):
if use_new_kcache_layout and use_alibi_slopes:
# TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
# the code (alibi kernel) will be refactored later to avoid code duplication, when
# the whole triton flow with new k cache layout has been supported and tested.
# And tests for the alibi kernel using new kcache layout will be added then.
return
2025-04-11T04:23:18.5637531Z
torch.manual_seed(123)
# It's necessary to clear cache here.
torch.cuda.empty_cache()
>       torch.cuda.synchronize()
2025-04-11T04:23:18.5637904Z
tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.5638193Z
device = None
2025-04-11T04:23:18.5638290Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.5638665Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5640317Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
____________ test_context_attention[True-False-False-4-16-8-32-32] _____________
2025-04-11T04:23:18.5640734Z
bsz = 32, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
kv_group_num = 4, same_context_len = False, use_alibi_slopes = False
use_new_kcache_layout = True
2025-04-11T04:23:18.5641121Z
@pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
@pytest.mark.parametrize("bsz", [7, 32])
@pytest.mark.parametrize("block_size", [16, 32])
@pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
@pytest.mark.parametrize("num_attn_heads", [16])
@pytest.mark.parametrize("kv_group_num", [1, 4])
@pytest.mark.parametrize("same_context_len", [True, False])
@pytest.mark.parametrize("use_alibi_slopes", [True, False])
@pytest.mark.parametrize("use_new_kcache_layout", [True, False])
def test_context_attention(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_attn_heads: int,
kv_group_num: int,
same_context_len: bool,
use_alibi_slopes: bool,
use_new_kcache_layout: bool,
):
if use_new_kcache_layout and use_alibi_slopes:
# TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
# the code (alibi kernel) will be refactored later to avoid code duplication, when
# the whole triton flow with new k cache layout has been supported and tested.
# And tests for the alibi kernel using new kcache layout will be added then.
return
2025-04-11T04:23:18.5644294Z
torch.manual_seed(123)
# It's necessary to clear cache here.
torch.cuda.empty_cache()
>       torch.cuda.synchronize()
2025-04-11T04:23:18.5644659Z
tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.5644948Z
device = None
2025-04-11T04:23:18.5645123Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.5645473Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5647047Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
____________ test_context_attention[True-False-False-4-16-16-16-7] _____________
2025-04-11T04:23:18.5647460Z
bsz = 7, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
kv_group_num = 4, same_context_len = False, use_alibi_slopes = False
use_new_kcache_layout = True
2025-04-11T04:23:18.5647854Z
@pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
@pytest.mark.parametrize("bsz", [7, 32])
@pytest.mark.parametrize("block_size", [16, 32])
@pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
@pytest.mark.parametrize("num_attn_heads", [16])
@pytest.mark.parametrize("kv_group_num", [1, 4])
@pytest.mark.parametrize("same_context_len", [True, False])
@pytest.mark.parametrize("use_alibi_slopes", [True, False])
@pytest.mark.parametrize("use_new_kcache_layout", [True, False])
def test_context_attention(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_attn_heads: int,
kv_group_num: int,
same_context_len: bool,
use_alibi_slopes: bool,
use_new_kcache_layout: bool,
):
if use_new_kcache_layout and use_alibi_slopes:
# TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
# the code (alibi kernel) will be refactored later to avoid code duplication, when
# the whole triton flow with new k cache layout has been supported and tested.
# And tests for the alibi kernel using new kcache layout will be added then.
return
2025-04-11T04:23:18.5651022Z
torch.manual_seed(123)
# It's necessary to clear cache here.
torch.cuda.empty_cache()
>       torch.cuda.synchronize()
2025-04-11T04:23:18.5651492Z
tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.5651780Z
device = None
2025-04-11T04:23:18.5651859Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.5652195Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5653805Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
____________ test_context_attention[True-False-False-4-16-16-16-32] ____________
2025-04-11T04:23:18.5654230Z
bsz = 32, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
kv_group_num = 4, same_context_len = False, use_alibi_slopes = False
use_new_kcache_layout = True
2025-04-11T04:23:18.5654627Z
@pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
@pytest.mark.parametrize("bsz", [7, 32])
@pytest.mark.parametrize("block_size", [16, 32])
@pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
@pytest.mark.parametrize("num_attn_heads", [16])
@pytest.mark.parametrize("kv_group_num", [1, 4])
@pytest.mark.parametrize("same_context_len", [True, False])
@pytest.mark.parametrize("use_alibi_slopes", [True, False])
@pytest.mark.parametrize("use_new_kcache_layout", [True, False])
def test_context_attention(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_attn_heads: int,
kv_group_num: int,
same_context_len: bool,
use_alibi_slopes: bool,
use_new_kcache_layout: bool,
):
if use_new_kcache_layout and use_alibi_slopes:
# TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
# the code (alibi kernel) will be refactored later to avoid code duplication, when
# the whole triton flow with new k cache layout has been supported and tested.
# And tests for the alibi kernel using new kcache layout will be added then.
return
2025-04-11T04:23:18.5657903Z
torch.manual_seed(123)
# It's necessary to clear cache here.
torch.cuda.empty_cache()
>       torch.cuda.synchronize()
2025-04-11T04:23:18.5658284Z
tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.5658574Z
device = None
2025-04-11T04:23:18.5658658Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.5658997Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5660547Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
____________ test_context_attention[True-False-False-4-16-16-32-7] _____________
2025-04-11T04:23:18.5660958Z
bsz = 7, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
kv_group_num = 4, same_context_len = False, use_alibi_slopes = False
use_new_kcache_layout = True
2025-04-11T04:23:18.5661340Z
@pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
@pytest.mark.parametrize("bsz", [7, 32])
@pytest.mark.parametrize("block_size", [16, 32])
@pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
@pytest.mark.parametrize("num_attn_heads", [16])
@pytest.mark.parametrize("kv_group_num", [1, 4])
@pytest.mark.parametrize("same_context_len", [True, False])
@pytest.mark.parametrize("use_alibi_slopes", [True, False])
@pytest.mark.parametrize("use_new_kcache_layout", [True, False])
def test_context_attention(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_attn_heads: int,
kv_group_num: int,
same_context_len: bool,
use_alibi_slopes: bool,
use_new_kcache_layout: bool,
):
if use_new_kcache_layout and use_alibi_slopes:
# TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
# the code (alibi kernel) will be refactored later to avoid code duplication, when
# the whole triton flow with new k cache layout has been supported and tested.
# And tests for the alibi kernel using new kcache layout will be added then.
return
2025-04-11T04:23:18.5664571Z
torch.manual_seed(123)
# It's necessary to clear cache here.
torch.cuda.empty_cache()
>       torch.cuda.synchronize()
2025-04-11T04:23:18.5664944Z
tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.5665232Z
device = None
2025-04-11T04:23:18.5665312Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.5665647Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5667193Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
____________ test_context_attention[True-False-False-4-16-16-32-32] ____________
2025-04-11T04:23:18.5667605Z
bsz = 32, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
kv_group_num = 4, same_context_len = False, use_alibi_slopes = False
use_new_kcache_layout = True
2025-04-11T04:23:18.5668099Z
@pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
@pytest.mark.parametrize("bsz", [7, 32])
@pytest.mark.parametrize("block_size", [16, 32])
@pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
@pytest.mark.parametrize("num_attn_heads", [16])
@pytest.mark.parametrize("kv_group_num", [1, 4])
@pytest.mark.parametrize("same_context_len", [True, False])
@pytest.mark.parametrize("use_alibi_slopes", [True, False])
@pytest.mark.parametrize("use_new_kcache_layout", [True, False])
def test_context_attention(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_attn_heads: int,
kv_group_num: int,
same_context_len: bool,
use_alibi_slopes: bool,
use_new_kcache_layout: bool,
):
if use_new_kcache_layout and use_alibi_slopes:
# TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
# the code (alibi kernel) will be refactored later to avoid code duplication, when
# the whole triton flow with new k cache layout has been supported and tested.
# And tests for the alibi kernel using new kcache layout will be added then.
return
2025-04-11T04:23:18.5671279Z
torch.manual_seed(123)
# It's necessary to clear cache here.
torch.cuda.empty_cache()
>       torch.cuda.synchronize()
2025-04-11T04:23:18.5671658Z
tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.5671940Z
device = None
2025-04-11T04:23:18.5672023Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.5672363Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5677947Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
_____________ test_context_attention[False-True-True-1-16-8-16-7] ______________
2025-04-11T04:23:18.5678372Z
bsz = 7, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
kv_group_num = 1, same_context_len = True, use_alibi_slopes = True
use_new_kcache_layout = False
2025-04-11T04:23:18.5678765Z
@pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
@pytest.mark.parametrize("bsz", [7, 32])
@pytest.mark.parametrize("block_size", [16, 32])
@pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
@pytest.mark.parametrize("num_attn_heads", [16])
@pytest.mark.parametrize("kv_group_num", [1, 4])
@pytest.mark.parametrize("same_context_len", [True, False])
@pytest.mark.parametrize("use_alibi_slopes", [True, False])
@pytest.mark.parametrize("use_new_kcache_layout", [True, False])
def test_context_attention(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_attn_heads: int,
kv_group_num: int,
same_context_len: bool,
use_alibi_slopes: bool,
use_new_kcache_layout: bool,
):
if use_new_kcache_layout and use_alibi_slopes:
# TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
# the code (alibi kernel) will be refactored later to avoid code duplication, when
# the whole triton flow with new k cache layout has been supported and tested.
# And tests for the alibi kernel using new kcache layout will be added then.
return
2025-04-11T04:23:18.5682008Z
torch.manual_seed(123)
# It's necessary to clear cache here.
torch.cuda.empty_cache()
>       torch.cuda.synchronize()
2025-04-11T04:23:18.5682373Z
tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.5682657Z
device = None
2025-04-11T04:23:18.5682742Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.5683082Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5684743Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
_____________ test_context_attention[False-True-True-1-16-8-16-32] _____________
2025-04-11T04:23:18.5685158Z
bsz = 32, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
kv_group_num = 1, same_context_len = True, use_alibi_slopes = True
use_new_kcache_layout = False
2025-04-11T04:23:18.5685548Z
@pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
@pytest.mark.parametrize("bsz", [7, 32])
@pytest.mark.parametrize("block_size", [16, 32])
@pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
@pytest.mark.parametrize("num_attn_heads", [16])
@pytest.mark.parametrize("kv_group_num", [1, 4])
@pytest.mark.parametrize("same_context_len", [True, False])
@pytest.mark.parametrize("use_alibi_slopes", [True, False])
@pytest.mark.parametrize("use_new_kcache_layout", [True, False])
def test_context_attention(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_attn_heads: int,
kv_group_num: int,
same_context_len: bool,
use_alibi_slopes: bool,
use_new_kcache_layout: bool,
):
if use_new_kcache_layout and use_alibi_slopes:
# TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
# the code (alibi kernel) will be refactored later to avoid code duplication, when
# the whole triton flow with new k cache layout has been supported and tested.
# And tests for the alibi kernel using new kcache layout will be added then.
return
2025-04-11T04:23:18.5688708Z
torch.manual_seed(123)
# It's necessary to clear cache here.
torch.cuda.empty_cache()
>       torch.cuda.synchronize()
2025-04-11T04:23:18.5689092Z
tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.5689375Z
device = None
2025-04-11T04:23:18.5689455Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.5689792Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5691455Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
_____________ test_context_attention[False-True-True-1-16-8-32-7] ______________
2025-04-11T04:23:18.5691873Z
bsz = 7, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
kv_group_num = 1, same_context_len = True, use_alibi_slopes = True
use_new_kcache_layout = False
2025-04-11T04:23:18.5692348Z
@pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
@pytest.mark.parametrize("bsz", [7, 32])
@pytest.mark.parametrize("block_size", [16, 32])
@pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
@pytest.mark.parametrize("num_attn_heads", [16])
@pytest.mark.parametrize("kv_group_num", [1, 4])
@pytest.mark.parametrize("same_context_len", [True, False])
@pytest.mark.parametrize("use_alibi_slopes", [True, False])
@pytest.mark.parametrize("use_new_kcache_layout", [True, False])
def test_context_attention(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_attn_heads: int,
kv_group_num: int,
same_context_len: bool,
use_alibi_slopes: bool,
use_new_kcache_layout: bool,
):
if use_new_kcache_layout and use_alibi_slopes:
# TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
# the code (alibi kernel) will be refactored later to avoid code duplication, when
# the whole triton flow with new k cache layout has been supported and tested.
# And tests for the alibi kernel using new kcache layout will be added then.
return
2025-04-11T04:23:18.5695403Z
torch.manual_seed(123)
# It's necessary to clear cache here.
torch.cuda.empty_cache()
>       torch.cuda.synchronize()
2025-04-11T04:23:18.5695777Z
tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.5696075Z
device = None
2025-04-11T04:23:18.5696161Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.5696596Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5698217Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
_____________ test_context_attention[False-True-True-1-16-8-32-32] _____________
2025-04-11T04:23:18.5698750Z
bsz = 32, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
kv_group_num = 1, same_context_len = True, use_alibi_slopes = True
use_new_kcache_layout = False
2025-04-11T04:23:18.5699146Z
@pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
@pytest.mark.parametrize("bsz", [7, 32])
@pytest.mark.parametrize("block_size", [16, 32])
@pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
@pytest.mark.parametrize("num_attn_heads", [16])
@pytest.mark.parametrize("kv_group_num", [1, 4])
@pytest.mark.parametrize("same_context_len", [True, False])
@pytest.mark.parametrize("use_alibi_slopes", [True, False])
@pytest.mark.parametrize("use_new_kcache_layout", [True, False])
def test_context_attention(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_attn_heads: int,
kv_group_num: int,
same_context_len: bool,
use_alibi_slopes: bool,
use_new_kcache_layout: bool,
):
if use_new_kcache_layout and use_alibi_slopes:
# TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
# the code (alibi kernel) will be refactored later to avoid code duplication, when
# the whole triton flow with new k cache layout has been supported and tested.
# And tests for the alibi kernel using new kcache layout will be added then.
return
2025-04-11T04:23:18.5738595Z
torch.manual_seed(123)
# It's necessary to clear cache here.
torch.cuda.empty_cache()
>       torch.cuda.synchronize()
2025-04-11T04:23:18.5739005Z
tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.5739432Z
device = None
2025-04-11T04:23:18.5739526Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.5739879Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5741661Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
_____________ test_context_attention[False-True-True-1-16-16-16-7] _____________
2025-04-11T04:23:18.5742079Z
bsz = 7, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
kv_group_num = 1, same_context_len = True, use_alibi_slopes = True
use_new_kcache_layout = False
2025-04-11T04:23:18.5742488Z
@pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
@pytest.mark.parametrize("bsz", [7, 32])
@pytest.mark.parametrize("block_size", [16, 32])
@pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
@pytest.mark.parametrize("num_attn_heads", [16])
@pytest.mark.parametrize("kv_group_num", [1, 4])
@pytest.mark.parametrize("same_context_len", [True, False])
@pytest.mark.parametrize("use_alibi_slopes", [True, False])
@pytest.mark.parametrize("use_new_kcache_layout", [True, False])
def test_context_attention(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_attn_heads: int,
kv_group_num: int,
same_context_len: bool,
use_alibi_slopes: bool,
use_new_kcache_layout: bool,
):
if use_new_kcache_layout and use_alibi_slopes:
# TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
# the code (alibi kernel) will be refactored later to avoid code duplication, when
# the whole triton flow with new k cache layout has been supported and tested.
# And tests for the alibi kernel using new kcache layout will be added then.
return
2025-04-11T04:23:18.5745743Z
torch.manual_seed(123)
# It's necessary to clear cache here.
torch.cuda.empty_cache()
>       torch.cuda.synchronize()
2025-04-11T04:23:18.5746123Z
tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.5746411Z
device = None
2025-04-11T04:23:18.5746492Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.5746833Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5748570Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
____________ test_context_attention[False-True-True-1-16-16-16-32] _____________
2025-04-11T04:23:18.5749015Z
bsz = 32, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
kv_group_num = 1, same_context_len = True, use_alibi_slopes = True
use_new_kcache_layout = False
2025-04-11T04:23:18.5749406Z
@pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
@pytest.mark.parametrize("bsz", [7, 32])
@pytest.mark.parametrize("block_size", [16, 32])
@pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
@pytest.mark.parametrize("num_attn_heads", [16])
@pytest.mark.parametrize("kv_group_num", [1, 4])
@pytest.mark.parametrize("same_context_len", [True, False])
@pytest.mark.parametrize("use_alibi_slopes", [True, False])
@pytest.mark.parametrize("use_new_kcache_layout", [True, False])
def test_context_attention(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_attn_heads: int,
kv_group_num: int,
same_context_len: bool,
use_alibi_slopes: bool,
use_new_kcache_layout: bool,
):
if use_new_kcache_layout and use_alibi_slopes:
# TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
# the code (alibi kernel) will be refactored later to avoid code duplication, when
# the whole triton flow with new k cache layout has been supported and tested.
# And tests for the alibi kernel using new kcache layout will be added then.
return
2025-04-11T04:23:18.5752566Z
torch.manual_seed(123)
# It's necessary to clear cache here.
torch.cuda.empty_cache()
>       torch.cuda.synchronize()
2025-04-11T04:23:18.5752936Z
tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.5753230Z
device = None
2025-04-11T04:23:18.5753315Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.5753762Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5755333Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
_____________ test_context_attention[False-True-True-1-16-16-32-7] _____________
2025-04-11T04:23:18.5755744Z
bsz = 7, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
kv_group_num = 1, same_context_len = True, use_alibi_slopes = True
use_new_kcache_layout = False
2025-04-11T04:23:18.5756134Z
@pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
@pytest.mark.parametrize("bsz", [7, 32])
@pytest.mark.parametrize("block_size", [16, 32])
@pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
@pytest.mark.parametrize("num_attn_heads", [16])
@pytest.mark.parametrize("kv_group_num", [1, 4])
@pytest.mark.parametrize("same_context_len", [True, False])
@pytest.mark.parametrize("use_alibi_slopes", [True, False])
@pytest.mark.parametrize("use_new_kcache_layout", [True, False])
def test_context_attention(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_attn_heads: int,
kv_group_num: int,
same_context_len: bool,
use_alibi_slopes: bool,
use_new_kcache_layout: bool,
):
if use_new_kcache_layout and use_alibi_slopes:
# TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
# the code (alibi kernel) will be refactored later to avoid code duplication, when
# the whole triton flow with new k cache layout has been supported and tested.
# And tests for the alibi kernel using new kcache layout will be added then.
return
2025-04-11T04:23:18.5759227Z
torch.manual_seed(123)
# It's necessary to clear cache here.
torch.cuda.empty_cache()
>       torch.cuda.synchronize()
2025-04-11T04:23:18.5759597Z
tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.5759968Z
device = None
2025-04-11T04:23:18.5760050Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.5760417Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5761979Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
____________ test_context_attention[False-True-True-1-16-16-32-32] _____________
2025-04-11T04:23:18.5762396Z
bsz = 32, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
kv_group_num = 1, same_context_len = True, use_alibi_slopes = True
use_new_kcache_layout = False
2025-04-11T04:23:18.5762801Z
@pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
@pytest.mark.parametrize("bsz", [7, 32])
@pytest.mark.parametrize("block_size", [16, 32])
@pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
@pytest.mark.parametrize("num_attn_heads", [16])
@pytest.mark.parametrize("kv_group_num", [1, 4])
@pytest.mark.parametrize("same_context_len", [True, False])
@pytest.mark.parametrize("use_alibi_slopes", [True, False])
@pytest.mark.parametrize("use_new_kcache_layout", [True, False])
def test_context_attention(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_attn_heads: int,
kv_group_num: int,
same_context_len: bool,
use_alibi_slopes: bool,
use_new_kcache_layout: bool,
):
if use_new_kcache_layout and use_alibi_slopes:
# TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
# the code (alibi kernel) will be refactored later to avoid code duplication, when
# the whole triton flow with new k cache layout has been supported and tested.
# And tests for the alibi kernel using new kcache layout will be added then.
return
2025-04-11T04:23:18.5766154Z
torch.manual_seed(123)
# It's necessary to clear cache here.
torch.cuda.empty_cache()
>       torch.cuda.synchronize()
2025-04-11T04:23:18.5766526Z
tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.5766809Z
device = None
2025-04-11T04:23:18.5766890Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.5767230Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5768777Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
_____________ test_context_attention[False-True-True-4-16-8-16-7] ______________
2025-04-11T04:23:18.5769193Z
bsz = 7, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
kv_group_num = 4, same_context_len = True, use_alibi_slopes = True
use_new_kcache_layout = False
2025-04-11T04:23:18.5769608Z
@pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
@pytest.mark.parametrize("bsz", [7, 32])
@pytest.mark.parametrize("block_size", [16, 32])
@pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
@pytest.mark.parametrize("num_attn_heads", [16])
@pytest.mark.parametrize("kv_group_num", [1, 4])
@pytest.mark.parametrize("same_context_len", [True, False])
@pytest.mark.parametrize("use_alibi_slopes", [True, False])
@pytest.mark.parametrize("use_new_kcache_layout", [True, False])
def test_context_attention(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_attn_heads: int,
kv_group_num: int,
same_context_len: bool,
use_alibi_slopes: bool,
use_new_kcache_layout: bool,
):
if use_new_kcache_layout and use_alibi_slopes:
# TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
# the code (alibi kernel) will be refactored later to avoid code duplication, when
# the whole triton flow with new k cache layout has been supported and tested.
# And tests for the alibi kernel using new kcache layout will be added then.
return
2025-04-11T04:23:18.5772857Z
torch.manual_seed(123)
# It's necessary to clear cache here.
torch.cuda.empty_cache()
>       torch.cuda.synchronize()
2025-04-11T04:23:18.5773227Z
tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.5773517Z
device = None
2025-04-11T04:23:18.5773606Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.5773948Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5775597Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
_____________ test_context_attention[False-True-True-4-16-8-16-32] _____________
2025-04-11T04:23:18.5776007Z
bsz = 32, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
kv_group_num = 4, same_context_len = True, use_alibi_slopes = True
use_new_kcache_layout = False
2025-04-11T04:23:18.5776481Z
@pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
@pytest.mark.parametrize("bsz", [7, 32])
@pytest.mark.parametrize("block_size", [16, 32])
@pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
@pytest.mark.parametrize("num_attn_heads", [16])
@pytest.mark.parametrize("kv_group_num", [1, 4])
@pytest.mark.parametrize("same_context_len", [True, False])
@pytest.mark.parametrize("use_alibi_slopes", [True, False])
@pytest.mark.parametrize("use_new_kcache_layout", [True, False])
def test_context_attention(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_attn_heads: int,
kv_group_num: int,
same_context_len: bool,
use_alibi_slopes: bool,
use_new_kcache_layout: bool,
):
if use_new_kcache_layout and use_alibi_slopes:
# TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
# the code (alibi kernel) will be refactored later to avoid code duplication, when
# the whole triton flow with new k cache layout has been supported and tested.
# And tests for the alibi kernel using new kcache layout will be added then.
return
2025-04-11T04:23:18.5779634Z
torch.manual_seed(123)
# It's necessary to clear cache here.
torch.cuda.empty_cache()
>       torch.cuda.synchronize()
2025-04-11T04:23:18.5780035Z
tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.5780329Z
device = None
2025-04-11T04:23:18.5780412Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.5780757Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5782349Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
_____________ test_context_attention[False-True-True-4-16-8-32-7] ______________
2025-04-11T04:23:18.5782847Z
bsz = 7, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
kv_group_num = 4, same_context_len = True, use_alibi_slopes = True
use_new_kcache_layout = False
2025-04-11T04:23:18.5783241Z
@pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
@pytest.mark.parametrize("bsz", [7, 32])
@pytest.mark.parametrize("block_size", [16, 32])
@pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
@pytest.mark.parametrize("num_attn_heads", [16])
@pytest.mark.parametrize("kv_group_num", [1, 4])
@pytest.mark.parametrize("same_context_len", [True, False])
@pytest.mark.parametrize("use_alibi_slopes", [True, False])
@pytest.mark.parametrize("use_new_kcache_layout", [True, False])
def test_context_attention(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_attn_heads: int,
kv_group_num: int,
same_context_len: bool,
use_alibi_slopes: bool,
use_new_kcache_layout: bool,
):
if use_new_kcache_layout and use_alibi_slopes:
# TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
# the code (alibi kernel) will be refactored later to avoid code duplication, when
# the whole triton flow with new k cache layout has been supported and tested.
# And tests for the alibi kernel using new kcache layout will be added then.
return
2025-04-11T04:23:18.5786407Z
torch.manual_seed(123)
# It's necessary to clear cache here.
torch.cuda.empty_cache()
>       torch.cuda.synchronize()
2025-04-11T04:23:18.5786775Z
tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.5787058Z
device = None
2025-04-11T04:23:18.5787143Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.5787489Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5789223Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
_____________ test_context_attention[False-True-True-4-16-8-32-32] _____________
2025-04-11T04:23:18.5789643Z
bsz = 32, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
kv_group_num = 4, same_context_len = True, use_alibi_slopes = True
use_new_kcache_layout = False
2025-04-11T04:23:18.5790039Z
@pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
@pytest.mark.parametrize("bsz", [7, 32])
@pytest.mark.parametrize("block_size", [16, 32])
@pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
@pytest.mark.parametrize("num_attn_heads", [16])
@pytest.mark.parametrize("kv_group_num", [1, 4])
@pytest.mark.parametrize("same_context_len", [True, False])
@pytest.mark.parametrize("use_alibi_slopes", [True, False])
@pytest.mark.parametrize("use_new_kcache_layout", [True, False])
def test_context_attention(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_attn_heads: int,
kv_group_num: int,
same_context_len: bool,
use_alibi_slopes: bool,
use_new_kcache_layout: bool,
):
if use_new_kcache_layout and use_alibi_slopes:
# TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
# the code (alibi kernel) will be refactored later to avoid code duplication, when
# the whole triton flow with new k cache layout has been supported and tested.
# And tests for the alibi kernel using new kcache layout will be added then.
return
2025-04-11T04:23:18.5793261Z
torch.manual_seed(123)
# It's necessary to clear cache here.
torch.cuda.empty_cache()
>       torch.cuda.synchronize()
2025-04-11T04:23:18.5793633Z
tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.5793920Z
device = None
2025-04-11T04:23:18.5794001Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.5794342Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5796037Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
_____________ test_context_attention[False-True-True-4-16-16-16-7] _____________
2025-04-11T04:23:18.5796441Z
bsz = 7, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
kv_group_num = 4, same_context_len = True, use_alibi_slopes = True
use_new_kcache_layout = False
2025-04-11T04:23:18.5796831Z
@pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
@pytest.mark.parametrize("bsz", [7, 32])
@pytest.mark.parametrize("block_size", [16, 32])
@pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
@pytest.mark.parametrize("num_attn_heads", [16])
@pytest.mark.parametrize("kv_group_num", [1, 4])
@pytest.mark.parametrize("same_context_len", [True, False])
@pytest.mark.parametrize("use_alibi_slopes", [True, False])
@pytest.mark.parametrize("use_new_kcache_layout", [True, False])
def test_context_attention(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_attn_heads: int,
kv_group_num: int,
same_context_len: bool,
use_alibi_slopes: bool,
use_new_kcache_layout: bool,
):
if use_new_kcache_layout and use_alibi_slopes:
# TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
# the code (alibi kernel) will be refactored later to avoid code duplication, when
# the whole triton flow with new k cache layout has been supported and tested.
# And tests for the alibi kernel using new kcache layout will be added then.
return
2025-04-11T04:23:18.5799970Z
torch.manual_seed(123)
# It's necessary to clear cache here.
torch.cuda.empty_cache()
>       torch.cuda.synchronize()
2025-04-11T04:23:18.5800342Z
tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.5800627Z
device = None
2025-04-11T04:23:18.5800707Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.5801048Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5802725Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
____________ test_context_attention[False-True-True-4-16-16-16-32] _____________
2025-04-11T04:23:18.5803216Z
bsz = 32, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
kv_group_num = 4, same_context_len = True, use_alibi_slopes = True
use_new_kcache_layout = False
2025-04-11T04:23:18.5803607Z
@pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
@pytest.mark.parametrize("bsz", [7, 32])
@pytest.mark.parametrize("block_size", [16, 32])
@pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
@pytest.mark.parametrize("num_attn_heads", [16])
@pytest.mark.parametrize("kv_group_num", [1, 4])
@pytest.mark.parametrize("same_context_len", [True, False])
@pytest.mark.parametrize("use_alibi_slopes", [True, False])
@pytest.mark.parametrize("use_new_kcache_layout", [True, False])
def test_context_attention(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_attn_heads: int,
kv_group_num: int,
same_context_len: bool,
use_alibi_slopes: bool,
use_new_kcache_layout: bool,
):
if use_new_kcache_layout and use_alibi_slopes:
# TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
# the code (alibi kernel) will be refactored later to avoid code duplication, when
# the whole triton flow with new k cache layout has been supported and tested.
# And tests for the alibi kernel using new kcache layout will be added then.
return
2025-04-11T04:23:18.5806693Z
torch.manual_seed(123)
# It's necessary to clear cache here.
torch.cuda.empty_cache()
>       torch.cuda.synchronize()
2025-04-11T04:23:18.5807066Z
tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.5807443Z
device = None
2025-04-11T04:23:18.5807532Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.5807879Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5809546Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
_____________ test_context_attention[False-True-True-4-16-16-32-7] _____________
2025-04-11T04:23:18.5809967Z
bsz = 7, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
kv_group_num = 4, same_context_len = True, use_alibi_slopes = True
use_new_kcache_layout = False
2025-04-11T04:23:18.5810367Z
@pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
@pytest.mark.parametrize("bsz", [7, 32])
@pytest.mark.parametrize("block_size", [16, 32])
@pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
@pytest.mark.parametrize("num_attn_heads", [16])
@pytest.mark.parametrize("kv_group_num", [1, 4])
@pytest.mark.parametrize("same_context_len", [True, False])
@pytest.mark.parametrize("use_alibi_slopes", [True, False])
@pytest.mark.parametrize("use_new_kcache_layout", [True, False])
def test_context_attention(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_attn_heads: int,
kv_group_num: int,
same_context_len: bool,
use_alibi_slopes: bool,
use_new_kcache_layout: bool,
):
if use_new_kcache_layout and use_alibi_slopes:
# TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
# the code (alibi kernel) will be refactored later to avoid code duplication, when
# the whole triton flow with new k cache layout has been supported and tested.
# And tests for the alibi kernel using new kcache layout will be added then.
return
2025-04-11T04:23:18.5813446Z
torch.manual_seed(123)
# It's necessary to clear cache here.
torch.cuda.empty_cache()
>       torch.cuda.synchronize()
2025-04-11T04:23:18.5814035Z
tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.5814321Z
device = None
2025-04-11T04:23:18.5814404Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.5814751Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5816425Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
____________ test_context_attention[False-True-True-4-16-16-32-32] _____________
2025-04-11T04:23:18.5816854Z
bsz = 32, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
kv_group_num = 4, same_context_len = True, use_alibi_slopes = True
use_new_kcache_layout = False
2025-04-11T04:23:18.5817272Z
@pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
@pytest.mark.parametrize("bsz", [7, 32])
@pytest.mark.parametrize("block_size", [16, 32])
@pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
@pytest.mark.parametrize("num_attn_heads", [16])
@pytest.mark.parametrize("kv_group_num", [1, 4])
@pytest.mark.parametrize("same_context_len", [True, False])
@pytest.mark.parametrize("use_alibi_slopes", [True, False])
@pytest.mark.parametrize("use_new_kcache_layout", [True, False])
def test_context_attention(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_attn_heads: int,
kv_group_num: int,
same_context_len: bool,
use_alibi_slopes: bool,
use_new_kcache_layout: bool,
):
if use_new_kcache_layout and use_alibi_slopes:
# TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
# the code (alibi kernel) will be refactored later to avoid code duplication, when
# the whole triton flow with new k cache layout has been supported and tested.
# And tests for the alibi kernel using new kcache layout will be added then.
return
2025-04-11T04:23:18.5820453Z
torch.manual_seed(123)
# It's necessary to clear cache here.
torch.cuda.empty_cache()
>       torch.cuda.synchronize()
2025-04-11T04:23:18.5820829Z
tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.5821114Z
device = None
2025-04-11T04:23:18.5821200Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.5821545Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5823213Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
_____________ test_context_attention[False-True-False-1-16-8-16-7] _____________
2025-04-11T04:23:18.5823644Z
bsz = 7, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
kv_group_num = 1, same_context_len = False, use_alibi_slopes = True
use_new_kcache_layout = False
2025-04-11T04:23:18.5824042Z
@pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
@pytest.mark.parametrize("bsz", [7, 32])
@pytest.mark.parametrize("block_size", [16, 32])
@pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
@pytest.mark.parametrize("num_attn_heads", [16])
@pytest.mark.parametrize("kv_group_num", [1, 4])
@pytest.mark.parametrize("same_context_len", [True, False])
@pytest.mark.parametrize("use_alibi_slopes", [True, False])
@pytest.mark.parametrize("use_new_kcache_layout", [True, False])
def test_context_attention(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_attn_heads: int,
kv_group_num: int,
same_context_len: bool,
use_alibi_slopes: bool,
use_new_kcache_layout: bool,
):
if use_new_kcache_layout and use_alibi_slopes:
# TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
# the code (alibi kernel) will be refactored later to avoid code duplication, when
# the whole triton flow with new k cache layout has been supported and tested.
# And tests for the alibi kernel using new kcache layout will be added then.
return
2025-04-11T04:23:18.5827198Z
torch.manual_seed(123)
# It's necessary to clear cache here.
torch.cuda.empty_cache()
>       torch.cuda.synchronize()
2025-04-11T04:23:18.5827580Z
tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.5827947Z
device = None
2025-04-11T04:23:18.5828044Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.5828402Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5830019Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
____________ test_context_attention[False-True-False-1-16-8-16-32] _____________
2025-04-11T04:23:18.5830457Z
bsz = 32, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
kv_group_num = 1, same_context_len = False, use_alibi_slopes = True
use_new_kcache_layout = False
2025-04-11T04:23:18.5830874Z
@pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
@pytest.mark.parametrize("bsz", [7, 32])
@pytest.mark.parametrize("block_size", [16, 32])
@pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
@pytest.mark.parametrize("num_attn_heads", [16])
@pytest.mark.parametrize("kv_group_num", [1, 4])
@pytest.mark.parametrize("same_context_len", [True, False])
@pytest.mark.parametrize("use_alibi_slopes", [True, False])
@pytest.mark.parametrize("use_new_kcache_layout", [True, False])
def test_context_attention(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_attn_heads: int,
kv_group_num: int,
same_context_len: bool,
use_alibi_slopes: bool,
use_new_kcache_layout: bool,
):
if use_new_kcache_layout and use_alibi_slopes:
# TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
# the code (alibi kernel) will be refactored later to avoid code duplication, when
# the whole triton flow with new k cache layout has been supported and tested.
# And tests for the alibi kernel using new kcache layout will be added then.
return
2025-04-11T04:23:18.5834017Z
torch.manual_seed(123)
# It's necessary to clear cache here.
torch.cuda.empty_cache()
>       torch.cuda.synchronize()
2025-04-11T04:23:18.5834507Z
tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.5834798Z
device = None
2025-04-11T04:23:18.5834888Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.5835247Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5836821Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
_____________ test_context_attention[False-True-False-1-16-8-32-7] _____________
2025-04-11T04:23:18.5837242Z
bsz = 7, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
kv_group_num = 1, same_context_len = False, use_alibi_slopes = True
use_new_kcache_layout = False
2025-04-11T04:23:18.5837650Z
@pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
@pytest.mark.parametrize("bsz", [7, 32])
@pytest.mark.parametrize("block_size", [16, 32])
@pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
@pytest.mark.parametrize("num_attn_heads", [16])
@pytest.mark.parametrize("kv_group_num", [1, 4])
@pytest.mark.parametrize("same_context_len", [True, False])
@pytest.mark.parametrize("use_alibi_slopes", [True, False])
@pytest.mark.parametrize("use_new_kcache_layout", [True, False])
def test_context_attention(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_attn_heads: int,
kv_group_num: int,
same_context_len: bool,
use_alibi_slopes: bool,
use_new_kcache_layout: bool,
):
if use_new_kcache_layout and use_alibi_slopes:
# TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
# the code (alibi kernel) will be refactored later to avoid code duplication, when
# the whole triton flow with new k cache layout has been supported and tested.
# And tests for the alibi kernel using new kcache layout will be added then.
return
2025-04-11T04:23:18.5840934Z
torch.manual_seed(123)
# It's necessary to clear cache here.
torch.cuda.empty_cache()
>       torch.cuda.synchronize()
2025-04-11T04:23:18.5841316Z
tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.5841604Z
device = None
2025-04-11T04:23:18.5841689Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.5842033Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5843615Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
____________ test_context_attention[False-True-False-1-16-8-32-32] _____________
2025-04-11T04:23:18.5844030Z
bsz = 32, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
kv_group_num = 1, same_context_len = False, use_alibi_slopes = True
use_new_kcache_layout = False
2025-04-11T04:23:18.5844429Z
@pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
@pytest.mark.parametrize("bsz", [7, 32])
@pytest.mark.parametrize("block_size", [16, 32])
@pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
@pytest.mark.parametrize("num_attn_heads", [16])
@pytest.mark.parametrize("kv_group_num", [1, 4])
@pytest.mark.parametrize("same_context_len", [True, False])
@pytest.mark.parametrize("use_alibi_slopes", [True, False])
@pytest.mark.parametrize("use_new_kcache_layout", [True, False])
def test_context_attention(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_attn_heads: int,
kv_group_num: int,
same_context_len: bool,
use_alibi_slopes: bool,
use_new_kcache_layout: bool,
):
if use_new_kcache_layout and use_alibi_slopes:
# TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
# the code (alibi kernel) will be refactored later to avoid code duplication, when
# the whole triton flow with new k cache layout has been supported and tested.
# And tests for the alibi kernel using new kcache layout will be added then.
return
2025-04-11T04:23:18.5847701Z
torch.manual_seed(123)
# It's necessary to clear cache here.
torch.cuda.empty_cache()
>       torch.cuda.synchronize()
2025-04-11T04:23:18.5848078Z
tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.5848370Z
device = None
2025-04-11T04:23:18.5848454Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.5848800Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5850396Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
____________ test_context_attention[False-True-False-1-16-16-16-7] _____________
2025-04-11T04:23:18.5850894Z
bsz = 7, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
kv_group_num = 1, same_context_len = False, use_alibi_slopes = True
use_new_kcache_layout = False
2025-04-11T04:23:18.5851295Z
@pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
@pytest.mark.parametrize("bsz", [7, 32])
@pytest.mark.parametrize("block_size", [16, 32])
@pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
@pytest.mark.parametrize("num_attn_heads", [16])
@pytest.mark.parametrize("kv_group_num", [1, 4])
@pytest.mark.parametrize("same_context_len", [True, False])
@pytest.mark.parametrize("use_alibi_slopes", [True, False])
@pytest.mark.parametrize("use_new_kcache_layout", [True, False])
def test_context_attention(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_attn_heads: int,
kv_group_num: int,
same_context_len: bool,
use_alibi_slopes: bool,
use_new_kcache_layout: bool,
):
if use_new_kcache_layout and use_alibi_slopes:
# TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
# the code (alibi kernel) will be refactored later to avoid code duplication, when
# the whole triton flow with new k cache layout has been supported and tested.
# And tests for the alibi kernel using new kcache layout will be added then.
return
2025-04-11T04:23:18.5854440Z
torch.manual_seed(123)
# It's necessary to clear cache here.
torch.cuda.empty_cache()
>       torch.cuda.synchronize()
2025-04-11T04:23:18.5854820Z
tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.5855118Z
device = None
2025-04-11T04:23:18.5855199Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.5855550Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5857191Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
____________ test_context_attention[False-True-False-1-16-16-16-32] ____________
2025-04-11T04:23:18.5857616Z
bsz = 32, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
kv_group_num = 1, same_context_len = False, use_alibi_slopes = True
use_new_kcache_layout = False
2025-04-11T04:23:18.5858011Z
@pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
@pytest.mark.parametrize("bsz", [7, 32])
@pytest.mark.parametrize("block_size", [16, 32])
@pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
@pytest.mark.parametrize("num_attn_heads", [16])
@pytest.mark.parametrize("kv_group_num", [1, 4])
@pytest.mark.parametrize("same_context_len", [True, False])
@pytest.mark.parametrize("use_alibi_slopes", [True, False])
@pytest.mark.parametrize("use_new_kcache_layout", [True, False])
def test_context_attention(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_attn_heads: int,
kv_group_num: int,
same_context_len: bool,
use_alibi_slopes: bool,
use_new_kcache_layout: bool,
):
if use_new_kcache_layout and use_alibi_slopes:
# TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
# the code (alibi kernel) will be refactored later to avoid code duplication, when
# the whole triton flow with new k cache layout has been supported and tested.
# And tests for the alibi kernel using new kcache layout will be added then.
return
2025-04-11T04:23:18.5861178Z
torch.manual_seed(123)
# It's necessary to clear cache here.
torch.cuda.empty_cache()
>       torch.cuda.synchronize()
2025-04-11T04:23:18.5861546Z
tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.5861833Z
device = None
2025-04-11T04:23:18.5861923Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.5862259Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5863921Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
____________ test_context_attention[False-True-False-1-16-16-32-7] _____________
2025-04-11T04:23:18.5864336Z
bsz = 7, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
kv_group_num = 1, same_context_len = False, use_alibi_slopes = True
use_new_kcache_layout = False
2025-04-11T04:23:18.5864729Z
@pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
@pytest.mark.parametrize("bsz", [7, 32])
@pytest.mark.parametrize("block_size", [16, 32])
@pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
@pytest.mark.parametrize("num_attn_heads", [16])
@pytest.mark.parametrize("kv_group_num", [1, 4])
@pytest.mark.parametrize("same_context_len", [True, False])
@pytest.mark.parametrize("use_alibi_slopes", [True, False])
@pytest.mark.parametrize("use_new_kcache_layout", [True, False])
def test_context_attention(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_attn_heads: int,
kv_group_num: int,
same_context_len: bool,
use_alibi_slopes: bool,
use_new_kcache_layout: bool,
):
if use_new_kcache_layout and use_alibi_slopes:
# TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
# the code (alibi kernel) will be refactored later to avoid code duplication, when
# the whole triton flow with new k cache layout has been supported and tested.
# And tests for the alibi kernel using new kcache layout will be added then.
return
2025-04-11T04:23:18.5867890Z
torch.manual_seed(123)
# It's necessary to clear cache here.
torch.cuda.empty_cache()
>       torch.cuda.synchronize()
2025-04-11T04:23:18.5868263Z
tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.5868588Z
device = None
2025-04-11T04:23:18.5868669Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.5869008Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5870671Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
____________ test_context_attention[False-True-False-1-16-16-32-32] ____________
2025-04-11T04:23:18.5871096Z
bsz = 32, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
kv_group_num = 1, same_context_len = False, use_alibi_slopes = True
use_new_kcache_layout = False
2025-04-11T04:23:18.5871597Z
@pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
@pytest.mark.parametrize("bsz", [7, 32])
@pytest.mark.parametrize("block_size", [16, 32])
@pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
@pytest.mark.parametrize("num_attn_heads", [16])
@pytest.mark.parametrize("kv_group_num", [1, 4])
@pytest.mark.parametrize("same_context_len", [True, False])
@pytest.mark.parametrize("use_alibi_slopes", [True, False])
@pytest.mark.parametrize("use_new_kcache_layout", [True, False])
def test_context_attention(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_attn_heads: int,
kv_group_num: int,
same_context_len: bool,
use_alibi_slopes: bool,
use_new_kcache_layout: bool,
):
if use_new_kcache_layout and use_alibi_slopes:
# TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
# the code (alibi kernel) will be refactored later to avoid code duplication, when
# the whole triton flow with new k cache layout has been supported and tested.
# And tests for the alibi kernel using new kcache layout will be added then.
return
2025-04-11T04:23:18.5874629Z
torch.manual_seed(123)
# It's necessary to clear cache here.
torch.cuda.empty_cache()
>       torch.cuda.synchronize()
2025-04-11T04:23:18.5874997Z
tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.5875279Z
device = None
2025-04-11T04:23:18.5875449Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.5875797Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5877469Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
_____________ test_context_attention[False-True-False-4-16-8-16-7] _____________
2025-04-11T04:23:18.5877887Z
bsz = 7, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
kv_group_num = 4, same_context_len = False, use_alibi_slopes = True
use_new_kcache_layout = False
2025-04-11T04:23:18.5878280Z
@pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
@pytest.mark.parametrize("bsz", [7, 32])
@pytest.mark.parametrize("block_size", [16, 32])
@pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
@pytest.mark.parametrize("num_attn_heads", [16])
@pytest.mark.parametrize("kv_group_num", [1, 4])
@pytest.mark.parametrize("same_context_len", [True, False])
@pytest.mark.parametrize("use_alibi_slopes", [True, False])
@pytest.mark.parametrize("use_new_kcache_layout", [True, False])
def test_context_attention(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_attn_heads: int,
kv_group_num: int,
same_context_len: bool,
use_alibi_slopes: bool,
use_new_kcache_layout: bool,
):
if use_new_kcache_layout and use_alibi_slopes:
# TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
# the code (alibi kernel) will be refactored later to avoid code duplication, when
# the whole triton flow with new k cache layout has been supported and tested.
# And tests for the alibi kernel using new kcache layout will be added then.
return
2025-04-11T04:23:18.5881325Z
torch.manual_seed(123)
# It's necessary to clear cache here.
torch.cuda.empty_cache()
>       torch.cuda.synchronize()
2025-04-11T04:23:18.5881830Z
tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.5882121Z
device = None
2025-04-11T04:23:18.5882201Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.5882542Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5884191Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
____________ test_context_attention[False-True-False-4-16-8-16-32] _____________
2025-04-11T04:23:18.5884607Z
bsz = 32, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
kv_group_num = 4, same_context_len = False, use_alibi_slopes = True
use_new_kcache_layout = False
2025-04-11T04:23:18.5885003Z
@pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
@pytest.mark.parametrize("bsz", [7, 32])
@pytest.mark.parametrize("block_size", [16, 32])
@pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
@pytest.mark.parametrize("num_attn_heads", [16])
@pytest.mark.parametrize("kv_group_num", [1, 4])
@pytest.mark.parametrize("same_context_len", [True, False])
@pytest.mark.parametrize("use_alibi_slopes", [True, False])
@pytest.mark.parametrize("use_new_kcache_layout", [True, False])
def test_context_attention(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_attn_heads: int,
kv_group_num: int,
same_context_len: bool,
use_alibi_slopes: bool,
use_new_kcache_layout: bool,
):
if use_new_kcache_layout and use_alibi_slopes:
# TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
# the code (alibi kernel) will be refactored later to avoid code duplication, when
# the whole triton flow with new k cache layout has been supported and tested.
# And tests for the alibi kernel using new kcache layout will be added then.
return
2025-04-11T04:23:18.5888148Z
torch.manual_seed(123)
# It's necessary to clear cache here.
torch.cuda.empty_cache()
>       torch.cuda.synchronize()
2025-04-11T04:23:18.5888520Z
tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.5888805Z
device = None
2025-04-11T04:23:18.5888887Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.5889230Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5890873Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
_____________ test_context_attention[False-True-False-4-16-8-32-7] _____________
2025-04-11T04:23:18.5891293Z
bsz = 7, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
kv_group_num = 4, same_context_len = False, use_alibi_slopes = True
use_new_kcache_layout = False
2025-04-11T04:23:18.5891689Z
@pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
@pytest.mark.parametrize("bsz", [7, 32])
@pytest.mark.parametrize("block_size", [16, 32])
@pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
@pytest.mark.parametrize("num_attn_heads", [16])
@pytest.mark.parametrize("kv_group_num", [1, 4])
@pytest.mark.parametrize("same_context_len", [True, False])
@pytest.mark.parametrize("use_alibi_slopes", [True, False])
@pytest.mark.parametrize("use_new_kcache_layout", [True, False])
def test_context_attention(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_attn_heads: int,
kv_group_num: int,
same_context_len: bool,
use_alibi_slopes: bool,
use_new_kcache_layout: bool,
):
if use_new_kcache_layout and use_alibi_slopes:
# TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
# the code (alibi kernel) will be refactored later to avoid code duplication, when
# the whole triton flow with new k cache layout has been supported and tested.
# And tests for the alibi kernel using new kcache layout will be added then.
return
2025-04-11T04:23:18.5894828Z
torch.manual_seed(123)
# It's necessary to clear cache here.
torch.cuda.empty_cache()
>       torch.cuda.synchronize()
2025-04-11T04:23:18.5895198Z
tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.5895484Z
device = None
2025-04-11T04:23:18.5895567Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.5895997Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5897554Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
____________ test_context_attention[False-True-False-4-16-8-32-32] _____________
2025-04-11T04:23:18.5897967Z
bsz = 32, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
kv_group_num = 4, same_context_len = False, use_alibi_slopes = True
use_new_kcache_layout = False
2025-04-11T04:23:18.5898356Z
@pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
@pytest.mark.parametrize("bsz", [7, 32])
@pytest.mark.parametrize("block_size", [16, 32])
@pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
@pytest.mark.parametrize("num_attn_heads", [16])
@pytest.mark.parametrize("kv_group_num", [1, 4])
@pytest.mark.parametrize("same_context_len", [True, False])
@pytest.mark.parametrize("use_alibi_slopes", [True, False])
@pytest.mark.parametrize("use_new_kcache_layout", [True, False])
def test_context_attention(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_attn_heads: int,
kv_group_num: int,
same_context_len: bool,
use_alibi_slopes: bool,
use_new_kcache_layout: bool,
):
if use_new_kcache_layout and use_alibi_slopes:
# TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
# the code (alibi kernel) will be refactored later to avoid code duplication, when
# the whole triton flow with new k cache layout has been supported and tested.
# And tests for the alibi kernel using new kcache layout will be added then.
return
2025-04-11T04:23:18.5901499Z
torch.manual_seed(123)
# It's necessary to clear cache here.
torch.cuda.empty_cache()
>       torch.cuda.synchronize()
2025-04-11T04:23:18.5901985Z
tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.5902273Z
device = None
2025-04-11T04:23:18.5902355Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.5902696Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5904261Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
____________ test_context_attention[False-True-False-4-16-16-16-7] _____________
2025-04-11T04:23:18.5904671Z
bsz = 7, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
kv_group_num = 4, same_context_len = False, use_alibi_slopes = True
use_new_kcache_layout = False
2025-04-11T04:23:18.5905068Z
@pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
@pytest.mark.parametrize("bsz", [7, 32])
@pytest.mark.parametrize("block_size", [16, 32])
@pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
@pytest.mark.parametrize("num_attn_heads", [16])
@pytest.mark.parametrize("kv_group_num", [1, 4])
@pytest.mark.parametrize("same_context_len", [True, False])
@pytest.mark.parametrize("use_alibi_slopes", [True, False])
@pytest.mark.parametrize("use_new_kcache_layout", [True, False])
def test_context_attention(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_attn_heads: int,
kv_group_num: int,
same_context_len: bool,
use_alibi_slopes: bool,
use_new_kcache_layout: bool,
):
if use_new_kcache_layout and use_alibi_slopes:
# TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
# the code (alibi kernel) will be refactored later to avoid code duplication, when
# the whole triton flow with new k cache layout has been supported and tested.
# And tests for the alibi kernel using new kcache layout will be added then.
return
2025-04-11T04:23:18.5908295Z
torch.manual_seed(123)
# It's necessary to clear cache here.
torch.cuda.empty_cache()
>       torch.cuda.synchronize()
2025-04-11T04:23:18.5908704Z
tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.5908989Z
device = None
2025-04-11T04:23:18.5909074Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.5909419Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5910991Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
____________ test_context_attention[False-True-False-4-16-16-16-32] ____________
2025-04-11T04:23:18.5911412Z
bsz = 32, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
kv_group_num = 4, same_context_len = False, use_alibi_slopes = True
use_new_kcache_layout = False
2025-04-11T04:23:18.5911811Z
@pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
@pytest.mark.parametrize("bsz", [7, 32])
@pytest.mark.parametrize("block_size", [16, 32])
@pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
@pytest.mark.parametrize("num_attn_heads", [16])
@pytest.mark.parametrize("kv_group_num", [1, 4])
@pytest.mark.parametrize("same_context_len", [True, False])
@pytest.mark.parametrize("use_alibi_slopes", [True, False])
@pytest.mark.parametrize("use_new_kcache_layout", [True, False])
def test_context_attention(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_attn_heads: int,
kv_group_num: int,
same_context_len: bool,
use_alibi_slopes: bool,
use_new_kcache_layout: bool,
):
if use_new_kcache_layout and use_alibi_slopes:
# TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
# the code (alibi kernel) will be refactored later to avoid code duplication, when
# the whole triton flow with new k cache layout has been supported and tested.
# And tests for the alibi kernel using new kcache layout will be added then.
return
2025-04-11T04:23:18.5915173Z
torch.manual_seed(123)
# It's necessary to clear cache here.
torch.cuda.empty_cache()
>       torch.cuda.synchronize()
2025-04-11T04:23:18.5915570Z
tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.5915865Z
device = None
2025-04-11T04:23:18.5915951Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.5916303Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5917918Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
____________ test_context_attention[False-True-False-4-16-16-32-7] _____________
2025-04-11T04:23:18.5918336Z
bsz = 7, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
kv_group_num = 4, same_context_len = False, use_alibi_slopes = True
use_new_kcache_layout = False
2025-04-11T04:23:18.5918821Z
@pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
@pytest.mark.parametrize("bsz", [7, 32])
@pytest.mark.parametrize("block_size", [16, 32])
@pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
@pytest.mark.parametrize("num_attn_heads", [16])
@pytest.mark.parametrize("kv_group_num", [1, 4])
@pytest.mark.parametrize("same_context_len", [True, False])
@pytest.mark.parametrize("use_alibi_slopes", [True, False])
@pytest.mark.parametrize("use_new_kcache_layout", [True, False])
def test_context_attention(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_attn_heads: int,
kv_group_num: int,
same_context_len: bool,
use_alibi_slopes: bool,
use_new_kcache_layout: bool,
):
if use_new_kcache_layout and use_alibi_slopes:
# TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
# the code (alibi kernel) will be refactored later to avoid code duplication, when
# the whole triton flow with new k cache layout has been supported and tested.
# And tests for the alibi kernel using new kcache layout will be added then.
return
2025-04-11T04:23:18.5921991Z
torch.manual_seed(123)
# It's necessary to clear cache here.
torch.cuda.empty_cache()
>       torch.cuda.synchronize()
2025-04-11T04:23:18.5922371Z
tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.5922659Z
device = None
2025-04-11T04:23:18.5922740Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.5923094Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5924662Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
____________ test_context_attention[False-True-False-4-16-16-32-32] ____________
2025-04-11T04:23:18.5925172Z
bsz = 32, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
kv_group_num = 4, same_context_len = False, use_alibi_slopes = True
use_new_kcache_layout = False
2025-04-11T04:23:18.5925570Z
@pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
@pytest.mark.parametrize("bsz", [7, 32])
@pytest.mark.parametrize("block_size", [16, 32])
@pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
@pytest.mark.parametrize("num_attn_heads", [16])
@pytest.mark.parametrize("kv_group_num", [1, 4])
@pytest.mark.parametrize("same_context_len", [True, False])
@pytest.mark.parametrize("use_alibi_slopes", [True, False])
@pytest.mark.parametrize("use_new_kcache_layout", [True, False])
def test_context_attention(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_attn_heads: int,
kv_group_num: int,
same_context_len: bool,
use_alibi_slopes: bool,
use_new_kcache_layout: bool,
):
if use_new_kcache_layout and use_alibi_slopes:
# TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
# the code (alibi kernel) will be refactored later to avoid code duplication, when
# the whole triton flow with new k cache layout has been supported and tested.
# And tests for the alibi kernel using new kcache layout will be added then.
return
2025-04-11T04:23:18.5928785Z
torch.manual_seed(123)
# It's necessary to clear cache here.
torch.cuda.empty_cache()
>       torch.cuda.synchronize()
2025-04-11T04:23:18.5929169Z
tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.5929464Z
device = None
2025-04-11T04:23:18.5929551Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.5929904Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5931549Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
_____________ test_context_attention[False-False-True-1-16-8-16-7] _____________
2025-04-11T04:23:18.5931967Z
bsz = 7, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
kv_group_num = 1, same_context_len = True, use_alibi_slopes = False
use_new_kcache_layout = False
2025-04-11T04:23:18.5932366Z
@pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
@pytest.mark.parametrize("bsz", [7, 32])
@pytest.mark.parametrize("block_size", [16, 32])
@pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
@pytest.mark.parametrize("num_attn_heads", [16])
@pytest.mark.parametrize("kv_group_num", [1, 4])
@pytest.mark.parametrize("same_context_len", [True, False])
@pytest.mark.parametrize("use_alibi_slopes", [True, False])
@pytest.mark.parametrize("use_new_kcache_layout", [True, False])
def test_context_attention(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_attn_heads: int,
kv_group_num: int,
same_context_len: bool,
use_alibi_slopes: bool,
use_new_kcache_layout: bool,
):
if use_new_kcache_layout and use_alibi_slopes:
# TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
# the code (alibi kernel) will be refactored later to avoid code duplication, when
# the whole triton flow with new k cache layout has been supported and tested.
# And tests for the alibi kernel using new kcache layout will be added then.
return
2025-04-11T04:23:18.5935517Z
torch.manual_seed(123)
# It's necessary to clear cache here.
torch.cuda.empty_cache()
>       torch.cuda.synchronize()
2025-04-11T04:23:18.5935892Z
tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.5936180Z
device = None
2025-04-11T04:23:18.5936262Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.5936600Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5938268Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
____________ test_context_attention[False-False-True-1-16-8-16-32] _____________
2025-04-11T04:23:18.5938691Z
bsz = 32, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
kv_group_num = 1, same_context_len = True, use_alibi_slopes = False
use_new_kcache_layout = False
2025-04-11T04:23:18.5939171Z
@pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
@pytest.mark.parametrize("bsz", [7, 32])
@pytest.mark.parametrize("block_size", [16, 32])
@pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
@pytest.mark.parametrize("num_attn_heads", [16])
@pytest.mark.parametrize("kv_group_num", [1, 4])
@pytest.mark.parametrize("same_context_len", [True, False])
@pytest.mark.parametrize("use_alibi_slopes", [True, False])
@pytest.mark.parametrize("use_new_kcache_layout", [True, False])
def test_context_attention(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_attn_heads: int,
kv_group_num: int,
same_context_len: bool,
use_alibi_slopes: bool,
use_new_kcache_layout: bool,
):
if use_new_kcache_layout and use_alibi_slopes:
# TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
# the code (alibi kernel) will be refactored later to avoid code duplication, when
# the whole triton flow with new k cache layout has been supported and tested.
# And tests for the alibi kernel using new kcache layout will be added then.
return
2025-04-11T04:23:18.5942238Z
torch.manual_seed(123)
# It's necessary to clear cache here.
torch.cuda.empty_cache()
>       torch.cuda.synchronize()
2025-04-11T04:23:18.5942605Z
tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.5942893Z
device = None
2025-04-11T04:23:18.5942977Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.5943406Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5944993Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
_____________ test_context_attention[False-False-True-1-16-8-32-7] _____________
2025-04-11T04:23:18.5945515Z
bsz = 7, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
kv_group_num = 1, same_context_len = True, use_alibi_slopes = False
use_new_kcache_layout = False
2025-04-11T04:23:18.5945910Z
@pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
@pytest.mark.parametrize("bsz", [7, 32])
@pytest.mark.parametrize("block_size", [16, 32])
@pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
@pytest.mark.parametrize("num_attn_heads", [16])
@pytest.mark.parametrize("kv_group_num", [1, 4])
@pytest.mark.parametrize("same_context_len", [True, False])
@pytest.mark.parametrize("use_alibi_slopes", [True, False])
@pytest.mark.parametrize("use_new_kcache_layout", [True, False])
def test_context_attention(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_attn_heads: int,
kv_group_num: int,
same_context_len: bool,
use_alibi_slopes: bool,
use_new_kcache_layout: bool,
):
if use_new_kcache_layout and use_alibi_slopes:
# TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
# the code (alibi kernel) will be refactored later to avoid code duplication, when
# the whole triton flow with new k cache layout has been supported and tested.
# And tests for the alibi kernel using new kcache layout will be added then.
return
2025-04-11T04:23:18.5949018Z
torch.manual_seed(123)
# It's necessary to clear cache here.
torch.cuda.empty_cache()
>       torch.cuda.synchronize()
2025-04-11T04:23:18.5949392Z
tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.5949846Z
device = None
2025-04-11T04:23:18.5949930Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.5950275Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5951943Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
____________ test_context_attention[False-False-True-1-16-8-32-32] _____________
2025-04-11T04:23:18.5952359Z
bsz = 32, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
kv_group_num = 1, same_context_len = True, use_alibi_slopes = False
use_new_kcache_layout = False
2025-04-11T04:23:18.5952751Z
@pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
@pytest.mark.parametrize("bsz", [7, 32])
@pytest.mark.parametrize("block_size", [16, 32])
@pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
@pytest.mark.parametrize("num_attn_heads", [16])
@pytest.mark.parametrize("kv_group_num", [1, 4])
@pytest.mark.parametrize("same_context_len", [True, False])
@pytest.mark.parametrize("use_alibi_slopes", [True, False])
@pytest.mark.parametrize("use_new_kcache_layout", [True, False])
def test_context_attention(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_attn_heads: int,
kv_group_num: int,
same_context_len: bool,
use_alibi_slopes: bool,
use_new_kcache_layout: bool,
):
if use_new_kcache_layout and use_alibi_slopes:
# TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
# the code (alibi kernel) will be refactored later to avoid code duplication, when
# the whole triton flow with new k cache layout has been supported and tested.
# And tests for the alibi kernel using new kcache layout will be added then.
return
2025-04-11T04:23:18.5955904Z
torch.manual_seed(123)
# It's necessary to clear cache here.
torch.cuda.empty_cache()
>       torch.cuda.synchronize()
2025-04-11T04:23:18.5956288Z
tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.5956574Z
device = None
2025-04-11T04:23:18.5956654Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.5956995Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5958649Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
____________ test_context_attention[False-False-True-1-16-16-16-7] _____________
2025-04-11T04:23:18.5959066Z
bsz = 7, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
kv_group_num = 1, same_context_len = True, use_alibi_slopes = False
use_new_kcache_layout = False
2025-04-11T04:23:18.5959468Z
@pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
@pytest.mark.parametrize("bsz", [7, 32])
@pytest.mark.parametrize("block_size", [16, 32])
@pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
@pytest.mark.parametrize("num_attn_heads", [16])
@pytest.mark.parametrize("kv_group_num", [1, 4])
@pytest.mark.parametrize("same_context_len", [True, False])
@pytest.mark.parametrize("use_alibi_slopes", [True, False])
@pytest.mark.parametrize("use_new_kcache_layout", [True, False])
def test_context_attention(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_attn_heads: int,
kv_group_num: int,
same_context_len: bool,
use_alibi_slopes: bool,
use_new_kcache_layout: bool,
):
if use_new_kcache_layout and use_alibi_slopes:
# TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
# the code (alibi kernel) will be refactored later to avoid code duplication, when
# the whole triton flow with new k cache layout has been supported and tested.
# And tests for the alibi kernel using new kcache layout will be added then.
return
2025-04-11T04:23:18.5962611Z
torch.manual_seed(123)
# It's necessary to clear cache here.
torch.cuda.empty_cache()
>       torch.cuda.synchronize()
2025-04-11T04:23:18.5962980Z
tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.5963265Z
device = None
2025-04-11T04:23:18.5963351Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.5963788Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5965420Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
____________ test_context_attention[False-False-True-1-16-16-16-32] ____________
2025-04-11T04:23:18.5965844Z
bsz = 32, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
kv_group_num = 1, same_context_len = True, use_alibi_slopes = False
use_new_kcache_layout = False
2025-04-11T04:23:18.5966250Z
@pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
@pytest.mark.parametrize("bsz", [7, 32])
@pytest.mark.parametrize("block_size", [16, 32])
@pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
@pytest.mark.parametrize("num_attn_heads", [16])
@pytest.mark.parametrize("kv_group_num", [1, 4])
@pytest.mark.parametrize("same_context_len", [True, False])
@pytest.mark.parametrize("use_alibi_slopes", [True, False])
@pytest.mark.parametrize("use_new_kcache_layout", [True, False])
def test_context_attention(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_attn_heads: int,
kv_group_num: int,
same_context_len: bool,
use_alibi_slopes: bool,
use_new_kcache_layout: bool,
):
if use_new_kcache_layout and use_alibi_slopes:
# TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
# the code (alibi kernel) will be refactored later to avoid code duplication, when
# the whole triton flow with new k cache layout has been supported and tested.
# And tests for the alibi kernel using new kcache layout will be added then.
return
2025-04-11T04:23:18.5969450Z
torch.manual_seed(123)
# It's necessary to clear cache here.
torch.cuda.empty_cache()
>       torch.cuda.synchronize()
2025-04-11T04:23:18.5969827Z
tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.5970197Z
device = None
2025-04-11T04:23:18.5970280Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.5970621Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5972211Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
____________ test_context_attention[False-False-True-1-16-16-32-7] _____________
2025-04-11T04:23:18.5972643Z
bsz = 7, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
kv_group_num = 1, same_context_len = True, use_alibi_slopes = False
use_new_kcache_layout = False
2025-04-11T04:23:18.5973055Z
@pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
@pytest.mark.parametrize("bsz", [7, 32])
@pytest.mark.parametrize("block_size", [16, 32])
@pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
@pytest.mark.parametrize("num_attn_heads", [16])
@pytest.mark.parametrize("kv_group_num", [1, 4])
@pytest.mark.parametrize("same_context_len", [True, False])
@pytest.mark.parametrize("use_alibi_slopes", [True, False])
@pytest.mark.parametrize("use_new_kcache_layout", [True, False])
def test_context_attention(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_attn_heads: int,
kv_group_num: int,
same_context_len: bool,
use_alibi_slopes: bool,
use_new_kcache_layout: bool,
):
if use_new_kcache_layout and use_alibi_slopes:
# TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
# the code (alibi kernel) will be refactored later to avoid code duplication, when
# the whole triton flow with new k cache layout has been supported and tested.
# And tests for the alibi kernel using new kcache layout will be added then.
return
2025-04-11T04:23:18.5976291Z
torch.manual_seed(123)
# It's necessary to clear cache here.
torch.cuda.empty_cache()
>       torch.cuda.synchronize()
2025-04-11T04:23:18.5976662Z
tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.5976946Z
device = None
2025-04-11T04:23:18.5977033Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.5977379Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5978937Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
____________ test_context_attention[False-False-True-1-16-16-32-32] ____________
2025-04-11T04:23:18.5979358Z
bsz = 32, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
kv_group_num = 1, same_context_len = True, use_alibi_slopes = False
use_new_kcache_layout = False
2025-04-11T04:23:18.5979751Z
@pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
@pytest.mark.parametrize("bsz", [7, 32])
@pytest.mark.parametrize("block_size", [16, 32])
@pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
@pytest.mark.parametrize("num_attn_heads", [16])
@pytest.mark.parametrize("kv_group_num", [1, 4])
@pytest.mark.parametrize("same_context_len", [True, False])
@pytest.mark.parametrize("use_alibi_slopes", [True, False])
@pytest.mark.parametrize("use_new_kcache_layout", [True, False])
def test_context_attention(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_attn_heads: int,
kv_group_num: int,
same_context_len: bool,
use_alibi_slopes: bool,
use_new_kcache_layout: bool,
):
if use_new_kcache_layout and use_alibi_slopes:
# TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
# the code (alibi kernel) will be refactored later to avoid code duplication, when
# the whole triton flow with new k cache layout has been supported and tested.
# And tests for the alibi kernel using new kcache layout will be added then.
return
2025-04-11T04:23:18.5982982Z
torch.manual_seed(123)
# It's necessary to clear cache here.
torch.cuda.empty_cache()
>       torch.cuda.synchronize()
2025-04-11T04:23:18.5983363Z
tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.5983657Z
device = None
2025-04-11T04:23:18.5983745Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.5984102Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5985671Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
_____________ test_context_attention[False-False-True-4-16-8-16-7] _____________
2025-04-11T04:23:18.5986083Z
bsz = 7, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
kv_group_num = 4, same_context_len = True, use_alibi_slopes = False
use_new_kcache_layout = False
2025-04-11T04:23:18.5986560Z
@pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
@pytest.mark.parametrize("bsz", [7, 32])
@pytest.mark.parametrize("block_size", [16, 32])
@pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
@pytest.mark.parametrize("num_attn_heads", [16])
@pytest.mark.parametrize("kv_group_num", [1, 4])
@pytest.mark.parametrize("same_context_len", [True, False])
@pytest.mark.parametrize("use_alibi_slopes", [True, False])
@pytest.mark.parametrize("use_new_kcache_layout", [True, False])
def test_context_attention(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_attn_heads: int,
kv_group_num: int,
same_context_len: bool,
use_alibi_slopes: bool,
use_new_kcache_layout: bool,
):
if use_new_kcache_layout and use_alibi_slopes:
# TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
# the code (alibi kernel) will be refactored later to avoid code duplication, when
# the whole triton flow with new k cache layout has been supported and tested.
# And tests for the alibi kernel using new kcache layout will be added then.
return
2025-04-11T04:23:18.5989721Z
torch.manual_seed(123)
# It's necessary to clear cache here.
torch.cuda.empty_cache()
>       torch.cuda.synchronize()
2025-04-11T04:23:18.5990094Z
tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.5990379Z
device = None
2025-04-11T04:23:18.5990460Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.5990799Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5992365Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
____________ test_context_attention[False-False-True-4-16-8-16-32] _____________
2025-04-11T04:23:18.5992876Z
bsz = 32, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
kv_group_num = 4, same_context_len = True, use_alibi_slopes = False
use_new_kcache_layout = False
2025-04-11T04:23:18.5993275Z
@pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
@pytest.mark.parametrize("bsz", [7, 32])
@pytest.mark.parametrize("block_size", [16, 32])
@pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
@pytest.mark.parametrize("num_attn_heads", [16])
@pytest.mark.parametrize("kv_group_num", [1, 4])
@pytest.mark.parametrize("same_context_len", [True, False])
@pytest.mark.parametrize("use_alibi_slopes", [True, False])
@pytest.mark.parametrize("use_new_kcache_layout", [True, False])
def test_context_attention(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_attn_heads: int,
kv_group_num: int,
same_context_len: bool,
use_alibi_slopes: bool,
use_new_kcache_layout: bool,
):
if use_new_kcache_layout and use_alibi_slopes:
# TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
# the code (alibi kernel) will be refactored later to avoid code duplication, when
# the whole triton flow with new k cache layout has been supported and tested.
# And tests for the alibi kernel using new kcache layout will be added then.
return
2025-04-11T04:23:18.5996432Z
torch.manual_seed(123)
# It's necessary to clear cache here.
torch.cuda.empty_cache()
>       torch.cuda.synchronize()
2025-04-11T04:23:18.5996799Z
tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.5997082Z
device = None
2025-04-11T04:23:18.5997166Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.5997507Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.5999170Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
_____________ test_context_attention[False-False-True-4-16-8-32-7] _____________
2025-04-11T04:23:18.5999579Z
bsz = 7, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
kv_group_num = 4, same_context_len = True, use_alibi_slopes = False
use_new_kcache_layout = False
2025-04-11T04:23:18.5999973Z
@pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
@pytest.mark.parametrize("bsz", [7, 32])
@pytest.mark.parametrize("block_size", [16, 32])
@pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
@pytest.mark.parametrize("num_attn_heads", [16])
@pytest.mark.parametrize("kv_group_num", [1, 4])
@pytest.mark.parametrize("same_context_len", [True, False])
@pytest.mark.parametrize("use_alibi_slopes", [True, False])
@pytest.mark.parametrize("use_new_kcache_layout", [True, False])
def test_context_attention(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_attn_heads: int,
kv_group_num: int,
same_context_len: bool,
use_alibi_slopes: bool,
use_new_kcache_layout: bool,
):
if use_new_kcache_layout and use_alibi_slopes:
# TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
# the code (alibi kernel) will be refactored later to avoid code duplication, when
# the whole triton flow with new k cache layout has been supported and tested.
# And tests for the alibi kernel using new kcache layout will be added then.
return
2025-04-11T04:23:18.6003087Z
torch.manual_seed(123)
# It's necessary to clear cache here.
torch.cuda.empty_cache()
>       torch.cuda.synchronize()
2025-04-11T04:23:18.6003461Z
tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.6003750Z
device = None
2025-04-11T04:23:18.6003832Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6004171Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6005837Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
____________ test_context_attention[False-False-True-4-16-8-32-32] _____________
2025-04-11T04:23:18.6006257Z
bsz = 32, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
kv_group_num = 4, same_context_len = True, use_alibi_slopes = False
use_new_kcache_layout = False
2025-04-11T04:23:18.6006654Z
@pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
@pytest.mark.parametrize("bsz", [7, 32])
@pytest.mark.parametrize("block_size", [16, 32])
@pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
@pytest.mark.parametrize("num_attn_heads", [16])
@pytest.mark.parametrize("kv_group_num", [1, 4])
@pytest.mark.parametrize("same_context_len", [True, False])
@pytest.mark.parametrize("use_alibi_slopes", [True, False])
@pytest.mark.parametrize("use_new_kcache_layout", [True, False])
def test_context_attention(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_attn_heads: int,
kv_group_num: int,
same_context_len: bool,
use_alibi_slopes: bool,
use_new_kcache_layout: bool,
):
if use_new_kcache_layout and use_alibi_slopes:
# TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
# the code (alibi kernel) will be refactored later to avoid code duplication, when
# the whole triton flow with new k cache layout has been supported and tested.
# And tests for the alibi kernel using new kcache layout will be added then.
return
2025-04-11T04:23:18.6009900Z
torch.manual_seed(123)
# It's necessary to clear cache here.
torch.cuda.empty_cache()
>       torch.cuda.synchronize()
2025-04-11T04:23:18.6010271Z
tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.6010551Z
device = None
2025-04-11T04:23:18.6010635Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6010970Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6012624Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
____________ test_context_attention[False-False-True-4-16-16-16-7] _____________
2025-04-11T04:23:18.6013121Z
bsz = 7, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
kv_group_num = 4, same_context_len = True, use_alibi_slopes = False
use_new_kcache_layout = False
2025-04-11T04:23:18.6013513Z
@pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
@pytest.mark.parametrize("bsz", [7, 32])
@pytest.mark.parametrize("block_size", [16, 32])
@pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
@pytest.mark.parametrize("num_attn_heads", [16])
@pytest.mark.parametrize("kv_group_num", [1, 4])
@pytest.mark.parametrize("same_context_len", [True, False])
@pytest.mark.parametrize("use_alibi_slopes", [True, False])
@pytest.mark.parametrize("use_new_kcache_layout", [True, False])
def test_context_attention(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_attn_heads: int,
kv_group_num: int,
same_context_len: bool,
use_alibi_slopes: bool,
use_new_kcache_layout: bool,
):
if use_new_kcache_layout and use_alibi_slopes:
# TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
# the code (alibi kernel) will be refactored later to avoid code duplication, when
# the whole triton flow with new k cache layout has been supported and tested.
# And tests for the alibi kernel using new kcache layout will be added then.
return
2025-04-11T04:23:18.6016546Z
torch.manual_seed(123)
# It's necessary to clear cache here.
torch.cuda.empty_cache()
>       torch.cuda.synchronize()
2025-04-11T04:23:18.6016924Z
tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.6017300Z
device = None
2025-04-11T04:23:18.6017384Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6017738Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6019408Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
____________ test_context_attention[False-False-True-4-16-16-16-32] ____________
2025-04-11T04:23:18.6019823Z
bsz = 32, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
kv_group_num = 4, same_context_len = True, use_alibi_slopes = False
use_new_kcache_layout = False
2025-04-11T04:23:18.6020214Z
@pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
@pytest.mark.parametrize("bsz", [7, 32])
@pytest.mark.parametrize("block_size", [16, 32])
@pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
@pytest.mark.parametrize("num_attn_heads", [16])
@pytest.mark.parametrize("kv_group_num", [1, 4])
@pytest.mark.parametrize("same_context_len", [True, False])
@pytest.mark.parametrize("use_alibi_slopes", [True, False])
@pytest.mark.parametrize("use_new_kcache_layout", [True, False])
def test_context_attention(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_attn_heads: int,
kv_group_num: int,
same_context_len: bool,
use_alibi_slopes: bool,
use_new_kcache_layout: bool,
):
if use_new_kcache_layout and use_alibi_slopes:
# TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
# the code (alibi kernel) will be refactored later to avoid code duplication, when
# the whole triton flow with new k cache layout has been supported and tested.
# And tests for the alibi kernel using new kcache layout will be added then.
return
2025-04-11T04:23:18.6023271Z
torch.manual_seed(123)
# It's necessary to clear cache here.
torch.cuda.empty_cache()
>       torch.cuda.synchronize()
2025-04-11T04:23:18.6023740Z
tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.6024024Z
device = None
2025-04-11T04:23:18.6024107Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6024446Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6026126Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
____________ test_context_attention[False-False-True-4-16-16-32-7] _____________
2025-04-11T04:23:18.6026543Z
bsz = 7, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
kv_group_num = 4, same_context_len = True, use_alibi_slopes = False
use_new_kcache_layout = False
2025-04-11T04:23:18.6026940Z
@pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
@pytest.mark.parametrize("bsz", [7, 32])
@pytest.mark.parametrize("block_size", [16, 32])
@pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
@pytest.mark.parametrize("num_attn_heads", [16])
@pytest.mark.parametrize("kv_group_num", [1, 4])
@pytest.mark.parametrize("same_context_len", [True, False])
@pytest.mark.parametrize("use_alibi_slopes", [True, False])
@pytest.mark.parametrize("use_new_kcache_layout", [True, False])
def test_context_attention(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_attn_heads: int,
kv_group_num: int,
same_context_len: bool,
use_alibi_slopes: bool,
use_new_kcache_layout: bool,
):
if use_new_kcache_layout and use_alibi_slopes:
# TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
# the code (alibi kernel) will be refactored later to avoid code duplication, when
# the whole triton flow with new k cache layout has been supported and tested.
# And tests for the alibi kernel using new kcache layout will be added then.
return
2025-04-11T04:23:18.6030121Z
torch.manual_seed(123)
# It's necessary to clear cache here.
torch.cuda.empty_cache()
>       torch.cuda.synchronize()
2025-04-11T04:23:18.6030494Z
tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.6030779Z
device = None
2025-04-11T04:23:18.6030864Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6031208Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6032882Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
____________ test_context_attention[False-False-True-4-16-16-32-32] ____________
2025-04-11T04:23:18.6033299Z
bsz = 32, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
kv_group_num = 4, same_context_len = True, use_alibi_slopes = False
use_new_kcache_layout = False
2025-04-11T04:23:18.6033691Z
@pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
@pytest.mark.parametrize("bsz", [7, 32])
@pytest.mark.parametrize("block_size", [16, 32])
@pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
@pytest.mark.parametrize("num_attn_heads", [16])
@pytest.mark.parametrize("kv_group_num", [1, 4])
@pytest.mark.parametrize("same_context_len", [True, False])
@pytest.mark.parametrize("use_alibi_slopes", [True, False])
@pytest.mark.parametrize("use_new_kcache_layout", [True, False])
def test_context_attention(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_attn_heads: int,
kv_group_num: int,
same_context_len: bool,
use_alibi_slopes: bool,
use_new_kcache_layout: bool,
):
if use_new_kcache_layout and use_alibi_slopes:
# TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
# the code (alibi kernel) will be refactored later to avoid code duplication, when
# the whole triton flow with new k cache layout has been supported and tested.
# And tests for the alibi kernel using new kcache layout will be added then.
return
2025-04-11T04:23:18.6036808Z
torch.manual_seed(123)
# It's necessary to clear cache here.
torch.cuda.empty_cache()
>       torch.cuda.synchronize()
2025-04-11T04:23:18.6037183Z
tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.6037550Z
device = None
2025-04-11T04:23:18.6037635Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6037975Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6039558Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
____________ test_context_attention[False-False-False-1-16-8-16-7] _____________
2025-04-11T04:23:18.6039969Z
bsz = 7, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
kv_group_num = 1, same_context_len = False, use_alibi_slopes = False
use_new_kcache_layout = False
2025-04-11T04:23:18.6040366Z
@pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
@pytest.mark.parametrize("bsz", [7, 32])
@pytest.mark.parametrize("block_size", [16, 32])
@pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
@pytest.mark.parametrize("num_attn_heads", [16])
@pytest.mark.parametrize("kv_group_num", [1, 4])
@pytest.mark.parametrize("same_context_len", [True, False])
@pytest.mark.parametrize("use_alibi_slopes", [True, False])
@pytest.mark.parametrize("use_new_kcache_layout", [True, False])
def test_context_attention(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_attn_heads: int,
kv_group_num: int,
same_context_len: bool,
use_alibi_slopes: bool,
use_new_kcache_layout: bool,
):
if use_new_kcache_layout and use_alibi_slopes:
# TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
# the code (alibi kernel) will be refactored later to avoid code duplication, when
# the whole triton flow with new k cache layout has been supported and tested.
# And tests for the alibi kernel using new kcache layout will be added then.
return
2025-04-11T04:23:18.6043490Z
torch.manual_seed(123)
# It's necessary to clear cache here.
torch.cuda.empty_cache()
>       torch.cuda.synchronize()
2025-04-11T04:23:18.6043966Z
tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.6044251Z
device = None
2025-04-11T04:23:18.6044337Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6044674Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6046229Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
____________ test_context_attention[False-False-False-1-16-8-16-32] ____________
2025-04-11T04:23:18.6046647Z
bsz = 32, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
kv_group_num = 1, same_context_len = False, use_alibi_slopes = False
use_new_kcache_layout = False
2025-04-11T04:23:18.6047039Z
@pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
@pytest.mark.parametrize("bsz", [7, 32])
@pytest.mark.parametrize("block_size", [16, 32])
@pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
@pytest.mark.parametrize("num_attn_heads", [16])
@pytest.mark.parametrize("kv_group_num", [1, 4])
@pytest.mark.parametrize("same_context_len", [True, False])
@pytest.mark.parametrize("use_alibi_slopes", [True, False])
@pytest.mark.parametrize("use_new_kcache_layout", [True, False])
def test_context_attention(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_attn_heads: int,
kv_group_num: int,
same_context_len: bool,
use_alibi_slopes: bool,
use_new_kcache_layout: bool,
):
if use_new_kcache_layout and use_alibi_slopes:
# TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
# the code (alibi kernel) will be refactored later to avoid code duplication, when
# the whole triton flow with new k cache layout has been supported and tested.
# And tests for the alibi kernel using new kcache layout will be added then.
return
2025-04-11T04:23:18.6050236Z
torch.manual_seed(123)
# It's necessary to clear cache here.
torch.cuda.empty_cache()
>       torch.cuda.synchronize()
2025-04-11T04:23:18.6050609Z
tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.6050901Z
device = None
2025-04-11T04:23:18.6050982Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6051336Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6052947Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
____________ test_context_attention[False-False-False-1-16-8-32-7] _____________
2025-04-11T04:23:18.6053365Z
bsz = 7, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
kv_group_num = 1, same_context_len = False, use_alibi_slopes = False
use_new_kcache_layout = False
2025-04-11T04:23:18.6053758Z
@pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
@pytest.mark.parametrize("bsz", [7, 32])
@pytest.mark.parametrize("block_size", [16, 32])
@pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
@pytest.mark.parametrize("num_attn_heads", [16])
@pytest.mark.parametrize("kv_group_num", [1, 4])
@pytest.mark.parametrize("same_context_len", [True, False])
@pytest.mark.parametrize("use_alibi_slopes", [True, False])
@pytest.mark.parametrize("use_new_kcache_layout", [True, False])
def test_context_attention(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_attn_heads: int,
kv_group_num: int,
same_context_len: bool,
use_alibi_slopes: bool,
use_new_kcache_layout: bool,
):
if use_new_kcache_layout and use_alibi_slopes:
# TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
# the code (alibi kernel) will be refactored later to avoid code duplication, when
# the whole triton flow with new k cache layout has been supported and tested.
# And tests for the alibi kernel using new kcache layout will be added then.
return
2025-04-11T04:23:18.6056956Z
torch.manual_seed(123)
# It's necessary to clear cache here.
torch.cuda.empty_cache()
>       torch.cuda.synchronize()
2025-04-11T04:23:18.6057334Z
tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.6057626Z
device = None
2025-04-11T04:23:18.6057708Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6058048Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6059612Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
____________ test_context_attention[False-False-False-1-16-8-32-32] ____________
2025-04-11T04:23:18.6060028Z
bsz = 32, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
kv_group_num = 1, same_context_len = False, use_alibi_slopes = False
use_new_kcache_layout = False
2025-04-11T04:23:18.6060513Z
@pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
@pytest.mark.parametrize("bsz", [7, 32])
@pytest.mark.parametrize("block_size", [16, 32])
@pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
@pytest.mark.parametrize("num_attn_heads", [16])
@pytest.mark.parametrize("kv_group_num", [1, 4])
@pytest.mark.parametrize("same_context_len", [True, False])
@pytest.mark.parametrize("use_alibi_slopes", [True, False])
@pytest.mark.parametrize("use_new_kcache_layout", [True, False])
def test_context_attention(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_attn_heads: int,
kv_group_num: int,
same_context_len: bool,
use_alibi_slopes: bool,
use_new_kcache_layout: bool,
):
if use_new_kcache_layout and use_alibi_slopes:
# TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
# the code (alibi kernel) will be refactored later to avoid code duplication, when
# the whole triton flow with new k cache layout has been supported and tested.
# And tests for the alibi kernel using new kcache layout will be added then.
return
2025-04-11T04:23:18.6063678Z
torch.manual_seed(123)
# It's necessary to clear cache here.
torch.cuda.empty_cache()
>       torch.cuda.synchronize()
2025-04-11T04:23:18.6064050Z
tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.6064334Z
device = None
2025-04-11T04:23:18.6064417Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6064764Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6066448Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
____________ test_context_attention[False-False-False-1-16-16-16-7] ____________
2025-04-11T04:23:18.6066871Z
bsz = 7, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
kv_group_num = 1, same_context_len = False, use_alibi_slopes = False
use_new_kcache_layout = False
2025-04-11T04:23:18.6067264Z
@pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
@pytest.mark.parametrize("bsz", [7, 32])
@pytest.mark.parametrize("block_size", [16, 32])
@pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
@pytest.mark.parametrize("num_attn_heads", [16])
@pytest.mark.parametrize("kv_group_num", [1, 4])
@pytest.mark.parametrize("same_context_len", [True, False])
@pytest.mark.parametrize("use_alibi_slopes", [True, False])
@pytest.mark.parametrize("use_new_kcache_layout", [True, False])
def test_context_attention(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_attn_heads: int,
kv_group_num: int,
same_context_len: bool,
use_alibi_slopes: bool,
use_new_kcache_layout: bool,
):
if use_new_kcache_layout and use_alibi_slopes:
# TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
# the code (alibi kernel) will be refactored later to avoid code duplication, when
# the whole triton flow with new k cache layout has been supported and tested.
# And tests for the alibi kernel using new kcache layout will be added then.
return
2025-04-11T04:23:18.6070398Z
torch.manual_seed(123)
# It's necessary to clear cache here.
torch.cuda.empty_cache()
>       torch.cuda.synchronize()
2025-04-11T04:23:18.6070767Z
tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.6071053Z
device = None
2025-04-11T04:23:18.6071139Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6071476Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6073128Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
___________ test_context_attention[False-False-False-1-16-16-16-32] ____________
2025-04-11T04:23:18.6073544Z
bsz = 32, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
kv_group_num = 1, same_context_len = False, use_alibi_slopes = False
use_new_kcache_layout = False
2025-04-11T04:23:18.6073939Z
@pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
@pytest.mark.parametrize("bsz", [7, 32])
@pytest.mark.parametrize("block_size", [16, 32])
@pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
@pytest.mark.parametrize("num_attn_heads", [16])
@pytest.mark.parametrize("kv_group_num", [1, 4])
@pytest.mark.parametrize("same_context_len", [True, False])
@pytest.mark.parametrize("use_alibi_slopes", [True, False])
@pytest.mark.parametrize("use_new_kcache_layout", [True, False])
def test_context_attention(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_attn_heads: int,
kv_group_num: int,
same_context_len: bool,
use_alibi_slopes: bool,
use_new_kcache_layout: bool,
):
if use_new_kcache_layout and use_alibi_slopes:
# TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
# the code (alibi kernel) will be refactored later to avoid code duplication, when
# the whole triton flow with new k cache layout has been supported and tested.
# And tests for the alibi kernel using new kcache layout will be added then.
return
2025-04-11T04:23:18.6077079Z
torch.manual_seed(123)
# It's necessary to clear cache here.
torch.cuda.empty_cache()
>       torch.cuda.synchronize()
2025-04-11T04:23:18.6077449Z
tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.6077732Z
device = None
2025-04-11T04:23:18.6077814Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6078153Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6079807Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
____________ test_context_attention[False-False-False-1-16-16-32-7] ____________
2025-04-11T04:23:18.6080222Z
bsz = 7, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
kv_group_num = 1, same_context_len = False, use_alibi_slopes = False
use_new_kcache_layout = False
2025-04-11T04:23:18.6080704Z
@pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
@pytest.mark.parametrize("bsz", [7, 32])
@pytest.mark.parametrize("block_size", [16, 32])
@pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
@pytest.mark.parametrize("num_attn_heads", [16])
@pytest.mark.parametrize("kv_group_num", [1, 4])
@pytest.mark.parametrize("same_context_len", [True, False])
@pytest.mark.parametrize("use_alibi_slopes", [True, False])
@pytest.mark.parametrize("use_new_kcache_layout", [True, False])
def test_context_attention(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_attn_heads: int,
kv_group_num: int,
same_context_len: bool,
use_alibi_slopes: bool,
use_new_kcache_layout: bool,
):
if use_new_kcache_layout and use_alibi_slopes:
# TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
# the code (alibi kernel) will be refactored later to avoid code duplication, when
# the whole triton flow with new k cache layout has been supported and tested.
# And tests for the alibi kernel using new kcache layout will be added then.
return
2025-04-11T04:23:18.6083753Z
torch.manual_seed(123)
# It's necessary to clear cache here.
torch.cuda.empty_cache()
>       torch.cuda.synchronize()
2025-04-11T04:23:18.6084127Z
tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.6084411Z
device = None
2025-04-11T04:23:18.6084577Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6084929Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6086606Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
___________ test_context_attention[False-False-False-1-16-16-32-32] ____________
2025-04-11T04:23:18.6087027Z
bsz = 32, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
kv_group_num = 1, same_context_len = False, use_alibi_slopes = False
use_new_kcache_layout = False
2025-04-11T04:23:18.6087421Z
@pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
@pytest.mark.parametrize("bsz", [7, 32])
@pytest.mark.parametrize("block_size", [16, 32])
@pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
@pytest.mark.parametrize("num_attn_heads", [16])
@pytest.mark.parametrize("kv_group_num", [1, 4])
@pytest.mark.parametrize("same_context_len", [True, False])
@pytest.mark.parametrize("use_alibi_slopes", [True, False])
@pytest.mark.parametrize("use_new_kcache_layout", [True, False])
def test_context_attention(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_attn_heads: int,
kv_group_num: int,
same_context_len: bool,
use_alibi_slopes: bool,
use_new_kcache_layout: bool,
):
if use_new_kcache_layout and use_alibi_slopes:
# TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
# the code (alibi kernel) will be refactored later to avoid code duplication, when
# the whole triton flow with new k cache layout has been supported and tested.
# And tests for the alibi kernel using new kcache layout will be added then.
return
2025-04-11T04:23:18.6090451Z
torch.manual_seed(123)
# It's necessary to clear cache here.
torch.cuda.empty_cache()
>       torch.cuda.synchronize()
2025-04-11T04:23:18.6090924Z
tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.6091214Z
device = None
2025-04-11T04:23:18.6091296Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6091637Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6093277Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
____________ test_context_attention[False-False-False-4-16-8-16-7] _____________
2025-04-11T04:23:18.6093687Z
bsz = 7, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
kv_group_num = 4, same_context_len = False, use_alibi_slopes = False
use_new_kcache_layout = False
2025-04-11T04:23:18.6094090Z
@pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
@pytest.mark.parametrize("bsz", [7, 32])
@pytest.mark.parametrize("block_size", [16, 32])
@pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
@pytest.mark.parametrize("num_attn_heads", [16])
@pytest.mark.parametrize("kv_group_num", [1, 4])
@pytest.mark.parametrize("same_context_len", [True, False])
@pytest.mark.parametrize("use_alibi_slopes", [True, False])
@pytest.mark.parametrize("use_new_kcache_layout", [True, False])
def test_context_attention(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_attn_heads: int,
kv_group_num: int,
same_context_len: bool,
use_alibi_slopes: bool,
use_new_kcache_layout: bool,
):
if use_new_kcache_layout and use_alibi_slopes:
# TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
# the code (alibi kernel) will be refactored later to avoid code duplication, when
# the whole triton flow with new k cache layout has been supported and tested.
# And tests for the alibi kernel using new kcache layout will be added then.
return
2025-04-11T04:23:18.6097210Z
torch.manual_seed(123)
# It's necessary to clear cache here.
torch.cuda.empty_cache()
>       torch.cuda.synchronize()
2025-04-11T04:23:18.6097583Z
tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.6097877Z
device = None
2025-04-11T04:23:18.6097960Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6098303Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6099958Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
____________ test_context_attention[False-False-False-4-16-8-16-32] ____________
2025-04-11T04:23:18.6100382Z
bsz = 32, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
kv_group_num = 4, same_context_len = False, use_alibi_slopes = False
use_new_kcache_layout = False
2025-04-11T04:23:18.6100779Z
@pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
@pytest.mark.parametrize("bsz", [7, 32])
@pytest.mark.parametrize("block_size", [16, 32])
@pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
@pytest.mark.parametrize("num_attn_heads", [16])
@pytest.mark.parametrize("kv_group_num", [1, 4])
@pytest.mark.parametrize("same_context_len", [True, False])
@pytest.mark.parametrize("use_alibi_slopes", [True, False])
@pytest.mark.parametrize("use_new_kcache_layout", [True, False])
def test_context_attention(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_attn_heads: int,
kv_group_num: int,
same_context_len: bool,
use_alibi_slopes: bool,
use_new_kcache_layout: bool,
):
if use_new_kcache_layout and use_alibi_slopes:
# TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
# the code (alibi kernel) will be refactored later to avoid code duplication, when
# the whole triton flow with new k cache layout has been supported and tested.
# And tests for the alibi kernel using new kcache layout will be added then.
return
2025-04-11T04:23:18.6103925Z
torch.manual_seed(123)
# It's necessary to clear cache here.
torch.cuda.empty_cache()
>       torch.cuda.synchronize()
2025-04-11T04:23:18.6104297Z
tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.6104587Z
device = None
2025-04-11T04:23:18.6104668Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6105186Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6106769Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
____________ test_context_attention[False-False-False-4-16-8-32-7] _____________
2025-04-11T04:23:18.6107184Z
bsz = 7, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
kv_group_num = 4, same_context_len = False, use_alibi_slopes = False
use_new_kcache_layout = False
2025-04-11T04:23:18.6107575Z
@pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
@pytest.mark.parametrize("bsz", [7, 32])
@pytest.mark.parametrize("block_size", [16, 32])
@pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
@pytest.mark.parametrize("num_attn_heads", [16])
@pytest.mark.parametrize("kv_group_num", [1, 4])
@pytest.mark.parametrize("same_context_len", [True, False])
@pytest.mark.parametrize("use_alibi_slopes", [True, False])
@pytest.mark.parametrize("use_new_kcache_layout", [True, False])
def test_context_attention(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_attn_heads: int,
kv_group_num: int,
same_context_len: bool,
use_alibi_slopes: bool,
use_new_kcache_layout: bool,
):
if use_new_kcache_layout and use_alibi_slopes:
# TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
# the code (alibi kernel) will be refactored later to avoid code duplication, when
# the whole triton flow with new k cache layout has been supported and tested.
# And tests for the alibi kernel using new kcache layout will be added then.
return
2025-04-11T04:23:18.6110784Z
torch.manual_seed(123)
# It's necessary to clear cache here.
torch.cuda.empty_cache()
>       torch.cuda.synchronize()
2025-04-11T04:23:18.6111266Z
tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.6111551Z
device = None
2025-04-11T04:23:18.6111634Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6111974Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6113525Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
____________ test_context_attention[False-False-False-4-16-8-32-32] ____________
2025-04-11T04:23:18.6113946Z
bsz = 32, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
kv_group_num = 4, same_context_len = False, use_alibi_slopes = False
use_new_kcache_layout = False
2025-04-11T04:23:18.6114343Z
@pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
@pytest.mark.parametrize("bsz", [7, 32])
@pytest.mark.parametrize("block_size", [16, 32])
@pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
@pytest.mark.parametrize("num_attn_heads", [16])
@pytest.mark.parametrize("kv_group_num", [1, 4])
@pytest.mark.parametrize("same_context_len", [True, False])
@pytest.mark.parametrize("use_alibi_slopes", [True, False])
@pytest.mark.parametrize("use_new_kcache_layout", [True, False])
def test_context_attention(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_attn_heads: int,
kv_group_num: int,
same_context_len: bool,
use_alibi_slopes: bool,
use_new_kcache_layout: bool,
):
if use_new_kcache_layout and use_alibi_slopes:
# TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
# the code (alibi kernel) will be refactored later to avoid code duplication, when
# the whole triton flow with new k cache layout has been supported and tested.
# And tests for the alibi kernel using new kcache layout will be added then.
return
2025-04-11T04:23:18.6117571Z
torch.manual_seed(123)
# It's necessary to clear cache here.
torch.cuda.empty_cache()
>       torch.cuda.synchronize()
2025-04-11T04:23:18.6117951Z
tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.6118237Z
device = None
2025-04-11T04:23:18.6118319Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6118664Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6120237Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
____________ test_context_attention[False-False-False-4-16-16-16-7] ____________
2025-04-11T04:23:18.6120652Z
bsz = 7, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
kv_group_num = 4, same_context_len = False, use_alibi_slopes = False
use_new_kcache_layout = False
2025-04-11T04:23:18.6121041Z
@pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
@pytest.mark.parametrize("bsz", [7, 32])
@pytest.mark.parametrize("block_size", [16, 32])
@pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
@pytest.mark.parametrize("num_attn_heads", [16])
@pytest.mark.parametrize("kv_group_num", [1, 4])
@pytest.mark.parametrize("same_context_len", [True, False])
@pytest.mark.parametrize("use_alibi_slopes", [True, False])
@pytest.mark.parametrize("use_new_kcache_layout", [True, False])
def test_context_attention(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_attn_heads: int,
kv_group_num: int,
same_context_len: bool,
use_alibi_slopes: bool,
use_new_kcache_layout: bool,
):
if use_new_kcache_layout and use_alibi_slopes:
# TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
# the code (alibi kernel) will be refactored later to avoid code duplication, when
# the whole triton flow with new k cache layout has been supported and tested.
# And tests for the alibi kernel using new kcache layout will be added then.
return
2025-04-11T04:23:18.6124279Z
torch.manual_seed(123)
# It's necessary to clear cache here.
torch.cuda.empty_cache()
>       torch.cuda.synchronize()
2025-04-11T04:23:18.6124657Z
tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.6124948Z
device = None
2025-04-11T04:23:18.6125029Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6125376Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6126954Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
___________ test_context_attention[False-False-False-4-16-16-16-32] ____________
2025-04-11T04:23:18.6127382Z
bsz = 32, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
kv_group_num = 4, same_context_len = False, use_alibi_slopes = False
use_new_kcache_layout = False
2025-04-11T04:23:18.6127872Z
@pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
@pytest.mark.parametrize("bsz", [7, 32])
@pytest.mark.parametrize("block_size", [16, 32])
@pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
@pytest.mark.parametrize("num_attn_heads", [16])
@pytest.mark.parametrize("kv_group_num", [1, 4])
@pytest.mark.parametrize("same_context_len", [True, False])
@pytest.mark.parametrize("use_alibi_slopes", [True, False])
@pytest.mark.parametrize("use_new_kcache_layout", [True, False])
def test_context_attention(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_attn_heads: int,
kv_group_num: int,
same_context_len: bool,
use_alibi_slopes: bool,
use_new_kcache_layout: bool,
):
if use_new_kcache_layout and use_alibi_slopes:
# TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
# the code (alibi kernel) will be refactored later to avoid code duplication, when
# the whole triton flow with new k cache layout has been supported and tested.
# And tests for the alibi kernel using new kcache layout will be added then.
return
2025-04-11T04:23:18.6131056Z
torch.manual_seed(123)
# It's necessary to clear cache here.
torch.cuda.empty_cache()
>       torch.cuda.synchronize()
2025-04-11T04:23:18.6131434Z
tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.6131717Z
device = None
2025-04-11T04:23:18.6131805Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6132146Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6133793Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
____________ test_context_attention[False-False-False-4-16-16-32-7] ____________
2025-04-11T04:23:18.6134217Z
bsz = 7, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
kv_group_num = 4, same_context_len = False, use_alibi_slopes = False
use_new_kcache_layout = False
2025-04-11T04:23:18.6134608Z
@pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
@pytest.mark.parametrize("bsz", [7, 32])
@pytest.mark.parametrize("block_size", [16, 32])
@pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
@pytest.mark.parametrize("num_attn_heads", [16])
@pytest.mark.parametrize("kv_group_num", [1, 4])
@pytest.mark.parametrize("same_context_len", [True, False])
@pytest.mark.parametrize("use_alibi_slopes", [True, False])
@pytest.mark.parametrize("use_new_kcache_layout", [True, False])
def test_context_attention(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_attn_heads: int,
kv_group_num: int,
same_context_len: bool,
use_alibi_slopes: bool,
use_new_kcache_layout: bool,
):
if use_new_kcache_layout and use_alibi_slopes:
# TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
# the code (alibi kernel) will be refactored later to avoid code duplication, when
# the whole triton flow with new k cache layout has been supported and tested.
# And tests for the alibi kernel using new kcache layout will be added then.
return
2025-04-11T04:23:18.6137758Z
torch.manual_seed(123)
# It's necessary to clear cache here.
torch.cuda.empty_cache()
>       torch.cuda.synchronize()
2025-04-11T04:23:18.6138132Z
tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.6138420Z
device = None
2025-04-11T04:23:18.6138501Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6138843Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6140507Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
___________ test_context_attention[False-False-False-4-16-16-32-32] ____________
2025-04-11T04:23:18.6140930Z
bsz = 32, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
kv_group_num = 4, same_context_len = False, use_alibi_slopes = False
use_new_kcache_layout = False
2025-04-11T04:23:18.6141327Z
@pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
@pytest.mark.parametrize("bsz", [7, 32])
@pytest.mark.parametrize("block_size", [16, 32])
@pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
@pytest.mark.parametrize("num_attn_heads", [16])
@pytest.mark.parametrize("kv_group_num", [1, 4])
@pytest.mark.parametrize("same_context_len", [True, False])
@pytest.mark.parametrize("use_alibi_slopes", [True, False])
@pytest.mark.parametrize("use_new_kcache_layout", [True, False])
def test_context_attention(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_attn_heads: int,
kv_group_num: int,
same_context_len: bool,
use_alibi_slopes: bool,
use_new_kcache_layout: bool,
):
if use_new_kcache_layout and use_alibi_slopes:
# TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
# the code (alibi kernel) will be refactored later to avoid code duplication, when
# the whole triton flow with new k cache layout has been supported and tested.
# And tests for the alibi kernel using new kcache layout will be added then.
return
2025-04-11T04:23:18.6144524Z
torch.manual_seed(123)
# It's necessary to clear cache here.
torch.cuda.empty_cache()
>       torch.cuda.synchronize()
2025-04-11T04:23:18.6144908Z
tests/test_infer/test_kernels/triton/test_context_attn_unpad.py:117:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.6145199Z
device = None
2025-04-11T04:23:18.6145285Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6145624Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6147278Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
______________ test_flash_decoding[True-False-1-True-1-16-8-16-7] ______________
2025-04-11T04:23:18.6147687Z
bsz = 7, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
kv_group_num = 1, same_context_len = True, q_len = 1, use_alibi_slopes = False
use_new_kcache_layout = True
2025-04-11T04:23:18.6148181Z
@pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
@pytest.mark.parametrize("bsz", [7, 16])
@pytest.mark.parametrize("block_size", [16, 32])
@pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
@pytest.mark.parametrize("num_attn_heads", [16])
@pytest.mark.parametrize("kv_group_num", [1, 4])
@pytest.mark.parametrize("same_context_len", [True, False])
@pytest.mark.parametrize("q_len", [1, 5])
@pytest.mark.parametrize("use_alibi_slopes", [True, False])
@pytest.mark.parametrize("use_new_kcache_layout", [True, False])
def test_flash_decoding(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_attn_heads: int,
kv_group_num: int,
same_context_len: bool,
q_len: int,
use_alibi_slopes: bool,
use_new_kcache_layout: bool,
):
if use_new_kcache_layout and use_alibi_slopes:
# TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
# the code (alibi kernel) will be refactored later to avoid code duplication, when
# the whole triton flow with new k cache layout has been supported and tested.
# And tests for the alibi kernel using new kcache layout will be added then.
pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.6151561Z
torch.manual_seed(123)
torch.cuda.empty_cache()
>       torch.cuda.synchronize()
2025-04-11T04:23:18.6151837Z
tests/test_infer/test_kernels/triton/test_decoding_attn.py:100:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.6152108Z
device = None
2025-04-11T04:23:18.6152285Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6152634Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6154338Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
_____________ test_flash_decoding[True-False-1-True-1-16-8-16-16] ______________
2025-04-11T04:23:18.6154750Z
bsz = 16, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
kv_group_num = 1, same_context_len = True, q_len = 1, use_alibi_slopes = False
use_new_kcache_layout = True
2025-04-11T04:23:18.6155159Z
@pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
@pytest.mark.parametrize("bsz", [7, 16])
@pytest.mark.parametrize("block_size", [16, 32])
@pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
@pytest.mark.parametrize("num_attn_heads", [16])
@pytest.mark.parametrize("kv_group_num", [1, 4])
@pytest.mark.parametrize("same_context_len", [True, False])
@pytest.mark.parametrize("q_len", [1, 5])
@pytest.mark.parametrize("use_alibi_slopes", [True, False])
@pytest.mark.parametrize("use_new_kcache_layout", [True, False])
def test_flash_decoding(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_attn_heads: int,
kv_group_num: int,
same_context_len: bool,
q_len: int,
use_alibi_slopes: bool,
use_new_kcache_layout: bool,
):
if use_new_kcache_layout and use_alibi_slopes:
# TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
# the code (alibi kernel) will be refactored later to avoid code duplication, when
# the whole triton flow with new k cache layout has been supported and tested.
# And tests for the alibi kernel using new kcache layout will be added then.
pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.6158468Z
torch.manual_seed(123)
torch.cuda.empty_cache()
>       torch.cuda.synchronize()
2025-04-11T04:23:18.6158844Z
tests/test_infer/test_kernels/triton/test_decoding_attn.py:100:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.6159122Z
device = None
2025-04-11T04:23:18.6159207Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6159548Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6161210Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
______________ test_flash_decoding[True-False-1-True-1-16-8-32-7] ______________
2025-04-11T04:23:18.6161632Z
bsz = 7, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
kv_group_num = 1, same_context_len = True, q_len = 1, use_alibi_slopes = False
use_new_kcache_layout = True
2025-04-11T04:23:18.6162060Z
@pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
@pytest.mark.parametrize("bsz", [7, 16])
@pytest.mark.parametrize("block_size", [16, 32])
@pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
@pytest.mark.parametrize("num_attn_heads", [16])
@pytest.mark.parametrize("kv_group_num", [1, 4])
@pytest.mark.parametrize("same_context_len", [True, False])
@pytest.mark.parametrize("q_len", [1, 5])
@pytest.mark.parametrize("use_alibi_slopes", [True, False])
@pytest.mark.parametrize("use_new_kcache_layout", [True, False])
def test_flash_decoding(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_attn_heads: int,
kv_group_num: int,
same_context_len: bool,
q_len: int,
use_alibi_slopes: bool,
use_new_kcache_layout: bool,
):
if use_new_kcache_layout and use_alibi_slopes:
# TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
# the code (alibi kernel) will be refactored later to avoid code duplication, when
# the whole triton flow with new k cache layout has been supported and tested.
# And tests for the alibi kernel using new kcache layout will be added then.
pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.6165578Z
torch.manual_seed(123)
torch.cuda.empty_cache()
>       torch.cuda.synchronize()
2025-04-11T04:23:18.6165851Z
tests/test_infer/test_kernels/triton/test_decoding_attn.py:100:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.6166124Z
device = None
2025-04-11T04:23:18.6166208Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6166639Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6168207Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
_____________ test_flash_decoding[True-False-1-True-1-16-8-32-16] ______________
2025-04-11T04:23:18.6168619Z
bsz = 16, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
kv_group_num = 1, same_context_len = True, q_len = 1, use_alibi_slopes = False
use_new_kcache_layout = True
2025-04-11T04:23:18.6169029Z
@pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
@pytest.mark.parametrize("bsz", [7, 16])
@pytest.mark.parametrize("block_size", [16, 32])
@pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
@pytest.mark.parametrize("num_attn_heads", [16])
@pytest.mark.parametrize("kv_group_num", [1, 4])
@pytest.mark.parametrize("same_context_len", [True, False])
@pytest.mark.parametrize("q_len", [1, 5])
@pytest.mark.parametrize("use_alibi_slopes", [True, False])
@pytest.mark.parametrize("use_new_kcache_layout", [True, False])
def test_flash_decoding(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_attn_heads: int,
kv_group_num: int,
same_context_len: bool,
q_len: int,
use_alibi_slopes: bool,
use_new_kcache_layout: bool,
):
if use_new_kcache_layout and use_alibi_slopes:
# TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
# the code (alibi kernel) will be refactored later to avoid code duplication, when
# the whole triton flow with new k cache layout has been supported and tested.
# And tests for the alibi kernel using new kcache layout will be added then.
pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.6172418Z
torch.manual_seed(123)
torch.cuda.empty_cache()
>       torch.cuda.synchronize()
2025-04-11T04:23:18.6172700Z
tests/test_infer/test_kernels/triton/test_decoding_attn.py:100:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.6173056Z
device = None
2025-04-11T04:23:18.6173137Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6173479Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6175068Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
_____________ test_flash_decoding[True-False-1-True-1-16-16-16-7] ______________
2025-04-11T04:23:18.6175485Z
bsz = 7, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
kv_group_num = 1, same_context_len = True, q_len = 1, use_alibi_slopes = False
use_new_kcache_layout = True
2025-04-11T04:23:18.6175900Z
@pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
@pytest.mark.parametrize("bsz", [7, 16])
@pytest.mark.parametrize("block_size", [16, 32])
@pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
@pytest.mark.parametrize("num_attn_heads", [16])
@pytest.mark.parametrize("kv_group_num", [1, 4])
@pytest.mark.parametrize("same_context_len", [True, False])
@pytest.mark.parametrize("q_len", [1, 5])
@pytest.mark.parametrize("use_alibi_slopes", [True, False])
@pytest.mark.parametrize("use_new_kcache_layout", [True, False])
def test_flash_decoding(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_attn_heads: int,
kv_group_num: int,
same_context_len: bool,
q_len: int,
use_alibi_slopes: bool,
use_new_kcache_layout: bool,
):
if use_new_kcache_layout and use_alibi_slopes:
# TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
# the code (alibi kernel) will be refactored later to avoid code duplication, when
# the whole triton flow with new k cache layout has been supported and tested.
# And tests for the alibi kernel using new kcache layout will be added then.
pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.6179377Z
torch.manual_seed(123)
torch.cuda.empty_cache()
>       torch.cuda.synchronize()
2025-04-11T04:23:18.6179654Z
tests/test_infer/test_kernels/triton/test_decoding_attn.py:100:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.6179927Z
device = None
2025-04-11T04:23:18.6180010Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6180361Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6181923Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
_____________ test_flash_decoding[True-False-1-True-1-16-16-16-16] _____________
2025-04-11T04:23:18.6182342Z
bsz = 16, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
kv_group_num = 1, same_context_len = True, q_len = 1, use_alibi_slopes = False
use_new_kcache_layout = True
2025-04-11T04:23:18.6182758Z
@pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
@pytest.mark.parametrize("bsz", [7, 16])
@pytest.mark.parametrize("block_size", [16, 32])
@pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
@pytest.mark.parametrize("num_attn_heads", [16])
@pytest.mark.parametrize("kv_group_num", [1, 4])
@pytest.mark.parametrize("same_context_len", [True, False])
@pytest.mark.parametrize("q_len", [1, 5])
@pytest.mark.parametrize("use_alibi_slopes", [True, False])
@pytest.mark.parametrize("use_new_kcache_layout", [True, False])
def test_flash_decoding(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_attn_heads: int,
kv_group_num: int,
same_context_len: bool,
q_len: int,
use_alibi_slopes: bool,
use_new_kcache_layout: bool,
):
if use_new_kcache_layout and use_alibi_slopes:
# TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
# the code (alibi kernel) will be refactored later to avoid code duplication, when
# the whole triton flow with new k cache layout has been supported and tested.
# And tests for the alibi kernel using new kcache layout will be added then.
pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.6186331Z
torch.manual_seed(123)
torch.cuda.empty_cache()
>       torch.cuda.synchronize()
2025-04-11T04:23:18.6186612Z
tests/test_infer/test_kernels/triton/test_decoding_attn.py:100:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.6186892Z
device = None
2025-04-11T04:23:18.6186975Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6187322Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6189093Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
_____________ test_flash_decoding[True-False-1-True-1-16-16-32-7] ______________
2025-04-11T04:23:18.6189601Z
bsz = 7, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
kv_group_num = 1, same_context_len = True, q_len = 1, use_alibi_slopes = False
use_new_kcache_layout = True
2025-04-11T04:23:18.6190026Z
@pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
@pytest.mark.parametrize("bsz", [7, 16])
@pytest.mark.parametrize("block_size", [16, 32])
@pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
@pytest.mark.parametrize("num_attn_heads", [16])
@pytest.mark.parametrize("kv_group_num", [1, 4])
@pytest.mark.parametrize("same_context_len", [True, False])
@pytest.mark.parametrize("q_len", [1, 5])
@pytest.mark.parametrize("use_alibi_slopes", [True, False])
@pytest.mark.parametrize("use_new_kcache_layout", [True, False])
def test_flash_decoding(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_attn_heads: int,
kv_group_num: int,
same_context_len: bool,
q_len: int,
use_alibi_slopes: bool,
use_new_kcache_layout: bool,
):
if use_new_kcache_layout and use_alibi_slopes:
# TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
# the code (alibi kernel) will be refactored later to avoid code duplication, when
# the whole triton flow with new k cache layout has been supported and tested.
# And tests for the alibi kernel using new kcache layout will be added then.
pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.6193446Z
torch.manual_seed(123)
torch.cuda.empty_cache()
>       torch.cuda.synchronize()
2025-04-11T04:23:18.6193722Z
tests/test_infer/test_kernels/triton/test_decoding_attn.py:100:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.6193994Z
device = None
2025-04-11T04:23:18.6194077Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6194421Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6196073Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
_____________ test_flash_decoding[True-False-1-True-1-16-16-32-16] _____________
2025-04-11T04:23:18.6196484Z
bsz = 16, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
kv_group_num = 1, same_context_len = True, q_len = 1, use_alibi_slopes = False
use_new_kcache_layout = True
2025-04-11T04:23:18.6196898Z
@pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
@pytest.mark.parametrize("bsz", [7, 16])
@pytest.mark.parametrize("block_size", [16, 32])
@pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
@pytest.mark.parametrize("num_attn_heads", [16])
@pytest.mark.parametrize("kv_group_num", [1, 4])
@pytest.mark.parametrize("same_context_len", [True, False])
@pytest.mark.parametrize("q_len", [1, 5])
@pytest.mark.parametrize("use_alibi_slopes", [True, False])
@pytest.mark.parametrize("use_new_kcache_layout", [True, False])
def test_flash_decoding(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_attn_heads: int,
kv_group_num: int,
same_context_len: bool,
q_len: int,
use_alibi_slopes: bool,
use_new_kcache_layout: bool,
):
if use_new_kcache_layout and use_alibi_slopes:
# TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
# the code (alibi kernel) will be refactored later to avoid code duplication, when
# the whole triton flow with new k cache layout has been supported and tested.
# And tests for the alibi kernel using new kcache layout will be added then.
pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.6200308Z
torch.manual_seed(123)
torch.cuda.empty_cache()
>       torch.cuda.synchronize()
2025-04-11T04:23:18.6200584Z
tests/test_infer/test_kernels/triton/test_decoding_attn.py:100:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.6200857Z
device = None
2025-04-11T04:23:18.6200938Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6201276Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6203004Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
______________ test_flash_decoding[True-False-1-True-4-16-8-16-7] ______________
2025-04-11T04:23:18.6203418Z
bsz = 7, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
kv_group_num = 4, same_context_len = True, q_len = 1, use_alibi_slopes = False
use_new_kcache_layout = True
2025-04-11T04:23:18.6203928Z
@pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
@pytest.mark.parametrize("bsz", [7, 16])
@pytest.mark.parametrize("block_size", [16, 32])
@pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
@pytest.mark.parametrize("num_attn_heads", [16])
@pytest.mark.parametrize("kv_group_num", [1, 4])
@pytest.mark.parametrize("same_context_len", [True, False])
@pytest.mark.parametrize("q_len", [1, 5])
@pytest.mark.parametrize("use_alibi_slopes", [True, False])
@pytest.mark.parametrize("use_new_kcache_layout", [True, False])
def test_flash_decoding(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_attn_heads: int,
kv_group_num: int,
same_context_len: bool,
q_len: int,
use_alibi_slopes: bool,
use_new_kcache_layout: bool,
):
if use_new_kcache_layout and use_alibi_slopes:
# TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
# the code (alibi kernel) will be refactored later to avoid code duplication, when
# the whole triton flow with new k cache layout has been supported and tested.
# And tests for the alibi kernel using new kcache layout will be added then.
pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.6207241Z
torch.manual_seed(123)
torch.cuda.empty_cache()
>       torch.cuda.synchronize()
2025-04-11T04:23:18.6207513Z
tests/test_infer/test_kernels/triton/test_decoding_attn.py:100:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.6207785Z
device = None
2025-04-11T04:23:18.6207867Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6208290Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6209970Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
_____________ test_flash_decoding[True-False-1-True-4-16-8-16-16] ______________
2025-04-11T04:23:18.6210394Z
bsz = 16, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
kv_group_num = 4, same_context_len = True, q_len = 1, use_alibi_slopes = False
use_new_kcache_layout = True
2025-04-11T04:23:18.6210813Z
@pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
@pytest.mark.parametrize("bsz", [7, 16])
@pytest.mark.parametrize("block_size", [16, 32])
@pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
@pytest.mark.parametrize("num_attn_heads", [16])
@pytest.mark.parametrize("kv_group_num", [1, 4])
@pytest.mark.parametrize("same_context_len", [True, False])
@pytest.mark.parametrize("q_len", [1, 5])
@pytest.mark.parametrize("use_alibi_slopes", [True, False])
@pytest.mark.parametrize("use_new_kcache_layout", [True, False])
def test_flash_decoding(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_attn_heads: int,
kv_group_num: int,
same_context_len: bool,
q_len: int,
use_alibi_slopes: bool,
use_new_kcache_layout: bool,
):
if use_new_kcache_layout and use_alibi_slopes:
# TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
# the code (alibi kernel) will be refactored later to avoid code duplication, when
# the whole triton flow with new k cache layout has been supported and tested.
# And tests for the alibi kernel using new kcache layout will be added then.
pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.6214159Z
torch.manual_seed(123)
torch.cuda.empty_cache()
>       torch.cuda.synchronize()
2025-04-11T04:23:18.6214523Z
tests/test_infer/test_kernels/triton/test_decoding_attn.py:100:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.6214805Z
device = None
2025-04-11T04:23:18.6214886Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6215225Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6216881Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
______________ test_flash_decoding[True-False-1-True-4-16-8-32-7] ______________
2025-04-11T04:23:18.6217287Z
bsz = 7, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
kv_group_num = 4, same_context_len = True, q_len = 1, use_alibi_slopes = False
use_new_kcache_layout = True
2025-04-11T04:23:18.6217706Z
@pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
@pytest.mark.parametrize("bsz", [7, 16])
@pytest.mark.parametrize("block_size", [16, 32])
@pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
@pytest.mark.parametrize("num_attn_heads", [16])
@pytest.mark.parametrize("kv_group_num", [1, 4])
@pytest.mark.parametrize("same_context_len", [True, False])
@pytest.mark.parametrize("q_len", [1, 5])
@pytest.mark.parametrize("use_alibi_slopes", [True, False])
@pytest.mark.parametrize("use_new_kcache_layout", [True, False])
def test_flash_decoding(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_attn_heads: int,
kv_group_num: int,
same_context_len: bool,
q_len: int,
use_alibi_slopes: bool,
use_new_kcache_layout: bool,
):
if use_new_kcache_layout and use_alibi_slopes:
# TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
# the code (alibi kernel) will be refactored later to avoid code duplication, when
# the whole triton flow with new k cache layout has been supported and tested.
# And tests for the alibi kernel using new kcache layout will be added then.
pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.6221125Z
torch.manual_seed(123)
torch.cuda.empty_cache()
>       torch.cuda.synchronize()
2025-04-11T04:23:18.6221401Z
tests/test_infer/test_kernels/triton/test_decoding_attn.py:100:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.6221675Z
device = None
2025-04-11T04:23:18.6221757Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6222105Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6223758Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
_____________ test_flash_decoding[True-False-1-True-4-16-8-32-16] ______________
2025-04-11T04:23:18.6224165Z
bsz = 16, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
kv_group_num = 4, same_context_len = True, q_len = 1, use_alibi_slopes = False
use_new_kcache_layout = True
2025-04-11T04:23:18.6224574Z
@pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
@pytest.mark.parametrize("bsz", [7, 16])
@pytest.mark.parametrize("block_size", [16, 32])
@pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
@pytest.mark.parametrize("num_attn_heads", [16])
@pytest.mark.parametrize("kv_group_num", [1, 4])
@pytest.mark.parametrize("same_context_len", [True, False])
@pytest.mark.parametrize("q_len", [1, 5])
@pytest.mark.parametrize("use_alibi_slopes", [True, False])
@pytest.mark.parametrize("use_new_kcache_layout", [True, False])
def test_flash_decoding(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_attn_heads: int,
kv_group_num: int,
same_context_len: bool,
q_len: int,
use_alibi_slopes: bool,
use_new_kcache_layout: bool,
):
if use_new_kcache_layout and use_alibi_slopes:
# TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
# the code (alibi kernel) will be refactored later to avoid code duplication, when
# the whole triton flow with new k cache layout has been supported and tested.
# And tests for the alibi kernel using new kcache layout will be added then.
pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.6228014Z
torch.manual_seed(123)
torch.cuda.empty_cache()
>       torch.cuda.synchronize()
2025-04-11T04:23:18.6228292Z
tests/test_infer/test_kernels/triton/test_decoding_attn.py:100:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.6228710Z
device = None
2025-04-11T04:23:18.6228793Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6229147Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6230760Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
_____________ test_flash_decoding[True-False-1-True-4-16-16-16-7] ______________
2025-04-11T04:23:18.6231173Z
bsz = 7, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
kv_group_num = 4, same_context_len = True, q_len = 1, use_alibi_slopes = False
use_new_kcache_layout = True
2025-04-11T04:23:18.6231590Z
@pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
@pytest.mark.parametrize("bsz", [7, 16])
@pytest.mark.parametrize("block_size", [16, 32])
@pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
@pytest.mark.parametrize("num_attn_heads", [16])
@pytest.mark.parametrize("kv_group_num", [1, 4])
@pytest.mark.parametrize("same_context_len", [True, False])
@pytest.mark.parametrize("q_len", [1, 5])
@pytest.mark.parametrize("use_alibi_slopes", [True, False])
@pytest.mark.parametrize("use_new_kcache_layout", [True, False])
def test_flash_decoding(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_attn_heads: int,
kv_group_num: int,
same_context_len: bool,
q_len: int,
use_alibi_slopes: bool,
use_new_kcache_layout: bool,
):
if use_new_kcache_layout and use_alibi_slopes:
# TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
# the code (alibi kernel) will be refactored later to avoid code duplication, when
# the whole triton flow with new k cache layout has been supported and tested.
# And tests for the alibi kernel using new kcache layout will be added then.
pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.6235083Z
torch.manual_seed(123)
torch.cuda.empty_cache()
>       torch.cuda.synchronize()
2025-04-11T04:23:18.6235355Z
tests/test_infer/test_kernels/triton/test_decoding_attn.py:100:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.6235626Z
device = None
2025-04-11T04:23:18.6235708Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6236055Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6237620Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
_____________ test_flash_decoding[True-False-1-True-4-16-16-16-16] _____________
2025-04-11T04:23:18.6238030Z
bsz = 16, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
kv_group_num = 4, same_context_len = True, q_len = 1, use_alibi_slopes = False
use_new_kcache_layout = True
2025-04-11T04:23:18.6238444Z
@pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
@pytest.mark.parametrize("bsz", [7, 16])
@pytest.mark.parametrize("block_size", [16, 32])
@pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
@pytest.mark.parametrize("num_attn_heads", [16])
@pytest.mark.parametrize("kv_group_num", [1, 4])
@pytest.mark.parametrize("same_context_len", [True, False])
@pytest.mark.parametrize("q_len", [1, 5])
@pytest.mark.parametrize("use_alibi_slopes", [True, False])
@pytest.mark.parametrize("use_new_kcache_layout", [True, False])
def test_flash_decoding(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_attn_heads: int,
kv_group_num: int,
same_context_len: bool,
q_len: int,
use_alibi_slopes: bool,
use_new_kcache_layout: bool,
):
if use_new_kcache_layout and use_alibi_slopes:
# TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
# the code (alibi kernel) will be refactored later to avoid code duplication, when
# the whole triton flow with new k cache layout has been supported and tested.
# And tests for the alibi kernel using new kcache layout will be added then.
pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.6241934Z
torch.manual_seed(123)
torch.cuda.empty_cache()
>       torch.cuda.synchronize()
2025-04-11T04:23:18.6242223Z
tests/test_infer/test_kernels/triton/test_decoding_attn.py:100:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.6242509Z
device = None
2025-04-11T04:23:18.6242591Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6242943Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6244518Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
_____________ test_flash_decoding[True-False-1-True-4-16-16-32-7] ______________
2025-04-11T04:23:18.6244930Z
bsz = 7, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
kv_group_num = 4, same_context_len = True, q_len = 1, use_alibi_slopes = False
use_new_kcache_layout = True
2025-04-11T04:23:18.6245449Z
@pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
@pytest.mark.parametrize("bsz", [7, 16])
@pytest.mark.parametrize("block_size", [16, 32])
@pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
@pytest.mark.parametrize("num_attn_heads", [16])
@pytest.mark.parametrize("kv_group_num", [1, 4])
@pytest.mark.parametrize("same_context_len", [True, False])
@pytest.mark.parametrize("q_len", [1, 5])
@pytest.mark.parametrize("use_alibi_slopes", [True, False])
@pytest.mark.parametrize("use_new_kcache_layout", [True, False])
def test_flash_decoding(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_attn_heads: int,
kv_group_num: int,
same_context_len: bool,
q_len: int,
use_alibi_slopes: bool,
use_new_kcache_layout: bool,
):
if use_new_kcache_layout and use_alibi_slopes:
# TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
# the code (alibi kernel) will be refactored later to avoid code duplication, when
# the whole triton flow with new k cache layout has been supported and tested.
# And tests for the alibi kernel using new kcache layout will be added then.
pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.6248848Z
torch.manual_seed(123)
torch.cuda.empty_cache()
>       torch.cuda.synchronize()
2025-04-11T04:23:18.6249124Z
tests/test_infer/test_kernels/triton/test_decoding_attn.py:100:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.6249396Z
device = None
2025-04-11T04:23:18.6249477Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6249823Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6251483Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
_____________ test_flash_decoding[True-False-1-True-4-16-16-32-16] _____________
2025-04-11T04:23:18.6251898Z
bsz = 16, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
kv_group_num = 4, same_context_len = True, q_len = 1, use_alibi_slopes = False
use_new_kcache_layout = True
2025-04-11T04:23:18.6252308Z
@pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
@pytest.mark.parametrize("bsz", [7, 16])
@pytest.mark.parametrize("block_size", [16, 32])
@pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
@pytest.mark.parametrize("num_attn_heads", [16])
@pytest.mark.parametrize("kv_group_num", [1, 4])
@pytest.mark.parametrize("same_context_len", [True, False])
@pytest.mark.parametrize("q_len", [1, 5])
@pytest.mark.parametrize("use_alibi_slopes", [True, False])
@pytest.mark.parametrize("use_new_kcache_layout", [True, False])
def test_flash_decoding(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_attn_heads: int,
kv_group_num: int,
same_context_len: bool,
q_len: int,
use_alibi_slopes: bool,
use_new_kcache_layout: bool,
):
if use_new_kcache_layout and use_alibi_slopes:
# TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
# the code (alibi kernel) will be refactored later to avoid code duplication, when
# the whole triton flow with new k cache layout has been supported and tested.
# And tests for the alibi kernel using new kcache layout will be added then.
pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.6255720Z
torch.manual_seed(123)
torch.cuda.empty_cache()
>       torch.cuda.synchronize()
2025-04-11T04:23:18.6255994Z
tests/test_infer/test_kernels/triton/test_decoding_attn.py:100:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.6256270Z
device = None
2025-04-11T04:23:18.6256351Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6256688Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6258364Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
_____________ test_flash_decoding[True-False-1-False-1-16-8-16-7] ______________
2025-04-11T04:23:18.6258776Z
bsz = 7, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
kv_group_num = 1, same_context_len = False, q_len = 1, use_alibi_slopes = False
use_new_kcache_layout = True
2025-04-11T04:23:18.6259273Z
@pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
@pytest.mark.parametrize("bsz", [7, 16])
@pytest.mark.parametrize("block_size", [16, 32])
@pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
@pytest.mark.parametrize("num_attn_heads", [16])
@pytest.mark.parametrize("kv_group_num", [1, 4])
@pytest.mark.parametrize("same_context_len", [True, False])
@pytest.mark.parametrize("q_len", [1, 5])
@pytest.mark.parametrize("use_alibi_slopes", [True, False])
@pytest.mark.parametrize("use_new_kcache_layout", [True, False])
def test_flash_decoding(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_attn_heads: int,
kv_group_num: int,
same_context_len: bool,
q_len: int,
use_alibi_slopes: bool,
use_new_kcache_layout: bool,
):
if use_new_kcache_layout and use_alibi_slopes:
# TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
# the code (alibi kernel) will be refactored later to avoid code duplication, when
# the whole triton flow with new k cache layout has been supported and tested.
# And tests for the alibi kernel using new kcache layout will be added then.
pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.6262620Z
torch.manual_seed(123)
torch.cuda.empty_cache()
>       torch.cuda.synchronize()
2025-04-11T04:23:18.6262893Z
tests/test_infer/test_kernels/triton/test_decoding_attn.py:100:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.6263166Z
device = None
2025-04-11T04:23:18.6263248Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6263683Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6265339Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
_____________ test_flash_decoding[True-False-1-False-1-16-8-16-16] _____________
2025-04-11T04:23:18.6265760Z
bsz = 16, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
kv_group_num = 1, same_context_len = False, q_len = 1, use_alibi_slopes = False
use_new_kcache_layout = True
2025-04-11T04:23:18.6266172Z
@pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
@pytest.mark.parametrize("bsz", [7, 16])
@pytest.mark.parametrize("block_size", [16, 32])
@pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
@pytest.mark.parametrize("num_attn_heads", [16])
@pytest.mark.parametrize("kv_group_num", [1, 4])
@pytest.mark.parametrize("same_context_len", [True, False])
@pytest.mark.parametrize("q_len", [1, 5])
@pytest.mark.parametrize("use_alibi_slopes", [True, False])
@pytest.mark.parametrize("use_new_kcache_layout", [True, False])
def test_flash_decoding(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_attn_heads: int,
kv_group_num: int,
same_context_len: bool,
q_len: int,
use_alibi_slopes: bool,
use_new_kcache_layout: bool,
):
if use_new_kcache_layout and use_alibi_slopes:
# TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
# the code (alibi kernel) will be refactored later to avoid code duplication, when
# the whole triton flow with new k cache layout has been supported and tested.
# And tests for the alibi kernel using new kcache layout will be added then.
pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.6269543Z
torch.manual_seed(123)
torch.cuda.empty_cache()
>       torch.cuda.synchronize()
2025-04-11T04:23:18.6269918Z
tests/test_infer/test_kernels/triton/test_decoding_attn.py:100:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.6270197Z
device = None
2025-04-11T04:23:18.6270282Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6270628Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6272319Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
_____________ test_flash_decoding[True-False-1-False-1-16-8-32-7] ______________
2025-04-11T04:23:18.6272734Z
bsz = 7, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
kv_group_num = 1, same_context_len = False, q_len = 1, use_alibi_slopes = False
use_new_kcache_layout = True
2025-04-11T04:23:18.6273153Z
@pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
@pytest.mark.parametrize("bsz", [7, 16])
@pytest.mark.parametrize("block_size", [16, 32])
@pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
@pytest.mark.parametrize("num_attn_heads", [16])
@pytest.mark.parametrize("kv_group_num", [1, 4])
@pytest.mark.parametrize("same_context_len", [True, False])
@pytest.mark.parametrize("q_len", [1, 5])
@pytest.mark.parametrize("use_alibi_slopes", [True, False])
@pytest.mark.parametrize("use_new_kcache_layout", [True, False])
def test_flash_decoding(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_attn_heads: int,
kv_group_num: int,
same_context_len: bool,
q_len: int,
use_alibi_slopes: bool,
use_new_kcache_layout: bool,
):
if use_new_kcache_layout and use_alibi_slopes:
# TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
# the code (alibi kernel) will be refactored later to avoid code duplication, when
# the whole triton flow with new k cache layout has been supported and tested.
# And tests for the alibi kernel using new kcache layout will be added then.
pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.6276578Z
torch.manual_seed(123)
torch.cuda.empty_cache()
>       torch.cuda.synchronize()
2025-04-11T04:23:18.6276852Z
tests/test_infer/test_kernels/triton/test_decoding_attn.py:100:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.6277123Z
device = None
2025-04-11T04:23:18.6277204Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6277552Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6279200Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
_____________ test_flash_decoding[True-False-1-False-1-16-8-32-16] _____________
2025-04-11T04:23:18.6279612Z
bsz = 16, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
kv_group_num = 1, same_context_len = False, q_len = 1, use_alibi_slopes = False
use_new_kcache_layout = True
2025-04-11T04:23:18.6280022Z
@pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
@pytest.mark.parametrize("bsz", [7, 16])
@pytest.mark.parametrize("block_size", [16, 32])
@pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
@pytest.mark.parametrize("num_attn_heads", [16])
@pytest.mark.parametrize("kv_group_num", [1, 4])
@pytest.mark.parametrize("same_context_len", [True, False])
@pytest.mark.parametrize("q_len", [1, 5])
@pytest.mark.parametrize("use_alibi_slopes", [True, False])
@pytest.mark.parametrize("use_new_kcache_layout", [True, False])
def test_flash_decoding(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_attn_heads: int,
kv_group_num: int,
same_context_len: bool,
q_len: int,
use_alibi_slopes: bool,
use_new_kcache_layout: bool,
):
if use_new_kcache_layout and use_alibi_slopes:
# TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
# the code (alibi kernel) will be refactored later to avoid code duplication, when
# the whole triton flow with new k cache layout has been supported and tested.
# And tests for the alibi kernel using new kcache layout will be added then.
pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.6283461Z
torch.manual_seed(123)
torch.cuda.empty_cache()
>       torch.cuda.synchronize()
2025-04-11T04:23:18.6283737Z
tests/test_infer/test_kernels/triton/test_decoding_attn.py:100:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.6284095Z
device = None
2025-04-11T04:23:18.6284179Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6284523Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6286082Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
_____________ test_flash_decoding[True-False-1-False-1-16-16-16-7] _____________
2025-04-11T04:23:18.6286493Z
bsz = 7, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
kv_group_num = 1, same_context_len = False, q_len = 1, use_alibi_slopes = False
use_new_kcache_layout = True
2025-04-11T04:23:18.6286910Z
@pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
@pytest.mark.parametrize("bsz", [7, 16])
@pytest.mark.parametrize("block_size", [16, 32])
@pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
@pytest.mark.parametrize("num_attn_heads", [16])
@pytest.mark.parametrize("kv_group_num", [1, 4])
@pytest.mark.parametrize("same_context_len", [True, False])
@pytest.mark.parametrize("q_len", [1, 5])
@pytest.mark.parametrize("use_alibi_slopes", [True, False])
@pytest.mark.parametrize("use_new_kcache_layout", [True, False])
def test_flash_decoding(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_attn_heads: int,
kv_group_num: int,
same_context_len: bool,
q_len: int,
use_alibi_slopes: bool,
use_new_kcache_layout: bool,
):
if use_new_kcache_layout and use_alibi_slopes:
# TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
# the code (alibi kernel) will be refactored later to avoid code duplication, when
# the whole triton flow with new k cache layout has been supported and tested.
# And tests for the alibi kernel using new kcache layout will be added then.
pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.6290392Z
torch.manual_seed(123)
torch.cuda.empty_cache()
>       torch.cuda.synchronize()
2025-04-11T04:23:18.6290679Z
tests/test_infer/test_kernels/triton/test_decoding_attn.py:100:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.6290958Z
device = None
2025-04-11T04:23:18.6291044Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6291394Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6292953Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
____________ test_flash_decoding[True-False-1-False-1-16-16-16-16] _____________
2025-04-11T04:23:18.6293371Z
bsz = 16, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
kv_group_num = 1, same_context_len = False, q_len = 1, use_alibi_slopes = False
use_new_kcache_layout = True
2025-04-11T04:23:18.6293779Z
@pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
@pytest.mark.parametrize("bsz", [7, 16])
@pytest.mark.parametrize("block_size", [16, 32])
@pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
@pytest.mark.parametrize("num_attn_heads", [16])
@pytest.mark.parametrize("kv_group_num", [1, 4])
@pytest.mark.parametrize("same_context_len", [True, False])
@pytest.mark.parametrize("q_len", [1, 5])
@pytest.mark.parametrize("use_alibi_slopes", [True, False])
@pytest.mark.parametrize("use_new_kcache_layout", [True, False])
def test_flash_decoding(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_attn_heads: int,
kv_group_num: int,
same_context_len: bool,
q_len: int,
use_alibi_slopes: bool,
use_new_kcache_layout: bool,
):
if use_new_kcache_layout and use_alibi_slopes:
# TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
# the code (alibi kernel) will be refactored later to avoid code duplication, when
# the whole triton flow with new k cache layout has been supported and tested.
# And tests for the alibi kernel using new kcache layout will be added then.
pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.6297280Z
torch.manual_seed(123)
torch.cuda.empty_cache()
>       torch.cuda.synchronize()
2025-04-11T04:23:18.6297559Z
tests/test_infer/test_kernels/triton/test_decoding_attn.py:100:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.6297836Z
device = None
2025-04-11T04:23:18.6297921Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6298261Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6299831Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
_____________ test_flash_decoding[True-False-1-False-1-16-16-32-7] _____________
2025-04-11T04:23:18.6300247Z
bsz = 7, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
kv_group_num = 1, same_context_len = False, q_len = 1, use_alibi_slopes = False
use_new_kcache_layout = True
2025-04-11T04:23:18.6300843Z
@pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
@pytest.mark.parametrize("bsz", [7, 16])
@pytest.mark.parametrize("block_size", [16, 32])
@pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
@pytest.mark.parametrize("num_attn_heads", [16])
@pytest.mark.parametrize("kv_group_num", [1, 4])
@pytest.mark.parametrize("same_context_len", [True, False])
@pytest.mark.parametrize("q_len", [1, 5])
@pytest.mark.parametrize("use_alibi_slopes", [True, False])
@pytest.mark.parametrize("use_new_kcache_layout", [True, False])
def test_flash_decoding(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_attn_heads: int,
kv_group_num: int,
same_context_len: bool,
q_len: int,
use_alibi_slopes: bool,
use_new_kcache_layout: bool,
):
if use_new_kcache_layout and use_alibi_slopes:
# TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
# the code (alibi kernel) will be refactored later to avoid code duplication, when
# the whole triton flow with new k cache layout has been supported and tested.
# And tests for the alibi kernel using new kcache layout will be added then.
pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.6304268Z
torch.manual_seed(123)
torch.cuda.empty_cache()
>       torch.cuda.synchronize()
2025-04-11T04:23:18.6304536Z
tests/test_infer/test_kernels/triton/test_decoding_attn.py:100:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.6304806Z
device = None
2025-04-11T04:23:18.6304893Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6305257Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6306916Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
____________ test_flash_decoding[True-False-1-False-1-16-16-32-16] _____________
2025-04-11T04:23:18.6307344Z
bsz = 16, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
kv_group_num = 1, same_context_len = False, q_len = 1, use_alibi_slopes = False
use_new_kcache_layout = True
2025-04-11T04:23:18.6307756Z
@pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
@pytest.mark.parametrize("bsz", [7, 16])
@pytest.mark.parametrize("block_size", [16, 32])
@pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
@pytest.mark.parametrize("num_attn_heads", [16])
@pytest.mark.parametrize("kv_group_num", [1, 4])
@pytest.mark.parametrize("same_context_len", [True, False])
@pytest.mark.parametrize("q_len", [1, 5])
@pytest.mark.parametrize("use_alibi_slopes", [True, False])
@pytest.mark.parametrize("use_new_kcache_layout", [True, False])
def test_flash_decoding(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_attn_heads: int,
kv_group_num: int,
same_context_len: bool,
q_len: int,
use_alibi_slopes: bool,
use_new_kcache_layout: bool,
):
if use_new_kcache_layout and use_alibi_slopes:
# TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
# the code (alibi kernel) will be refactored later to avoid code duplication, when
# the whole triton flow with new k cache layout has been supported and tested.
# And tests for the alibi kernel using new kcache layout will be added then.
pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.6312212Z
torch.manual_seed(123)
torch.cuda.empty_cache()
>       torch.cuda.synchronize()
2025-04-11T04:23:18.6312693Z
tests/test_infer/test_kernels/triton/test_decoding_attn.py:100:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.6313081Z
device = None
2025-04-11T04:23:18.6313240Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6313700Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6315907Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
_____________ test_flash_decoding[True-False-1-False-4-16-8-16-7] ______________
2025-04-11T04:23:18.6316512Z
bsz = 7, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
kv_group_num = 4, same_context_len = False, q_len = 1, use_alibi_slopes = False
use_new_kcache_layout = True
2025-04-11T04:23:18.6317135Z
@pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
@pytest.mark.parametrize("bsz", [7, 16])
@pytest.mark.parametrize("block_size", [16, 32])
@pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
@pytest.mark.parametrize("num_attn_heads", [16])
@pytest.mark.parametrize("kv_group_num", [1, 4])
@pytest.mark.parametrize("same_context_len", [True, False])
@pytest.mark.parametrize("q_len", [1, 5])
@pytest.mark.parametrize("use_alibi_slopes", [True, False])
@pytest.mark.parametrize("use_new_kcache_layout", [True, False])
def test_flash_decoding(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_attn_heads: int,
kv_group_num: int,
same_context_len: bool,
q_len: int,
use_alibi_slopes: bool,
use_new_kcache_layout: bool,
):
if use_new_kcache_layout and use_alibi_slopes:
# TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
# the code (alibi kernel) will be refactored later to avoid code duplication, when
# the whole triton flow with new k cache layout has been supported and tested.
# And tests for the alibi kernel using new kcache layout will be added then.
pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.6321833Z
torch.manual_seed(123)
torch.cuda.empty_cache()
>       torch.cuda.synchronize()
2025-04-11T04:23:18.6322284Z
tests/test_infer/test_kernels/triton/test_decoding_attn.py:100:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.6322604Z
device = None
2025-04-11T04:23:18.6322762Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6323325Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6325509Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
_____________ test_flash_decoding[True-False-1-False-4-16-8-16-16] _____________
2025-04-11T04:23:18.6326110Z
bsz = 16, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
kv_group_num = 4, same_context_len = False, q_len = 1, use_alibi_slopes = False
use_new_kcache_layout = True
2025-04-11T04:23:18.6326662Z
@pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
@pytest.mark.parametrize("bsz", [7, 16])
@pytest.mark.parametrize("block_size", [16, 32])
@pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
@pytest.mark.parametrize("num_attn_heads", [16])
@pytest.mark.parametrize("kv_group_num", [1, 4])
@pytest.mark.parametrize("same_context_len", [True, False])
@pytest.mark.parametrize("q_len", [1, 5])
@pytest.mark.parametrize("use_alibi_slopes", [True, False])
@pytest.mark.parametrize("use_new_kcache_layout", [True, False])
def test_flash_decoding(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_attn_heads: int,
kv_group_num: int,
same_context_len: bool,
q_len: int,
use_alibi_slopes: bool,
use_new_kcache_layout: bool,
):
if use_new_kcache_layout and use_alibi_slopes:
# TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
# the code (alibi kernel) will be refactored later to avoid code duplication, when
# the whole triton flow with new k cache layout has been supported and tested.
# And tests for the alibi kernel using new kcache layout will be added then.
pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.6331329Z
torch.manual_seed(123)
torch.cuda.empty_cache()
>       torch.cuda.synchronize()
2025-04-11T04:23:18.6331905Z
tests/test_infer/test_kernels/triton/test_decoding_attn.py:100:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.6332278Z
device = None
2025-04-11T04:23:18.6332393Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6332878Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6363506Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
_____________ test_flash_decoding[True-False-1-False-4-16-8-32-7] ______________
2025-04-11T04:23:18.6363961Z
bsz = 7, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
kv_group_num = 4, same_context_len = False, q_len = 1, use_alibi_slopes = False
use_new_kcache_layout = True
2025-04-11T04:23:18.6364417Z
@pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
@pytest.mark.parametrize("bsz", [7, 16])
@pytest.mark.parametrize("block_size", [16, 32])
@pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
@pytest.mark.parametrize("num_attn_heads", [16])
@pytest.mark.parametrize("kv_group_num", [1, 4])
@pytest.mark.parametrize("same_context_len", [True, False])
@pytest.mark.parametrize("q_len", [1, 5])
@pytest.mark.parametrize("use_alibi_slopes", [True, False])
@pytest.mark.parametrize("use_new_kcache_layout", [True, False])
def test_flash_decoding(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_attn_heads: int,
kv_group_num: int,
same_context_len: bool,
q_len: int,
use_alibi_slopes: bool,
use_new_kcache_layout: bool,
):
if use_new_kcache_layout and use_alibi_slopes:
# TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
# the code (alibi kernel) will be refactored later to avoid code duplication, when
# the whole triton flow with new k cache layout has been supported and tested.
# And tests for the alibi kernel using new kcache layout will be added then.
pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.6368087Z
torch.manual_seed(123)
torch.cuda.empty_cache()
>       torch.cuda.synchronize()
2025-04-11T04:23:18.6368389Z
tests/test_infer/test_kernels/triton/test_decoding_attn.py:100:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.6368686Z
device = None
2025-04-11T04:23:18.6368773Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6369143Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6370864Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
_____________ test_flash_decoding[True-False-1-False-4-16-8-32-16] _____________
2025-04-11T04:23:18.6371307Z
bsz = 16, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
kv_group_num = 4, same_context_len = False, q_len = 1, use_alibi_slopes = False
use_new_kcache_layout = True
2025-04-11T04:23:18.6371734Z
@pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
@pytest.mark.parametrize("bsz", [7, 16])
@pytest.mark.parametrize("block_size", [16, 32])
@pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
@pytest.mark.parametrize("num_attn_heads", [16])
@pytest.mark.parametrize("kv_group_num", [1, 4])
@pytest.mark.parametrize("same_context_len", [True, False])
@pytest.mark.parametrize("q_len", [1, 5])
@pytest.mark.parametrize("use_alibi_slopes", [True, False])
@pytest.mark.parametrize("use_new_kcache_layout", [True, False])
def test_flash_decoding(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_attn_heads: int,
kv_group_num: int,
same_context_len: bool,
q_len: int,
use_alibi_slopes: bool,
use_new_kcache_layout: bool,
):
if use_new_kcache_layout and use_alibi_slopes:
# TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
# the code (alibi kernel) will be refactored later to avoid code duplication, when
# the whole triton flow with new k cache layout has been supported and tested.
# And tests for the alibi kernel using new kcache layout will be added then.
pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.6375265Z
torch.manual_seed(123)
torch.cuda.empty_cache()
>       torch.cuda.synchronize()
2025-04-11T04:23:18.6396154Z
tests/test_infer/test_kernels/triton/test_decoding_attn.py:100:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.6396550Z
device = None
2025-04-11T04:23:18.6396637Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6397018Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6398665Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
_____________ test_flash_decoding[True-False-1-False-4-16-16-16-7] _____________
2025-04-11T04:23:18.6399093Z
bsz = 7, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
kv_group_num = 4, same_context_len = False, q_len = 1, use_alibi_slopes = False
use_new_kcache_layout = True
2025-04-11T04:23:18.6399532Z
@pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
@pytest.mark.parametrize("bsz", [7, 16])
@pytest.mark.parametrize("block_size", [16, 32])
@pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
@pytest.mark.parametrize("num_attn_heads", [16])
@pytest.mark.parametrize("kv_group_num", [1, 4])
@pytest.mark.parametrize("same_context_len", [True, False])
@pytest.mark.parametrize("q_len", [1, 5])
@pytest.mark.parametrize("use_alibi_slopes", [True, False])
@pytest.mark.parametrize("use_new_kcache_layout", [True, False])
def test_flash_decoding(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_attn_heads: int,
kv_group_num: int,
same_context_len: bool,
q_len: int,
use_alibi_slopes: bool,
use_new_kcache_layout: bool,
):
if use_new_kcache_layout and use_alibi_slopes:
# TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
# the code (alibi kernel) will be refactored later to avoid code duplication, when
# the whole triton flow with new k cache layout has been supported and tested.
# And tests for the alibi kernel using new kcache layout will be added then.
pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.6403115Z
torch.manual_seed(123)
torch.cuda.empty_cache()
>       torch.cuda.synchronize()
2025-04-11T04:23:18.6403393Z
tests/test_infer/test_kernels/triton/test_decoding_attn.py:100:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.6403668Z
device = None
2025-04-11T04:23:18.6403749Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6404097Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6405668Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
____________ test_flash_decoding[True-False-1-False-4-16-16-16-16] _____________
2025-04-11T04:23:18.6406093Z
bsz = 16, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
kv_group_num = 4, same_context_len = False, q_len = 1, use_alibi_slopes = False
use_new_kcache_layout = True
2025-04-11T04:23:18.6406512Z
@pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
@pytest.mark.parametrize("bsz", [7, 16])
@pytest.mark.parametrize("block_size", [16, 32])
@pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
@pytest.mark.parametrize("num_attn_heads", [16])
@pytest.mark.parametrize("kv_group_num", [1, 4])
@pytest.mark.parametrize("same_context_len", [True, False])
@pytest.mark.parametrize("q_len", [1, 5])
@pytest.mark.parametrize("use_alibi_slopes", [True, False])
@pytest.mark.parametrize("use_new_kcache_layout", [True, False])
def test_flash_decoding(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_attn_heads: int,
kv_group_num: int,
same_context_len: bool,
q_len: int,
use_alibi_slopes: bool,
use_new_kcache_layout: bool,
):
if use_new_kcache_layout and use_alibi_slopes:
# TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
# the code (alibi kernel) will be refactored later to avoid code duplication, when
# the whole triton flow with new k cache layout has been supported and tested.
# And tests for the alibi kernel using new kcache layout will be added then.
pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.6430605Z
torch.manual_seed(123)
torch.cuda.empty_cache()
>       torch.cuda.synchronize()
2025-04-11T04:23:18.6430882Z
tests/test_infer/test_kernels/triton/test_decoding_attn.py:100:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.6431158Z
device = None
2025-04-11T04:23:18.6431240Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6431576Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6433123Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
_____________ test_flash_decoding[True-False-1-False-4-16-16-32-7] _____________
2025-04-11T04:23:18.6433533Z
bsz = 7, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
kv_group_num = 4, same_context_len = False, q_len = 1, use_alibi_slopes = False
use_new_kcache_layout = True
2025-04-11T04:23:18.6434046Z
@pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
@pytest.mark.parametrize("bsz", [7, 16])
@pytest.mark.parametrize("block_size", [16, 32])
@pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
@pytest.mark.parametrize("num_attn_heads", [16])
@pytest.mark.parametrize("kv_group_num", [1, 4])
@pytest.mark.parametrize("same_context_len", [True, False])
@pytest.mark.parametrize("q_len", [1, 5])
@pytest.mark.parametrize("use_alibi_slopes", [True, False])
@pytest.mark.parametrize("use_new_kcache_layout", [True, False])
def test_flash_decoding(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_attn_heads: int,
kv_group_num: int,
same_context_len: bool,
q_len: int,
use_alibi_slopes: bool,
use_new_kcache_layout: bool,
):
if use_new_kcache_layout and use_alibi_slopes:
# TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
# the code (alibi kernel) will be refactored later to avoid code duplication, when
# the whole triton flow with new k cache layout has been supported and tested.
# And tests for the alibi kernel using new kcache layout will be added then.
pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.6437452Z
torch.manual_seed(123)
torch.cuda.empty_cache()
>       torch.cuda.synchronize()
2025-04-11T04:23:18.6437735Z
tests/test_infer/test_kernels/triton/test_decoding_attn.py:100:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.6438004Z
device = None
2025-04-11T04:23:18.6438088Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6438431Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6440078Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
____________ test_flash_decoding[True-False-1-False-4-16-16-32-16] _____________
2025-04-11T04:23:18.6440499Z
bsz = 16, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
kv_group_num = 4, same_context_len = False, q_len = 1, use_alibi_slopes = False
use_new_kcache_layout = True
2025-04-11T04:23:18.6440910Z
@pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
@pytest.mark.parametrize("bsz", [7, 16])
@pytest.mark.parametrize("block_size", [16, 32])
@pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
@pytest.mark.parametrize("num_attn_heads", [16])
@pytest.mark.parametrize("kv_group_num", [1, 4])
@pytest.mark.parametrize("same_context_len", [True, False])
@pytest.mark.parametrize("q_len", [1, 5])
@pytest.mark.parametrize("use_alibi_slopes", [True, False])
@pytest.mark.parametrize("use_new_kcache_layout", [True, False])
def test_flash_decoding(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_attn_heads: int,
kv_group_num: int,
same_context_len: bool,
q_len: int,
use_alibi_slopes: bool,
use_new_kcache_layout: bool,
):
if use_new_kcache_layout and use_alibi_slopes:
# TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
# the code (alibi kernel) will be refactored later to avoid code duplication, when
# the whole triton flow with new k cache layout has been supported and tested.
# And tests for the alibi kernel using new kcache layout will be added then.
pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.6444312Z
torch.manual_seed(123)
torch.cuda.empty_cache()
>       torch.cuda.synchronize()
2025-04-11T04:23:18.6444591Z
tests/test_infer/test_kernels/triton/test_decoding_attn.py:100:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.6444865Z
device = None
2025-04-11T04:23:18.6444952Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6445295Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6446938Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
______________ test_flash_decoding[True-False-5-True-1-16-8-16-7] ______________
2025-04-11T04:23:18.6447357Z
bsz = 7, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
kv_group_num = 1, same_context_len = True, q_len = 5, use_alibi_slopes = False
use_new_kcache_layout = True
2025-04-11T04:23:18.6447865Z
@pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
@pytest.mark.parametrize("bsz", [7, 16])
@pytest.mark.parametrize("block_size", [16, 32])
@pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
@pytest.mark.parametrize("num_attn_heads", [16])
@pytest.mark.parametrize("kv_group_num", [1, 4])
@pytest.mark.parametrize("same_context_len", [True, False])
@pytest.mark.parametrize("q_len", [1, 5])
@pytest.mark.parametrize("use_alibi_slopes", [True, False])
@pytest.mark.parametrize("use_new_kcache_layout", [True, False])
def test_flash_decoding(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_attn_heads: int,
kv_group_num: int,
same_context_len: bool,
q_len: int,
use_alibi_slopes: bool,
use_new_kcache_layout: bool,
):
if use_new_kcache_layout and use_alibi_slopes:
# TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
# the code (alibi kernel) will be refactored later to avoid code duplication, when
# the whole triton flow with new k cache layout has been supported and tested.
# And tests for the alibi kernel using new kcache layout will be added then.
pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.6451168Z
torch.manual_seed(123)
torch.cuda.empty_cache()
>       torch.cuda.synchronize()
2025-04-11T04:23:18.6451438Z
tests/test_infer/test_kernels/triton/test_decoding_attn.py:100:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.6451710Z
device = None
2025-04-11T04:23:18.6451791Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6452286Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6453841Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
_____________ test_flash_decoding[True-False-5-True-1-16-8-16-16] ______________
2025-04-11T04:23:18.6454345Z
bsz = 16, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
kv_group_num = 1, same_context_len = True, q_len = 5, use_alibi_slopes = False
use_new_kcache_layout = True
2025-04-11T04:23:18.6454754Z
@pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
@pytest.mark.parametrize("bsz", [7, 16])
@pytest.mark.parametrize("block_size", [16, 32])
@pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
@pytest.mark.parametrize("num_attn_heads", [16])
@pytest.mark.parametrize("kv_group_num", [1, 4])
@pytest.mark.parametrize("same_context_len", [True, False])
@pytest.mark.parametrize("q_len", [1, 5])
@pytest.mark.parametrize("use_alibi_slopes", [True, False])
@pytest.mark.parametrize("use_new_kcache_layout", [True, False])
def test_flash_decoding(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_attn_heads: int,
kv_group_num: int,
same_context_len: bool,
q_len: int,
use_alibi_slopes: bool,
use_new_kcache_layout: bool,
):
if use_new_kcache_layout and use_alibi_slopes:
# TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
# the code (alibi kernel) will be refactored later to avoid code duplication, when
# the whole triton flow with new k cache layout has been supported and tested.
# And tests for the alibi kernel using new kcache layout will be added then.
pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.6458037Z
torch.manual_seed(123)
torch.cuda.empty_cache()
>       torch.cuda.synchronize()
2025-04-11T04:23:18.6458405Z
tests/test_infer/test_kernels/triton/test_decoding_attn.py:100:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.6458680Z
device = None
2025-04-11T04:23:18.6458763Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6459103Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6460730Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
______________ test_flash_decoding[True-False-5-True-1-16-8-32-7] ______________
2025-04-11T04:23:18.6461139Z
bsz = 7, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
kv_group_num = 1, same_context_len = True, q_len = 5, use_alibi_slopes = False
use_new_kcache_layout = True
2025-04-11T04:23:18.6461552Z
@pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
@pytest.mark.parametrize("bsz", [7, 16])
@pytest.mark.parametrize("block_size", [16, 32])
@pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
@pytest.mark.parametrize("num_attn_heads", [16])
@pytest.mark.parametrize("kv_group_num", [1, 4])
@pytest.mark.parametrize("same_context_len", [True, False])
@pytest.mark.parametrize("q_len", [1, 5])
@pytest.mark.parametrize("use_alibi_slopes", [True, False])
@pytest.mark.parametrize("use_new_kcache_layout", [True, False])
def test_flash_decoding(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_attn_heads: int,
kv_group_num: int,
same_context_len: bool,
q_len: int,
use_alibi_slopes: bool,
use_new_kcache_layout: bool,
):
if use_new_kcache_layout and use_alibi_slopes:
# TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
# the code (alibi kernel) will be refactored later to avoid code duplication, when
# the whole triton flow with new k cache layout has been supported and tested.
# And tests for the alibi kernel using new kcache layout will be added then.
pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.6464918Z
torch.manual_seed(123)
torch.cuda.empty_cache()
>       torch.cuda.synchronize()
2025-04-11T04:23:18.6465192Z
tests/test_infer/test_kernels/triton/test_decoding_attn.py:100:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.6465460Z
device = None
2025-04-11T04:23:18.6465541Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6465886Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6467528Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
_____________ test_flash_decoding[True-False-5-True-1-16-8-32-16] ______________
2025-04-11T04:23:18.6467943Z
bsz = 16, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
kv_group_num = 1, same_context_len = True, q_len = 5, use_alibi_slopes = False
use_new_kcache_layout = True
2025-04-11T04:23:18.6468351Z
@pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
@pytest.mark.parametrize("bsz", [7, 16])
@pytest.mark.parametrize("block_size", [16, 32])
@pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
@pytest.mark.parametrize("num_attn_heads", [16])
@pytest.mark.parametrize("kv_group_num", [1, 4])
@pytest.mark.parametrize("same_context_len", [True, False])
@pytest.mark.parametrize("q_len", [1, 5])
@pytest.mark.parametrize("use_alibi_slopes", [True, False])
@pytest.mark.parametrize("use_new_kcache_layout", [True, False])
def test_flash_decoding(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_attn_heads: int,
kv_group_num: int,
same_context_len: bool,
q_len: int,
use_alibi_slopes: bool,
use_new_kcache_layout: bool,
):
if use_new_kcache_layout and use_alibi_slopes:
# TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
# the code (alibi kernel) will be refactored later to avoid code duplication, when
# the whole triton flow with new k cache layout has been supported and tested.
# And tests for the alibi kernel using new kcache layout will be added then.
pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.6471810Z
torch.manual_seed(123)
torch.cuda.empty_cache()
>       torch.cuda.synchronize()
2025-04-11T04:23:18.6472086Z
tests/test_infer/test_kernels/triton/test_decoding_attn.py:100:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.6472468Z
device = None
2025-04-11T04:23:18.6472549Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6472890Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6474435Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
_____________ test_flash_decoding[True-False-5-True-1-16-16-16-7] ______________
2025-04-11T04:23:18.6474844Z
bsz = 7, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
kv_group_num = 1, same_context_len = True, q_len = 5, use_alibi_slopes = False
use_new_kcache_layout = True
2025-04-11T04:23:18.6475257Z
@pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
@pytest.mark.parametrize("bsz", [7, 16])
@pytest.mark.parametrize("block_size", [16, 32])
@pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
@pytest.mark.parametrize("num_attn_heads", [16])
@pytest.mark.parametrize("kv_group_num", [1, 4])
@pytest.mark.parametrize("same_context_len", [True, False])
@pytest.mark.parametrize("q_len", [1, 5])
@pytest.mark.parametrize("use_alibi_slopes", [True, False])
@pytest.mark.parametrize("use_new_kcache_layout", [True, False])
def test_flash_decoding(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_attn_heads: int,
kv_group_num: int,
same_context_len: bool,
q_len: int,
use_alibi_slopes: bool,
use_new_kcache_layout: bool,
):
if use_new_kcache_layout and use_alibi_slopes:
# TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
# the code (alibi kernel) will be refactored later to avoid code duplication, when
# the whole triton flow with new k cache layout has been supported and tested.
# And tests for the alibi kernel using new kcache layout will be added then.
pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.6478718Z
torch.manual_seed(123)
torch.cuda.empty_cache()
>       torch.cuda.synchronize()
2025-04-11T04:23:18.6478996Z
tests/test_infer/test_kernels/triton/test_decoding_attn.py:100:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.6479267Z
device = None
2025-04-11T04:23:18.6479349Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6479692Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6481264Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
_____________ test_flash_decoding[True-False-5-True-1-16-16-16-16] _____________
2025-04-11T04:23:18.6481681Z
bsz = 16, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
kv_group_num = 1, same_context_len = True, q_len = 5, use_alibi_slopes = False
use_new_kcache_layout = True
2025-04-11T04:23:18.6482093Z
@pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
@pytest.mark.parametrize("bsz", [7, 16])
@pytest.mark.parametrize("block_size", [16, 32])
@pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
@pytest.mark.parametrize("num_attn_heads", [16])
@pytest.mark.parametrize("kv_group_num", [1, 4])
@pytest.mark.parametrize("same_context_len", [True, False])
@pytest.mark.parametrize("q_len", [1, 5])
@pytest.mark.parametrize("use_alibi_slopes", [True, False])
@pytest.mark.parametrize("use_new_kcache_layout", [True, False])
def test_flash_decoding(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_attn_heads: int,
kv_group_num: int,
same_context_len: bool,
q_len: int,
use_alibi_slopes: bool,
use_new_kcache_layout: bool,
):
if use_new_kcache_layout and use_alibi_slopes:
# TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
# the code (alibi kernel) will be refactored later to avoid code duplication, when
# the whole triton flow with new k cache layout has been supported and tested.
# And tests for the alibi kernel using new kcache layout will be added then.
pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.6485565Z
torch.manual_seed(123)
torch.cuda.empty_cache()
>       torch.cuda.synchronize()
2025-04-11T04:23:18.6485842Z
tests/test_infer/test_kernels/triton/test_decoding_attn.py:100:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.6486118Z
device = None
2025-04-11T04:23:18.6486202Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6486539Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6488102Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
_____________ test_flash_decoding[True-False-5-True-1-16-16-32-7] ______________
2025-04-11T04:23:18.6488512Z
bsz = 7, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
kv_group_num = 1, same_context_len = True, q_len = 5, use_alibi_slopes = False
use_new_kcache_layout = True
2025-04-11T04:23:18.6489008Z
@pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
@pytest.mark.parametrize("bsz", [7, 16])
@pytest.mark.parametrize("block_size", [16, 32])
@pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
@pytest.mark.parametrize("num_attn_heads", [16])
@pytest.mark.parametrize("kv_group_num", [1, 4])
@pytest.mark.parametrize("same_context_len", [True, False])
@pytest.mark.parametrize("q_len", [1, 5])
@pytest.mark.parametrize("use_alibi_slopes", [True, False])
@pytest.mark.parametrize("use_new_kcache_layout", [True, False])
def test_flash_decoding(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_attn_heads: int,
kv_group_num: int,
same_context_len: bool,
q_len: int,
use_alibi_slopes: bool,
use_new_kcache_layout: bool,
):
if use_new_kcache_layout and use_alibi_slopes:
# TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
# the code (alibi kernel) will be refactored later to avoid code duplication, when
# the whole triton flow with new k cache layout has been supported and tested.
# And tests for the alibi kernel using new kcache layout will be added then.
pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.6492393Z
torch.manual_seed(123)
torch.cuda.empty_cache()
>       torch.cuda.synchronize()
2025-04-11T04:23:18.6492662Z
tests/test_infer/test_kernels/triton/test_decoding_attn.py:100:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.6492933Z
device = None
2025-04-11T04:23:18.6493014Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6493359Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6495017Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
_____________ test_flash_decoding[True-False-5-True-1-16-16-32-16] _____________
2025-04-11T04:23:18.6495428Z
bsz = 16, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
kv_group_num = 1, same_context_len = True, q_len = 5, use_alibi_slopes = False
use_new_kcache_layout = True
2025-04-11T04:23:18.6495842Z
@pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
@pytest.mark.parametrize("bsz", [7, 16])
@pytest.mark.parametrize("block_size", [16, 32])
@pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
@pytest.mark.parametrize("num_attn_heads", [16])
@pytest.mark.parametrize("kv_group_num", [1, 4])
@pytest.mark.parametrize("same_context_len", [True, False])
@pytest.mark.parametrize("q_len", [1, 5])
@pytest.mark.parametrize("use_alibi_slopes", [True, False])
@pytest.mark.parametrize("use_new_kcache_layout", [True, False])
def test_flash_decoding(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_attn_heads: int,
kv_group_num: int,
same_context_len: bool,
q_len: int,
use_alibi_slopes: bool,
use_new_kcache_layout: bool,
):
if use_new_kcache_layout and use_alibi_slopes:
# TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
# the code (alibi kernel) will be refactored later to avoid code duplication, when
# the whole triton flow with new k cache layout has been supported and tested.
# And tests for the alibi kernel using new kcache layout will be added then.
pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.6499236Z
torch.manual_seed(123)
torch.cuda.empty_cache()
>       torch.cuda.synchronize()
2025-04-11T04:23:18.6499508Z
tests/test_infer/test_kernels/triton/test_decoding_attn.py:100:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.6499786Z
device = None
2025-04-11T04:23:18.6499869Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6500208Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6501875Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
______________ test_flash_decoding[True-False-5-True-4-16-8-16-7] ______________
2025-04-11T04:23:18.6502293Z
bsz = 7, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
kv_group_num = 4, same_context_len = True, q_len = 5, use_alibi_slopes = False
use_new_kcache_layout = True
2025-04-11T04:23:18.6502703Z
@pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
@pytest.mark.parametrize("bsz", [7, 16])
@pytest.mark.parametrize("block_size", [16, 32])
@pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
@pytest.mark.parametrize("num_attn_heads", [16])
@pytest.mark.parametrize("kv_group_num", [1, 4])
@pytest.mark.parametrize("same_context_len", [True, False])
@pytest.mark.parametrize("q_len", [1, 5])
@pytest.mark.parametrize("use_alibi_slopes", [True, False])
@pytest.mark.parametrize("use_new_kcache_layout", [True, False])
def test_flash_decoding(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_attn_heads: int,
kv_group_num: int,
same_context_len: bool,
q_len: int,
use_alibi_slopes: bool,
use_new_kcache_layout: bool,
):
if use_new_kcache_layout and use_alibi_slopes:
# TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
# the code (alibi kernel) will be refactored later to avoid code duplication, when
# the whole triton flow with new k cache layout has been supported and tested.
# And tests for the alibi kernel using new kcache layout will be added then.
pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.6506103Z
torch.manual_seed(123)
torch.cuda.empty_cache()
>       torch.cuda.synchronize()
2025-04-11T04:23:18.6506372Z
tests/test_infer/test_kernels/triton/test_decoding_attn.py:100:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.6506642Z
device = None
2025-04-11T04:23:18.6506721Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6507141Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6508747Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
_____________ test_flash_decoding[True-False-5-True-4-16-8-16-16] ______________
2025-04-11T04:23:18.6509257Z
bsz = 16, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
kv_group_num = 4, same_context_len = True, q_len = 5, use_alibi_slopes = False
use_new_kcache_layout = True
2025-04-11T04:23:18.6509667Z
@pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
@pytest.mark.parametrize("bsz", [7, 16])
@pytest.mark.parametrize("block_size", [16, 32])
@pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
@pytest.mark.parametrize("num_attn_heads", [16])
@pytest.mark.parametrize("kv_group_num", [1, 4])
@pytest.mark.parametrize("same_context_len", [True, False])
@pytest.mark.parametrize("q_len", [1, 5])
@pytest.mark.parametrize("use_alibi_slopes", [True, False])
@pytest.mark.parametrize("use_new_kcache_layout", [True, False])
def test_flash_decoding(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_attn_heads: int,
kv_group_num: int,
same_context_len: bool,
q_len: int,
use_alibi_slopes: bool,
use_new_kcache_layout: bool,
):
if use_new_kcache_layout and use_alibi_slopes:
# TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
# the code (alibi kernel) will be refactored later to avoid code duplication, when
# the whole triton flow with new k cache layout has been supported and tested.
# And tests for the alibi kernel using new kcache layout will be added then.
pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.6512933Z
torch.manual_seed(123)
torch.cuda.empty_cache()
>       torch.cuda.synchronize()
2025-04-11T04:23:18.6513301Z
tests/test_infer/test_kernels/triton/test_decoding_attn.py:100:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.6513581Z
device = None
2025-04-11T04:23:18.6513663Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6514005Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6515636Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
______________ test_flash_decoding[True-False-5-True-4-16-8-32-7] ______________
2025-04-11T04:23:18.6516049Z
bsz = 7, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
kv_group_num = 4, same_context_len = True, q_len = 5, use_alibi_slopes = False
use_new_kcache_layout = True
2025-04-11T04:23:18.6516458Z
@pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
@pytest.mark.parametrize("bsz", [7, 16])
@pytest.mark.parametrize("block_size", [16, 32])
@pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
@pytest.mark.parametrize("num_attn_heads", [16])
@pytest.mark.parametrize("kv_group_num", [1, 4])
@pytest.mark.parametrize("same_context_len", [True, False])
@pytest.mark.parametrize("q_len", [1, 5])
@pytest.mark.parametrize("use_alibi_slopes", [True, False])
@pytest.mark.parametrize("use_new_kcache_layout", [True, False])
def test_flash_decoding(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_attn_heads: int,
kv_group_num: int,
same_context_len: bool,
q_len: int,
use_alibi_slopes: bool,
use_new_kcache_layout: bool,
):
if use_new_kcache_layout and use_alibi_slopes:
# TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
# the code (alibi kernel) will be refactored later to avoid code duplication, when
# the whole triton flow with new k cache layout has been supported and tested.
# And tests for the alibi kernel using new kcache layout will be added then.
pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.6519847Z
torch.manual_seed(123)
torch.cuda.empty_cache()
>       torch.cuda.synchronize()
2025-04-11T04:23:18.6520121Z
tests/test_infer/test_kernels/triton/test_decoding_attn.py:100:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.6520392Z
device = None
2025-04-11T04:23:18.6520473Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6520821Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6522461Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
_____________ test_flash_decoding[True-False-5-True-4-16-8-32-16] ______________
2025-04-11T04:23:18.6522877Z
bsz = 16, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
kv_group_num = 4, same_context_len = True, q_len = 5, use_alibi_slopes = False
use_new_kcache_layout = True
2025-04-11T04:23:18.6523282Z
@pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
@pytest.mark.parametrize("bsz", [7, 16])
@pytest.mark.parametrize("block_size", [16, 32])
@pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
@pytest.mark.parametrize("num_attn_heads", [16])
@pytest.mark.parametrize("kv_group_num", [1, 4])
@pytest.mark.parametrize("same_context_len", [True, False])
@pytest.mark.parametrize("q_len", [1, 5])
@pytest.mark.parametrize("use_alibi_slopes", [True, False])
@pytest.mark.parametrize("use_new_kcache_layout", [True, False])
def test_flash_decoding(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_attn_heads: int,
kv_group_num: int,
same_context_len: bool,
q_len: int,
use_alibi_slopes: bool,
use_new_kcache_layout: bool,
):
if use_new_kcache_layout and use_alibi_slopes:
# TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
# the code (alibi kernel) will be refactored later to avoid code duplication, when
# the whole triton flow with new k cache layout has been supported and tested.
# And tests for the alibi kernel using new kcache layout will be added then.
pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.6526658Z
torch.manual_seed(123)
torch.cuda.empty_cache()
>       torch.cuda.synchronize()
2025-04-11T04:23:18.6526930Z
tests/test_infer/test_kernels/triton/test_decoding_attn.py:100:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.6527287Z
device = None
2025-04-11T04:23:18.6527371Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6527713Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6529259Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
_____________ test_flash_decoding[True-False-5-True-4-16-16-16-7] ______________
2025-04-11T04:23:18.6529666Z
bsz = 7, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
kv_group_num = 4, same_context_len = True, q_len = 5, use_alibi_slopes = False
use_new_kcache_layout = True
2025-04-11T04:23:18.6530072Z
@pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
@pytest.mark.parametrize("bsz", [7, 16])
@pytest.mark.parametrize("block_size", [16, 32])
@pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
@pytest.mark.parametrize("num_attn_heads", [16])
@pytest.mark.parametrize("kv_group_num", [1, 4])
@pytest.mark.parametrize("same_context_len", [True, False])
@pytest.mark.parametrize("q_len", [1, 5])
@pytest.mark.parametrize("use_alibi_slopes", [True, False])
@pytest.mark.parametrize("use_new_kcache_layout", [True, False])
def test_flash_decoding(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_attn_heads: int,
kv_group_num: int,
same_context_len: bool,
q_len: int,
use_alibi_slopes: bool,
use_new_kcache_layout: bool,
):
if use_new_kcache_layout and use_alibi_slopes:
# TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
# the code (alibi kernel) will be refactored later to avoid code duplication, when
# the whole triton flow with new k cache layout has been supported and tested.
# And tests for the alibi kernel using new kcache layout will be added then.
pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.6533517Z
torch.manual_seed(123)
torch.cuda.empty_cache()
>       torch.cuda.synchronize()
2025-04-11T04:23:18.6533785Z
tests/test_infer/test_kernels/triton/test_decoding_attn.py:100:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.6534055Z
device = None
2025-04-11T04:23:18.6534135Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6534480Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6536024Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
_____________ test_flash_decoding[True-False-5-True-4-16-16-16-16] _____________
2025-04-11T04:23:18.6536438Z
bsz = 16, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
kv_group_num = 4, same_context_len = True, q_len = 5, use_alibi_slopes = False
use_new_kcache_layout = True
2025-04-11T04:23:18.6536844Z
@pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
@pytest.mark.parametrize("bsz", [7, 16])
@pytest.mark.parametrize("block_size", [16, 32])
@pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
@pytest.mark.parametrize("num_attn_heads", [16])
@pytest.mark.parametrize("kv_group_num", [1, 4])
@pytest.mark.parametrize("same_context_len", [True, False])
@pytest.mark.parametrize("q_len", [1, 5])
@pytest.mark.parametrize("use_alibi_slopes", [True, False])
@pytest.mark.parametrize("use_new_kcache_layout", [True, False])
def test_flash_decoding(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_attn_heads: int,
kv_group_num: int,
same_context_len: bool,
q_len: int,
use_alibi_slopes: bool,
use_new_kcache_layout: bool,
):
if use_new_kcache_layout and use_alibi_slopes:
# TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
# the code (alibi kernel) will be refactored later to avoid code duplication, when
# the whole triton flow with new k cache layout has been supported and tested.
# And tests for the alibi kernel using new kcache layout will be added then.
pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.6540300Z
torch.manual_seed(123)
torch.cuda.empty_cache()
>       torch.cuda.synchronize()
2025-04-11T04:23:18.6540570Z
tests/test_infer/test_kernels/triton/test_decoding_attn.py:100:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.6540844Z
device = None
2025-04-11T04:23:18.6540924Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6541265Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6542823Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
_____________ test_flash_decoding[True-False-5-True-4-16-16-32-7] ______________
2025-04-11T04:23:18.6543237Z
bsz = 7, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
kv_group_num = 4, same_context_len = True, q_len = 5, use_alibi_slopes = False
use_new_kcache_layout = True
2025-04-11T04:23:18.6543730Z
@pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
@pytest.mark.parametrize("bsz", [7, 16])
@pytest.mark.parametrize("block_size", [16, 32])
@pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
@pytest.mark.parametrize("num_attn_heads", [16])
@pytest.mark.parametrize("kv_group_num", [1, 4])
@pytest.mark.parametrize("same_context_len", [True, False])
@pytest.mark.parametrize("q_len", [1, 5])
@pytest.mark.parametrize("use_alibi_slopes", [True, False])
@pytest.mark.parametrize("use_new_kcache_layout", [True, False])
def test_flash_decoding(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_attn_heads: int,
kv_group_num: int,
same_context_len: bool,
q_len: int,
use_alibi_slopes: bool,
use_new_kcache_layout: bool,
):
if use_new_kcache_layout and use_alibi_slopes:
# TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
# the code (alibi kernel) will be refactored later to avoid code duplication, when
# the whole triton flow with new k cache layout has been supported and tested.
# And tests for the alibi kernel using new kcache layout will be added then.
pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.6547181Z
torch.manual_seed(123)
torch.cuda.empty_cache()
>       torch.cuda.synchronize()
2025-04-11T04:23:18.6547454Z
tests/test_infer/test_kernels/triton/test_decoding_attn.py:100:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.6547728Z
device = None
2025-04-11T04:23:18.6547808Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6548157Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6549827Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
_____________ test_flash_decoding[True-False-5-True-4-16-16-32-16] _____________
2025-04-11T04:23:18.6550241Z
bsz = 16, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
kv_group_num = 4, same_context_len = True, q_len = 5, use_alibi_slopes = False
use_new_kcache_layout = True
2025-04-11T04:23:18.6550646Z
@pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
@pytest.mark.parametrize("bsz", [7, 16])
@pytest.mark.parametrize("block_size", [16, 32])
@pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
@pytest.mark.parametrize("num_attn_heads", [16])
@pytest.mark.parametrize("kv_group_num", [1, 4])
@pytest.mark.parametrize("same_context_len", [True, False])
@pytest.mark.parametrize("q_len", [1, 5])
@pytest.mark.parametrize("use_alibi_slopes", [True, False])
@pytest.mark.parametrize("use_new_kcache_layout", [True, False])
def test_flash_decoding(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_attn_heads: int,
kv_group_num: int,
same_context_len: bool,
q_len: int,
use_alibi_slopes: bool,
use_new_kcache_layout: bool,
):
if use_new_kcache_layout and use_alibi_slopes:
# TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
# the code (alibi kernel) will be refactored later to avoid code duplication, when
# the whole triton flow with new k cache layout has been supported and tested.
# And tests for the alibi kernel using new kcache layout will be added then.
pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.6554023Z
torch.manual_seed(123)
torch.cuda.empty_cache()
>       torch.cuda.synchronize()
2025-04-11T04:23:18.6554293Z
tests/test_infer/test_kernels/triton/test_decoding_attn.py:100:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.6554564Z
device = None
2025-04-11T04:23:18.6554647Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6554989Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6556626Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
_____________ test_flash_decoding[True-False-5-False-1-16-8-16-7] ______________
2025-04-11T04:23:18.6557037Z
bsz = 7, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
kv_group_num = 1, same_context_len = False, q_len = 5, use_alibi_slopes = False
use_new_kcache_layout = True
2025-04-11T04:23:18.6557444Z
@pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
@pytest.mark.parametrize("bsz", [7, 16])
@pytest.mark.parametrize("block_size", [16, 32])
@pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
@pytest.mark.parametrize("num_attn_heads", [16])
@pytest.mark.parametrize("kv_group_num", [1, 4])
@pytest.mark.parametrize("same_context_len", [True, False])
@pytest.mark.parametrize("q_len", [1, 5])
@pytest.mark.parametrize("use_alibi_slopes", [True, False])
@pytest.mark.parametrize("use_new_kcache_layout", [True, False])
def test_flash_decoding(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_attn_heads: int,
kv_group_num: int,
same_context_len: bool,
q_len: int,
use_alibi_slopes: bool,
use_new_kcache_layout: bool,
):
if use_new_kcache_layout and use_alibi_slopes:
# TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
# the code (alibi kernel) will be refactored later to avoid code duplication, when
# the whole triton flow with new k cache layout has been supported and tested.
# And tests for the alibi kernel using new kcache layout will be added then.
pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.6560940Z
torch.manual_seed(123)
torch.cuda.empty_cache()
>       torch.cuda.synchronize()
2025-04-11T04:23:18.6561227Z
tests/test_infer/test_kernels/triton/test_decoding_attn.py:100:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.6561506Z
device = None
2025-04-11T04:23:18.6561596Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6562048Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6563610Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
_____________ test_flash_decoding[True-False-5-False-1-16-8-16-16] _____________
2025-04-11T04:23:18.6564142Z
bsz = 16, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
kv_group_num = 1, same_context_len = False, q_len = 5, use_alibi_slopes = False
use_new_kcache_layout = True
2025-04-11T04:23:18.6564550Z
@pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
@pytest.mark.parametrize("bsz", [7, 16])
@pytest.mark.parametrize("block_size", [16, 32])
@pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
@pytest.mark.parametrize("num_attn_heads", [16])
@pytest.mark.parametrize("kv_group_num", [1, 4])
@pytest.mark.parametrize("same_context_len", [True, False])
@pytest.mark.parametrize("q_len", [1, 5])
@pytest.mark.parametrize("use_alibi_slopes", [True, False])
@pytest.mark.parametrize("use_new_kcache_layout", [True, False])
def test_flash_decoding(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_attn_heads: int,
kv_group_num: int,
same_context_len: bool,
q_len: int,
use_alibi_slopes: bool,
use_new_kcache_layout: bool,
):
if use_new_kcache_layout and use_alibi_slopes:
# TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
# the code (alibi kernel) will be refactored later to avoid code duplication, when
# the whole triton flow with new k cache layout has been supported and tested.
# And tests for the alibi kernel using new kcache layout will be added then.
pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.6567841Z
torch.manual_seed(123)
torch.cuda.empty_cache()
>       torch.cuda.synchronize()
2025-04-11T04:23:18.6568198Z
tests/test_infer/test_kernels/triton/test_decoding_attn.py:100:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.6568476Z
device = None
2025-04-11T04:23:18.6568562Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6568899Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6570535Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
_____________ test_flash_decoding[True-False-5-False-1-16-8-32-7] ______________
2025-04-11T04:23:18.6570944Z
bsz = 7, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
kv_group_num = 1, same_context_len = False, q_len = 5, use_alibi_slopes = False
use_new_kcache_layout = True
2025-04-11T04:23:18.6571349Z
@pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
@pytest.mark.parametrize("bsz", [7, 16])
@pytest.mark.parametrize("block_size", [16, 32])
@pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
@pytest.mark.parametrize("num_attn_heads", [16])
@pytest.mark.parametrize("kv_group_num", [1, 4])
@pytest.mark.parametrize("same_context_len", [True, False])
@pytest.mark.parametrize("q_len", [1, 5])
@pytest.mark.parametrize("use_alibi_slopes", [True, False])
@pytest.mark.parametrize("use_new_kcache_layout", [True, False])
def test_flash_decoding(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_attn_heads: int,
kv_group_num: int,
same_context_len: bool,
q_len: int,
use_alibi_slopes: bool,
use_new_kcache_layout: bool,
):
if use_new_kcache_layout and use_alibi_slopes:
# TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
# the code (alibi kernel) will be refactored later to avoid code duplication, when
# the whole triton flow with new k cache layout has been supported and tested.
# And tests for the alibi kernel using new kcache layout will be added then.
pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.6574751Z
torch.manual_seed(123)
torch.cuda.empty_cache()
>       torch.cuda.synchronize()
2025-04-11T04:23:18.6575020Z
tests/test_infer/test_kernels/triton/test_decoding_attn.py:100:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.6575291Z
device = None
2025-04-11T04:23:18.6575375Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6575716Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6577355Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
_____________ test_flash_decoding[True-False-5-False-1-16-8-32-16] _____________
2025-04-11T04:23:18.6577773Z
bsz = 16, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
kv_group_num = 1, same_context_len = False, q_len = 5, use_alibi_slopes = False
use_new_kcache_layout = True
2025-04-11T04:23:18.6578181Z
@pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
@pytest.mark.parametrize("bsz", [7, 16])
@pytest.mark.parametrize("block_size", [16, 32])
@pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
@pytest.mark.parametrize("num_attn_heads", [16])
@pytest.mark.parametrize("kv_group_num", [1, 4])
@pytest.mark.parametrize("same_context_len", [True, False])
@pytest.mark.parametrize("q_len", [1, 5])
@pytest.mark.parametrize("use_alibi_slopes", [True, False])
@pytest.mark.parametrize("use_new_kcache_layout", [True, False])
def test_flash_decoding(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_attn_heads: int,
kv_group_num: int,
same_context_len: bool,
q_len: int,
use_alibi_slopes: bool,
use_new_kcache_layout: bool,
):
if use_new_kcache_layout and use_alibi_slopes:
# TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
# the code (alibi kernel) will be refactored later to avoid code duplication, when
# the whole triton flow with new k cache layout has been supported and tested.
# And tests for the alibi kernel using new kcache layout will be added then.
pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.6581560Z
torch.manual_seed(123)
torch.cuda.empty_cache()
>       torch.cuda.synchronize()
2025-04-11T04:23:18.6581839Z
tests/test_infer/test_kernels/triton/test_decoding_attn.py:100:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.6582200Z
device = None
2025-04-11T04:23:18.6582290Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6582633Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6584202Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
_____________ test_flash_decoding[True-False-5-False-1-16-16-16-7] _____________
2025-04-11T04:23:18.6584622Z
bsz = 7, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
kv_group_num = 1, same_context_len = False, q_len = 5, use_alibi_slopes = False
use_new_kcache_layout = True
2025-04-11T04:23:18.6585033Z
@pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
@pytest.mark.parametrize("bsz", [7, 16])
@pytest.mark.parametrize("block_size", [16, 32])
@pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
@pytest.mark.parametrize("num_attn_heads", [16])
@pytest.mark.parametrize("kv_group_num", [1, 4])
@pytest.mark.parametrize("same_context_len", [True, False])
@pytest.mark.parametrize("q_len", [1, 5])
@pytest.mark.parametrize("use_alibi_slopes", [True, False])
@pytest.mark.parametrize("use_new_kcache_layout", [True, False])
def test_flash_decoding(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_attn_heads: int,
kv_group_num: int,
same_context_len: bool,
q_len: int,
use_alibi_slopes: bool,
use_new_kcache_layout: bool,
):
if use_new_kcache_layout and use_alibi_slopes:
# TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
# the code (alibi kernel) will be refactored later to avoid code duplication, when
# the whole triton flow with new k cache layout has been supported and tested.
# And tests for the alibi kernel using new kcache layout will be added then.
pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.6588555Z
torch.manual_seed(123)
torch.cuda.empty_cache()
>       torch.cuda.synchronize()
2025-04-11T04:23:18.6588829Z
tests/test_infer/test_kernels/triton/test_decoding_attn.py:100:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.6589100Z
device = None
2025-04-11T04:23:18.6589186Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6589526Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6591078Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
____________ test_flash_decoding[True-False-5-False-1-16-16-16-16] _____________
2025-04-11T04:23:18.6591499Z
bsz = 16, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
kv_group_num = 1, same_context_len = False, q_len = 5, use_alibi_slopes = False
use_new_kcache_layout = True
2025-04-11T04:23:18.6591907Z
@pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
@pytest.mark.parametrize("bsz", [7, 16])
@pytest.mark.parametrize("block_size", [16, 32])
@pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
@pytest.mark.parametrize("num_attn_heads", [16])
@pytest.mark.parametrize("kv_group_num", [1, 4])
@pytest.mark.parametrize("same_context_len", [True, False])
@pytest.mark.parametrize("q_len", [1, 5])
@pytest.mark.parametrize("use_alibi_slopes", [True, False])
@pytest.mark.parametrize("use_new_kcache_layout", [True, False])
def test_flash_decoding(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_attn_heads: int,
kv_group_num: int,
same_context_len: bool,
q_len: int,
use_alibi_slopes: bool,
use_new_kcache_layout: bool,
):
if use_new_kcache_layout and use_alibi_slopes:
# TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
# the code (alibi kernel) will be refactored later to avoid code duplication, when
# the whole triton flow with new k cache layout has been supported and tested.
# And tests for the alibi kernel using new kcache layout will be added then.
pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.6595382Z
torch.manual_seed(123)
torch.cuda.empty_cache()
>       torch.cuda.synchronize()
2025-04-11T04:23:18.6595655Z
tests/test_infer/test_kernels/triton/test_decoding_attn.py:100:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.6595936Z
device = None
2025-04-11T04:23:18.6596027Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6596370Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6597921Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
_____________ test_flash_decoding[True-False-5-False-1-16-16-32-7] _____________
2025-04-11T04:23:18.6598333Z
bsz = 7, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
kv_group_num = 1, same_context_len = False, q_len = 5, use_alibi_slopes = False
use_new_kcache_layout = True
2025-04-11T04:23:18.6598833Z
@pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
@pytest.mark.parametrize("bsz", [7, 16])
@pytest.mark.parametrize("block_size", [16, 32])
@pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
@pytest.mark.parametrize("num_attn_heads", [16])
@pytest.mark.parametrize("kv_group_num", [1, 4])
@pytest.mark.parametrize("same_context_len", [True, False])
@pytest.mark.parametrize("q_len", [1, 5])
@pytest.mark.parametrize("use_alibi_slopes", [True, False])
@pytest.mark.parametrize("use_new_kcache_layout", [True, False])
def test_flash_decoding(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_attn_heads: int,
kv_group_num: int,
same_context_len: bool,
q_len: int,
use_alibi_slopes: bool,
use_new_kcache_layout: bool,
):
if use_new_kcache_layout and use_alibi_slopes:
# TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
# the code (alibi kernel) will be refactored later to avoid code duplication, when
# the whole triton flow with new k cache layout has been supported and tested.
# And tests for the alibi kernel using new kcache layout will be added then.
pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.6602207Z
torch.manual_seed(123)
torch.cuda.empty_cache()
>       torch.cuda.synchronize()
2025-04-11T04:23:18.6602479Z
tests/test_infer/test_kernels/triton/test_decoding_attn.py:100:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.6602752Z
device = None
2025-04-11T04:23:18.6602836Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6603180Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6604821Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
____________ test_flash_decoding[True-False-5-False-1-16-16-32-16] _____________
2025-04-11T04:23:18.6605248Z
bsz = 16, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
kv_group_num = 1, same_context_len = False, q_len = 5, use_alibi_slopes = False
use_new_kcache_layout = True
2025-04-11T04:23:18.6605657Z
@pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
@pytest.mark.parametrize("bsz", [7, 16])
@pytest.mark.parametrize("block_size", [16, 32])
@pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
@pytest.mark.parametrize("num_attn_heads", [16])
@pytest.mark.parametrize("kv_group_num", [1, 4])
@pytest.mark.parametrize("same_context_len", [True, False])
@pytest.mark.parametrize("q_len", [1, 5])
@pytest.mark.parametrize("use_alibi_slopes", [True, False])
@pytest.mark.parametrize("use_new_kcache_layout", [True, False])
def test_flash_decoding(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_attn_heads: int,
kv_group_num: int,
same_context_len: bool,
q_len: int,
use_alibi_slopes: bool,
use_new_kcache_layout: bool,
):
if use_new_kcache_layout and use_alibi_slopes:
# TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
# the code (alibi kernel) will be refactored later to avoid code duplication, when
# the whole triton flow with new k cache layout has been supported and tested.
# And tests for the alibi kernel using new kcache layout will be added then.
pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.6609038Z
torch.manual_seed(123)
torch.cuda.empty_cache()
>       torch.cuda.synchronize()
2025-04-11T04:23:18.6609317Z
tests/test_infer/test_kernels/triton/test_decoding_attn.py:100:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.6609594Z
device = None
2025-04-11T04:23:18.6609677Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6610013Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6611655Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
_____________ test_flash_decoding[True-False-5-False-4-16-8-16-7] ______________
2025-04-11T04:23:18.6612066Z
bsz = 7, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
kv_group_num = 4, same_context_len = False, q_len = 5, use_alibi_slopes = False
use_new_kcache_layout = True
2025-04-11T04:23:18.6612472Z
@pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
@pytest.mark.parametrize("bsz", [7, 16])
@pytest.mark.parametrize("block_size", [16, 32])
@pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
@pytest.mark.parametrize("num_attn_heads", [16])
@pytest.mark.parametrize("kv_group_num", [1, 4])
@pytest.mark.parametrize("same_context_len", [True, False])
@pytest.mark.parametrize("q_len", [1, 5])
@pytest.mark.parametrize("use_alibi_slopes", [True, False])
@pytest.mark.parametrize("use_new_kcache_layout", [True, False])
def test_flash_decoding(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_attn_heads: int,
kv_group_num: int,
same_context_len: bool,
q_len: int,
use_alibi_slopes: bool,
use_new_kcache_layout: bool,
):
if use_new_kcache_layout and use_alibi_slopes:
# TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
# the code (alibi kernel) will be refactored later to avoid code duplication, when
# the whole triton flow with new k cache layout has been supported and tested.
# And tests for the alibi kernel using new kcache layout will be added then.
pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.6615925Z
torch.manual_seed(123)
torch.cuda.empty_cache()
>       torch.cuda.synchronize()
2025-04-11T04:23:18.6616205Z
tests/test_infer/test_kernels/triton/test_decoding_attn.py:100:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.6616484Z
device = None
2025-04-11T04:23:18.6616571Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6617009Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6618562Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
_____________ test_flash_decoding[True-False-5-False-4-16-8-16-16] _____________
2025-04-11T04:23:18.6619057Z
bsz = 16, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
kv_group_num = 4, same_context_len = False, q_len = 5, use_alibi_slopes = False
use_new_kcache_layout = True
2025-04-11T04:23:18.6619462Z
@pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
@pytest.mark.parametrize("bsz", [7, 16])
@pytest.mark.parametrize("block_size", [16, 32])
@pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
@pytest.mark.parametrize("num_attn_heads", [16])
@pytest.mark.parametrize("kv_group_num", [1, 4])
@pytest.mark.parametrize("same_context_len", [True, False])
@pytest.mark.parametrize("q_len", [1, 5])
@pytest.mark.parametrize("use_alibi_slopes", [True, False])
@pytest.mark.parametrize("use_new_kcache_layout", [True, False])
def test_flash_decoding(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_attn_heads: int,
kv_group_num: int,
same_context_len: bool,
q_len: int,
use_alibi_slopes: bool,
use_new_kcache_layout: bool,
):
if use_new_kcache_layout and use_alibi_slopes:
# TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
# the code (alibi kernel) will be refactored later to avoid code duplication, when
# the whole triton flow with new k cache layout has been supported and tested.
# And tests for the alibi kernel using new kcache layout will be added then.
pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.6622730Z
torch.manual_seed(123)
torch.cuda.empty_cache()
>       torch.cuda.synchronize()
2025-04-11T04:23:18.6623099Z
tests/test_infer/test_kernels/triton/test_decoding_attn.py:100:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.6623377Z
device = None
2025-04-11T04:23:18.6623462Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6623801Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6625429Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
_____________ test_flash_decoding[True-False-5-False-4-16-8-32-7] ______________
2025-04-11T04:23:18.6625838Z
bsz = 7, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
kv_group_num = 4, same_context_len = False, q_len = 5, use_alibi_slopes = False
use_new_kcache_layout = True
2025-04-11T04:23:18.6626246Z
@pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
@pytest.mark.parametrize("bsz", [7, 16])
@pytest.mark.parametrize("block_size", [16, 32])
@pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
@pytest.mark.parametrize("num_attn_heads", [16])
@pytest.mark.parametrize("kv_group_num", [1, 4])
@pytest.mark.parametrize("same_context_len", [True, False])
@pytest.mark.parametrize("q_len", [1, 5])
@pytest.mark.parametrize("use_alibi_slopes", [True, False])
@pytest.mark.parametrize("use_new_kcache_layout", [True, False])
def test_flash_decoding(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_attn_heads: int,
kv_group_num: int,
same_context_len: bool,
q_len: int,
use_alibi_slopes: bool,
use_new_kcache_layout: bool,
):
if use_new_kcache_layout and use_alibi_slopes:
# TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
# the code (alibi kernel) will be refactored later to avoid code duplication, when
# the whole triton flow with new k cache layout has been supported and tested.
# And tests for the alibi kernel using new kcache layout will be added then.
pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.6629686Z
torch.manual_seed(123)
torch.cuda.empty_cache()
>       torch.cuda.synchronize()
2025-04-11T04:23:18.6629955Z
tests/test_infer/test_kernels/triton/test_decoding_attn.py:100:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.6630226Z
device = None
2025-04-11T04:23:18.6630312Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6630656Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6632314Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
_____________ test_flash_decoding[True-False-5-False-4-16-8-32-16] _____________
2025-04-11T04:23:18.6632729Z
bsz = 16, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
kv_group_num = 4, same_context_len = False, q_len = 5, use_alibi_slopes = False
use_new_kcache_layout = True
2025-04-11T04:23:18.6633133Z
@pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
@pytest.mark.parametrize("bsz", [7, 16])
@pytest.mark.parametrize("block_size", [16, 32])
@pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
@pytest.mark.parametrize("num_attn_heads", [16])
@pytest.mark.parametrize("kv_group_num", [1, 4])
@pytest.mark.parametrize("same_context_len", [True, False])
@pytest.mark.parametrize("q_len", [1, 5])
@pytest.mark.parametrize("use_alibi_slopes", [True, False])
@pytest.mark.parametrize("use_new_kcache_layout", [True, False])
def test_flash_decoding(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_attn_heads: int,
kv_group_num: int,
same_context_len: bool,
q_len: int,
use_alibi_slopes: bool,
use_new_kcache_layout: bool,
):
if use_new_kcache_layout and use_alibi_slopes:
# TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
# the code (alibi kernel) will be refactored later to avoid code duplication, when
# the whole triton flow with new k cache layout has been supported and tested.
# And tests for the alibi kernel using new kcache layout will be added then.
pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.6636475Z
torch.manual_seed(123)
torch.cuda.empty_cache()
>       torch.cuda.synchronize()
2025-04-11T04:23:18.6636746Z
tests/test_infer/test_kernels/triton/test_decoding_attn.py:100:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.6637100Z
device = None
2025-04-11T04:23:18.6637187Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6637524Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6639077Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
_____________ test_flash_decoding[True-False-5-False-4-16-16-16-7] _____________
2025-04-11T04:23:18.6639489Z
bsz = 7, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
kv_group_num = 4, same_context_len = False, q_len = 5, use_alibi_slopes = False
use_new_kcache_layout = True
2025-04-11T04:23:18.6639904Z
@pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
@pytest.mark.parametrize("bsz", [7, 16])
@pytest.mark.parametrize("block_size", [16, 32])
@pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
@pytest.mark.parametrize("num_attn_heads", [16])
@pytest.mark.parametrize("kv_group_num", [1, 4])
@pytest.mark.parametrize("same_context_len", [True, False])
@pytest.mark.parametrize("q_len", [1, 5])
@pytest.mark.parametrize("use_alibi_slopes", [True, False])
@pytest.mark.parametrize("use_new_kcache_layout", [True, False])
def test_flash_decoding(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_attn_heads: int,
kv_group_num: int,
same_context_len: bool,
q_len: int,
use_alibi_slopes: bool,
use_new_kcache_layout: bool,
):
if use_new_kcache_layout and use_alibi_slopes:
# TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
# the code (alibi kernel) will be refactored later to avoid code duplication, when
# the whole triton flow with new k cache layout has been supported and tested.
# And tests for the alibi kernel using new kcache layout will be added then.
pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.6643438Z
torch.manual_seed(123)
torch.cuda.empty_cache()
>       torch.cuda.synchronize()
2025-04-11T04:23:18.6643717Z
tests/test_infer/test_kernels/triton/test_decoding_attn.py:100:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.6643984Z
device = None
2025-04-11T04:23:18.6644069Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6644414Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6645960Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
____________ test_flash_decoding[True-False-5-False-4-16-16-16-16] _____________
2025-04-11T04:23:18.6646384Z
bsz = 16, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
kv_group_num = 4, same_context_len = False, q_len = 5, use_alibi_slopes = False
use_new_kcache_layout = True
2025-04-11T04:23:18.6646790Z
@pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
@pytest.mark.parametrize("bsz", [7, 16])
@pytest.mark.parametrize("block_size", [16, 32])
@pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
@pytest.mark.parametrize("num_attn_heads", [16])
@pytest.mark.parametrize("kv_group_num", [1, 4])
@pytest.mark.parametrize("same_context_len", [True, False])
@pytest.mark.parametrize("q_len", [1, 5])
@pytest.mark.parametrize("use_alibi_slopes", [True, False])
@pytest.mark.parametrize("use_new_kcache_layout", [True, False])
def test_flash_decoding(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_attn_heads: int,
kv_group_num: int,
same_context_len: bool,
q_len: int,
use_alibi_slopes: bool,
use_new_kcache_layout: bool,
):
if use_new_kcache_layout and use_alibi_slopes:
# TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
# the code (alibi kernel) will be refactored later to avoid code duplication, when
# the whole triton flow with new k cache layout has been supported and tested.
# And tests for the alibi kernel using new kcache layout will be added then.
pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.6650229Z
torch.manual_seed(123)
torch.cuda.empty_cache()
>       torch.cuda.synchronize()
2025-04-11T04:23:18.6650502Z
tests/test_infer/test_kernels/triton/test_decoding_attn.py:100:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.6650780Z
device = None
2025-04-11T04:23:18.6650862Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6651198Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6652750Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
_____________ test_flash_decoding[True-False-5-False-4-16-16-32-7] _____________
2025-04-11T04:23:18.6653165Z
bsz = 7, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
kv_group_num = 4, same_context_len = False, q_len = 5, use_alibi_slopes = False
use_new_kcache_layout = True
2025-04-11T04:23:18.6653659Z
@pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
@pytest.mark.parametrize("bsz", [7, 16])
@pytest.mark.parametrize("block_size", [16, 32])
@pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
@pytest.mark.parametrize("num_attn_heads", [16])
@pytest.mark.parametrize("kv_group_num", [1, 4])
@pytest.mark.parametrize("same_context_len", [True, False])
@pytest.mark.parametrize("q_len", [1, 5])
@pytest.mark.parametrize("use_alibi_slopes", [True, False])
@pytest.mark.parametrize("use_new_kcache_layout", [True, False])
def test_flash_decoding(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_attn_heads: int,
kv_group_num: int,
same_context_len: bool,
q_len: int,
use_alibi_slopes: bool,
use_new_kcache_layout: bool,
):
if use_new_kcache_layout and use_alibi_slopes:
# TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
# the code (alibi kernel) will be refactored later to avoid code duplication, when
# the whole triton flow with new k cache layout has been supported and tested.
# And tests for the alibi kernel using new kcache layout will be added then.
pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.6657028Z
torch.manual_seed(123)
torch.cuda.empty_cache()
>       torch.cuda.synchronize()
2025-04-11T04:23:18.6657299Z
tests/test_infer/test_kernels/triton/test_decoding_attn.py:100:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.6657567Z
device = None
2025-04-11T04:23:18.6657647Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6657986Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6659613Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
____________ test_flash_decoding[True-False-5-False-4-16-16-32-16] _____________
2025-04-11T04:23:18.6660037Z
bsz = 16, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
kv_group_num = 4, same_context_len = False, q_len = 5, use_alibi_slopes = False
use_new_kcache_layout = True
2025-04-11T04:23:18.6660447Z
@pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
@pytest.mark.parametrize("bsz", [7, 16])
@pytest.mark.parametrize("block_size", [16, 32])
@pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
@pytest.mark.parametrize("num_attn_heads", [16])
@pytest.mark.parametrize("kv_group_num", [1, 4])
@pytest.mark.parametrize("same_context_len", [True, False])
@pytest.mark.parametrize("q_len", [1, 5])
@pytest.mark.parametrize("use_alibi_slopes", [True, False])
@pytest.mark.parametrize("use_new_kcache_layout", [True, False])
def test_flash_decoding(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_attn_heads: int,
kv_group_num: int,
same_context_len: bool,
q_len: int,
use_alibi_slopes: bool,
use_new_kcache_layout: bool,
):
if use_new_kcache_layout and use_alibi_slopes:
# TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
# the code (alibi kernel) will be refactored later to avoid code duplication, when
# the whole triton flow with new k cache layout has been supported and tested.
# And tests for the alibi kernel using new kcache layout will be added then.
pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.6663811Z
torch.manual_seed(123)
torch.cuda.empty_cache()
>       torch.cuda.synchronize()
2025-04-11T04:23:18.6664089Z
tests/test_infer/test_kernels/triton/test_decoding_attn.py:100:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.6664368Z
device = None
2025-04-11T04:23:18.6664448Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6664781Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6666416Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
______________ test_flash_decoding[False-True-1-True-1-16-8-16-7] ______________
2025-04-11T04:23:18.6666824Z
bsz = 7, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
kv_group_num = 1, same_context_len = True, q_len = 1, use_alibi_slopes = True
use_new_kcache_layout = False
2025-04-11T04:23:18.6667239Z
@pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
@pytest.mark.parametrize("bsz", [7, 16])
@pytest.mark.parametrize("block_size", [16, 32])
@pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
@pytest.mark.parametrize("num_attn_heads", [16])
@pytest.mark.parametrize("kv_group_num", [1, 4])
@pytest.mark.parametrize("same_context_len", [True, False])
@pytest.mark.parametrize("q_len", [1, 5])
@pytest.mark.parametrize("use_alibi_slopes", [True, False])
@pytest.mark.parametrize("use_new_kcache_layout", [True, False])
def test_flash_decoding(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_attn_heads: int,
kv_group_num: int,
same_context_len: bool,
q_len: int,
use_alibi_slopes: bool,
use_new_kcache_layout: bool,
):
if use_new_kcache_layout and use_alibi_slopes:
# TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
# the code (alibi kernel) will be refactored later to avoid code duplication, when
# the whole triton flow with new k cache layout has been supported and tested.
# And tests for the alibi kernel using new kcache layout will be added then.
pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.6670636Z
torch.manual_seed(123)
torch.cuda.empty_cache()
>       torch.cuda.synchronize()
2025-04-11T04:23:18.6670914Z
tests/test_infer/test_kernels/triton/test_decoding_attn.py:100:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.6671185Z
device = None
2025-04-11T04:23:18.6671266Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6671695Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6673240Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
_____________ test_flash_decoding[False-True-1-True-1-16-8-16-16] ______________
2025-04-11T04:23:18.6673751Z
bsz = 16, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
kv_group_num = 1, same_context_len = True, q_len = 1, use_alibi_slopes = True
use_new_kcache_layout = False
2025-04-11T04:23:18.6674164Z
@pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
@pytest.mark.parametrize("bsz", [7, 16])
@pytest.mark.parametrize("block_size", [16, 32])
@pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
@pytest.mark.parametrize("num_attn_heads", [16])
@pytest.mark.parametrize("kv_group_num", [1, 4])
@pytest.mark.parametrize("same_context_len", [True, False])
@pytest.mark.parametrize("q_len", [1, 5])
@pytest.mark.parametrize("use_alibi_slopes", [True, False])
@pytest.mark.parametrize("use_new_kcache_layout", [True, False])
def test_flash_decoding(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_attn_heads: int,
kv_group_num: int,
same_context_len: bool,
q_len: int,
use_alibi_slopes: bool,
use_new_kcache_layout: bool,
):
if use_new_kcache_layout and use_alibi_slopes:
# TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
# the code (alibi kernel) will be refactored later to avoid code duplication, when
# the whole triton flow with new k cache layout has been supported and tested.
# And tests for the alibi kernel using new kcache layout will be added then.
pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.6677442Z
torch.manual_seed(123)
torch.cuda.empty_cache()
>       torch.cuda.synchronize()
2025-04-11T04:23:18.6677711Z
tests/test_infer/test_kernels/triton/test_decoding_attn.py:100:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.6678087Z
device = None
2025-04-11T04:23:18.6678169Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6678506Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6680147Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
______________ test_flash_decoding[False-True-1-True-1-16-8-32-7] ______________
2025-04-11T04:23:18.6680556Z
bsz = 7, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
kv_group_num = 1, same_context_len = True, q_len = 1, use_alibi_slopes = True
use_new_kcache_layout = False
2025-04-11T04:23:18.6680964Z
@pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
@pytest.mark.parametrize("bsz", [7, 16])
@pytest.mark.parametrize("block_size", [16, 32])
@pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
@pytest.mark.parametrize("num_attn_heads", [16])
@pytest.mark.parametrize("kv_group_num", [1, 4])
@pytest.mark.parametrize("same_context_len", [True, False])
@pytest.mark.parametrize("q_len", [1, 5])
@pytest.mark.parametrize("use_alibi_slopes", [True, False])
@pytest.mark.parametrize("use_new_kcache_layout", [True, False])
def test_flash_decoding(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_attn_heads: int,
kv_group_num: int,
same_context_len: bool,
q_len: int,
use_alibi_slopes: bool,
use_new_kcache_layout: bool,
):
if use_new_kcache_layout and use_alibi_slopes:
# TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
# the code (alibi kernel) will be refactored later to avoid code duplication, when
# the whole triton flow with new k cache layout has been supported and tested.
# And tests for the alibi kernel using new kcache layout will be added then.
pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.6684335Z
torch.manual_seed(123)
torch.cuda.empty_cache()
>       torch.cuda.synchronize()
2025-04-11T04:23:18.6684609Z
tests/test_infer/test_kernels/triton/test_decoding_attn.py:100:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.6684882Z
device = None
2025-04-11T04:23:18.6684962Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6685300Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6686987Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
_____________ test_flash_decoding[False-True-1-True-1-16-8-32-16] ______________
2025-04-11T04:23:18.6687410Z
bsz = 16, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
kv_group_num = 1, same_context_len = True, q_len = 1, use_alibi_slopes = True
use_new_kcache_layout = False
2025-04-11T04:23:18.6687835Z
@pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
@pytest.mark.parametrize("bsz", [7, 16])
@pytest.mark.parametrize("block_size", [16, 32])
@pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
@pytest.mark.parametrize("num_attn_heads", [16])
@pytest.mark.parametrize("kv_group_num", [1, 4])
@pytest.mark.parametrize("same_context_len", [True, False])
@pytest.mark.parametrize("q_len", [1, 5])
@pytest.mark.parametrize("use_alibi_slopes", [True, False])
@pytest.mark.parametrize("use_new_kcache_layout", [True, False])
def test_flash_decoding(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_attn_heads: int,
kv_group_num: int,
same_context_len: bool,
q_len: int,
use_alibi_slopes: bool,
use_new_kcache_layout: bool,
):
if use_new_kcache_layout and use_alibi_slopes:
# TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
# the code (alibi kernel) will be refactored later to avoid code duplication, when
# the whole triton flow with new k cache layout has been supported and tested.
# And tests for the alibi kernel using new kcache layout will be added then.
pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.6691289Z
torch.manual_seed(123)
torch.cuda.empty_cache()
>       torch.cuda.synchronize()
2025-04-11T04:23:18.6691559Z
tests/test_infer/test_kernels/triton/test_decoding_attn.py:100:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.6691916Z
device = None
2025-04-11T04:23:18.6692001Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6692339Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6693874Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
_____________ test_flash_decoding[False-True-1-True-1-16-16-16-7] ______________
2025-04-11T04:23:18.6694276Z
bsz = 7, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
kv_group_num = 1, same_context_len = True, q_len = 1, use_alibi_slopes = True
use_new_kcache_layout = False
2025-04-11T04:23:18.6694690Z
@pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
@pytest.mark.parametrize("bsz", [7, 16])
@pytest.mark.parametrize("block_size", [16, 32])
@pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
@pytest.mark.parametrize("num_attn_heads", [16])
@pytest.mark.parametrize("kv_group_num", [1, 4])
@pytest.mark.parametrize("same_context_len", [True, False])
@pytest.mark.parametrize("q_len", [1, 5])
@pytest.mark.parametrize("use_alibi_slopes", [True, False])
@pytest.mark.parametrize("use_new_kcache_layout", [True, False])
def test_flash_decoding(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_attn_heads: int,
kv_group_num: int,
same_context_len: bool,
q_len: int,
use_alibi_slopes: bool,
use_new_kcache_layout: bool,
):
if use_new_kcache_layout and use_alibi_slopes:
# TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
# the code (alibi kernel) will be refactored later to avoid code duplication, when
# the whole triton flow with new k cache layout has been supported and tested.
# And tests for the alibi kernel using new kcache layout will be added then.
pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.6698130Z
torch.manual_seed(123)
torch.cuda.empty_cache()
>       torch.cuda.synchronize()
2025-04-11T04:23:18.6698406Z
tests/test_infer/test_kernels/triton/test_decoding_attn.py:100:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.6698675Z
device = None
2025-04-11T04:23:18.6698757Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6699092Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6700637Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
_____________ test_flash_decoding[False-True-1-True-1-16-16-16-16] _____________
2025-04-11T04:23:18.6701048Z
bsz = 16, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
kv_group_num = 1, same_context_len = True, q_len = 1, use_alibi_slopes = True
use_new_kcache_layout = False
2025-04-11T04:23:18.6701456Z
@pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
@pytest.mark.parametrize("bsz", [7, 16])
@pytest.mark.parametrize("block_size", [16, 32])
@pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
@pytest.mark.parametrize("num_attn_heads", [16])
@pytest.mark.parametrize("kv_group_num", [1, 4])
@pytest.mark.parametrize("same_context_len", [True, False])
@pytest.mark.parametrize("q_len", [1, 5])
@pytest.mark.parametrize("use_alibi_slopes", [True, False])
@pytest.mark.parametrize("use_new_kcache_layout", [True, False])
def test_flash_decoding(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_attn_heads: int,
kv_group_num: int,
same_context_len: bool,
q_len: int,
use_alibi_slopes: bool,
use_new_kcache_layout: bool,
):
if use_new_kcache_layout and use_alibi_slopes:
# TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
# the code (alibi kernel) will be refactored later to avoid code duplication, when
# the whole triton flow with new k cache layout has been supported and tested.
# And tests for the alibi kernel using new kcache layout will be added then.
pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.6704901Z
torch.manual_seed(123)
torch.cuda.empty_cache()
>       torch.cuda.synchronize()
2025-04-11T04:23:18.6705169Z
tests/test_infer/test_kernels/triton/test_decoding_attn.py:100:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.6705441Z
device = None
2025-04-11T04:23:18.6705525Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6705857Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6707398Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
_____________ test_flash_decoding[False-True-1-True-1-16-16-32-7] ______________
2025-04-11T04:23:18.6707807Z
bsz = 7, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
kv_group_num = 1, same_context_len = True, q_len = 1, use_alibi_slopes = True
use_new_kcache_layout = False
2025-04-11T04:23:18.6708300Z
@pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
@pytest.mark.parametrize("bsz", [7, 16])
@pytest.mark.parametrize("block_size", [16, 32])
@pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
@pytest.mark.parametrize("num_attn_heads", [16])
@pytest.mark.parametrize("kv_group_num", [1, 4])
@pytest.mark.parametrize("same_context_len", [True, False])
@pytest.mark.parametrize("q_len", [1, 5])
@pytest.mark.parametrize("use_alibi_slopes", [True, False])
@pytest.mark.parametrize("use_new_kcache_layout", [True, False])
def test_flash_decoding(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_attn_heads: int,
kv_group_num: int,
same_context_len: bool,
q_len: int,
use_alibi_slopes: bool,
use_new_kcache_layout: bool,
):
if use_new_kcache_layout and use_alibi_slopes:
# TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
# the code (alibi kernel) will be refactored later to avoid code duplication, when
# the whole triton flow with new k cache layout has been supported and tested.
# And tests for the alibi kernel using new kcache layout will be added then.
pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.6711700Z
torch.manual_seed(123)
torch.cuda.empty_cache()
>       torch.cuda.synchronize()
2025-04-11T04:23:18.6711968Z
tests/test_infer/test_kernels/triton/test_decoding_attn.py:100:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.6712236Z
device = None
2025-04-11T04:23:18.6712315Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6712650Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6714271Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
_____________ test_flash_decoding[False-True-1-True-1-16-16-32-16] _____________
2025-04-11T04:23:18.6714685Z
bsz = 16, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
kv_group_num = 1, same_context_len = True, q_len = 1, use_alibi_slopes = True
use_new_kcache_layout = False
2025-04-11T04:23:18.6715090Z
@pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
@pytest.mark.parametrize("bsz", [7, 16])
@pytest.mark.parametrize("block_size", [16, 32])
@pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
@pytest.mark.parametrize("num_attn_heads", [16])
@pytest.mark.parametrize("kv_group_num", [1, 4])
@pytest.mark.parametrize("same_context_len", [True, False])
@pytest.mark.parametrize("q_len", [1, 5])
@pytest.mark.parametrize("use_alibi_slopes", [True, False])
@pytest.mark.parametrize("use_new_kcache_layout", [True, False])
def test_flash_decoding(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_attn_heads: int,
kv_group_num: int,
same_context_len: bool,
q_len: int,
use_alibi_slopes: bool,
use_new_kcache_layout: bool,
):
if use_new_kcache_layout and use_alibi_slopes:
# TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
# the code (alibi kernel) will be refactored later to avoid code duplication, when
# the whole triton flow with new k cache layout has been supported and tested.
# And tests for the alibi kernel using new kcache layout will be added then.
pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.6718421Z
torch.manual_seed(123)
torch.cuda.empty_cache()
>       torch.cuda.synchronize()
2025-04-11T04:23:18.6718690Z
tests/test_infer/test_kernels/triton/test_decoding_attn.py:100:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.6718959Z
device = None
2025-04-11T04:23:18.6719043Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6719378Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6721022Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
______________ test_flash_decoding[False-True-1-True-4-16-8-16-7] ______________
2025-04-11T04:23:18.6721433Z
bsz = 7, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
kv_group_num = 4, same_context_len = True, q_len = 1, use_alibi_slopes = True
use_new_kcache_layout = False
2025-04-11T04:23:18.6721844Z
@pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
@pytest.mark.parametrize("bsz", [7, 16])
@pytest.mark.parametrize("block_size", [16, 32])
@pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
@pytest.mark.parametrize("num_attn_heads", [16])
@pytest.mark.parametrize("kv_group_num", [1, 4])
@pytest.mark.parametrize("same_context_len", [True, False])
@pytest.mark.parametrize("q_len", [1, 5])
@pytest.mark.parametrize("use_alibi_slopes", [True, False])
@pytest.mark.parametrize("use_new_kcache_layout", [True, False])
def test_flash_decoding(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_attn_heads: int,
kv_group_num: int,
same_context_len: bool,
q_len: int,
use_alibi_slopes: bool,
use_new_kcache_layout: bool,
):
if use_new_kcache_layout and use_alibi_slopes:
# TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
# the code (alibi kernel) will be refactored later to avoid code duplication, when
# the whole triton flow with new k cache layout has been supported and tested.
# And tests for the alibi kernel using new kcache layout will be added then.
pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.6725215Z
torch.manual_seed(123)
torch.cuda.empty_cache()
>       torch.cuda.synchronize()
2025-04-11T04:23:18.6725485Z
tests/test_infer/test_kernels/triton/test_decoding_attn.py:100:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.6725753Z
device = None
2025-04-11T04:23:18.6725835Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6726170Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6727800Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
_____________ test_flash_decoding[False-True-1-True-4-16-8-16-16] ______________
2025-04-11T04:23:18.6728294Z
bsz = 16, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
kv_group_num = 4, same_context_len = True, q_len = 1, use_alibi_slopes = True
use_new_kcache_layout = False
2025-04-11T04:23:18.6728700Z
@pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
@pytest.mark.parametrize("bsz", [7, 16])
@pytest.mark.parametrize("block_size", [16, 32])
@pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
@pytest.mark.parametrize("num_attn_heads", [16])
@pytest.mark.parametrize("kv_group_num", [1, 4])
@pytest.mark.parametrize("same_context_len", [True, False])
@pytest.mark.parametrize("q_len", [1, 5])
@pytest.mark.parametrize("use_alibi_slopes", [True, False])
@pytest.mark.parametrize("use_new_kcache_layout", [True, False])
def test_flash_decoding(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_attn_heads: int,
kv_group_num: int,
same_context_len: bool,
q_len: int,
use_alibi_slopes: bool,
use_new_kcache_layout: bool,
):
if use_new_kcache_layout and use_alibi_slopes:
# TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
# the code (alibi kernel) will be refactored later to avoid code duplication, when
# the whole triton flow with new k cache layout has been supported and tested.
# And tests for the alibi kernel using new kcache layout will be added then.
pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.6731997Z
torch.manual_seed(123)
torch.cuda.empty_cache()
>       torch.cuda.synchronize()
2025-04-11T04:23:18.6732267Z
tests/test_infer/test_kernels/triton/test_decoding_attn.py:100:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.6732622Z
device = None
2025-04-11T04:23:18.6732707Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6733051Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6734679Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
______________ test_flash_decoding[False-True-1-True-4-16-8-32-7] ______________
2025-04-11T04:23:18.6735085Z
bsz = 7, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
kv_group_num = 4, same_context_len = True, q_len = 1, use_alibi_slopes = True
use_new_kcache_layout = False
2025-04-11T04:23:18.6735491Z
@pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
@pytest.mark.parametrize("bsz", [7, 16])
@pytest.mark.parametrize("block_size", [16, 32])
@pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
@pytest.mark.parametrize("num_attn_heads", [16])
@pytest.mark.parametrize("kv_group_num", [1, 4])
@pytest.mark.parametrize("same_context_len", [True, False])
@pytest.mark.parametrize("q_len", [1, 5])
@pytest.mark.parametrize("use_alibi_slopes", [True, False])
@pytest.mark.parametrize("use_new_kcache_layout", [True, False])
def test_flash_decoding(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_attn_heads: int,
kv_group_num: int,
same_context_len: bool,
q_len: int,
use_alibi_slopes: bool,
use_new_kcache_layout: bool,
):
if use_new_kcache_layout and use_alibi_slopes:
# TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
# the code (alibi kernel) will be refactored later to avoid code duplication, when
# the whole triton flow with new k cache layout has been supported and tested.
# And tests for the alibi kernel using new kcache layout will be added then.
pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.6738896Z
torch.manual_seed(123)
torch.cuda.empty_cache()
>       torch.cuda.synchronize()
2025-04-11T04:23:18.6739166Z
tests/test_infer/test_kernels/triton/test_decoding_attn.py:100:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.6739434Z
device = None
2025-04-11T04:23:18.6739514Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6739854Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6741516Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
_____________ test_flash_decoding[False-True-1-True-4-16-8-32-16] ______________
2025-04-11T04:23:18.6741930Z
bsz = 16, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
kv_group_num = 4, same_context_len = True, q_len = 1, use_alibi_slopes = True
use_new_kcache_layout = False
2025-04-11T04:23:18.6742334Z
@pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
@pytest.mark.parametrize("bsz", [7, 16])
@pytest.mark.parametrize("block_size", [16, 32])
@pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
@pytest.mark.parametrize("num_attn_heads", [16])
@pytest.mark.parametrize("kv_group_num", [1, 4])
@pytest.mark.parametrize("same_context_len", [True, False])
@pytest.mark.parametrize("q_len", [1, 5])
@pytest.mark.parametrize("use_alibi_slopes", [True, False])
@pytest.mark.parametrize("use_new_kcache_layout", [True, False])
def test_flash_decoding(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_attn_heads: int,
kv_group_num: int,
same_context_len: bool,
q_len: int,
use_alibi_slopes: bool,
use_new_kcache_layout: bool,
):
if use_new_kcache_layout and use_alibi_slopes:
# TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
# the code (alibi kernel) will be refactored later to avoid code duplication, when
# the whole triton flow with new k cache layout has been supported and tested.
# And tests for the alibi kernel using new kcache layout will be added then.
pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.6745701Z
torch.manual_seed(123)
torch.cuda.empty_cache()
>       torch.cuda.synchronize()
2025-04-11T04:23:18.6745969Z
tests/test_infer/test_kernels/triton/test_decoding_attn.py:100:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.6746240Z
device = None
2025-04-11T04:23:18.6746408Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6746753Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6748297Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
_____________ test_flash_decoding[False-True-1-True-4-16-16-16-7] ______________
2025-04-11T04:23:18.6748757Z
bsz = 7, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
kv_group_num = 4, same_context_len = True, q_len = 1, use_alibi_slopes = True
use_new_kcache_layout = False
2025-04-11T04:23:18.6749160Z
@pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
@pytest.mark.parametrize("bsz", [7, 16])
@pytest.mark.parametrize("block_size", [16, 32])
@pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
@pytest.mark.parametrize("num_attn_heads", [16])
@pytest.mark.parametrize("kv_group_num", [1, 4])
@pytest.mark.parametrize("same_context_len", [True, False])
@pytest.mark.parametrize("q_len", [1, 5])
@pytest.mark.parametrize("use_alibi_slopes", [True, False])
@pytest.mark.parametrize("use_new_kcache_layout", [True, False])
def test_flash_decoding(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_attn_heads: int,
kv_group_num: int,
same_context_len: bool,
q_len: int,
use_alibi_slopes: bool,
use_new_kcache_layout: bool,
):
if use_new_kcache_layout and use_alibi_slopes:
# TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
# the code (alibi kernel) will be refactored later to avoid code duplication, when
# the whole triton flow with new k cache layout has been supported and tested.
# And tests for the alibi kernel using new kcache layout will be added then.
pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.6752619Z
torch.manual_seed(123)
torch.cuda.empty_cache()
>       torch.cuda.synchronize()
2025-04-11T04:23:18.6752896Z
tests/test_infer/test_kernels/triton/test_decoding_attn.py:100:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.6753163Z
device = None
2025-04-11T04:23:18.6753244Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6753583Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6755136Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
_____________ test_flash_decoding[False-True-1-True-4-16-16-16-16] _____________
2025-04-11T04:23:18.6755548Z
bsz = 16, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
kv_group_num = 4, same_context_len = True, q_len = 1, use_alibi_slopes = True
use_new_kcache_layout = False
2025-04-11T04:23:18.6755951Z
@pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
@pytest.mark.parametrize("bsz", [7, 16])
@pytest.mark.parametrize("block_size", [16, 32])
@pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
@pytest.mark.parametrize("num_attn_heads", [16])
@pytest.mark.parametrize("kv_group_num", [1, 4])
@pytest.mark.parametrize("same_context_len", [True, False])
@pytest.mark.parametrize("q_len", [1, 5])
@pytest.mark.parametrize("use_alibi_slopes", [True, False])
@pytest.mark.parametrize("use_new_kcache_layout", [True, False])
def test_flash_decoding(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_attn_heads: int,
kv_group_num: int,
same_context_len: bool,
q_len: int,
use_alibi_slopes: bool,
use_new_kcache_layout: bool,
):
if use_new_kcache_layout and use_alibi_slopes:
# TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
# the code (alibi kernel) will be refactored later to avoid code duplication, when
# the whole triton flow with new k cache layout has been supported and tested.
# And tests for the alibi kernel using new kcache layout will be added then.
pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.6759394Z
torch.manual_seed(123)
torch.cuda.empty_cache()
>       torch.cuda.synchronize()
2025-04-11T04:23:18.6759677Z
tests/test_infer/test_kernels/triton/test_decoding_attn.py:100:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.6759959Z
device = None
2025-04-11T04:23:18.6760048Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6760397Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6761995Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
_____________ test_flash_decoding[False-True-1-True-4-16-16-32-7] ______________
2025-04-11T04:23:18.6762422Z
bsz = 7, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
kv_group_num = 4, same_context_len = True, q_len = 1, use_alibi_slopes = True
use_new_kcache_layout = False
2025-04-11T04:23:18.6762927Z
@pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
@pytest.mark.parametrize("bsz", [7, 16])
@pytest.mark.parametrize("block_size", [16, 32])
@pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
@pytest.mark.parametrize("num_attn_heads", [16])
@pytest.mark.parametrize("kv_group_num", [1, 4])
@pytest.mark.parametrize("same_context_len", [True, False])
@pytest.mark.parametrize("q_len", [1, 5])
@pytest.mark.parametrize("use_alibi_slopes", [True, False])
@pytest.mark.parametrize("use_new_kcache_layout", [True, False])
def test_flash_decoding(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_attn_heads: int,
kv_group_num: int,
same_context_len: bool,
q_len: int,
use_alibi_slopes: bool,
use_new_kcache_layout: bool,
):
if use_new_kcache_layout and use_alibi_slopes:
# TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
# the code (alibi kernel) will be refactored later to avoid code duplication, when
# the whole triton flow with new k cache layout has been supported and tested.
# And tests for the alibi kernel using new kcache layout will be added then.
pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.6766308Z
torch.manual_seed(123)
torch.cuda.empty_cache()
>       torch.cuda.synchronize()
2025-04-11T04:23:18.6766576Z
tests/test_infer/test_kernels/triton/test_decoding_attn.py:100:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.6766848Z
device = None
2025-04-11T04:23:18.6766934Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6767268Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6768820Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
_____________ test_flash_decoding[False-True-1-True-4-16-16-32-16] _____________
2025-04-11T04:23:18.6769348Z
bsz = 16, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
kv_group_num = 4, same_context_len = True, q_len = 1, use_alibi_slopes = True
use_new_kcache_layout = False
2025-04-11T04:23:18.6769755Z
@pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
@pytest.mark.parametrize("bsz", [7, 16])
@pytest.mark.parametrize("block_size", [16, 32])
@pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
@pytest.mark.parametrize("num_attn_heads", [16])
@pytest.mark.parametrize("kv_group_num", [1, 4])
@pytest.mark.parametrize("same_context_len", [True, False])
@pytest.mark.parametrize("q_len", [1, 5])
@pytest.mark.parametrize("use_alibi_slopes", [True, False])
@pytest.mark.parametrize("use_new_kcache_layout", [True, False])
def test_flash_decoding(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_attn_heads: int,
kv_group_num: int,
same_context_len: bool,
q_len: int,
use_alibi_slopes: bool,
use_new_kcache_layout: bool,
):
if use_new_kcache_layout and use_alibi_slopes:
# TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
# the code (alibi kernel) will be refactored later to avoid code duplication, when
# the whole triton flow with new k cache layout has been supported and tested.
# And tests for the alibi kernel using new kcache layout will be added then.
pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.6773119Z
torch.manual_seed(123)
torch.cuda.empty_cache()
>       torch.cuda.synchronize()
2025-04-11T04:23:18.6773390Z
tests/test_infer/test_kernels/triton/test_decoding_attn.py:100:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.6773660Z
device = None
2025-04-11T04:23:18.6773748Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6774081Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6775722Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
_____________ test_flash_decoding[False-True-1-False-1-16-8-16-7] ______________
2025-04-11T04:23:18.6776134Z
bsz = 7, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
kv_group_num = 1, same_context_len = False, q_len = 1, use_alibi_slopes = True
use_new_kcache_layout = False
2025-04-11T04:23:18.6776541Z
@pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
@pytest.mark.parametrize("bsz", [7, 16])
@pytest.mark.parametrize("block_size", [16, 32])
@pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
@pytest.mark.parametrize("num_attn_heads", [16])
@pytest.mark.parametrize("kv_group_num", [1, 4])
@pytest.mark.parametrize("same_context_len", [True, False])
@pytest.mark.parametrize("q_len", [1, 5])
@pytest.mark.parametrize("use_alibi_slopes", [True, False])
@pytest.mark.parametrize("use_new_kcache_layout", [True, False])
def test_flash_decoding(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_attn_heads: int,
kv_group_num: int,
same_context_len: bool,
q_len: int,
use_alibi_slopes: bool,
use_new_kcache_layout: bool,
):
if use_new_kcache_layout and use_alibi_slopes:
# TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
# the code (alibi kernel) will be refactored later to avoid code duplication, when
# the whole triton flow with new k cache layout has been supported and tested.
# And tests for the alibi kernel using new kcache layout will be added then.
pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.6779896Z
torch.manual_seed(123)
torch.cuda.empty_cache()
>       torch.cuda.synchronize()
2025-04-11T04:23:18.6780165Z
tests/test_infer/test_kernels/triton/test_decoding_attn.py:100:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.6780433Z
device = None
2025-04-11T04:23:18.6780516Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6780853Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6782486Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
_____________ test_flash_decoding[False-True-1-False-1-16-8-16-16] _____________
2025-04-11T04:23:18.6782982Z
bsz = 16, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
kv_group_num = 1, same_context_len = False, q_len = 1, use_alibi_slopes = True
use_new_kcache_layout = False
2025-04-11T04:23:18.6783394Z
@pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
@pytest.mark.parametrize("bsz", [7, 16])
@pytest.mark.parametrize("block_size", [16, 32])
@pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
@pytest.mark.parametrize("num_attn_heads", [16])
@pytest.mark.parametrize("kv_group_num", [1, 4])
@pytest.mark.parametrize("same_context_len", [True, False])
@pytest.mark.parametrize("q_len", [1, 5])
@pytest.mark.parametrize("use_alibi_slopes", [True, False])
@pytest.mark.parametrize("use_new_kcache_layout", [True, False])
def test_flash_decoding(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_attn_heads: int,
kv_group_num: int,
same_context_len: bool,
q_len: int,
use_alibi_slopes: bool,
use_new_kcache_layout: bool,
):
if use_new_kcache_layout and use_alibi_slopes:
# TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
# the code (alibi kernel) will be refactored later to avoid code duplication, when
# the whole triton flow with new k cache layout has been supported and tested.
# And tests for the alibi kernel using new kcache layout will be added then.
pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.6786651Z
torch.manual_seed(123)
torch.cuda.empty_cache()
>       torch.cuda.synchronize()
2025-04-11T04:23:18.6786919Z
tests/test_infer/test_kernels/triton/test_decoding_attn.py:100:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.6787273Z
device = None
2025-04-11T04:23:18.6787361Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6787697Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6789373Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
_____________ test_flash_decoding[False-True-1-False-1-16-8-32-7] ______________
2025-04-11T04:23:18.6789780Z
bsz = 7, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
kv_group_num = 1, same_context_len = False, q_len = 1, use_alibi_slopes = True
use_new_kcache_layout = False
2025-04-11T04:23:18.6790184Z
@pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
@pytest.mark.parametrize("bsz", [7, 16])
@pytest.mark.parametrize("block_size", [16, 32])
@pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
@pytest.mark.parametrize("num_attn_heads", [16])
@pytest.mark.parametrize("kv_group_num", [1, 4])
@pytest.mark.parametrize("same_context_len", [True, False])
@pytest.mark.parametrize("q_len", [1, 5])
@pytest.mark.parametrize("use_alibi_slopes", [True, False])
@pytest.mark.parametrize("use_new_kcache_layout", [True, False])
def test_flash_decoding(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_attn_heads: int,
kv_group_num: int,
same_context_len: bool,
q_len: int,
use_alibi_slopes: bool,
use_new_kcache_layout: bool,
):
if use_new_kcache_layout and use_alibi_slopes:
# TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
# the code (alibi kernel) will be refactored later to avoid code duplication, when
# the whole triton flow with new k cache layout has been supported and tested.
# And tests for the alibi kernel using new kcache layout will be added then.
pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.6793537Z
torch.manual_seed(123)
torch.cuda.empty_cache()
>       torch.cuda.synchronize()
2025-04-11T04:23:18.6793807Z
tests/test_infer/test_kernels/triton/test_decoding_attn.py:100:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.6794078Z
device = None
2025-04-11T04:23:18.6794162Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6794496Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6796130Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
_____________ test_flash_decoding[False-True-1-False-1-16-8-32-16] _____________
2025-04-11T04:23:18.6796550Z
bsz = 16, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
kv_group_num = 1, same_context_len = False, q_len = 1, use_alibi_slopes = True
use_new_kcache_layout = False
2025-04-11T04:23:18.6796957Z
@pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
@pytest.mark.parametrize("bsz", [7, 16])
@pytest.mark.parametrize("block_size", [16, 32])
@pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
@pytest.mark.parametrize("num_attn_heads", [16])
@pytest.mark.parametrize("kv_group_num", [1, 4])
@pytest.mark.parametrize("same_context_len", [True, False])
@pytest.mark.parametrize("q_len", [1, 5])
@pytest.mark.parametrize("use_alibi_slopes", [True, False])
@pytest.mark.parametrize("use_new_kcache_layout", [True, False])
def test_flash_decoding(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_attn_heads: int,
kv_group_num: int,
same_context_len: bool,
q_len: int,
use_alibi_slopes: bool,
use_new_kcache_layout: bool,
):
if use_new_kcache_layout and use_alibi_slopes:
# TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
# the code (alibi kernel) will be refactored later to avoid code duplication, when
# the whole triton flow with new k cache layout has been supported and tested.
# And tests for the alibi kernel using new kcache layout will be added then.
pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.6800334Z
torch.manual_seed(123)
torch.cuda.empty_cache()
>       torch.cuda.synchronize()
2025-04-11T04:23:18.6800603Z
tests/test_infer/test_kernels/triton/test_decoding_attn.py:100:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.6800878Z
device = None
2025-04-11T04:23:18.6801043Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6801382Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6802934Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
_____________ test_flash_decoding[False-True-1-False-1-16-16-16-7] _____________
2025-04-11T04:23:18.6803349Z
bsz = 7, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
kv_group_num = 1, same_context_len = False, q_len = 1, use_alibi_slopes = True
use_new_kcache_layout = False
2025-04-11T04:23:18.6803760Z
@pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
@pytest.mark.parametrize("bsz", [7, 16])
@pytest.mark.parametrize("block_size", [16, 32])
@pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
@pytest.mark.parametrize("num_attn_heads", [16])
@pytest.mark.parametrize("kv_group_num", [1, 4])
@pytest.mark.parametrize("same_context_len", [True, False])
@pytest.mark.parametrize("q_len", [1, 5])
@pytest.mark.parametrize("use_alibi_slopes", [True, False])
@pytest.mark.parametrize("use_new_kcache_layout", [True, False])
def test_flash_decoding(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_attn_heads: int,
kv_group_num: int,
same_context_len: bool,
q_len: int,
use_alibi_slopes: bool,
use_new_kcache_layout: bool,
):
if use_new_kcache_layout and use_alibi_slopes:
# TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
# the code (alibi kernel) will be refactored later to avoid code duplication, when
# the whole triton flow with new k cache layout has been supported and tested.
# And tests for the alibi kernel using new kcache layout will be added then.
pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.6807139Z
torch.manual_seed(123)
torch.cuda.empty_cache()
>       torch.cuda.synchronize()
2025-04-11T04:23:18.6807501Z
tests/test_infer/test_kernels/triton/test_decoding_attn.py:100:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.6807775Z
device = None
2025-04-11T04:23:18.6807859Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6808200Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6809768Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
____________ test_flash_decoding[False-True-1-False-1-16-16-16-16] _____________
2025-04-11T04:23:18.6810194Z
bsz = 16, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
kv_group_num = 1, same_context_len = False, q_len = 1, use_alibi_slopes = True
use_new_kcache_layout = False
2025-04-11T04:23:18.6810603Z
@pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
@pytest.mark.parametrize("bsz", [7, 16])
@pytest.mark.parametrize("block_size", [16, 32])
@pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
@pytest.mark.parametrize("num_attn_heads", [16])
@pytest.mark.parametrize("kv_group_num", [1, 4])
@pytest.mark.parametrize("same_context_len", [True, False])
@pytest.mark.parametrize("q_len", [1, 5])
@pytest.mark.parametrize("use_alibi_slopes", [True, False])
@pytest.mark.parametrize("use_new_kcache_layout", [True, False])
def test_flash_decoding(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_attn_heads: int,
kv_group_num: int,
same_context_len: bool,
q_len: int,
use_alibi_slopes: bool,
use_new_kcache_layout: bool,
):
if use_new_kcache_layout and use_alibi_slopes:
# TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
# the code (alibi kernel) will be refactored later to avoid code duplication, when
# the whole triton flow with new k cache layout has been supported and tested.
# And tests for the alibi kernel using new kcache layout will be added then.
pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.6814058Z
torch.manual_seed(123)
torch.cuda.empty_cache()
>       torch.cuda.synchronize()
2025-04-11T04:23:18.6814327Z
tests/test_infer/test_kernels/triton/test_decoding_attn.py:100:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.6814602Z
device = None
2025-04-11T04:23:18.6814692Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6815026Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6816578Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
_____________ test_flash_decoding[False-True-1-False-1-16-16-32-7] _____________
2025-04-11T04:23:18.6816985Z
bsz = 7, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
kv_group_num = 1, same_context_len = False, q_len = 1, use_alibi_slopes = True
use_new_kcache_layout = False
2025-04-11T04:23:18.6817472Z
@pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
@pytest.mark.parametrize("bsz", [7, 16])
@pytest.mark.parametrize("block_size", [16, 32])
@pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
@pytest.mark.parametrize("num_attn_heads", [16])
@pytest.mark.parametrize("kv_group_num", [1, 4])
@pytest.mark.parametrize("same_context_len", [True, False])
@pytest.mark.parametrize("q_len", [1, 5])
@pytest.mark.parametrize("use_alibi_slopes", [True, False])
@pytest.mark.parametrize("use_new_kcache_layout", [True, False])
def test_flash_decoding(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_attn_heads: int,
kv_group_num: int,
same_context_len: bool,
q_len: int,
use_alibi_slopes: bool,
use_new_kcache_layout: bool,
):
if use_new_kcache_layout and use_alibi_slopes:
# TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
# the code (alibi kernel) will be refactored later to avoid code duplication, when
# the whole triton flow with new k cache layout has been supported and tested.
# And tests for the alibi kernel using new kcache layout will be added then.
pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.6820844Z
torch.manual_seed(123)
torch.cuda.empty_cache()
>       torch.cuda.synchronize()
2025-04-11T04:23:18.6821121Z
tests/test_infer/test_kernels/triton/test_decoding_attn.py:100:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.6821387Z
device = None
2025-04-11T04:23:18.6821471Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6821806Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6823363Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
____________ test_flash_decoding[False-True-1-False-1-16-16-32-16] _____________
2025-04-11T04:23:18.6823871Z
bsz = 16, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
kv_group_num = 1, same_context_len = False, q_len = 1, use_alibi_slopes = True
use_new_kcache_layout = False
2025-04-11T04:23:18.6824277Z
@pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
@pytest.mark.parametrize("bsz", [7, 16])
@pytest.mark.parametrize("block_size", [16, 32])
@pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
@pytest.mark.parametrize("num_attn_heads", [16])
@pytest.mark.parametrize("kv_group_num", [1, 4])
@pytest.mark.parametrize("same_context_len", [True, False])
@pytest.mark.parametrize("q_len", [1, 5])
@pytest.mark.parametrize("use_alibi_slopes", [True, False])
@pytest.mark.parametrize("use_new_kcache_layout", [True, False])
def test_flash_decoding(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_attn_heads: int,
kv_group_num: int,
same_context_len: bool,
q_len: int,
use_alibi_slopes: bool,
use_new_kcache_layout: bool,
):
if use_new_kcache_layout and use_alibi_slopes:
# TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
# the code (alibi kernel) will be refactored later to avoid code duplication, when
# the whole triton flow with new k cache layout has been supported and tested.
# And tests for the alibi kernel using new kcache layout will be added then.
pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.6827625Z
torch.manual_seed(123)
torch.cuda.empty_cache()
>       torch.cuda.synchronize()
2025-04-11T04:23:18.6827897Z
tests/test_infer/test_kernels/triton/test_decoding_attn.py:100:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.6828164Z
device = None
2025-04-11T04:23:18.6828250Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6828629Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6830275Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
_____________ test_flash_decoding[False-True-1-False-4-16-8-16-7] ______________
2025-04-11T04:23:18.6830687Z
bsz = 7, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
kv_group_num = 4, same_context_len = False, q_len = 1, use_alibi_slopes = True
use_new_kcache_layout = False
2025-04-11T04:23:18.6831102Z
@pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
@pytest.mark.parametrize("bsz", [7, 16])
@pytest.mark.parametrize("block_size", [16, 32])
@pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
@pytest.mark.parametrize("num_attn_heads", [16])
@pytest.mark.parametrize("kv_group_num", [1, 4])
@pytest.mark.parametrize("same_context_len", [True, False])
@pytest.mark.parametrize("q_len", [1, 5])
@pytest.mark.parametrize("use_alibi_slopes", [True, False])
@pytest.mark.parametrize("use_new_kcache_layout", [True, False])
def test_flash_decoding(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_attn_heads: int,
kv_group_num: int,
same_context_len: bool,
q_len: int,
use_alibi_slopes: bool,
use_new_kcache_layout: bool,
):
if use_new_kcache_layout and use_alibi_slopes:
# TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
# the code (alibi kernel) will be refactored later to avoid code duplication, when
# the whole triton flow with new k cache layout has been supported and tested.
# And tests for the alibi kernel using new kcache layout will be added then.
pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.6834485Z
torch.manual_seed(123)
torch.cuda.empty_cache()
>       torch.cuda.synchronize()
2025-04-11T04:23:18.6834764Z
tests/test_infer/test_kernels/triton/test_decoding_attn.py:100:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.6835031Z
device = None
2025-04-11T04:23:18.6835115Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6835453Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6837191Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
_____________ test_flash_decoding[False-True-1-False-4-16-8-16-16] _____________
2025-04-11T04:23:18.6837709Z
bsz = 16, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
kv_group_num = 4, same_context_len = False, q_len = 1, use_alibi_slopes = True
use_new_kcache_layout = False
2025-04-11T04:23:18.6838122Z
@pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
@pytest.mark.parametrize("bsz", [7, 16])
@pytest.mark.parametrize("block_size", [16, 32])
@pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
@pytest.mark.parametrize("num_attn_heads", [16])
@pytest.mark.parametrize("kv_group_num", [1, 4])
@pytest.mark.parametrize("same_context_len", [True, False])
@pytest.mark.parametrize("q_len", [1, 5])
@pytest.mark.parametrize("use_alibi_slopes", [True, False])
@pytest.mark.parametrize("use_new_kcache_layout", [True, False])
def test_flash_decoding(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_attn_heads: int,
kv_group_num: int,
same_context_len: bool,
q_len: int,
use_alibi_slopes: bool,
use_new_kcache_layout: bool,
):
if use_new_kcache_layout and use_alibi_slopes:
# TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
# the code (alibi kernel) will be refactored later to avoid code duplication, when
# the whole triton flow with new k cache layout has been supported and tested.
# And tests for the alibi kernel using new kcache layout will be added then.
pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.6841388Z
torch.manual_seed(123)
torch.cuda.empty_cache()
>       torch.cuda.synchronize()
2025-04-11T04:23:18.6841657Z
tests/test_infer/test_kernels/triton/test_decoding_attn.py:100:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.6842011Z
device = None
2025-04-11T04:23:18.6842095Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6842434Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6844055Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
_____________ test_flash_decoding[False-True-1-False-4-16-8-32-7] ______________
2025-04-11T04:23:18.6844463Z
bsz = 7, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
kv_group_num = 4, same_context_len = False, q_len = 1, use_alibi_slopes = True
use_new_kcache_layout = False
2025-04-11T04:23:18.6844872Z
@pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
@pytest.mark.parametrize("bsz", [7, 16])
@pytest.mark.parametrize("block_size", [16, 32])
@pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
@pytest.mark.parametrize("num_attn_heads", [16])
@pytest.mark.parametrize("kv_group_num", [1, 4])
@pytest.mark.parametrize("same_context_len", [True, False])
@pytest.mark.parametrize("q_len", [1, 5])
@pytest.mark.parametrize("use_alibi_slopes", [True, False])
@pytest.mark.parametrize("use_new_kcache_layout", [True, False])
def test_flash_decoding(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_attn_heads: int,
kv_group_num: int,
same_context_len: bool,
q_len: int,
use_alibi_slopes: bool,
use_new_kcache_layout: bool,
):
if use_new_kcache_layout and use_alibi_slopes:
# TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
# the code (alibi kernel) will be refactored later to avoid code duplication, when
# the whole triton flow with new k cache layout has been supported and tested.
# And tests for the alibi kernel using new kcache layout will be added then.
pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.6848265Z
torch.manual_seed(123)
torch.cuda.empty_cache()
>       torch.cuda.synchronize()
2025-04-11T04:23:18.6848549Z
tests/test_infer/test_kernels/triton/test_decoding_attn.py:100:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.6848822Z
device = None
2025-04-11T04:23:18.6848902Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6849239Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6850895Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
_____________ test_flash_decoding[False-True-1-False-4-16-8-32-16] _____________
2025-04-11T04:23:18.6851318Z
bsz = 16, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
kv_group_num = 4, same_context_len = False, q_len = 1, use_alibi_slopes = True
use_new_kcache_layout = False
2025-04-11T04:23:18.6851731Z
@pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
@pytest.mark.parametrize("bsz", [7, 16])
@pytest.mark.parametrize("block_size", [16, 32])
@pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
@pytest.mark.parametrize("num_attn_heads", [16])
@pytest.mark.parametrize("kv_group_num", [1, 4])
@pytest.mark.parametrize("same_context_len", [True, False])
@pytest.mark.parametrize("q_len", [1, 5])
@pytest.mark.parametrize("use_alibi_slopes", [True, False])
@pytest.mark.parametrize("use_new_kcache_layout", [True, False])
def test_flash_decoding(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_attn_heads: int,
kv_group_num: int,
same_context_len: bool,
q_len: int,
use_alibi_slopes: bool,
use_new_kcache_layout: bool,
):
if use_new_kcache_layout and use_alibi_slopes:
# TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
# the code (alibi kernel) will be refactored later to avoid code duplication, when
# the whole triton flow with new k cache layout has been supported and tested.
# And tests for the alibi kernel using new kcache layout will be added then.
pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.6855128Z
torch.manual_seed(123)
torch.cuda.empty_cache()
>       torch.cuda.synchronize()
2025-04-11T04:23:18.6855404Z
tests/test_infer/test_kernels/triton/test_decoding_attn.py:100:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.6855682Z
device = None
2025-04-11T04:23:18.6855763Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6856190Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6857748Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
_____________ test_flash_decoding[False-True-1-False-4-16-16-16-7] _____________
2025-04-11T04:23:18.6858163Z
bsz = 7, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
kv_group_num = 4, same_context_len = False, q_len = 1, use_alibi_slopes = True
use_new_kcache_layout = False
2025-04-11T04:23:18.6858576Z
@pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
@pytest.mark.parametrize("bsz", [7, 16])
@pytest.mark.parametrize("block_size", [16, 32])
@pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
@pytest.mark.parametrize("num_attn_heads", [16])
@pytest.mark.parametrize("kv_group_num", [1, 4])
@pytest.mark.parametrize("same_context_len", [True, False])
@pytest.mark.parametrize("q_len", [1, 5])
@pytest.mark.parametrize("use_alibi_slopes", [True, False])
@pytest.mark.parametrize("use_new_kcache_layout", [True, False])
def test_flash_decoding(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_attn_heads: int,
kv_group_num: int,
same_context_len: bool,
q_len: int,
use_alibi_slopes: bool,
use_new_kcache_layout: bool,
):
if use_new_kcache_layout and use_alibi_slopes:
# TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
# the code (alibi kernel) will be refactored later to avoid code duplication, when
# the whole triton flow with new k cache layout has been supported and tested.
# And tests for the alibi kernel using new kcache layout will be added then.
pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.6861998Z
torch.manual_seed(123)
torch.cuda.empty_cache()
>       torch.cuda.synchronize()
2025-04-11T04:23:18.6862376Z
tests/test_infer/test_kernels/triton/test_decoding_attn.py:100:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.6862649Z
device = None
2025-04-11T04:23:18.6862729Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6863064Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6864633Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
____________ test_flash_decoding[False-True-1-False-4-16-16-16-16] _____________
2025-04-11T04:23:18.6865054Z
bsz = 16, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
kv_group_num = 4, same_context_len = False, q_len = 1, use_alibi_slopes = True
use_new_kcache_layout = False
2025-04-11T04:23:18.6865467Z
@pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
@pytest.mark.parametrize("bsz", [7, 16])
@pytest.mark.parametrize("block_size", [16, 32])
@pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
@pytest.mark.parametrize("num_attn_heads", [16])
@pytest.mark.parametrize("kv_group_num", [1, 4])
@pytest.mark.parametrize("same_context_len", [True, False])
@pytest.mark.parametrize("q_len", [1, 5])
@pytest.mark.parametrize("use_alibi_slopes", [True, False])
@pytest.mark.parametrize("use_new_kcache_layout", [True, False])
def test_flash_decoding(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_attn_heads: int,
kv_group_num: int,
same_context_len: bool,
q_len: int,
use_alibi_slopes: bool,
use_new_kcache_layout: bool,
):
if use_new_kcache_layout and use_alibi_slopes:
# TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
# the code (alibi kernel) will be refactored later to avoid code duplication, when
# the whole triton flow with new k cache layout has been supported and tested.
# And tests for the alibi kernel using new kcache layout will be added then.
pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.6868944Z
torch.manual_seed(123)
torch.cuda.empty_cache()
>       torch.cuda.synchronize()
2025-04-11T04:23:18.6869220Z
tests/test_infer/test_kernels/triton/test_decoding_attn.py:100:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.6869496Z
device = None
2025-04-11T04:23:18.6869575Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6869914Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6871466Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
_____________ test_flash_decoding[False-True-1-False-4-16-16-32-7] _____________
2025-04-11T04:23:18.6871876Z
bsz = 7, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
kv_group_num = 4, same_context_len = False, q_len = 1, use_alibi_slopes = True
use_new_kcache_layout = False
2025-04-11T04:23:18.6872385Z
@pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
@pytest.mark.parametrize("bsz", [7, 16])
@pytest.mark.parametrize("block_size", [16, 32])
@pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
@pytest.mark.parametrize("num_attn_heads", [16])
@pytest.mark.parametrize("kv_group_num", [1, 4])
@pytest.mark.parametrize("same_context_len", [True, False])
@pytest.mark.parametrize("q_len", [1, 5])
@pytest.mark.parametrize("use_alibi_slopes", [True, False])
@pytest.mark.parametrize("use_new_kcache_layout", [True, False])
def test_flash_decoding(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_attn_heads: int,
kv_group_num: int,
same_context_len: bool,
q_len: int,
use_alibi_slopes: bool,
use_new_kcache_layout: bool,
):
if use_new_kcache_layout and use_alibi_slopes:
# TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
# the code (alibi kernel) will be refactored later to avoid code duplication, when
# the whole triton flow with new k cache layout has been supported and tested.
# And tests for the alibi kernel using new kcache layout will be added then.
pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.6875755Z
torch.manual_seed(123)
torch.cuda.empty_cache()
>       torch.cuda.synchronize()
2025-04-11T04:23:18.6876029Z
tests/test_infer/test_kernels/triton/test_decoding_attn.py:100:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.6876301Z
device = None
2025-04-11T04:23:18.6876381Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6876717Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6878270Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
____________ test_flash_decoding[False-True-1-False-4-16-16-32-16] _____________
2025-04-11T04:23:18.6878777Z
bsz = 16, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
kv_group_num = 4, same_context_len = False, q_len = 1, use_alibi_slopes = True
use_new_kcache_layout = False
2025-04-11T04:23:18.6879192Z
@pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
@pytest.mark.parametrize("bsz", [7, 16])
@pytest.mark.parametrize("block_size", [16, 32])
@pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
@pytest.mark.parametrize("num_attn_heads", [16])
@pytest.mark.parametrize("kv_group_num", [1, 4])
@pytest.mark.parametrize("same_context_len", [True, False])
@pytest.mark.parametrize("q_len", [1, 5])
@pytest.mark.parametrize("use_alibi_slopes", [True, False])
@pytest.mark.parametrize("use_new_kcache_layout", [True, False])
def test_flash_decoding(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_attn_heads: int,
kv_group_num: int,
same_context_len: bool,
q_len: int,
use_alibi_slopes: bool,
use_new_kcache_layout: bool,
):
if use_new_kcache_layout and use_alibi_slopes:
# TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
# the code (alibi kernel) will be refactored later to avoid code duplication, when
# the whole triton flow with new k cache layout has been supported and tested.
# And tests for the alibi kernel using new kcache layout will be added then.
pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.6882554Z
torch.manual_seed(123)
torch.cuda.empty_cache()
>       torch.cuda.synchronize()
2025-04-11T04:23:18.6882825Z
tests/test_infer/test_kernels/triton/test_decoding_attn.py:100:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.6883096Z
device = None
2025-04-11T04:23:18.6883177Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6883514Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6885164Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
______________ test_flash_decoding[False-True-5-True-1-16-8-16-7] ______________
2025-04-11T04:23:18.6885579Z
bsz = 7, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
kv_group_num = 1, same_context_len = True, q_len = 5, use_alibi_slopes = True
use_new_kcache_layout = False
2025-04-11T04:23:18.6885984Z
@pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
@pytest.mark.parametrize("bsz", [7, 16])
@pytest.mark.parametrize("block_size", [16, 32])
@pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
@pytest.mark.parametrize("num_attn_heads", [16])
@pytest.mark.parametrize("kv_group_num", [1, 4])
@pytest.mark.parametrize("same_context_len", [True, False])
@pytest.mark.parametrize("q_len", [1, 5])
@pytest.mark.parametrize("use_alibi_slopes", [True, False])
@pytest.mark.parametrize("use_new_kcache_layout", [True, False])
def test_flash_decoding(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_attn_heads: int,
kv_group_num: int,
same_context_len: bool,
q_len: int,
use_alibi_slopes: bool,
use_new_kcache_layout: bool,
):
if use_new_kcache_layout and use_alibi_slopes:
# TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
# the code (alibi kernel) will be refactored later to avoid code duplication, when
# the whole triton flow with new k cache layout has been supported and tested.
# And tests for the alibi kernel using new kcache layout will be added then.
pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.6889367Z
torch.manual_seed(123)
torch.cuda.empty_cache()
>       torch.cuda.synchronize()
2025-04-11T04:23:18.6889644Z
tests/test_infer/test_kernels/triton/test_decoding_attn.py:100:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.6889911Z
device = None
2025-04-11T04:23:18.6889992Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6890341Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6892007Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
_____________ test_flash_decoding[False-True-5-True-1-16-8-16-16] ______________
2025-04-11T04:23:18.6892506Z
bsz = 16, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
kv_group_num = 1, same_context_len = True, q_len = 5, use_alibi_slopes = True
use_new_kcache_layout = False
2025-04-11T04:23:18.6892912Z
@pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
@pytest.mark.parametrize("bsz", [7, 16])
@pytest.mark.parametrize("block_size", [16, 32])
@pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
@pytest.mark.parametrize("num_attn_heads", [16])
@pytest.mark.parametrize("kv_group_num", [1, 4])
@pytest.mark.parametrize("same_context_len", [True, False])
@pytest.mark.parametrize("q_len", [1, 5])
@pytest.mark.parametrize("use_alibi_slopes", [True, False])
@pytest.mark.parametrize("use_new_kcache_layout", [True, False])
def test_flash_decoding(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_attn_heads: int,
kv_group_num: int,
same_context_len: bool,
q_len: int,
use_alibi_slopes: bool,
use_new_kcache_layout: bool,
):
if use_new_kcache_layout and use_alibi_slopes:
# TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
# the code (alibi kernel) will be refactored later to avoid code duplication, when
# the whole triton flow with new k cache layout has been supported and tested.
# And tests for the alibi kernel using new kcache layout will be added then.
pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.6896206Z
torch.manual_seed(123)
torch.cuda.empty_cache()
>       torch.cuda.synchronize()
2025-04-11T04:23:18.6896486Z
tests/test_infer/test_kernels/triton/test_decoding_attn.py:100:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.6896849Z
device = None
2025-04-11T04:23:18.6896936Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6897292Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6898931Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
______________ test_flash_decoding[False-True-5-True-1-16-8-32-7] ______________
2025-04-11T04:23:18.6899343Z
bsz = 7, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
kv_group_num = 1, same_context_len = True, q_len = 5, use_alibi_slopes = True
use_new_kcache_layout = False
2025-04-11T04:23:18.6899751Z
@pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
@pytest.mark.parametrize("bsz", [7, 16])
@pytest.mark.parametrize("block_size", [16, 32])
@pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
@pytest.mark.parametrize("num_attn_heads", [16])
@pytest.mark.parametrize("kv_group_num", [1, 4])
@pytest.mark.parametrize("same_context_len", [True, False])
@pytest.mark.parametrize("q_len", [1, 5])
@pytest.mark.parametrize("use_alibi_slopes", [True, False])
@pytest.mark.parametrize("use_new_kcache_layout", [True, False])
def test_flash_decoding(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_attn_heads: int,
kv_group_num: int,
same_context_len: bool,
q_len: int,
use_alibi_slopes: bool,
use_new_kcache_layout: bool,
):
if use_new_kcache_layout and use_alibi_slopes:
# TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
# the code (alibi kernel) will be refactored later to avoid code duplication, when
# the whole triton flow with new k cache layout has been supported and tested.
# And tests for the alibi kernel using new kcache layout will be added then.
pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.6903125Z
torch.manual_seed(123)
torch.cuda.empty_cache()
>       torch.cuda.synchronize()
2025-04-11T04:23:18.6903407Z
tests/test_infer/test_kernels/triton/test_decoding_attn.py:100:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.6903686Z
device = None
2025-04-11T04:23:18.6903769Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6904113Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6905760Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
_____________ test_flash_decoding[False-True-5-True-1-16-8-32-16] ______________
2025-04-11T04:23:18.6906174Z
bsz = 16, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
kv_group_num = 1, same_context_len = True, q_len = 5, use_alibi_slopes = True
use_new_kcache_layout = False
2025-04-11T04:23:18.6906582Z
@pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
@pytest.mark.parametrize("bsz", [7, 16])
@pytest.mark.parametrize("block_size", [16, 32])
@pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
@pytest.mark.parametrize("num_attn_heads", [16])
@pytest.mark.parametrize("kv_group_num", [1, 4])
@pytest.mark.parametrize("same_context_len", [True, False])
@pytest.mark.parametrize("q_len", [1, 5])
@pytest.mark.parametrize("use_alibi_slopes", [True, False])
@pytest.mark.parametrize("use_new_kcache_layout", [True, False])
def test_flash_decoding(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_attn_heads: int,
kv_group_num: int,
same_context_len: bool,
q_len: int,
use_alibi_slopes: bool,
use_new_kcache_layout: bool,
):
if use_new_kcache_layout and use_alibi_slopes:
# TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
# the code (alibi kernel) will be refactored later to avoid code duplication, when
# the whole triton flow with new k cache layout has been supported and tested.
# And tests for the alibi kernel using new kcache layout will be added then.
pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.6910020Z
torch.manual_seed(123)
torch.cuda.empty_cache()
>       torch.cuda.synchronize()
2025-04-11T04:23:18.6910298Z
tests/test_infer/test_kernels/triton/test_decoding_attn.py:100:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.6910577Z
device = None
2025-04-11T04:23:18.6910657Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6911093Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6912649Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
_____________ test_flash_decoding[False-True-5-True-1-16-16-16-7] ______________
2025-04-11T04:23:18.6913056Z
bsz = 7, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
kv_group_num = 1, same_context_len = True, q_len = 5, use_alibi_slopes = True
use_new_kcache_layout = False
2025-04-11T04:23:18.6913457Z
@pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
@pytest.mark.parametrize("bsz", [7, 16])
@pytest.mark.parametrize("block_size", [16, 32])
@pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
@pytest.mark.parametrize("num_attn_heads", [16])
@pytest.mark.parametrize("kv_group_num", [1, 4])
@pytest.mark.parametrize("same_context_len", [True, False])
@pytest.mark.parametrize("q_len", [1, 5])
@pytest.mark.parametrize("use_alibi_slopes", [True, False])
@pytest.mark.parametrize("use_new_kcache_layout", [True, False])
def test_flash_decoding(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_attn_heads: int,
kv_group_num: int,
same_context_len: bool,
q_len: int,
use_alibi_slopes: bool,
use_new_kcache_layout: bool,
):
if use_new_kcache_layout and use_alibi_slopes:
# TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
# the code (alibi kernel) will be refactored later to avoid code duplication, when
# the whole triton flow with new k cache layout has been supported and tested.
# And tests for the alibi kernel using new kcache layout will be added then.
pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.6916848Z
torch.manual_seed(123)
torch.cuda.empty_cache()
>       torch.cuda.synchronize()
2025-04-11T04:23:18.6917217Z
tests/test_infer/test_kernels/triton/test_decoding_attn.py:100:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.6917496Z
device = None
2025-04-11T04:23:18.6917576Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6917928Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6919488Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
_____________ test_flash_decoding[False-True-5-True-1-16-16-16-16] _____________
2025-04-11T04:23:18.6919901Z
bsz = 16, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
kv_group_num = 1, same_context_len = True, q_len = 5, use_alibi_slopes = True
use_new_kcache_layout = False
2025-04-11T04:23:18.6920307Z
@pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
@pytest.mark.parametrize("bsz", [7, 16])
@pytest.mark.parametrize("block_size", [16, 32])
@pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
@pytest.mark.parametrize("num_attn_heads", [16])
@pytest.mark.parametrize("kv_group_num", [1, 4])
@pytest.mark.parametrize("same_context_len", [True, False])
@pytest.mark.parametrize("q_len", [1, 5])
@pytest.mark.parametrize("use_alibi_slopes", [True, False])
@pytest.mark.parametrize("use_new_kcache_layout", [True, False])
def test_flash_decoding(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_attn_heads: int,
kv_group_num: int,
same_context_len: bool,
q_len: int,
use_alibi_slopes: bool,
use_new_kcache_layout: bool,
):
if use_new_kcache_layout and use_alibi_slopes:
# TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
# the code (alibi kernel) will be refactored later to avoid code duplication, when
# the whole triton flow with new k cache layout has been supported and tested.
# And tests for the alibi kernel using new kcache layout will be added then.
pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.6923772Z
torch.manual_seed(123)
torch.cuda.empty_cache()
>       torch.cuda.synchronize()
2025-04-11T04:23:18.6924039Z
tests/test_infer/test_kernels/triton/test_decoding_attn.py:100:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.6924313Z
device = None
2025-04-11T04:23:18.6924393Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6924737Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6926299Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
_____________ test_flash_decoding[False-True-5-True-1-16-16-32-7] ______________
2025-04-11T04:23:18.6926710Z
bsz = 7, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
kv_group_num = 1, same_context_len = True, q_len = 5, use_alibi_slopes = True
use_new_kcache_layout = False
2025-04-11T04:23:18.6927113Z
@pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
@pytest.mark.parametrize("bsz", [7, 16])
@pytest.mark.parametrize("block_size", [16, 32])
@pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
@pytest.mark.parametrize("num_attn_heads", [16])
@pytest.mark.parametrize("kv_group_num", [1, 4])
@pytest.mark.parametrize("same_context_len", [True, False])
@pytest.mark.parametrize("q_len", [1, 5])
@pytest.mark.parametrize("use_alibi_slopes", [True, False])
@pytest.mark.parametrize("use_new_kcache_layout", [True, False])
def test_flash_decoding(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_attn_heads: int,
kv_group_num: int,
same_context_len: bool,
q_len: int,
use_alibi_slopes: bool,
use_new_kcache_layout: bool,
):
if use_new_kcache_layout and use_alibi_slopes:
# TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
# the code (alibi kernel) will be refactored later to avoid code duplication, when
# the whole triton flow with new k cache layout has been supported and tested.
# And tests for the alibi kernel using new kcache layout will be added then.
pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.6930706Z
torch.manual_seed(123)
torch.cuda.empty_cache()
>       torch.cuda.synchronize()
2025-04-11T04:23:18.6930992Z
tests/test_infer/test_kernels/triton/test_decoding_attn.py:100:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.6931265Z
device = None
2025-04-11T04:23:18.6931346Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6931689Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6933261Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
_____________ test_flash_decoding[False-True-5-True-1-16-16-32-16] _____________
2025-04-11T04:23:18.6933762Z
bsz = 16, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
kv_group_num = 1, same_context_len = True, q_len = 5, use_alibi_slopes = True
use_new_kcache_layout = False
2025-04-11T04:23:18.6934173Z
@pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
@pytest.mark.parametrize("bsz", [7, 16])
@pytest.mark.parametrize("block_size", [16, 32])
@pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
@pytest.mark.parametrize("num_attn_heads", [16])
@pytest.mark.parametrize("kv_group_num", [1, 4])
@pytest.mark.parametrize("same_context_len", [True, False])
@pytest.mark.parametrize("q_len", [1, 5])
@pytest.mark.parametrize("use_alibi_slopes", [True, False])
@pytest.mark.parametrize("use_new_kcache_layout", [True, False])
def test_flash_decoding(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_attn_heads: int,
kv_group_num: int,
same_context_len: bool,
q_len: int,
use_alibi_slopes: bool,
use_new_kcache_layout: bool,
):
if use_new_kcache_layout and use_alibi_slopes:
# TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
# the code (alibi kernel) will be refactored later to avoid code duplication, when
# the whole triton flow with new k cache layout has been supported and tested.
# And tests for the alibi kernel using new kcache layout will be added then.
pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.6937541Z
torch.manual_seed(123)
torch.cuda.empty_cache()
>       torch.cuda.synchronize()
2025-04-11T04:23:18.6937810Z
tests/test_infer/test_kernels/triton/test_decoding_attn.py:100:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.6938078Z
device = None
2025-04-11T04:23:18.6938159Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6938508Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6940156Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
______________ test_flash_decoding[False-True-5-True-4-16-8-16-7] ______________
2025-04-11T04:23:18.6940571Z
bsz = 7, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
kv_group_num = 4, same_context_len = True, q_len = 5, use_alibi_slopes = True
use_new_kcache_layout = False
2025-04-11T04:23:18.6940975Z
@pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
@pytest.mark.parametrize("bsz", [7, 16])
@pytest.mark.parametrize("block_size", [16, 32])
@pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
@pytest.mark.parametrize("num_attn_heads", [16])
@pytest.mark.parametrize("kv_group_num", [1, 4])
@pytest.mark.parametrize("same_context_len", [True, False])
@pytest.mark.parametrize("q_len", [1, 5])
@pytest.mark.parametrize("use_alibi_slopes", [True, False])
@pytest.mark.parametrize("use_new_kcache_layout", [True, False])
def test_flash_decoding(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_attn_heads: int,
kv_group_num: int,
same_context_len: bool,
q_len: int,
use_alibi_slopes: bool,
use_new_kcache_layout: bool,
):
if use_new_kcache_layout and use_alibi_slopes:
# TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
# the code (alibi kernel) will be refactored later to avoid code duplication, when
# the whole triton flow with new k cache layout has been supported and tested.
# And tests for the alibi kernel using new kcache layout will be added then.
pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.6944327Z
torch.manual_seed(123)
torch.cuda.empty_cache()
>       torch.cuda.synchronize()
2025-04-11T04:23:18.6944601Z
tests/test_infer/test_kernels/triton/test_decoding_attn.py:100:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.6944871Z
device = None
2025-04-11T04:23:18.6944954Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6945290Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6946923Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
_____________ test_flash_decoding[False-True-5-True-4-16-8-16-16] ______________
2025-04-11T04:23:18.6947335Z
bsz = 16, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
kv_group_num = 4, same_context_len = True, q_len = 5, use_alibi_slopes = True
use_new_kcache_layout = False
2025-04-11T04:23:18.6947824Z
@pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
@pytest.mark.parametrize("bsz", [7, 16])
@pytest.mark.parametrize("block_size", [16, 32])
@pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
@pytest.mark.parametrize("num_attn_heads", [16])
@pytest.mark.parametrize("kv_group_num", [1, 4])
@pytest.mark.parametrize("same_context_len", [True, False])
@pytest.mark.parametrize("q_len", [1, 5])
@pytest.mark.parametrize("use_alibi_slopes", [True, False])
@pytest.mark.parametrize("use_new_kcache_layout", [True, False])
def test_flash_decoding(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_attn_heads: int,
kv_group_num: int,
same_context_len: bool,
q_len: int,
use_alibi_slopes: bool,
use_new_kcache_layout: bool,
):
if use_new_kcache_layout and use_alibi_slopes:
# TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
# the code (alibi kernel) will be refactored later to avoid code duplication, when
# the whole triton flow with new k cache layout has been supported and tested.
# And tests for the alibi kernel using new kcache layout will be added then.
pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.6951140Z
torch.manual_seed(123)
torch.cuda.empty_cache()
>       torch.cuda.synchronize()
2025-04-11T04:23:18.6951408Z
tests/test_infer/test_kernels/triton/test_decoding_attn.py:100:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.6951790Z
device = None
2025-04-11T04:23:18.6951876Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6952218Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6953861Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
______________ test_flash_decoding[False-True-5-True-4-16-8-32-7] ______________
2025-04-11T04:23:18.6954268Z
bsz = 7, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
kv_group_num = 4, same_context_len = True, q_len = 5, use_alibi_slopes = True
use_new_kcache_layout = False
2025-04-11T04:23:18.6954672Z
@pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
@pytest.mark.parametrize("bsz", [7, 16])
@pytest.mark.parametrize("block_size", [16, 32])
@pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
@pytest.mark.parametrize("num_attn_heads", [16])
@pytest.mark.parametrize("kv_group_num", [1, 4])
@pytest.mark.parametrize("same_context_len", [True, False])
@pytest.mark.parametrize("q_len", [1, 5])
@pytest.mark.parametrize("use_alibi_slopes", [True, False])
@pytest.mark.parametrize("use_new_kcache_layout", [True, False])
def test_flash_decoding(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_attn_heads: int,
kv_group_num: int,
same_context_len: bool,
q_len: int,
use_alibi_slopes: bool,
use_new_kcache_layout: bool,
):
if use_new_kcache_layout and use_alibi_slopes:
# TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
# the code (alibi kernel) will be refactored later to avoid code duplication, when
# the whole triton flow with new k cache layout has been supported and tested.
# And tests for the alibi kernel using new kcache layout will be added then.
pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.6958038Z
torch.manual_seed(123)
torch.cuda.empty_cache()
>       torch.cuda.synchronize()
2025-04-11T04:23:18.6958318Z
tests/test_infer/test_kernels/triton/test_decoding_attn.py:100:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.6958592Z
device = None
2025-04-11T04:23:18.6958676Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6959013Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6960645Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
_____________ test_flash_decoding[False-True-5-True-4-16-8-32-16] ______________
2025-04-11T04:23:18.6961060Z
bsz = 16, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
kv_group_num = 4, same_context_len = True, q_len = 5, use_alibi_slopes = True
use_new_kcache_layout = False
2025-04-11T04:23:18.6961472Z
@pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
@pytest.mark.parametrize("bsz", [7, 16])
@pytest.mark.parametrize("block_size", [16, 32])
@pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
@pytest.mark.parametrize("num_attn_heads", [16])
@pytest.mark.parametrize("kv_group_num", [1, 4])
@pytest.mark.parametrize("same_context_len", [True, False])
@pytest.mark.parametrize("q_len", [1, 5])
@pytest.mark.parametrize("use_alibi_slopes", [True, False])
@pytest.mark.parametrize("use_new_kcache_layout", [True, False])
def test_flash_decoding(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_attn_heads: int,
kv_group_num: int,
same_context_len: bool,
q_len: int,
use_alibi_slopes: bool,
use_new_kcache_layout: bool,
):
if use_new_kcache_layout and use_alibi_slopes:
# TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
# the code (alibi kernel) will be refactored later to avoid code duplication, when
# the whole triton flow with new k cache layout has been supported and tested.
# And tests for the alibi kernel using new kcache layout will be added then.
pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.6964850Z
torch.manual_seed(123)
torch.cuda.empty_cache()
>       torch.cuda.synchronize()
2025-04-11T04:23:18.6965126Z
tests/test_infer/test_kernels/triton/test_decoding_attn.py:100:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.6965397Z
device = None
2025-04-11T04:23:18.6965481Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6965904Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6967452Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
_____________ test_flash_decoding[False-True-5-True-4-16-16-16-7] ______________
2025-04-11T04:23:18.6967861Z
bsz = 7, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
kv_group_num = 4, same_context_len = True, q_len = 5, use_alibi_slopes = True
use_new_kcache_layout = False
2025-04-11T04:23:18.6968265Z
@pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
@pytest.mark.parametrize("bsz", [7, 16])
@pytest.mark.parametrize("block_size", [16, 32])
@pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
@pytest.mark.parametrize("num_attn_heads", [16])
@pytest.mark.parametrize("kv_group_num", [1, 4])
@pytest.mark.parametrize("same_context_len", [True, False])
@pytest.mark.parametrize("q_len", [1, 5])
@pytest.mark.parametrize("use_alibi_slopes", [True, False])
@pytest.mark.parametrize("use_new_kcache_layout", [True, False])
def test_flash_decoding(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_attn_heads: int,
kv_group_num: int,
same_context_len: bool,
q_len: int,
use_alibi_slopes: bool,
use_new_kcache_layout: bool,
):
if use_new_kcache_layout and use_alibi_slopes:
# TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
# the code (alibi kernel) will be refactored later to avoid code duplication, when
# the whole triton flow with new k cache layout has been supported and tested.
# And tests for the alibi kernel using new kcache layout will be added then.
pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.6971668Z
torch.manual_seed(123)
torch.cuda.empty_cache()
>       torch.cuda.synchronize()
2025-04-11T04:23:18.6972024Z
tests/test_infer/test_kernels/triton/test_decoding_attn.py:100:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.6972297Z
device = None
2025-04-11T04:23:18.6972380Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6972717Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6974263Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
_____________ test_flash_decoding[False-True-5-True-4-16-16-16-16] _____________
2025-04-11T04:23:18.6974684Z
bsz = 16, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
kv_group_num = 4, same_context_len = True, q_len = 5, use_alibi_slopes = True
use_new_kcache_layout = False
2025-04-11T04:23:18.6975102Z
@pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
@pytest.mark.parametrize("bsz", [7, 16])
@pytest.mark.parametrize("block_size", [16, 32])
@pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
@pytest.mark.parametrize("num_attn_heads", [16])
@pytest.mark.parametrize("kv_group_num", [1, 4])
@pytest.mark.parametrize("same_context_len", [True, False])
@pytest.mark.parametrize("q_len", [1, 5])
@pytest.mark.parametrize("use_alibi_slopes", [True, False])
@pytest.mark.parametrize("use_new_kcache_layout", [True, False])
def test_flash_decoding(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_attn_heads: int,
kv_group_num: int,
same_context_len: bool,
q_len: int,
use_alibi_slopes: bool,
use_new_kcache_layout: bool,
):
if use_new_kcache_layout and use_alibi_slopes:
# TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
# the code (alibi kernel) will be refactored later to avoid code duplication, when
# the whole triton flow with new k cache layout has been supported and tested.
# And tests for the alibi kernel using new kcache layout will be added then.
pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.6978587Z
torch.manual_seed(123)
torch.cuda.empty_cache()
>       torch.cuda.synchronize()
2025-04-11T04:23:18.6978858Z
tests/test_infer/test_kernels/triton/test_decoding_attn.py:100:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.6979134Z
device = None
2025-04-11T04:23:18.6979218Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6979557Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6981108Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
_____________ test_flash_decoding[False-True-5-True-4-16-16-32-7] ______________
2025-04-11T04:23:18.6981516Z
bsz = 7, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
kv_group_num = 4, same_context_len = True, q_len = 5, use_alibi_slopes = True
use_new_kcache_layout = False
2025-04-11T04:23:18.6981922Z
@pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
@pytest.mark.parametrize("bsz", [7, 16])
@pytest.mark.parametrize("block_size", [16, 32])
@pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
@pytest.mark.parametrize("num_attn_heads", [16])
@pytest.mark.parametrize("kv_group_num", [1, 4])
@pytest.mark.parametrize("same_context_len", [True, False])
@pytest.mark.parametrize("q_len", [1, 5])
@pytest.mark.parametrize("use_alibi_slopes", [True, False])
@pytest.mark.parametrize("use_new_kcache_layout", [True, False])
def test_flash_decoding(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_attn_heads: int,
kv_group_num: int,
same_context_len: bool,
q_len: int,
use_alibi_slopes: bool,
use_new_kcache_layout: bool,
):
if use_new_kcache_layout and use_alibi_slopes:
# TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
# the code (alibi kernel) will be refactored later to avoid code duplication, when
# the whole triton flow with new k cache layout has been supported and tested.
# And tests for the alibi kernel using new kcache layout will be added then.
pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.6985366Z
torch.manual_seed(123)
torch.cuda.empty_cache()
>       torch.cuda.synchronize()
2025-04-11T04:23:18.6985641Z
tests/test_infer/test_kernels/triton/test_decoding_attn.py:100:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.6985910Z
device = None
2025-04-11T04:23:18.6985992Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6986327Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6987926Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
_____________ test_flash_decoding[False-True-5-True-4-16-16-32-16] _____________
2025-04-11T04:23:18.6988463Z
bsz = 16, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
kv_group_num = 4, same_context_len = True, q_len = 5, use_alibi_slopes = True
use_new_kcache_layout = False
2025-04-11T04:23:18.6988877Z
@pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
@pytest.mark.parametrize("bsz", [7, 16])
@pytest.mark.parametrize("block_size", [16, 32])
@pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
@pytest.mark.parametrize("num_attn_heads", [16])
@pytest.mark.parametrize("kv_group_num", [1, 4])
@pytest.mark.parametrize("same_context_len", [True, False])
@pytest.mark.parametrize("q_len", [1, 5])
@pytest.mark.parametrize("use_alibi_slopes", [True, False])
@pytest.mark.parametrize("use_new_kcache_layout", [True, False])
def test_flash_decoding(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_attn_heads: int,
kv_group_num: int,
same_context_len: bool,
q_len: int,
use_alibi_slopes: bool,
use_new_kcache_layout: bool,
):
if use_new_kcache_layout and use_alibi_slopes:
# TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
# the code (alibi kernel) will be refactored later to avoid code duplication, when
# the whole triton flow with new k cache layout has been supported and tested.
# And tests for the alibi kernel using new kcache layout will be added then.
pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.6992364Z
torch.manual_seed(123)
torch.cuda.empty_cache()
>       torch.cuda.synchronize()
2025-04-11T04:23:18.6992653Z
tests/test_infer/test_kernels/triton/test_decoding_attn.py:100:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.6992940Z
device = None
2025-04-11T04:23:18.6993031Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.6993385Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.6995081Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
_____________ test_flash_decoding[False-True-5-False-1-16-8-16-7] ______________
2025-04-11T04:23:18.6995496Z
bsz = 7, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
kv_group_num = 1, same_context_len = False, q_len = 5, use_alibi_slopes = True
use_new_kcache_layout = False
2025-04-11T04:23:18.6995907Z
@pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
@pytest.mark.parametrize("bsz", [7, 16])
@pytest.mark.parametrize("block_size", [16, 32])
@pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
@pytest.mark.parametrize("num_attn_heads", [16])
@pytest.mark.parametrize("kv_group_num", [1, 4])
@pytest.mark.parametrize("same_context_len", [True, False])
@pytest.mark.parametrize("q_len", [1, 5])
@pytest.mark.parametrize("use_alibi_slopes", [True, False])
@pytest.mark.parametrize("use_new_kcache_layout", [True, False])
def test_flash_decoding(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_attn_heads: int,
kv_group_num: int,
same_context_len: bool,
q_len: int,
use_alibi_slopes: bool,
use_new_kcache_layout: bool,
):
if use_new_kcache_layout and use_alibi_slopes:
# TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
# the code (alibi kernel) will be refactored later to avoid code duplication, when
# the whole triton flow with new k cache layout has been supported and tested.
# And tests for the alibi kernel using new kcache layout will be added then.
pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.6999364Z
torch.manual_seed(123)
torch.cuda.empty_cache()
>       torch.cuda.synchronize()
2025-04-11T04:23:18.6999645Z
tests/test_infer/test_kernels/triton/test_decoding_attn.py:100:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.7046285Z
device = None
2025-04-11T04:23:18.7046423Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7046838Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7048705Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
_____________ test_flash_decoding[False-True-5-False-1-16-8-16-16] _____________
2025-04-11T04:23:18.7049146Z
bsz = 16, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
kv_group_num = 1, same_context_len = False, q_len = 5, use_alibi_slopes = True
use_new_kcache_layout = False
2025-04-11T04:23:18.7049692Z
@pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
@pytest.mark.parametrize("bsz", [7, 16])
@pytest.mark.parametrize("block_size", [16, 32])
@pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
@pytest.mark.parametrize("num_attn_heads", [16])
@pytest.mark.parametrize("kv_group_num", [1, 4])
@pytest.mark.parametrize("same_context_len", [True, False])
@pytest.mark.parametrize("q_len", [1, 5])
@pytest.mark.parametrize("use_alibi_slopes", [True, False])
@pytest.mark.parametrize("use_new_kcache_layout", [True, False])
def test_flash_decoding(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_attn_heads: int,
kv_group_num: int,
same_context_len: bool,
q_len: int,
use_alibi_slopes: bool,
use_new_kcache_layout: bool,
):
if use_new_kcache_layout and use_alibi_slopes:
# TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
# the code (alibi kernel) will be refactored later to avoid code duplication, when
# the whole triton flow with new k cache layout has been supported and tested.
# And tests for the alibi kernel using new kcache layout will be added then.
pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.7053071Z
torch.manual_seed(123)
torch.cuda.empty_cache()
>       torch.cuda.synchronize()
2025-04-11T04:23:18.7053348Z
tests/test_infer/test_kernels/triton/test_decoding_attn.py:100:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.7053719Z
device = None
2025-04-11T04:23:18.7053804Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7054157Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7055827Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
_____________ test_flash_decoding[False-True-5-False-1-16-8-32-7] ______________
2025-04-11T04:23:18.7056249Z
bsz = 7, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
kv_group_num = 1, same_context_len = False, q_len = 5, use_alibi_slopes = True
use_new_kcache_layout = False
2025-04-11T04:23:18.7056666Z
@pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
@pytest.mark.parametrize("bsz", [7, 16])
@pytest.mark.parametrize("block_size", [16, 32])
@pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
@pytest.mark.parametrize("num_attn_heads", [16])
@pytest.mark.parametrize("kv_group_num", [1, 4])
@pytest.mark.parametrize("same_context_len", [True, False])
@pytest.mark.parametrize("q_len", [1, 5])
@pytest.mark.parametrize("use_alibi_slopes", [True, False])
@pytest.mark.parametrize("use_new_kcache_layout", [True, False])
def test_flash_decoding(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_attn_heads: int,
kv_group_num: int,
same_context_len: bool,
q_len: int,
use_alibi_slopes: bool,
use_new_kcache_layout: bool,
):
if use_new_kcache_layout and use_alibi_slopes:
# TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
# the code (alibi kernel) will be refactored later to avoid code duplication, when
# the whole triton flow with new k cache layout has been supported and tested.
# And tests for the alibi kernel using new kcache layout will be added then.
pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.7060162Z
torch.manual_seed(123)
torch.cuda.empty_cache()
>       torch.cuda.synchronize()
2025-04-11T04:23:18.7060443Z
tests/test_infer/test_kernels/triton/test_decoding_attn.py:100:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.7060719Z
device = None
2025-04-11T04:23:18.7060799Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7061139Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7062805Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
_____________ test_flash_decoding[False-True-5-False-1-16-8-32-16] _____________
2025-04-11T04:23:18.7063222Z
bsz = 16, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
kv_group_num = 1, same_context_len = False, q_len = 5, use_alibi_slopes = True
use_new_kcache_layout = False
2025-04-11T04:23:18.7063631Z
@pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
@pytest.mark.parametrize("bsz", [7, 16])
@pytest.mark.parametrize("block_size", [16, 32])
@pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
@pytest.mark.parametrize("num_attn_heads", [16])
@pytest.mark.parametrize("kv_group_num", [1, 4])
@pytest.mark.parametrize("same_context_len", [True, False])
@pytest.mark.parametrize("q_len", [1, 5])
@pytest.mark.parametrize("use_alibi_slopes", [True, False])
@pytest.mark.parametrize("use_new_kcache_layout", [True, False])
def test_flash_decoding(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_attn_heads: int,
kv_group_num: int,
same_context_len: bool,
q_len: int,
use_alibi_slopes: bool,
use_new_kcache_layout: bool,
):
if use_new_kcache_layout and use_alibi_slopes:
# TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
# the code (alibi kernel) will be refactored later to avoid code duplication, when
# the whole triton flow with new k cache layout has been supported and tested.
# And tests for the alibi kernel using new kcache layout will be added then.
pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.7067004Z
torch.manual_seed(123)
torch.cuda.empty_cache()
>       torch.cuda.synchronize()
2025-04-11T04:23:18.7067274Z
tests/test_infer/test_kernels/triton/test_decoding_attn.py:100:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.7067549Z
device = None
2025-04-11T04:23:18.7067628Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7068072Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7069679Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
_____________ test_flash_decoding[False-True-5-False-1-16-16-16-7] _____________
2025-04-11T04:23:18.7070091Z
bsz = 7, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
kv_group_num = 1, same_context_len = False, q_len = 5, use_alibi_slopes = True
use_new_kcache_layout = False
2025-04-11T04:23:18.7070501Z
@pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
@pytest.mark.parametrize("bsz", [7, 16])
@pytest.mark.parametrize("block_size", [16, 32])
@pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
@pytest.mark.parametrize("num_attn_heads", [16])
@pytest.mark.parametrize("kv_group_num", [1, 4])
@pytest.mark.parametrize("same_context_len", [True, False])
@pytest.mark.parametrize("q_len", [1, 5])
@pytest.mark.parametrize("use_alibi_slopes", [True, False])
@pytest.mark.parametrize("use_new_kcache_layout", [True, False])
def test_flash_decoding(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_attn_heads: int,
kv_group_num: int,
same_context_len: bool,
q_len: int,
use_alibi_slopes: bool,
use_new_kcache_layout: bool,
):
if use_new_kcache_layout and use_alibi_slopes:
# TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
# the code (alibi kernel) will be refactored later to avoid code duplication, when
# the whole triton flow with new k cache layout has been supported and tested.
# And tests for the alibi kernel using new kcache layout will be added then.
pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.7073902Z
torch.manual_seed(123)
torch.cuda.empty_cache()
>       torch.cuda.synchronize()
2025-04-11T04:23:18.7074267Z
tests/test_infer/test_kernels/triton/test_decoding_attn.py:100:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.7074540Z
device = None
2025-04-11T04:23:18.7074619Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7074961Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7076504Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
____________ test_flash_decoding[False-True-5-False-1-16-16-16-16] _____________
2025-04-11T04:23:18.7076917Z
bsz = 16, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
kv_group_num = 1, same_context_len = False, q_len = 5, use_alibi_slopes = True
use_new_kcache_layout = False
2025-04-11T04:23:18.7077329Z
@pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
@pytest.mark.parametrize("bsz", [7, 16])
@pytest.mark.parametrize("block_size", [16, 32])
@pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
@pytest.mark.parametrize("num_attn_heads", [16])
@pytest.mark.parametrize("kv_group_num", [1, 4])
@pytest.mark.parametrize("same_context_len", [True, False])
@pytest.mark.parametrize("q_len", [1, 5])
@pytest.mark.parametrize("use_alibi_slopes", [True, False])
@pytest.mark.parametrize("use_new_kcache_layout", [True, False])
def test_flash_decoding(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_attn_heads: int,
kv_group_num: int,
same_context_len: bool,
q_len: int,
use_alibi_slopes: bool,
use_new_kcache_layout: bool,
):
if use_new_kcache_layout and use_alibi_slopes:
# TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
# the code (alibi kernel) will be refactored later to avoid code duplication, when
# the whole triton flow with new k cache layout has been supported and tested.
# And tests for the alibi kernel using new kcache layout will be added then.
pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.7080764Z
torch.manual_seed(123)
torch.cuda.empty_cache()
>       torch.cuda.synchronize()
2025-04-11T04:23:18.7081045Z
tests/test_infer/test_kernels/triton/test_decoding_attn.py:100:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.7081328Z
device = None
2025-04-11T04:23:18.7081410Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7081763Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7083357Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
_____________ test_flash_decoding[False-True-5-False-1-16-16-32-7] _____________
2025-04-11T04:23:18.7083776Z
bsz = 7, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
kv_group_num = 1, same_context_len = False, q_len = 5, use_alibi_slopes = True
use_new_kcache_layout = False
2025-04-11T04:23:18.7084195Z
@pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
@pytest.mark.parametrize("bsz", [7, 16])
@pytest.mark.parametrize("block_size", [16, 32])
@pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
@pytest.mark.parametrize("num_attn_heads", [16])
@pytest.mark.parametrize("kv_group_num", [1, 4])
@pytest.mark.parametrize("same_context_len", [True, False])
@pytest.mark.parametrize("q_len", [1, 5])
@pytest.mark.parametrize("use_alibi_slopes", [True, False])
@pytest.mark.parametrize("use_new_kcache_layout", [True, False])
def test_flash_decoding(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_attn_heads: int,
kv_group_num: int,
same_context_len: bool,
q_len: int,
use_alibi_slopes: bool,
use_new_kcache_layout: bool,
):
if use_new_kcache_layout and use_alibi_slopes:
# TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
# the code (alibi kernel) will be refactored later to avoid code duplication, when
# the whole triton flow with new k cache layout has been supported and tested.
# And tests for the alibi kernel using new kcache layout will be added then.
pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.7087684Z
torch.manual_seed(123)
torch.cuda.empty_cache()
>       torch.cuda.synchronize()
2025-04-11T04:23:18.7087951Z
tests/test_infer/test_kernels/triton/test_decoding_attn.py:100:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.7088223Z
device = None
2025-04-11T04:23:18.7088302Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7088646Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7090204Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
____________ test_flash_decoding[False-True-5-False-1-16-16-32-16] _____________
2025-04-11T04:23:18.7090722Z
bsz = 16, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
kv_group_num = 1, same_context_len = False, q_len = 5, use_alibi_slopes = True
use_new_kcache_layout = False
2025-04-11T04:23:18.7091135Z
@pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
@pytest.mark.parametrize("bsz", [7, 16])
@pytest.mark.parametrize("block_size", [16, 32])
@pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
@pytest.mark.parametrize("num_attn_heads", [16])
@pytest.mark.parametrize("kv_group_num", [1, 4])
@pytest.mark.parametrize("same_context_len", [True, False])
@pytest.mark.parametrize("q_len", [1, 5])
@pytest.mark.parametrize("use_alibi_slopes", [True, False])
@pytest.mark.parametrize("use_new_kcache_layout", [True, False])
def test_flash_decoding(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_attn_heads: int,
kv_group_num: int,
same_context_len: bool,
q_len: int,
use_alibi_slopes: bool,
use_new_kcache_layout: bool,
):
if use_new_kcache_layout and use_alibi_slopes:
# TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
# the code (alibi kernel) will be refactored later to avoid code duplication, when
# the whole triton flow with new k cache layout has been supported and tested.
# And tests for the alibi kernel using new kcache layout will be added then.
pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.7094495Z
torch.manual_seed(123)
torch.cuda.empty_cache()
>       torch.cuda.synchronize()
2025-04-11T04:23:18.7094765Z
tests/test_infer/test_kernels/triton/test_decoding_attn.py:100:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.7095040Z
device = None
2025-04-11T04:23:18.7095124Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7095462Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7097092Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
_____________ test_flash_decoding[False-True-5-False-4-16-8-16-7] ______________
2025-04-11T04:23:18.7097503Z
bsz = 7, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
kv_group_num = 4, same_context_len = False, q_len = 5, use_alibi_slopes = True
use_new_kcache_layout = False
2025-04-11T04:23:18.7097908Z
@pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
@pytest.mark.parametrize("bsz", [7, 16])
@pytest.mark.parametrize("block_size", [16, 32])
@pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
@pytest.mark.parametrize("num_attn_heads", [16])
@pytest.mark.parametrize("kv_group_num", [1, 4])
@pytest.mark.parametrize("same_context_len", [True, False])
@pytest.mark.parametrize("q_len", [1, 5])
@pytest.mark.parametrize("use_alibi_slopes", [True, False])
@pytest.mark.parametrize("use_new_kcache_layout", [True, False])
def test_flash_decoding(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_attn_heads: int,
kv_group_num: int,
same_context_len: bool,
q_len: int,
use_alibi_slopes: bool,
use_new_kcache_layout: bool,
):
if use_new_kcache_layout and use_alibi_slopes:
# TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
# the code (alibi kernel) will be refactored later to avoid code duplication, when
# the whole triton flow with new k cache layout has been supported and tested.
# And tests for the alibi kernel using new kcache layout will be added then.
pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.7101239Z
torch.manual_seed(123)
torch.cuda.empty_cache()
>       torch.cuda.synchronize()
2025-04-11T04:23:18.7101506Z
tests/test_infer/test_kernels/triton/test_decoding_attn.py:100:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.7101777Z
device = None
2025-04-11T04:23:18.7101859Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7102192Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7103824Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
_____________ test_flash_decoding[False-True-5-False-4-16-8-16-16] _____________
2025-04-11T04:23:18.7104237Z
bsz = 16, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
kv_group_num = 4, same_context_len = False, q_len = 5, use_alibi_slopes = True
use_new_kcache_layout = False
2025-04-11T04:23:18.7104729Z
@pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
@pytest.mark.parametrize("bsz", [7, 16])
@pytest.mark.parametrize("block_size", [16, 32])
@pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
@pytest.mark.parametrize("num_attn_heads", [16])
@pytest.mark.parametrize("kv_group_num", [1, 4])
@pytest.mark.parametrize("same_context_len", [True, False])
@pytest.mark.parametrize("q_len", [1, 5])
@pytest.mark.parametrize("use_alibi_slopes", [True, False])
@pytest.mark.parametrize("use_new_kcache_layout", [True, False])
def test_flash_decoding(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_attn_heads: int,
kv_group_num: int,
same_context_len: bool,
q_len: int,
use_alibi_slopes: bool,
use_new_kcache_layout: bool,
):
if use_new_kcache_layout and use_alibi_slopes:
# TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
# the code (alibi kernel) will be refactored later to avoid code duplication, when
# the whole triton flow with new k cache layout has been supported and tested.
# And tests for the alibi kernel using new kcache layout will be added then.
pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.7107974Z
torch.manual_seed(123)
torch.cuda.empty_cache()
>       torch.cuda.synchronize()
2025-04-11T04:23:18.7108233Z
tests/test_infer/test_kernels/triton/test_decoding_attn.py:100:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.7108644Z
device = None
2025-04-11T04:23:18.7108731Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7109075Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7110714Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
_____________ test_flash_decoding[False-True-5-False-4-16-8-32-7] ______________
2025-04-11T04:23:18.7111123Z
bsz = 7, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
kv_group_num = 4, same_context_len = False, q_len = 5, use_alibi_slopes = True
use_new_kcache_layout = False
2025-04-11T04:23:18.7111528Z
@pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
@pytest.mark.parametrize("bsz", [7, 16])
@pytest.mark.parametrize("block_size", [16, 32])
@pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
@pytest.mark.parametrize("num_attn_heads", [16])
@pytest.mark.parametrize("kv_group_num", [1, 4])
@pytest.mark.parametrize("same_context_len", [True, False])
@pytest.mark.parametrize("q_len", [1, 5])
@pytest.mark.parametrize("use_alibi_slopes", [True, False])
@pytest.mark.parametrize("use_new_kcache_layout", [True, False])
def test_flash_decoding(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_attn_heads: int,
kv_group_num: int,
same_context_len: bool,
q_len: int,
use_alibi_slopes: bool,
use_new_kcache_layout: bool,
):
if use_new_kcache_layout and use_alibi_slopes:
# TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
# the code (alibi kernel) will be refactored later to avoid code duplication, when
# the whole triton flow with new k cache layout has been supported and tested.
# And tests for the alibi kernel using new kcache layout will be added then.
pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.7114933Z
torch.manual_seed(123)
torch.cuda.empty_cache()
>       torch.cuda.synchronize()
2025-04-11T04:23:18.7115209Z
tests/test_infer/test_kernels/triton/test_decoding_attn.py:100:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.7115481Z
device = None
2025-04-11T04:23:18.7115562Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7115896Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7117528Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
_____________ test_flash_decoding[False-True-5-False-4-16-8-32-16] _____________
2025-04-11T04:23:18.7117936Z
bsz = 16, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
kv_group_num = 4, same_context_len = False, q_len = 5, use_alibi_slopes = True
use_new_kcache_layout = False
2025-04-11T04:23:18.7118344Z
@pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
@pytest.mark.parametrize("bsz", [7, 16])
@pytest.mark.parametrize("block_size", [16, 32])
@pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
@pytest.mark.parametrize("num_attn_heads", [16])
@pytest.mark.parametrize("kv_group_num", [1, 4])
@pytest.mark.parametrize("same_context_len", [True, False])
@pytest.mark.parametrize("q_len", [1, 5])
@pytest.mark.parametrize("use_alibi_slopes", [True, False])
@pytest.mark.parametrize("use_new_kcache_layout", [True, False])
def test_flash_decoding(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_attn_heads: int,
kv_group_num: int,
same_context_len: bool,
q_len: int,
use_alibi_slopes: bool,
use_new_kcache_layout: bool,
):
if use_new_kcache_layout and use_alibi_slopes:
# TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
# the code (alibi kernel) will be refactored later to avoid code duplication, when
# the whole triton flow with new k cache layout has been supported and tested.
# And tests for the alibi kernel using new kcache layout will be added then.
pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.7121694Z
torch.manual_seed(123)
torch.cuda.empty_cache()
>       torch.cuda.synchronize()
2025-04-11T04:23:18.7121956Z
tests/test_infer/test_kernels/triton/test_decoding_attn.py:100:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.7122231Z
device = None
2025-04-11T04:23:18.7122313Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7122731Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7124283Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
_____________ test_flash_decoding[False-True-5-False-4-16-16-16-7] _____________
2025-04-11T04:23:18.7124691Z
bsz = 7, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
kv_group_num = 4, same_context_len = False, q_len = 5, use_alibi_slopes = True
use_new_kcache_layout = False
2025-04-11T04:23:18.7125099Z
@pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
@pytest.mark.parametrize("bsz", [7, 16])
@pytest.mark.parametrize("block_size", [16, 32])
@pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
@pytest.mark.parametrize("num_attn_heads", [16])
@pytest.mark.parametrize("kv_group_num", [1, 4])
@pytest.mark.parametrize("same_context_len", [True, False])
@pytest.mark.parametrize("q_len", [1, 5])
@pytest.mark.parametrize("use_alibi_slopes", [True, False])
@pytest.mark.parametrize("use_new_kcache_layout", [True, False])
def test_flash_decoding(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_attn_heads: int,
kv_group_num: int,
same_context_len: bool,
q_len: int,
use_alibi_slopes: bool,
use_new_kcache_layout: bool,
):
if use_new_kcache_layout and use_alibi_slopes:
# TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
# the code (alibi kernel) will be refactored later to avoid code duplication, when
# the whole triton flow with new k cache layout has been supported and tested.
# And tests for the alibi kernel using new kcache layout will be added then.
pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.7128468Z
torch.manual_seed(123)
torch.cuda.empty_cache()
>       torch.cuda.synchronize()
2025-04-11T04:23:18.7128816Z
tests/test_infer/test_kernels/triton/test_decoding_attn.py:100:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.7129088Z
device = None
2025-04-11T04:23:18.7129168Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7129503Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7131051Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
____________ test_flash_decoding[False-True-5-False-4-16-16-16-16] _____________
2025-04-11T04:23:18.7131466Z
bsz = 16, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
kv_group_num = 4, same_context_len = False, q_len = 5, use_alibi_slopes = True
use_new_kcache_layout = False
2025-04-11T04:23:18.7131877Z
@pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
@pytest.mark.parametrize("bsz", [7, 16])
@pytest.mark.parametrize("block_size", [16, 32])
@pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
@pytest.mark.parametrize("num_attn_heads", [16])
@pytest.mark.parametrize("kv_group_num", [1, 4])
@pytest.mark.parametrize("same_context_len", [True, False])
@pytest.mark.parametrize("q_len", [1, 5])
@pytest.mark.parametrize("use_alibi_slopes", [True, False])
@pytest.mark.parametrize("use_new_kcache_layout", [True, False])
def test_flash_decoding(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_attn_heads: int,
kv_group_num: int,
same_context_len: bool,
q_len: int,
use_alibi_slopes: bool,
use_new_kcache_layout: bool,
):
if use_new_kcache_layout and use_alibi_slopes:
# TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
# the code (alibi kernel) will be refactored later to avoid code duplication, when
# the whole triton flow with new k cache layout has been supported and tested.
# And tests for the alibi kernel using new kcache layout will be added then.
pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.7135326Z
torch.manual_seed(123)
torch.cuda.empty_cache()
>       torch.cuda.synchronize()
2025-04-11T04:23:18.7135587Z
tests/test_infer/test_kernels/triton/test_decoding_attn.py:100:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.7135859Z
device = None
2025-04-11T04:23:18.7135940Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7136275Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7137821Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
_____________ test_flash_decoding[False-True-5-False-4-16-16-32-7] _____________
2025-04-11T04:23:18.7138229Z
bsz = 7, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
kv_group_num = 4, same_context_len = False, q_len = 5, use_alibi_slopes = True
use_new_kcache_layout = False
2025-04-11T04:23:18.7138633Z
@pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
@pytest.mark.parametrize("bsz", [7, 16])
@pytest.mark.parametrize("block_size", [16, 32])
@pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
@pytest.mark.parametrize("num_attn_heads", [16])
@pytest.mark.parametrize("kv_group_num", [1, 4])
@pytest.mark.parametrize("same_context_len", [True, False])
@pytest.mark.parametrize("q_len", [1, 5])
@pytest.mark.parametrize("use_alibi_slopes", [True, False])
@pytest.mark.parametrize("use_new_kcache_layout", [True, False])
def test_flash_decoding(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_attn_heads: int,
kv_group_num: int,
same_context_len: bool,
q_len: int,
use_alibi_slopes: bool,
use_new_kcache_layout: bool,
):
if use_new_kcache_layout and use_alibi_slopes:
# TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
# the code (alibi kernel) will be refactored later to avoid code duplication, when
# the whole triton flow with new k cache layout has been supported and tested.
# And tests for the alibi kernel using new kcache layout will be added then.
pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.7142084Z
torch.manual_seed(123)
torch.cuda.empty_cache()
>       torch.cuda.synchronize()
2025-04-11T04:23:18.7142351Z
tests/test_infer/test_kernels/triton/test_decoding_attn.py:100:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.7142623Z
device = None
2025-04-11T04:23:18.7142706Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7143044Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7144588Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
____________ test_flash_decoding[False-True-5-False-4-16-16-32-16] _____________
2025-04-11T04:23:18.7145109Z
bsz = 16, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
kv_group_num = 4, same_context_len = False, q_len = 5, use_alibi_slopes = True
use_new_kcache_layout = False
2025-04-11T04:23:18.7145520Z
@pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
@pytest.mark.parametrize("bsz", [7, 16])
@pytest.mark.parametrize("block_size", [16, 32])
@pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
@pytest.mark.parametrize("num_attn_heads", [16])
@pytest.mark.parametrize("kv_group_num", [1, 4])
@pytest.mark.parametrize("same_context_len", [True, False])
@pytest.mark.parametrize("q_len", [1, 5])
@pytest.mark.parametrize("use_alibi_slopes", [True, False])
@pytest.mark.parametrize("use_new_kcache_layout", [True, False])
def test_flash_decoding(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_attn_heads: int,
kv_group_num: int,
same_context_len: bool,
q_len: int,
use_alibi_slopes: bool,
use_new_kcache_layout: bool,
):
if use_new_kcache_layout and use_alibi_slopes:
# TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
# the code (alibi kernel) will be refactored later to avoid code duplication, when
# the whole triton flow with new k cache layout has been supported and tested.
# And tests for the alibi kernel using new kcache layout will be added then.
pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.7148911Z
torch.manual_seed(123)
torch.cuda.empty_cache()
>       torch.cuda.synchronize()
2025-04-11T04:23:18.7149173Z
tests/test_infer/test_kernels/triton/test_decoding_attn.py:100:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.7149444Z
device = None
2025-04-11T04:23:18.7149530Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7149870Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7151514Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
_____________ test_flash_decoding[False-False-1-True-1-16-8-16-7] ______________
2025-04-11T04:23:18.7151922Z
bsz = 7, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
kv_group_num = 1, same_context_len = True, q_len = 1, use_alibi_slopes = False
use_new_kcache_layout = False
2025-04-11T04:23:18.7152329Z
@pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
@pytest.mark.parametrize("bsz", [7, 16])
@pytest.mark.parametrize("block_size", [16, 32])
@pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
@pytest.mark.parametrize("num_attn_heads", [16])
@pytest.mark.parametrize("kv_group_num", [1, 4])
@pytest.mark.parametrize("same_context_len", [True, False])
@pytest.mark.parametrize("q_len", [1, 5])
@pytest.mark.parametrize("use_alibi_slopes", [True, False])
@pytest.mark.parametrize("use_new_kcache_layout", [True, False])
def test_flash_decoding(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_attn_heads: int,
kv_group_num: int,
same_context_len: bool,
q_len: int,
use_alibi_slopes: bool,
use_new_kcache_layout: bool,
):
if use_new_kcache_layout and use_alibi_slopes:
# TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
# the code (alibi kernel) will be refactored later to avoid code duplication, when
# the whole triton flow with new k cache layout has been supported and tested.
# And tests for the alibi kernel using new kcache layout will be added then.
pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.7155707Z
torch.manual_seed(123)
torch.cuda.empty_cache()
>       torch.cuda.synchronize()
2025-04-11T04:23:18.7155981Z
tests/test_infer/test_kernels/triton/test_decoding_attn.py:100:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.7156247Z
device = None
2025-04-11T04:23:18.7156331Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7156665Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7158445Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
_____________ test_flash_decoding[False-False-1-True-1-16-8-16-16] _____________
2025-04-11T04:23:18.7158861Z
bsz = 16, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
kv_group_num = 1, same_context_len = True, q_len = 1, use_alibi_slopes = False
use_new_kcache_layout = False
2025-04-11T04:23:18.7159371Z
@pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
@pytest.mark.parametrize("bsz", [7, 16])
@pytest.mark.parametrize("block_size", [16, 32])
@pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
@pytest.mark.parametrize("num_attn_heads", [16])
@pytest.mark.parametrize("kv_group_num", [1, 4])
@pytest.mark.parametrize("same_context_len", [True, False])
@pytest.mark.parametrize("q_len", [1, 5])
@pytest.mark.parametrize("use_alibi_slopes", [True, False])
@pytest.mark.parametrize("use_new_kcache_layout", [True, False])
def test_flash_decoding(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_attn_heads: int,
kv_group_num: int,
same_context_len: bool,
q_len: int,
use_alibi_slopes: bool,
use_new_kcache_layout: bool,
):
if use_new_kcache_layout and use_alibi_slopes:
# TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
# the code (alibi kernel) will be refactored later to avoid code duplication, when
# the whole triton flow with new k cache layout has been supported and tested.
# And tests for the alibi kernel using new kcache layout will be added then.
pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.7162652Z
torch.manual_seed(123)
torch.cuda.empty_cache()
>       torch.cuda.synchronize()
2025-04-11T04:23:18.7162928Z
tests/test_infer/test_kernels/triton/test_decoding_attn.py:100:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.7163194Z
device = None
2025-04-11T04:23:18.7163362Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7163704Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7165341Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
_____________ test_flash_decoding[False-False-1-True-1-16-8-32-7] ______________
2025-04-11T04:23:18.7165757Z
bsz = 7, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
kv_group_num = 1, same_context_len = True, q_len = 1, use_alibi_slopes = False
use_new_kcache_layout = False
2025-04-11T04:23:18.7166170Z
@pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
@pytest.mark.parametrize("bsz", [7, 16])
@pytest.mark.parametrize("block_size", [16, 32])
@pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
@pytest.mark.parametrize("num_attn_heads", [16])
@pytest.mark.parametrize("kv_group_num", [1, 4])
@pytest.mark.parametrize("same_context_len", [True, False])
@pytest.mark.parametrize("q_len", [1, 5])
@pytest.mark.parametrize("use_alibi_slopes", [True, False])
@pytest.mark.parametrize("use_new_kcache_layout", [True, False])
def test_flash_decoding(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_attn_heads: int,
kv_group_num: int,
same_context_len: bool,
q_len: int,
use_alibi_slopes: bool,
use_new_kcache_layout: bool,
):
if use_new_kcache_layout and use_alibi_slopes:
# TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
# the code (alibi kernel) will be refactored later to avoid code duplication, when
# the whole triton flow with new k cache layout has been supported and tested.
# And tests for the alibi kernel using new kcache layout will be added then.
pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.7169544Z
torch.manual_seed(123)
torch.cuda.empty_cache()
>       torch.cuda.synchronize()
2025-04-11T04:23:18.7169824Z
tests/test_infer/test_kernels/triton/test_decoding_attn.py:100:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.7170092Z
device = None
2025-04-11T04:23:18.7170179Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7170516Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7172149Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
_____________ test_flash_decoding[False-False-1-True-1-16-8-32-16] _____________
2025-04-11T04:23:18.7172558Z
bsz = 16, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
kv_group_num = 1, same_context_len = True, q_len = 1, use_alibi_slopes = False
use_new_kcache_layout = False
2025-04-11T04:23:18.7172969Z
@pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
@pytest.mark.parametrize("bsz", [7, 16])
@pytest.mark.parametrize("block_size", [16, 32])
@pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
@pytest.mark.parametrize("num_attn_heads", [16])
@pytest.mark.parametrize("kv_group_num", [1, 4])
@pytest.mark.parametrize("same_context_len", [True, False])
@pytest.mark.parametrize("q_len", [1, 5])
@pytest.mark.parametrize("use_alibi_slopes", [True, False])
@pytest.mark.parametrize("use_new_kcache_layout", [True, False])
def test_flash_decoding(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_attn_heads: int,
kv_group_num: int,
same_context_len: bool,
q_len: int,
use_alibi_slopes: bool,
use_new_kcache_layout: bool,
):
if use_new_kcache_layout and use_alibi_slopes:
# TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
# the code (alibi kernel) will be refactored later to avoid code duplication, when
# the whole triton flow with new k cache layout has been supported and tested.
# And tests for the alibi kernel using new kcache layout will be added then.
pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.7176334Z
torch.manual_seed(123)
torch.cuda.empty_cache()
>       torch.cuda.synchronize()
2025-04-11T04:23:18.7176600Z
tests/test_infer/test_kernels/triton/test_decoding_attn.py:100:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.7176873Z
device = None
2025-04-11T04:23:18.7176955Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7177385Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7178933Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
_____________ test_flash_decoding[False-False-1-True-1-16-16-16-7] _____________
2025-04-11T04:23:18.7179343Z
bsz = 7, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
kv_group_num = 1, same_context_len = True, q_len = 1, use_alibi_slopes = False
use_new_kcache_layout = False
2025-04-11T04:23:18.7179758Z
@pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
@pytest.mark.parametrize("bsz", [7, 16])
@pytest.mark.parametrize("block_size", [16, 32])
@pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
@pytest.mark.parametrize("num_attn_heads", [16])
@pytest.mark.parametrize("kv_group_num", [1, 4])
@pytest.mark.parametrize("same_context_len", [True, False])
@pytest.mark.parametrize("q_len", [1, 5])
@pytest.mark.parametrize("use_alibi_slopes", [True, False])
@pytest.mark.parametrize("use_new_kcache_layout", [True, False])
def test_flash_decoding(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_attn_heads: int,
kv_group_num: int,
same_context_len: bool,
q_len: int,
use_alibi_slopes: bool,
use_new_kcache_layout: bool,
):
if use_new_kcache_layout and use_alibi_slopes:
# TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
# the code (alibi kernel) will be refactored later to avoid code duplication, when
# the whole triton flow with new k cache layout has been supported and tested.
# And tests for the alibi kernel using new kcache layout will be added then.
pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.7183146Z
torch.manual_seed(123)
torch.cuda.empty_cache()
>       torch.cuda.synchronize()
2025-04-11T04:23:18.7183500Z
tests/test_infer/test_kernels/triton/test_decoding_attn.py:100:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.7183775Z
device = None
2025-04-11T04:23:18.7183854Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7184192Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7185738Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
____________ test_flash_decoding[False-False-1-True-1-16-16-16-16] _____________
2025-04-11T04:23:18.7186153Z
bsz = 16, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
kv_group_num = 1, same_context_len = True, q_len = 1, use_alibi_slopes = False
use_new_kcache_layout = False
2025-04-11T04:23:18.7186569Z
@pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
@pytest.mark.parametrize("bsz", [7, 16])
@pytest.mark.parametrize("block_size", [16, 32])
@pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
@pytest.mark.parametrize("num_attn_heads", [16])
@pytest.mark.parametrize("kv_group_num", [1, 4])
@pytest.mark.parametrize("same_context_len", [True, False])
@pytest.mark.parametrize("q_len", [1, 5])
@pytest.mark.parametrize("use_alibi_slopes", [True, False])
@pytest.mark.parametrize("use_new_kcache_layout", [True, False])
def test_flash_decoding(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_attn_heads: int,
kv_group_num: int,
same_context_len: bool,
q_len: int,
use_alibi_slopes: bool,
use_new_kcache_layout: bool,
):
if use_new_kcache_layout and use_alibi_slopes:
# TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
# the code (alibi kernel) will be refactored later to avoid code duplication, when
# the whole triton flow with new k cache layout has been supported and tested.
# And tests for the alibi kernel using new kcache layout will be added then.
pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.7190035Z
torch.manual_seed(123)
torch.cuda.empty_cache()
>       torch.cuda.synchronize()
2025-04-11T04:23:18.7190306Z
tests/test_infer/test_kernels/triton/test_decoding_attn.py:100:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.7190578Z
device = None
2025-04-11T04:23:18.7190661Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7191002Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7192548Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
_____________ test_flash_decoding[False-False-1-True-1-16-16-32-7] _____________
2025-04-11T04:23:18.7192960Z
bsz = 7, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
kv_group_num = 1, same_context_len = True, q_len = 1, use_alibi_slopes = False
use_new_kcache_layout = False
2025-04-11T04:23:18.7193372Z
@pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
@pytest.mark.parametrize("bsz", [7, 16])
@pytest.mark.parametrize("block_size", [16, 32])
@pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
@pytest.mark.parametrize("num_attn_heads", [16])
@pytest.mark.parametrize("kv_group_num", [1, 4])
@pytest.mark.parametrize("same_context_len", [True, False])
@pytest.mark.parametrize("q_len", [1, 5])
@pytest.mark.parametrize("use_alibi_slopes", [True, False])
@pytest.mark.parametrize("use_new_kcache_layout", [True, False])
def test_flash_decoding(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_attn_heads: int,
kv_group_num: int,
same_context_len: bool,
q_len: int,
use_alibi_slopes: bool,
use_new_kcache_layout: bool,
):
if use_new_kcache_layout and use_alibi_slopes:
# TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
# the code (alibi kernel) will be refactored later to avoid code duplication, when
# the whole triton flow with new k cache layout has been supported and tested.
# And tests for the alibi kernel using new kcache layout will be added then.
pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.7196830Z
torch.manual_seed(123)
torch.cuda.empty_cache()
>       torch.cuda.synchronize()
2025-04-11T04:23:18.7197102Z
tests/test_infer/test_kernels/triton/test_decoding_attn.py:100:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.7197374Z
device = None
2025-04-11T04:23:18.7197454Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7197790Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7199343Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
____________ test_flash_decoding[False-False-1-True-1-16-16-32-16] _____________
2025-04-11T04:23:18.7199845Z
bsz = 16, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
kv_group_num = 1, same_context_len = True, q_len = 1, use_alibi_slopes = False
use_new_kcache_layout = False
2025-04-11T04:23:18.7200269Z
@pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
@pytest.mark.parametrize("bsz", [7, 16])
@pytest.mark.parametrize("block_size", [16, 32])
@pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
@pytest.mark.parametrize("num_attn_heads", [16])
@pytest.mark.parametrize("kv_group_num", [1, 4])
@pytest.mark.parametrize("same_context_len", [True, False])
@pytest.mark.parametrize("q_len", [1, 5])
@pytest.mark.parametrize("use_alibi_slopes", [True, False])
@pytest.mark.parametrize("use_new_kcache_layout", [True, False])
def test_flash_decoding(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_attn_heads: int,
kv_group_num: int,
same_context_len: bool,
q_len: int,
use_alibi_slopes: bool,
use_new_kcache_layout: bool,
):
if use_new_kcache_layout and use_alibi_slopes:
# TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
# the code (alibi kernel) will be refactored later to avoid code duplication, when
# the whole triton flow with new k cache layout has been supported and tested.
# And tests for the alibi kernel using new kcache layout will be added then.
pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.7203668Z
torch.manual_seed(123)
torch.cuda.empty_cache()
>       torch.cuda.synchronize()
2025-04-11T04:23:18.7203937Z
tests/test_infer/test_kernels/triton/test_decoding_attn.py:100:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.7204204Z
device = None
2025-04-11T04:23:18.7204287Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7204628Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7206293Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
_____________ test_flash_decoding[False-False-1-True-4-16-8-16-7] ______________
2025-04-11T04:23:18.7206705Z
bsz = 7, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
kv_group_num = 4, same_context_len = True, q_len = 1, use_alibi_slopes = False
use_new_kcache_layout = False
2025-04-11T04:23:18.7207114Z
@pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
@pytest.mark.parametrize("bsz", [7, 16])
@pytest.mark.parametrize("block_size", [16, 32])
@pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
@pytest.mark.parametrize("num_attn_heads", [16])
@pytest.mark.parametrize("kv_group_num", [1, 4])
@pytest.mark.parametrize("same_context_len", [True, False])
@pytest.mark.parametrize("q_len", [1, 5])
@pytest.mark.parametrize("use_alibi_slopes", [True, False])
@pytest.mark.parametrize("use_new_kcache_layout", [True, False])
def test_flash_decoding(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_attn_heads: int,
kv_group_num: int,
same_context_len: bool,
q_len: int,
use_alibi_slopes: bool,
use_new_kcache_layout: bool,
):
if use_new_kcache_layout and use_alibi_slopes:
# TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
# the code (alibi kernel) will be refactored later to avoid code duplication, when
# the whole triton flow with new k cache layout has been supported and tested.
# And tests for the alibi kernel using new kcache layout will be added then.
pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.7210470Z
torch.manual_seed(123)
torch.cuda.empty_cache()
>       torch.cuda.synchronize()
2025-04-11T04:23:18.7210737Z
tests/test_infer/test_kernels/triton/test_decoding_attn.py:100:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.7211002Z
device = None
2025-04-11T04:23:18.7211080Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7211415Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7213057Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
_____________ test_flash_decoding[False-False-1-True-4-16-8-16-16] _____________
2025-04-11T04:23:18.7213465Z
bsz = 16, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
kv_group_num = 4, same_context_len = True, q_len = 1, use_alibi_slopes = False
use_new_kcache_layout = False
2025-04-11T04:23:18.7213965Z
@pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
@pytest.mark.parametrize("bsz", [7, 16])
@pytest.mark.parametrize("block_size", [16, 32])
@pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
@pytest.mark.parametrize("num_attn_heads", [16])
@pytest.mark.parametrize("kv_group_num", [1, 4])
@pytest.mark.parametrize("same_context_len", [True, False])
@pytest.mark.parametrize("q_len", [1, 5])
@pytest.mark.parametrize("use_alibi_slopes", [True, False])
@pytest.mark.parametrize("use_new_kcache_layout", [True, False])
def test_flash_decoding(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_attn_heads: int,
kv_group_num: int,
same_context_len: bool,
q_len: int,
use_alibi_slopes: bool,
use_new_kcache_layout: bool,
):
if use_new_kcache_layout and use_alibi_slopes:
# TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
# the code (alibi kernel) will be refactored later to avoid code duplication, when
# the whole triton flow with new k cache layout has been supported and tested.
# And tests for the alibi kernel using new kcache layout will be added then.
pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.7217252Z
torch.manual_seed(123)
torch.cuda.empty_cache()
>       torch.cuda.synchronize()
2025-04-11T04:23:18.7217519Z
tests/test_infer/test_kernels/triton/test_decoding_attn.py:100:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.7217787Z
device = None
2025-04-11T04:23:18.7217953Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7218296Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7219945Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
_____________ test_flash_decoding[False-False-1-True-4-16-8-32-7] ______________
2025-04-11T04:23:18.7220347Z
bsz = 7, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
kv_group_num = 4, same_context_len = True, q_len = 1, use_alibi_slopes = False
use_new_kcache_layout = False
2025-04-11T04:23:18.7220755Z
@pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
@pytest.mark.parametrize("bsz", [7, 16])
@pytest.mark.parametrize("block_size", [16, 32])
@pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
@pytest.mark.parametrize("num_attn_heads", [16])
@pytest.mark.parametrize("kv_group_num", [1, 4])
@pytest.mark.parametrize("same_context_len", [True, False])
@pytest.mark.parametrize("q_len", [1, 5])
@pytest.mark.parametrize("use_alibi_slopes", [True, False])
@pytest.mark.parametrize("use_new_kcache_layout", [True, False])
def test_flash_decoding(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_attn_heads: int,
kv_group_num: int,
same_context_len: bool,
q_len: int,
use_alibi_slopes: bool,
use_new_kcache_layout: bool,
):
if use_new_kcache_layout and use_alibi_slopes:
# TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
# the code (alibi kernel) will be refactored later to avoid code duplication, when
# the whole triton flow with new k cache layout has been supported and tested.
# And tests for the alibi kernel using new kcache layout will be added then.
pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.7224102Z
torch.manual_seed(123)
torch.cuda.empty_cache()
>       torch.cuda.synchronize()
2025-04-11T04:23:18.7224375Z
tests/test_infer/test_kernels/triton/test_decoding_attn.py:100:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.7224644Z
device = None
2025-04-11T04:23:18.7224723Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7225058Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7226700Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
_____________ test_flash_decoding[False-False-1-True-4-16-8-32-16] _____________
2025-04-11T04:23:18.7227108Z
bsz = 16, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
kv_group_num = 4, same_context_len = True, q_len = 1, use_alibi_slopes = False
use_new_kcache_layout = False
2025-04-11T04:23:18.7227516Z
@pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
@pytest.mark.parametrize("bsz", [7, 16])
@pytest.mark.parametrize("block_size", [16, 32])
@pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
@pytest.mark.parametrize("num_attn_heads", [16])
@pytest.mark.parametrize("kv_group_num", [1, 4])
@pytest.mark.parametrize("same_context_len", [True, False])
@pytest.mark.parametrize("q_len", [1, 5])
@pytest.mark.parametrize("use_alibi_slopes", [True, False])
@pytest.mark.parametrize("use_new_kcache_layout", [True, False])
def test_flash_decoding(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_attn_heads: int,
kv_group_num: int,
same_context_len: bool,
q_len: int,
use_alibi_slopes: bool,
use_new_kcache_layout: bool,
):
if use_new_kcache_layout and use_alibi_slopes:
# TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
# the code (alibi kernel) will be refactored later to avoid code duplication, when
# the whole triton flow with new k cache layout has been supported and tested.
# And tests for the alibi kernel using new kcache layout will be added then.
pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.7230906Z
torch.manual_seed(123)
torch.cuda.empty_cache()
>       torch.cuda.synchronize()
2025-04-11T04:23:18.7231170Z
tests/test_infer/test_kernels/triton/test_decoding_attn.py:100:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.7231438Z
device = None
2025-04-11T04:23:18.7231520Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7231945Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7233491Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
_____________ test_flash_decoding[False-False-1-True-4-16-16-16-7] _____________
2025-04-11T04:23:18.7233897Z
bsz = 7, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
kv_group_num = 4, same_context_len = True, q_len = 1, use_alibi_slopes = False
use_new_kcache_layout = False
2025-04-11T04:23:18.7234304Z
@pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
@pytest.mark.parametrize("bsz", [7, 16])
@pytest.mark.parametrize("block_size", [16, 32])
@pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
@pytest.mark.parametrize("num_attn_heads", [16])
@pytest.mark.parametrize("kv_group_num", [1, 4])
@pytest.mark.parametrize("same_context_len", [True, False])
@pytest.mark.parametrize("q_len", [1, 5])
@pytest.mark.parametrize("use_alibi_slopes", [True, False])
@pytest.mark.parametrize("use_new_kcache_layout", [True, False])
def test_flash_decoding(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_attn_heads: int,
kv_group_num: int,
same_context_len: bool,
q_len: int,
use_alibi_slopes: bool,
use_new_kcache_layout: bool,
):
if use_new_kcache_layout and use_alibi_slopes:
# TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
# the code (alibi kernel) will be refactored later to avoid code duplication, when
# the whole triton flow with new k cache layout has been supported and tested.
# And tests for the alibi kernel using new kcache layout will be added then.
pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.7237681Z
torch.manual_seed(123)
torch.cuda.empty_cache()
>       torch.cuda.synchronize()
2025-04-11T04:23:18.7238034Z
tests/test_infer/test_kernels/triton/test_decoding_attn.py:100:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.7238305Z
device = None
2025-04-11T04:23:18.7238383Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7238721Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7240264Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
____________ test_flash_decoding[False-False-1-True-4-16-16-16-16] _____________
2025-04-11T04:23:18.7240678Z
bsz = 16, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
kv_group_num = 4, same_context_len = True, q_len = 1, use_alibi_slopes = False
use_new_kcache_layout = False
2025-04-11T04:23:18.7241087Z
@pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
@pytest.mark.parametrize("bsz", [7, 16])
@pytest.mark.parametrize("block_size", [16, 32])
@pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
@pytest.mark.parametrize("num_attn_heads", [16])
@pytest.mark.parametrize("kv_group_num", [1, 4])
@pytest.mark.parametrize("same_context_len", [True, False])
@pytest.mark.parametrize("q_len", [1, 5])
@pytest.mark.parametrize("use_alibi_slopes", [True, False])
@pytest.mark.parametrize("use_new_kcache_layout", [True, False])
def test_flash_decoding(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_attn_heads: int,
kv_group_num: int,
same_context_len: bool,
q_len: int,
use_alibi_slopes: bool,
use_new_kcache_layout: bool,
):
if use_new_kcache_layout and use_alibi_slopes:
# TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
# the code (alibi kernel) will be refactored later to avoid code duplication, when
# the whole triton flow with new k cache layout has been supported and tested.
# And tests for the alibi kernel using new kcache layout will be added then.
pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.7244509Z
torch.manual_seed(123)
torch.cuda.empty_cache()
>       torch.cuda.synchronize()
2025-04-11T04:23:18.7244776Z
tests/test_infer/test_kernels/triton/test_decoding_attn.py:100:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.7245046Z
device = None
2025-04-11T04:23:18.7245127Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7245467Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7247004Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
_____________ test_flash_decoding[False-False-1-True-4-16-16-32-7] _____________
2025-04-11T04:23:18.7247410Z
bsz = 7, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
kv_group_num = 4, same_context_len = True, q_len = 1, use_alibi_slopes = False
use_new_kcache_layout = False
2025-04-11T04:23:18.7247813Z
@pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
@pytest.mark.parametrize("bsz", [7, 16])
@pytest.mark.parametrize("block_size", [16, 32])
@pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
@pytest.mark.parametrize("num_attn_heads", [16])
@pytest.mark.parametrize("kv_group_num", [1, 4])
@pytest.mark.parametrize("same_context_len", [True, False])
@pytest.mark.parametrize("q_len", [1, 5])
@pytest.mark.parametrize("use_alibi_slopes", [True, False])
@pytest.mark.parametrize("use_new_kcache_layout", [True, False])
def test_flash_decoding(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_attn_heads: int,
kv_group_num: int,
same_context_len: bool,
q_len: int,
use_alibi_slopes: bool,
use_new_kcache_layout: bool,
):
if use_new_kcache_layout and use_alibi_slopes:
# TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
# the code (alibi kernel) will be refactored later to avoid code duplication, when
# the whole triton flow with new k cache layout has been supported and tested.
# And tests for the alibi kernel using new kcache layout will be added then.
pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.7251425Z
torch.manual_seed(123)
torch.cuda.empty_cache()
>       torch.cuda.synchronize()
2025-04-11T04:23:18.7251694Z
tests/test_infer/test_kernels/triton/test_decoding_attn.py:100:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.7251966Z
device = None
2025-04-11T04:23:18.7252044Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7252388Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7253924Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
____________ test_flash_decoding[False-False-1-True-4-16-16-32-16] _____________
2025-04-11T04:23:18.7254419Z
bsz = 16, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
kv_group_num = 4, same_context_len = True, q_len = 1, use_alibi_slopes = False
use_new_kcache_layout = False
2025-04-11T04:23:18.7254833Z
@pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
@pytest.mark.parametrize("bsz", [7, 16])
@pytest.mark.parametrize("block_size", [16, 32])
@pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
@pytest.mark.parametrize("num_attn_heads", [16])
@pytest.mark.parametrize("kv_group_num", [1, 4])
@pytest.mark.parametrize("same_context_len", [True, False])
@pytest.mark.parametrize("q_len", [1, 5])
@pytest.mark.parametrize("use_alibi_slopes", [True, False])
@pytest.mark.parametrize("use_new_kcache_layout", [True, False])
def test_flash_decoding(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_attn_heads: int,
kv_group_num: int,
same_context_len: bool,
q_len: int,
use_alibi_slopes: bool,
use_new_kcache_layout: bool,
):
if use_new_kcache_layout and use_alibi_slopes:
# TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
# the code (alibi kernel) will be refactored later to avoid code duplication, when
# the whole triton flow with new k cache layout has been supported and tested.
# And tests for the alibi kernel using new kcache layout will be added then.
pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.7258191Z
torch.manual_seed(123)
torch.cuda.empty_cache()
>       torch.cuda.synchronize()
2025-04-11T04:23:18.7258457Z
tests/test_infer/test_kernels/triton/test_decoding_attn.py:100:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.7258728Z
device = None
2025-04-11T04:23:18.7258813Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7259156Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7260778Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
_____________ test_flash_decoding[False-False-1-False-1-16-8-16-7] _____________
2025-04-11T04:23:18.7261186Z
bsz = 7, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
kv_group_num = 1, same_context_len = False, q_len = 1, use_alibi_slopes = False
use_new_kcache_layout = False
2025-04-11T04:23:18.7261592Z
@pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
@pytest.mark.parametrize("bsz", [7, 16])
@pytest.mark.parametrize("block_size", [16, 32])
@pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
@pytest.mark.parametrize("num_attn_heads", [16])
@pytest.mark.parametrize("kv_group_num", [1, 4])
@pytest.mark.parametrize("same_context_len", [True, False])
@pytest.mark.parametrize("q_len", [1, 5])
@pytest.mark.parametrize("use_alibi_slopes", [True, False])
@pytest.mark.parametrize("use_new_kcache_layout", [True, False])
def test_flash_decoding(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_attn_heads: int,
kv_group_num: int,
same_context_len: bool,
q_len: int,
use_alibi_slopes: bool,
use_new_kcache_layout: bool,
):
if use_new_kcache_layout and use_alibi_slopes:
# TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
# the code (alibi kernel) will be refactored later to avoid code duplication, when
# the whole triton flow with new k cache layout has been supported and tested.
# And tests for the alibi kernel using new kcache layout will be added then.
pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.7264962Z
torch.manual_seed(123)
torch.cuda.empty_cache()
>       torch.cuda.synchronize()
2025-04-11T04:23:18.7265229Z
tests/test_infer/test_kernels/triton/test_decoding_attn.py:100:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.7265496Z
device = None
2025-04-11T04:23:18.7265574Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7265910Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7267536Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
____________ test_flash_decoding[False-False-1-False-1-16-8-16-16] _____________
2025-04-11T04:23:18.7267950Z
bsz = 16, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
kv_group_num = 1, same_context_len = False, q_len = 1, use_alibi_slopes = False
use_new_kcache_layout = False
2025-04-11T04:23:18.7268500Z
@pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
@pytest.mark.parametrize("bsz", [7, 16])
@pytest.mark.parametrize("block_size", [16, 32])
@pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
@pytest.mark.parametrize("num_attn_heads", [16])
@pytest.mark.parametrize("kv_group_num", [1, 4])
@pytest.mark.parametrize("same_context_len", [True, False])
@pytest.mark.parametrize("q_len", [1, 5])
@pytest.mark.parametrize("use_alibi_slopes", [True, False])
@pytest.mark.parametrize("use_new_kcache_layout", [True, False])
def test_flash_decoding(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_attn_heads: int,
kv_group_num: int,
same_context_len: bool,
q_len: int,
use_alibi_slopes: bool,
use_new_kcache_layout: bool,
):
if use_new_kcache_layout and use_alibi_slopes:
# TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
# the code (alibi kernel) will be refactored later to avoid code duplication, when
# the whole triton flow with new k cache layout has been supported and tested.
# And tests for the alibi kernel using new kcache layout will be added then.
pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.7271801Z
torch.manual_seed(123)
torch.cuda.empty_cache()
>       torch.cuda.synchronize()
2025-04-11T04:23:18.7272085Z
tests/test_infer/test_kernels/triton/test_decoding_attn.py:100:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.7272363Z
device = None
2025-04-11T04:23:18.7272551Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7272912Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7274569Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
_____________ test_flash_decoding[False-False-1-False-1-16-8-32-7] _____________
2025-04-11T04:23:18.7274989Z
bsz = 7, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
kv_group_num = 1, same_context_len = False, q_len = 1, use_alibi_slopes = False
use_new_kcache_layout = False
2025-04-11T04:23:18.7275398Z
@pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
@pytest.mark.parametrize("bsz", [7, 16])
@pytest.mark.parametrize("block_size", [16, 32])
@pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
@pytest.mark.parametrize("num_attn_heads", [16])
@pytest.mark.parametrize("kv_group_num", [1, 4])
@pytest.mark.parametrize("same_context_len", [True, False])
@pytest.mark.parametrize("q_len", [1, 5])
@pytest.mark.parametrize("use_alibi_slopes", [True, False])
@pytest.mark.parametrize("use_new_kcache_layout", [True, False])
def test_flash_decoding(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_attn_heads: int,
kv_group_num: int,
same_context_len: bool,
q_len: int,
use_alibi_slopes: bool,
use_new_kcache_layout: bool,
):
if use_new_kcache_layout and use_alibi_slopes:
# TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
# the code (alibi kernel) will be refactored later to avoid code duplication, when
# the whole triton flow with new k cache layout has been supported and tested.
# And tests for the alibi kernel using new kcache layout will be added then.
pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.7278765Z
torch.manual_seed(123)
torch.cuda.empty_cache()
>       torch.cuda.synchronize()
2025-04-11T04:23:18.7279042Z
tests/test_infer/test_kernels/triton/test_decoding_attn.py:100:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.7279315Z
device = None
2025-04-11T04:23:18.7279398Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7279738Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7281377Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
____________ test_flash_decoding[False-False-1-False-1-16-8-32-16] _____________
2025-04-11T04:23:18.7281797Z
bsz = 16, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
kv_group_num = 1, same_context_len = False, q_len = 1, use_alibi_slopes = False
use_new_kcache_layout = False
2025-04-11T04:23:18.7282210Z
@pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
@pytest.mark.parametrize("bsz", [7, 16])
@pytest.mark.parametrize("block_size", [16, 32])
@pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
@pytest.mark.parametrize("num_attn_heads", [16])
@pytest.mark.parametrize("kv_group_num", [1, 4])
@pytest.mark.parametrize("same_context_len", [True, False])
@pytest.mark.parametrize("q_len", [1, 5])
@pytest.mark.parametrize("use_alibi_slopes", [True, False])
@pytest.mark.parametrize("use_new_kcache_layout", [True, False])
def test_flash_decoding(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_attn_heads: int,
kv_group_num: int,
same_context_len: bool,
q_len: int,
use_alibi_slopes: bool,
use_new_kcache_layout: bool,
):
if use_new_kcache_layout and use_alibi_slopes:
# TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
# the code (alibi kernel) will be refactored later to avoid code duplication, when
# the whole triton flow with new k cache layout has been supported and tested.
# And tests for the alibi kernel using new kcache layout will be added then.
pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.7285576Z
torch.manual_seed(123)
torch.cuda.empty_cache()
>       torch.cuda.synchronize()
2025-04-11T04:23:18.7285844Z
tests/test_infer/test_kernels/triton/test_decoding_attn.py:100:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.7286112Z
device = None
2025-04-11T04:23:18.7286196Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7286613Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7288172Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
____________ test_flash_decoding[False-False-1-False-1-16-16-16-7] _____________
2025-04-11T04:23:18.7288587Z
bsz = 7, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
kv_group_num = 1, same_context_len = False, q_len = 1, use_alibi_slopes = False
use_new_kcache_layout = False
2025-04-11T04:23:18.7289000Z
@pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
@pytest.mark.parametrize("bsz", [7, 16])
@pytest.mark.parametrize("block_size", [16, 32])
@pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
@pytest.mark.parametrize("num_attn_heads", [16])
@pytest.mark.parametrize("kv_group_num", [1, 4])
@pytest.mark.parametrize("same_context_len", [True, False])
@pytest.mark.parametrize("q_len", [1, 5])
@pytest.mark.parametrize("use_alibi_slopes", [True, False])
@pytest.mark.parametrize("use_new_kcache_layout", [True, False])
def test_flash_decoding(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_attn_heads: int,
kv_group_num: int,
same_context_len: bool,
q_len: int,
use_alibi_slopes: bool,
use_new_kcache_layout: bool,
):
if use_new_kcache_layout and use_alibi_slopes:
# TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
# the code (alibi kernel) will be refactored later to avoid code duplication, when
# the whole triton flow with new k cache layout has been supported and tested.
# And tests for the alibi kernel using new kcache layout will be added then.
pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.7292378Z
torch.manual_seed(123)
torch.cuda.empty_cache()
>       torch.cuda.synchronize()
2025-04-11T04:23:18.7292731Z
tests/test_infer/test_kernels/triton/test_decoding_attn.py:100:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.7293006Z
device = None
2025-04-11T04:23:18.7293090Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7293430Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7294978Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
____________ test_flash_decoding[False-False-1-False-1-16-16-16-16] ____________
2025-04-11T04:23:18.7295389Z
bsz = 16, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
kv_group_num = 1, same_context_len = False, q_len = 1, use_alibi_slopes = False
use_new_kcache_layout = False
2025-04-11T04:23:18.7295798Z
@pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
@pytest.mark.parametrize("bsz", [7, 16])
@pytest.mark.parametrize("block_size", [16, 32])
@pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
@pytest.mark.parametrize("num_attn_heads", [16])
@pytest.mark.parametrize("kv_group_num", [1, 4])
@pytest.mark.parametrize("same_context_len", [True, False])
@pytest.mark.parametrize("q_len", [1, 5])
@pytest.mark.parametrize("use_alibi_slopes", [True, False])
@pytest.mark.parametrize("use_new_kcache_layout", [True, False])
def test_flash_decoding(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_attn_heads: int,
kv_group_num: int,
same_context_len: bool,
q_len: int,
use_alibi_slopes: bool,
use_new_kcache_layout: bool,
):
if use_new_kcache_layout and use_alibi_slopes:
# TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
# the code (alibi kernel) will be refactored later to avoid code duplication, when
# the whole triton flow with new k cache layout has been supported and tested.
# And tests for the alibi kernel using new kcache layout will be added then.
pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.7299257Z
torch.manual_seed(123)
torch.cuda.empty_cache()
>       torch.cuda.synchronize()
2025-04-11T04:23:18.7299528Z
tests/test_infer/test_kernels/triton/test_decoding_attn.py:100:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.7299803Z
device = None
2025-04-11T04:23:18.7299890Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7300230Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7301772Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
____________ test_flash_decoding[False-False-1-False-1-16-16-32-7] _____________
2025-04-11T04:23:18.7302183Z
bsz = 7, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
kv_group_num = 1, same_context_len = False, q_len = 1, use_alibi_slopes = False
use_new_kcache_layout = False
2025-04-11T04:23:18.7302588Z
@pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
@pytest.mark.parametrize("bsz", [7, 16])
@pytest.mark.parametrize("block_size", [16, 32])
@pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
@pytest.mark.parametrize("num_attn_heads", [16])
@pytest.mark.parametrize("kv_group_num", [1, 4])
@pytest.mark.parametrize("same_context_len", [True, False])
@pytest.mark.parametrize("q_len", [1, 5])
@pytest.mark.parametrize("use_alibi_slopes", [True, False])
@pytest.mark.parametrize("use_new_kcache_layout", [True, False])
def test_flash_decoding(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_attn_heads: int,
kv_group_num: int,
same_context_len: bool,
q_len: int,
use_alibi_slopes: bool,
use_new_kcache_layout: bool,
):
if use_new_kcache_layout and use_alibi_slopes:
# TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
# the code (alibi kernel) will be refactored later to avoid code duplication, when
# the whole triton flow with new k cache layout has been supported and tested.
# And tests for the alibi kernel using new kcache layout will be added then.
pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.7306022Z
torch.manual_seed(123)
torch.cuda.empty_cache()
>       torch.cuda.synchronize()
2025-04-11T04:23:18.7306292Z
tests/test_infer/test_kernels/triton/test_decoding_attn.py:100:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.7306562Z
device = None
2025-04-11T04:23:18.7306642Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7306981Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7308559Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
____________ test_flash_decoding[False-False-1-False-1-16-16-32-16] ____________
2025-04-11T04:23:18.7309068Z
bsz = 16, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
kv_group_num = 1, same_context_len = False, q_len = 1, use_alibi_slopes = False
use_new_kcache_layout = False
2025-04-11T04:23:18.7309484Z
@pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
@pytest.mark.parametrize("bsz", [7, 16])
@pytest.mark.parametrize("block_size", [16, 32])
@pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
@pytest.mark.parametrize("num_attn_heads", [16])
@pytest.mark.parametrize("kv_group_num", [1, 4])
@pytest.mark.parametrize("same_context_len", [True, False])
@pytest.mark.parametrize("q_len", [1, 5])
@pytest.mark.parametrize("use_alibi_slopes", [True, False])
@pytest.mark.parametrize("use_new_kcache_layout", [True, False])
def test_flash_decoding(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_attn_heads: int,
kv_group_num: int,
same_context_len: bool,
q_len: int,
use_alibi_slopes: bool,
use_new_kcache_layout: bool,
):
if use_new_kcache_layout and use_alibi_slopes:
# TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
# the code (alibi kernel) will be refactored later to avoid code duplication, when
# the whole triton flow with new k cache layout has been supported and tested.
# And tests for the alibi kernel using new kcache layout will be added then.
pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.7312846Z
torch.manual_seed(123)
torch.cuda.empty_cache()
>       torch.cuda.synchronize()
2025-04-11T04:23:18.7313111Z
tests/test_infer/test_kernels/triton/test_decoding_attn.py:100:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.7313378Z
device = None
2025-04-11T04:23:18.7313461Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7313798Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7315427Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
_____________ test_flash_decoding[False-False-1-False-4-16-8-16-7] _____________
2025-04-11T04:23:18.7315835Z
bsz = 7, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
kv_group_num = 4, same_context_len = False, q_len = 1, use_alibi_slopes = False
use_new_kcache_layout = False
2025-04-11T04:23:18.7316242Z
@pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
@pytest.mark.parametrize("bsz", [7, 16])
@pytest.mark.parametrize("block_size", [16, 32])
@pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
@pytest.mark.parametrize("num_attn_heads", [16])
@pytest.mark.parametrize("kv_group_num", [1, 4])
@pytest.mark.parametrize("same_context_len", [True, False])
@pytest.mark.parametrize("q_len", [1, 5])
@pytest.mark.parametrize("use_alibi_slopes", [True, False])
@pytest.mark.parametrize("use_new_kcache_layout", [True, False])
def test_flash_decoding(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_attn_heads: int,
kv_group_num: int,
same_context_len: bool,
q_len: int,
use_alibi_slopes: bool,
use_new_kcache_layout: bool,
):
if use_new_kcache_layout and use_alibi_slopes:
# TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
# the code (alibi kernel) will be refactored later to avoid code duplication, when
# the whole triton flow with new k cache layout has been supported and tested.
# And tests for the alibi kernel using new kcache layout will be added then.
pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.7319619Z
torch.manual_seed(123)
torch.cuda.empty_cache()
>       torch.cuda.synchronize()
2025-04-11T04:23:18.7319891Z
tests/test_infer/test_kernels/triton/test_decoding_attn.py:100:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.7320161Z
device = None
2025-04-11T04:23:18.7320243Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7320574Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7322222Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
____________ test_flash_decoding[False-False-1-False-4-16-8-16-16] _____________
2025-04-11T04:23:18.7322637Z
bsz = 16, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
kv_group_num = 4, same_context_len = False, q_len = 1, use_alibi_slopes = False
use_new_kcache_layout = False
2025-04-11T04:23:18.7323131Z
@pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
@pytest.mark.parametrize("bsz", [7, 16])
@pytest.mark.parametrize("block_size", [16, 32])
@pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
@pytest.mark.parametrize("num_attn_heads", [16])
@pytest.mark.parametrize("kv_group_num", [1, 4])
@pytest.mark.parametrize("same_context_len", [True, False])
@pytest.mark.parametrize("q_len", [1, 5])
@pytest.mark.parametrize("use_alibi_slopes", [True, False])
@pytest.mark.parametrize("use_new_kcache_layout", [True, False])
def test_flash_decoding(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_attn_heads: int,
kv_group_num: int,
same_context_len: bool,
q_len: int,
use_alibi_slopes: bool,
use_new_kcache_layout: bool,
):
if use_new_kcache_layout and use_alibi_slopes:
# TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
# the code (alibi kernel) will be refactored later to avoid code duplication, when
# the whole triton flow with new k cache layout has been supported and tested.
# And tests for the alibi kernel using new kcache layout will be added then.
pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.7326406Z
torch.manual_seed(123)
torch.cuda.empty_cache()
>       torch.cuda.synchronize()
2025-04-11T04:23:18.7326678Z
tests/test_infer/test_kernels/triton/test_decoding_attn.py:100:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.7326949Z
device = None
2025-04-11T04:23:18.7327116Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7327467Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7329128Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
_____________ test_flash_decoding[False-False-1-False-4-16-8-32-7] _____________
2025-04-11T04:23:18.7329548Z
bsz = 7, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
kv_group_num = 4, same_context_len = False, q_len = 1, use_alibi_slopes = False
use_new_kcache_layout = False
2025-04-11T04:23:18.7329957Z
@pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
@pytest.mark.parametrize("bsz", [7, 16])
@pytest.mark.parametrize("block_size", [16, 32])
@pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
@pytest.mark.parametrize("num_attn_heads", [16])
@pytest.mark.parametrize("kv_group_num", [1, 4])
@pytest.mark.parametrize("same_context_len", [True, False])
@pytest.mark.parametrize("q_len", [1, 5])
@pytest.mark.parametrize("use_alibi_slopes", [True, False])
@pytest.mark.parametrize("use_new_kcache_layout", [True, False])
def test_flash_decoding(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_attn_heads: int,
kv_group_num: int,
same_context_len: bool,
q_len: int,
use_alibi_slopes: bool,
use_new_kcache_layout: bool,
):
if use_new_kcache_layout and use_alibi_slopes:
# TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
# the code (alibi kernel) will be refactored later to avoid code duplication, when
# the whole triton flow with new k cache layout has been supported and tested.
# And tests for the alibi kernel using new kcache layout will be added then.
pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.7333232Z
torch.manual_seed(123)
torch.cuda.empty_cache()
>       torch.cuda.synchronize()
2025-04-11T04:23:18.7333589Z
tests/test_infer/test_kernels/triton/test_decoding_attn.py:100:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.7333863Z
device = None
2025-04-11T04:23:18.7333944Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7334279Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7335928Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
____________ test_flash_decoding[False-False-1-False-4-16-8-32-16] _____________
2025-04-11T04:23:18.7336350Z
bsz = 16, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
kv_group_num = 4, same_context_len = False, q_len = 1, use_alibi_slopes = False
use_new_kcache_layout = False
2025-04-11T04:23:18.7336774Z
@pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
@pytest.mark.parametrize("bsz", [7, 16])
@pytest.mark.parametrize("block_size", [16, 32])
@pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
@pytest.mark.parametrize("num_attn_heads", [16])
@pytest.mark.parametrize("kv_group_num", [1, 4])
@pytest.mark.parametrize("same_context_len", [True, False])
@pytest.mark.parametrize("q_len", [1, 5])
@pytest.mark.parametrize("use_alibi_slopes", [True, False])
@pytest.mark.parametrize("use_new_kcache_layout", [True, False])
def test_flash_decoding(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_attn_heads: int,
kv_group_num: int,
same_context_len: bool,
q_len: int,
use_alibi_slopes: bool,
use_new_kcache_layout: bool,
):
if use_new_kcache_layout and use_alibi_slopes:
# TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
# the code (alibi kernel) will be refactored later to avoid code duplication, when
# the whole triton flow with new k cache layout has been supported and tested.
# And tests for the alibi kernel using new kcache layout will be added then.
pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.7340244Z
torch.manual_seed(123)
torch.cuda.empty_cache()
>       torch.cuda.synchronize()
2025-04-11T04:23:18.7340513Z
tests/test_infer/test_kernels/triton/test_decoding_attn.py:100:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.7340788Z
device = None
2025-04-11T04:23:18.7340869Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7341310Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7342879Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
____________ test_flash_decoding[False-False-1-False-4-16-16-16-7] _____________
2025-04-11T04:23:18.7343291Z
bsz = 7, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
kv_group_num = 4, same_context_len = False, q_len = 1, use_alibi_slopes = False
use_new_kcache_layout = False
2025-04-11T04:23:18.7343703Z
@pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
@pytest.mark.parametrize("bsz", [7, 16])
@pytest.mark.parametrize("block_size", [16, 32])
@pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
@pytest.mark.parametrize("num_attn_heads", [16])
@pytest.mark.parametrize("kv_group_num", [1, 4])
@pytest.mark.parametrize("same_context_len", [True, False])
@pytest.mark.parametrize("q_len", [1, 5])
@pytest.mark.parametrize("use_alibi_slopes", [True, False])
@pytest.mark.parametrize("use_new_kcache_layout", [True, False])
def test_flash_decoding(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_attn_heads: int,
kv_group_num: int,
same_context_len: bool,
q_len: int,
use_alibi_slopes: bool,
use_new_kcache_layout: bool,
):
if use_new_kcache_layout and use_alibi_slopes:
# TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
# the code (alibi kernel) will be refactored later to avoid code duplication, when
# the whole triton flow with new k cache layout has been supported and tested.
# And tests for the alibi kernel using new kcache layout will be added then.
pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.7347067Z
torch.manual_seed(123)
torch.cuda.empty_cache()
>       torch.cuda.synchronize()
2025-04-11T04:23:18.7347337Z
tests/test_infer/test_kernels/triton/test_decoding_attn.py:100:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.7347780Z
device = None
2025-04-11T04:23:18.7347862Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7348198Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7349793Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
____________ test_flash_decoding[False-False-1-False-4-16-16-16-16] ____________
2025-04-11T04:23:18.7350212Z
bsz = 16, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
kv_group_num = 4, same_context_len = False, q_len = 1, use_alibi_slopes = False
use_new_kcache_layout = False
2025-04-11T04:23:18.7350629Z
@pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
@pytest.mark.parametrize("bsz", [7, 16])
@pytest.mark.parametrize("block_size", [16, 32])
@pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
@pytest.mark.parametrize("num_attn_heads", [16])
@pytest.mark.parametrize("kv_group_num", [1, 4])
@pytest.mark.parametrize("same_context_len", [True, False])
@pytest.mark.parametrize("q_len", [1, 5])
@pytest.mark.parametrize("use_alibi_slopes", [True, False])
@pytest.mark.parametrize("use_new_kcache_layout", [True, False])
def test_flash_decoding(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_attn_heads: int,
kv_group_num: int,
same_context_len: bool,
q_len: int,
use_alibi_slopes: bool,
use_new_kcache_layout: bool,
):
if use_new_kcache_layout and use_alibi_slopes:
# TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
# the code (alibi kernel) will be refactored later to avoid code duplication, when
# the whole triton flow with new k cache layout has been supported and tested.
# And tests for the alibi kernel using new kcache layout will be added then.
pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.7354082Z
torch.manual_seed(123)
torch.cuda.empty_cache()
>       torch.cuda.synchronize()
2025-04-11T04:23:18.7354351Z
tests/test_infer/test_kernels/triton/test_decoding_attn.py:100:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.7354616Z
device = None
2025-04-11T04:23:18.7354696Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7355039Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7356571Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
____________ test_flash_decoding[False-False-1-False-4-16-16-32-7] _____________
2025-04-11T04:23:18.7356978Z
bsz = 7, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
kv_group_num = 4, same_context_len = False, q_len = 1, use_alibi_slopes = False
use_new_kcache_layout = False
2025-04-11T04:23:18.7357383Z
@pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
@pytest.mark.parametrize("bsz", [7, 16])
@pytest.mark.parametrize("block_size", [16, 32])
@pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
@pytest.mark.parametrize("num_attn_heads", [16])
@pytest.mark.parametrize("kv_group_num", [1, 4])
@pytest.mark.parametrize("same_context_len", [True, False])
@pytest.mark.parametrize("q_len", [1, 5])
@pytest.mark.parametrize("use_alibi_slopes", [True, False])
@pytest.mark.parametrize("use_new_kcache_layout", [True, False])
def test_flash_decoding(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_attn_heads: int,
kv_group_num: int,
same_context_len: bool,
q_len: int,
use_alibi_slopes: bool,
use_new_kcache_layout: bool,
):
if use_new_kcache_layout and use_alibi_slopes:
# TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
# the code (alibi kernel) will be refactored later to avoid code duplication, when
# the whole triton flow with new k cache layout has been supported and tested.
# And tests for the alibi kernel using new kcache layout will be added then.
pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.7360839Z
torch.manual_seed(123)
torch.cuda.empty_cache()
>       torch.cuda.synchronize()
2025-04-11T04:23:18.7361113Z
tests/test_infer/test_kernels/triton/test_decoding_attn.py:100:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.7361391Z
device = None
2025-04-11T04:23:18.7361470Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7361810Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7363353Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
____________ test_flash_decoding[False-False-1-False-4-16-16-32-16] ____________
2025-04-11T04:23:18.7363875Z
bsz = 16, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
kv_group_num = 4, same_context_len = False, q_len = 1, use_alibi_slopes = False
use_new_kcache_layout = False
2025-04-11T04:23:18.7364291Z
@pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
@pytest.mark.parametrize("bsz", [7, 16])
@pytest.mark.parametrize("block_size", [16, 32])
@pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
@pytest.mark.parametrize("num_attn_heads", [16])
@pytest.mark.parametrize("kv_group_num", [1, 4])
@pytest.mark.parametrize("same_context_len", [True, False])
@pytest.mark.parametrize("q_len", [1, 5])
@pytest.mark.parametrize("use_alibi_slopes", [True, False])
@pytest.mark.parametrize("use_new_kcache_layout", [True, False])
def test_flash_decoding(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_attn_heads: int,
kv_group_num: int,
same_context_len: bool,
q_len: int,
use_alibi_slopes: bool,
use_new_kcache_layout: bool,
):
if use_new_kcache_layout and use_alibi_slopes:
# TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
# the code (alibi kernel) will be refactored later to avoid code duplication, when
# the whole triton flow with new k cache layout has been supported and tested.
# And tests for the alibi kernel using new kcache layout will be added then.
pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.7367647Z
torch.manual_seed(123)
torch.cuda.empty_cache()
>       torch.cuda.synchronize()
2025-04-11T04:23:18.7367916Z
tests/test_infer/test_kernels/triton/test_decoding_attn.py:100:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.7368186Z
device = None
2025-04-11T04:23:18.7368264Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7368606Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7370234Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
_____________ test_flash_decoding[False-False-5-True-1-16-8-16-7] ______________
2025-04-11T04:23:18.7370649Z
bsz = 7, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
kv_group_num = 1, same_context_len = True, q_len = 5, use_alibi_slopes = False
use_new_kcache_layout = False
2025-04-11T04:23:18.7371060Z
@pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
@pytest.mark.parametrize("bsz", [7, 16])
@pytest.mark.parametrize("block_size", [16, 32])
@pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
@pytest.mark.parametrize("num_attn_heads", [16])
@pytest.mark.parametrize("kv_group_num", [1, 4])
@pytest.mark.parametrize("same_context_len", [True, False])
@pytest.mark.parametrize("q_len", [1, 5])
@pytest.mark.parametrize("use_alibi_slopes", [True, False])
@pytest.mark.parametrize("use_new_kcache_layout", [True, False])
def test_flash_decoding(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_attn_heads: int,
kv_group_num: int,
same_context_len: bool,
q_len: int,
use_alibi_slopes: bool,
use_new_kcache_layout: bool,
):
if use_new_kcache_layout and use_alibi_slopes:
# TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
# the code (alibi kernel) will be refactored later to avoid code duplication, when
# the whole triton flow with new k cache layout has been supported and tested.
# And tests for the alibi kernel using new kcache layout will be added then.
pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.7374422Z
torch.manual_seed(123)
torch.cuda.empty_cache()
>       torch.cuda.synchronize()
2025-04-11T04:23:18.7374698Z
tests/test_infer/test_kernels/triton/test_decoding_attn.py:100:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.7374974Z
device = None
2025-04-11T04:23:18.7375052Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7375386Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7377031Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
_____________ test_flash_decoding[False-False-5-True-1-16-8-16-16] _____________
2025-04-11T04:23:18.7377452Z
bsz = 16, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
kv_group_num = 1, same_context_len = True, q_len = 5, use_alibi_slopes = False
use_new_kcache_layout = False
2025-04-11T04:23:18.7377952Z
@pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
@pytest.mark.parametrize("bsz", [7, 16])
@pytest.mark.parametrize("block_size", [16, 32])
@pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
@pytest.mark.parametrize("num_attn_heads", [16])
@pytest.mark.parametrize("kv_group_num", [1, 4])
@pytest.mark.parametrize("same_context_len", [True, False])
@pytest.mark.parametrize("q_len", [1, 5])
@pytest.mark.parametrize("use_alibi_slopes", [True, False])
@pytest.mark.parametrize("use_new_kcache_layout", [True, False])
def test_flash_decoding(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_attn_heads: int,
kv_group_num: int,
same_context_len: bool,
q_len: int,
use_alibi_slopes: bool,
use_new_kcache_layout: bool,
):
if use_new_kcache_layout and use_alibi_slopes:
# TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
# the code (alibi kernel) will be refactored later to avoid code duplication, when
# the whole triton flow with new k cache layout has been supported and tested.
# And tests for the alibi kernel using new kcache layout will be added then.
pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.7381208Z
torch.manual_seed(123)
torch.cuda.empty_cache()
>       torch.cuda.synchronize()
2025-04-11T04:23:18.7381476Z
tests/test_infer/test_kernels/triton/test_decoding_attn.py:100:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.7381747Z
device = None
2025-04-11T04:23:18.7381825Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7382270Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7383904Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
_____________ test_flash_decoding[False-False-5-True-1-16-8-32-7] ______________
2025-04-11T04:23:18.7384316Z
bsz = 7, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
kv_group_num = 1, same_context_len = True, q_len = 5, use_alibi_slopes = False
use_new_kcache_layout = False
2025-04-11T04:23:18.7384725Z
@pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
@pytest.mark.parametrize("bsz", [7, 16])
@pytest.mark.parametrize("block_size", [16, 32])
@pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
@pytest.mark.parametrize("num_attn_heads", [16])
@pytest.mark.parametrize("kv_group_num", [1, 4])
@pytest.mark.parametrize("same_context_len", [True, False])
@pytest.mark.parametrize("q_len", [1, 5])
@pytest.mark.parametrize("use_alibi_slopes", [True, False])
@pytest.mark.parametrize("use_new_kcache_layout", [True, False])
def test_flash_decoding(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_attn_heads: int,
kv_group_num: int,
same_context_len: bool,
q_len: int,
use_alibi_slopes: bool,
use_new_kcache_layout: bool,
):
if use_new_kcache_layout and use_alibi_slopes:
# TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
# the code (alibi kernel) will be refactored later to avoid code duplication, when
# the whole triton flow with new k cache layout has been supported and tested.
# And tests for the alibi kernel using new kcache layout will be added then.
pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.7388005Z
torch.manual_seed(123)
torch.cuda.empty_cache()
>       torch.cuda.synchronize()
2025-04-11T04:23:18.7388382Z
tests/test_infer/test_kernels/triton/test_decoding_attn.py:100:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.7388714Z
device = None
2025-04-11T04:23:18.7388791Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7389129Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7390763Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
_____________ test_flash_decoding[False-False-5-True-1-16-8-32-16] _____________
2025-04-11T04:23:18.7391186Z
bsz = 16, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
kv_group_num = 1, same_context_len = True, q_len = 5, use_alibi_slopes = False
use_new_kcache_layout = False
2025-04-11T04:23:18.7391601Z
@pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
@pytest.mark.parametrize("bsz", [7, 16])
@pytest.mark.parametrize("block_size", [16, 32])
@pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
@pytest.mark.parametrize("num_attn_heads", [16])
@pytest.mark.parametrize("kv_group_num", [1, 4])
@pytest.mark.parametrize("same_context_len", [True, False])
@pytest.mark.parametrize("q_len", [1, 5])
@pytest.mark.parametrize("use_alibi_slopes", [True, False])
@pytest.mark.parametrize("use_new_kcache_layout", [True, False])
def test_flash_decoding(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_attn_heads: int,
kv_group_num: int,
same_context_len: bool,
q_len: int,
use_alibi_slopes: bool,
use_new_kcache_layout: bool,
):
if use_new_kcache_layout and use_alibi_slopes:
# TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
# the code (alibi kernel) will be refactored later to avoid code duplication, when
# the whole triton flow with new k cache layout has been supported and tested.
# And tests for the alibi kernel using new kcache layout will be added then.
pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.7394976Z
torch.manual_seed(123)
torch.cuda.empty_cache()
>       torch.cuda.synchronize()
2025-04-11T04:23:18.7395244Z
tests/test_infer/test_kernels/triton/test_decoding_attn.py:100:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.7395515Z
device = None
2025-04-11T04:23:18.7395593Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7395931Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7397570Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
_____________ test_flash_decoding[False-False-5-True-1-16-16-16-7] _____________
2025-04-11T04:23:18.7397977Z
bsz = 7, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
kv_group_num = 1, same_context_len = True, q_len = 5, use_alibi_slopes = False
use_new_kcache_layout = False
2025-04-11T04:23:18.7398387Z
@pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
@pytest.mark.parametrize("bsz", [7, 16])
@pytest.mark.parametrize("block_size", [16, 32])
@pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
@pytest.mark.parametrize("num_attn_heads", [16])
@pytest.mark.parametrize("kv_group_num", [1, 4])
@pytest.mark.parametrize("same_context_len", [True, False])
@pytest.mark.parametrize("q_len", [1, 5])
@pytest.mark.parametrize("use_alibi_slopes", [True, False])
@pytest.mark.parametrize("use_new_kcache_layout", [True, False])
def test_flash_decoding(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_attn_heads: int,
kv_group_num: int,
same_context_len: bool,
q_len: int,
use_alibi_slopes: bool,
use_new_kcache_layout: bool,
):
if use_new_kcache_layout and use_alibi_slopes:
# TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
# the code (alibi kernel) will be refactored later to avoid code duplication, when
# the whole triton flow with new k cache layout has been supported and tested.
# And tests for the alibi kernel using new kcache layout will be added then.
pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.7401747Z
torch.manual_seed(123)
torch.cuda.empty_cache()
>       torch.cuda.synchronize()
2025-04-11T04:23:18.7402021Z
tests/test_infer/test_kernels/triton/test_decoding_attn.py:100:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.7402381Z
device = None
2025-04-11T04:23:18.7402459Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7402798Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7404355Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
____________ test_flash_decoding[False-False-5-True-1-16-16-16-16] _____________
2025-04-11T04:23:18.7404768Z
bsz = 16, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
kv_group_num = 1, same_context_len = True, q_len = 5, use_alibi_slopes = False
use_new_kcache_layout = False
2025-04-11T04:23:18.7405187Z
@pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
@pytest.mark.parametrize("bsz", [7, 16])
@pytest.mark.parametrize("block_size", [16, 32])
@pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
@pytest.mark.parametrize("num_attn_heads", [16])
@pytest.mark.parametrize("kv_group_num", [1, 4])
@pytest.mark.parametrize("same_context_len", [True, False])
@pytest.mark.parametrize("q_len", [1, 5])
@pytest.mark.parametrize("use_alibi_slopes", [True, False])
@pytest.mark.parametrize("use_new_kcache_layout", [True, False])
def test_flash_decoding(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_attn_heads: int,
kv_group_num: int,
same_context_len: bool,
q_len: int,
use_alibi_slopes: bool,
use_new_kcache_layout: bool,
):
if use_new_kcache_layout and use_alibi_slopes:
# TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
# the code (alibi kernel) will be refactored later to avoid code duplication, when
# the whole triton flow with new k cache layout has been supported and tested.
# And tests for the alibi kernel using new kcache layout will be added then.
pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.7408648Z
torch.manual_seed(123)
torch.cuda.empty_cache()
>       torch.cuda.synchronize()
2025-04-11T04:23:18.7408921Z
tests/test_infer/test_kernels/triton/test_decoding_attn.py:100:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.7409194Z
device = None
2025-04-11T04:23:18.7409274Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7409620Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7411169Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
_____________ test_flash_decoding[False-False-5-True-1-16-16-32-7] _____________
2025-04-11T04:23:18.7411579Z
bsz = 7, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
kv_group_num = 1, same_context_len = True, q_len = 5, use_alibi_slopes = False
use_new_kcache_layout = False
2025-04-11T04:23:18.7411984Z
@pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
@pytest.mark.parametrize("bsz", [7, 16])
@pytest.mark.parametrize("block_size", [16, 32])
@pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
@pytest.mark.parametrize("num_attn_heads", [16])
@pytest.mark.parametrize("kv_group_num", [1, 4])
@pytest.mark.parametrize("same_context_len", [True, False])
@pytest.mark.parametrize("q_len", [1, 5])
@pytest.mark.parametrize("use_alibi_slopes", [True, False])
@pytest.mark.parametrize("use_new_kcache_layout", [True, False])
def test_flash_decoding(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_attn_heads: int,
kv_group_num: int,
same_context_len: bool,
q_len: int,
use_alibi_slopes: bool,
use_new_kcache_layout: bool,
):
if use_new_kcache_layout and use_alibi_slopes:
# TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
# the code (alibi kernel) will be refactored later to avoid code duplication, when
# the whole triton flow with new k cache layout has been supported and tested.
# And tests for the alibi kernel using new kcache layout will be added then.
pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.7415439Z
torch.manual_seed(123)
torch.cuda.empty_cache()
>       torch.cuda.synchronize()
2025-04-11T04:23:18.7415713Z
tests/test_infer/test_kernels/triton/test_decoding_attn.py:100:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.7415994Z
device = None
2025-04-11T04:23:18.7416073Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7416418Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7417980Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
____________ test_flash_decoding[False-False-5-True-1-16-16-32-16] _____________
2025-04-11T04:23:18.7418400Z
bsz = 16, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
kv_group_num = 1, same_context_len = True, q_len = 5, use_alibi_slopes = False
use_new_kcache_layout = False
2025-04-11T04:23:18.7418899Z
@pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
@pytest.mark.parametrize("bsz", [7, 16])
@pytest.mark.parametrize("block_size", [16, 32])
@pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
@pytest.mark.parametrize("num_attn_heads", [16])
@pytest.mark.parametrize("kv_group_num", [1, 4])
@pytest.mark.parametrize("same_context_len", [True, False])
@pytest.mark.parametrize("q_len", [1, 5])
@pytest.mark.parametrize("use_alibi_slopes", [True, False])
@pytest.mark.parametrize("use_new_kcache_layout", [True, False])
def test_flash_decoding(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_attn_heads: int,
kv_group_num: int,
same_context_len: bool,
q_len: int,
use_alibi_slopes: bool,
use_new_kcache_layout: bool,
):
if use_new_kcache_layout and use_alibi_slopes:
# TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
# the code (alibi kernel) will be refactored later to avoid code duplication, when
# the whole triton flow with new k cache layout has been supported and tested.
# And tests for the alibi kernel using new kcache layout will be added then.
pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.7422284Z
torch.manual_seed(123)
torch.cuda.empty_cache()
>       torch.cuda.synchronize()
2025-04-11T04:23:18.7422551Z
tests/test_infer/test_kernels/triton/test_decoding_attn.py:100:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.7422821Z
device = None
2025-04-11T04:23:18.7422899Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7423242Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7424869Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
_____________ test_flash_decoding[False-False-5-True-4-16-8-16-7] ______________
2025-04-11T04:23:18.7425283Z
bsz = 7, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
kv_group_num = 4, same_context_len = True, q_len = 5, use_alibi_slopes = False
use_new_kcache_layout = False
2025-04-11T04:23:18.7425687Z
@pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
@pytest.mark.parametrize("bsz", [7, 16])
@pytest.mark.parametrize("block_size", [16, 32])
@pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
@pytest.mark.parametrize("num_attn_heads", [16])
@pytest.mark.parametrize("kv_group_num", [1, 4])
@pytest.mark.parametrize("same_context_len", [True, False])
@pytest.mark.parametrize("q_len", [1, 5])
@pytest.mark.parametrize("use_alibi_slopes", [True, False])
@pytest.mark.parametrize("use_new_kcache_layout", [True, False])
def test_flash_decoding(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_attn_heads: int,
kv_group_num: int,
same_context_len: bool,
q_len: int,
use_alibi_slopes: bool,
use_new_kcache_layout: bool,
):
if use_new_kcache_layout and use_alibi_slopes:
# TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
# the code (alibi kernel) will be refactored later to avoid code duplication, when
# the whole triton flow with new k cache layout has been supported and tested.
# And tests for the alibi kernel using new kcache layout will be added then.
pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.7429082Z
torch.manual_seed(123)
torch.cuda.empty_cache()
>       torch.cuda.synchronize()
2025-04-11T04:23:18.7429351Z
tests/test_infer/test_kernels/triton/test_decoding_attn.py:100:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.7429627Z
device = None
2025-04-11T04:23:18.7429705Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7430047Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7431688Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
_____________ test_flash_decoding[False-False-5-True-4-16-8-16-16] _____________
2025-04-11T04:23:18.7432095Z
bsz = 16, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
kv_group_num = 4, same_context_len = True, q_len = 5, use_alibi_slopes = False
use_new_kcache_layout = False
2025-04-11T04:23:18.7432607Z
@pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
@pytest.mark.parametrize("bsz", [7, 16])
@pytest.mark.parametrize("block_size", [16, 32])
@pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
@pytest.mark.parametrize("num_attn_heads", [16])
@pytest.mark.parametrize("kv_group_num", [1, 4])
@pytest.mark.parametrize("same_context_len", [True, False])
@pytest.mark.parametrize("q_len", [1, 5])
@pytest.mark.parametrize("use_alibi_slopes", [True, False])
@pytest.mark.parametrize("use_new_kcache_layout", [True, False])
def test_flash_decoding(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_attn_heads: int,
kv_group_num: int,
same_context_len: bool,
q_len: int,
use_alibi_slopes: bool,
use_new_kcache_layout: bool,
):
if use_new_kcache_layout and use_alibi_slopes:
# TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
# the code (alibi kernel) will be refactored later to avoid code duplication, when
# the whole triton flow with new k cache layout has been supported and tested.
# And tests for the alibi kernel using new kcache layout will be added then.
pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.7437617Z
torch.manual_seed(123)
torch.cuda.empty_cache()
>       torch.cuda.synchronize()
2025-04-11T04:23:18.7437892Z
tests/test_infer/test_kernels/triton/test_decoding_attn.py:100:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.7438164Z
device = None
2025-04-11T04:23:18.7438244Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7438672Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7440294Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
_____________ test_flash_decoding[False-False-5-True-4-16-8-32-7] ______________
2025-04-11T04:23:18.7440706Z
bsz = 7, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
kv_group_num = 4, same_context_len = True, q_len = 5, use_alibi_slopes = False
use_new_kcache_layout = False
2025-04-11T04:23:18.7441198Z
@pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
@pytest.mark.parametrize("bsz", [7, 16])
@pytest.mark.parametrize("block_size", [16, 32])
@pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
@pytest.mark.parametrize("num_attn_heads", [16])
@pytest.mark.parametrize("kv_group_num", [1, 4])
@pytest.mark.parametrize("same_context_len", [True, False])
@pytest.mark.parametrize("q_len", [1, 5])
@pytest.mark.parametrize("use_alibi_slopes", [True, False])
@pytest.mark.parametrize("use_new_kcache_layout", [True, False])
def test_flash_decoding(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_attn_heads: int,
kv_group_num: int,
same_context_len: bool,
q_len: int,
use_alibi_slopes: bool,
use_new_kcache_layout: bool,
):
if use_new_kcache_layout and use_alibi_slopes:
# TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
# the code (alibi kernel) will be refactored later to avoid code duplication, when
# the whole triton flow with new k cache layout has been supported and tested.
# And tests for the alibi kernel using new kcache layout will be added then.
pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.7444476Z
torch.manual_seed(123)
torch.cuda.empty_cache()
>       torch.cuda.synchronize()
2025-04-11T04:23:18.7444928Z
tests/test_infer/test_kernels/triton/test_decoding_attn.py:100:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.7445208Z
device = None
2025-04-11T04:23:18.7445290Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7445632Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7447295Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
_____________ test_flash_decoding[False-False-5-True-4-16-8-32-16] _____________
2025-04-11T04:23:18.7447707Z
bsz = 16, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
kv_group_num = 4, same_context_len = True, q_len = 5, use_alibi_slopes = False
use_new_kcache_layout = False
2025-04-11T04:23:18.7448126Z
@pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
@pytest.mark.parametrize("bsz", [7, 16])
@pytest.mark.parametrize("block_size", [16, 32])
@pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
@pytest.mark.parametrize("num_attn_heads", [16])
@pytest.mark.parametrize("kv_group_num", [1, 4])
@pytest.mark.parametrize("same_context_len", [True, False])
@pytest.mark.parametrize("q_len", [1, 5])
@pytest.mark.parametrize("use_alibi_slopes", [True, False])
@pytest.mark.parametrize("use_new_kcache_layout", [True, False])
def test_flash_decoding(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_attn_heads: int,
kv_group_num: int,
same_context_len: bool,
q_len: int,
use_alibi_slopes: bool,
use_new_kcache_layout: bool,
):
if use_new_kcache_layout and use_alibi_slopes:
# TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
# the code (alibi kernel) will be refactored later to avoid code duplication, when
# the whole triton flow with new k cache layout has been supported and tested.
# And tests for the alibi kernel using new kcache layout will be added then.
pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.7451490Z
torch.manual_seed(123)
torch.cuda.empty_cache()
>       torch.cuda.synchronize()
2025-04-11T04:23:18.7451762Z
tests/test_infer/test_kernels/triton/test_decoding_attn.py:100:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.7452036Z
device = None
2025-04-11T04:23:18.7452114Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7452451Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7454117Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
_____________ test_flash_decoding[False-False-5-True-4-16-16-16-7] _____________
2025-04-11T04:23:18.7454523Z
bsz = 7, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
kv_group_num = 4, same_context_len = True, q_len = 5, use_alibi_slopes = False
use_new_kcache_layout = False
2025-04-11T04:23:18.7454935Z
@pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
@pytest.mark.parametrize("bsz", [7, 16])
@pytest.mark.parametrize("block_size", [16, 32])
@pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
@pytest.mark.parametrize("num_attn_heads", [16])
@pytest.mark.parametrize("kv_group_num", [1, 4])
@pytest.mark.parametrize("same_context_len", [True, False])
@pytest.mark.parametrize("q_len", [1, 5])
@pytest.mark.parametrize("use_alibi_slopes", [True, False])
@pytest.mark.parametrize("use_new_kcache_layout", [True, False])
def test_flash_decoding(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_attn_heads: int,
kv_group_num: int,
same_context_len: bool,
q_len: int,
use_alibi_slopes: bool,
use_new_kcache_layout: bool,
):
if use_new_kcache_layout and use_alibi_slopes:
# TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
# the code (alibi kernel) will be refactored later to avoid code duplication, when
# the whole triton flow with new k cache layout has been supported and tested.
# And tests for the alibi kernel using new kcache layout will be added then.
pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.7458319Z
torch.manual_seed(123)
torch.cuda.empty_cache()
>       torch.cuda.synchronize()
2025-04-11T04:23:18.7458592Z
tests/test_infer/test_kernels/triton/test_decoding_attn.py:100:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.7458913Z
device = None
2025-04-11T04:23:18.7458993Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7459331Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7460910Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
____________ test_flash_decoding[False-False-5-True-4-16-16-16-16] _____________
2025-04-11T04:23:18.7461324Z
bsz = 16, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
kv_group_num = 4, same_context_len = True, q_len = 5, use_alibi_slopes = False
use_new_kcache_layout = False
2025-04-11T04:23:18.7461737Z
@pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
@pytest.mark.parametrize("bsz", [7, 16])
@pytest.mark.parametrize("block_size", [16, 32])
@pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
@pytest.mark.parametrize("num_attn_heads", [16])
@pytest.mark.parametrize("kv_group_num", [1, 4])
@pytest.mark.parametrize("same_context_len", [True, False])
@pytest.mark.parametrize("q_len", [1, 5])
@pytest.mark.parametrize("use_alibi_slopes", [True, False])
@pytest.mark.parametrize("use_new_kcache_layout", [True, False])
def test_flash_decoding(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_attn_heads: int,
kv_group_num: int,
same_context_len: bool,
q_len: int,
use_alibi_slopes: bool,
use_new_kcache_layout: bool,
):
if use_new_kcache_layout and use_alibi_slopes:
# TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
# the code (alibi kernel) will be refactored later to avoid code duplication, when
# the whole triton flow with new k cache layout has been supported and tested.
# And tests for the alibi kernel using new kcache layout will be added then.
pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.7465150Z
torch.manual_seed(123)
torch.cuda.empty_cache()
>       torch.cuda.synchronize()
2025-04-11T04:23:18.7465419Z
tests/test_infer/test_kernels/triton/test_decoding_attn.py:100:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.7465743Z
device = None
2025-04-11T04:23:18.7465822Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7466162Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7467710Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
_____________ test_flash_decoding[False-False-5-True-4-16-16-32-7] _____________
2025-04-11T04:23:18.7468118Z
bsz = 7, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
kv_group_num = 4, same_context_len = True, q_len = 5, use_alibi_slopes = False
use_new_kcache_layout = False
2025-04-11T04:23:18.7468561Z
@pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
@pytest.mark.parametrize("bsz", [7, 16])
@pytest.mark.parametrize("block_size", [16, 32])
@pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
@pytest.mark.parametrize("num_attn_heads", [16])
@pytest.mark.parametrize("kv_group_num", [1, 4])
@pytest.mark.parametrize("same_context_len", [True, False])
@pytest.mark.parametrize("q_len", [1, 5])
@pytest.mark.parametrize("use_alibi_slopes", [True, False])
@pytest.mark.parametrize("use_new_kcache_layout", [True, False])
def test_flash_decoding(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_attn_heads: int,
kv_group_num: int,
same_context_len: bool,
q_len: int,
use_alibi_slopes: bool,
use_new_kcache_layout: bool,
):
if use_new_kcache_layout and use_alibi_slopes:
# TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
# the code (alibi kernel) will be refactored later to avoid code duplication, when
# the whole triton flow with new k cache layout has been supported and tested.
# And tests for the alibi kernel using new kcache layout will be added then.
pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.7472065Z
torch.manual_seed(123)
torch.cuda.empty_cache()
>       torch.cuda.synchronize()
2025-04-11T04:23:18.7472341Z
tests/test_infer/test_kernels/triton/test_decoding_attn.py:100:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.7472617Z
device = None
2025-04-11T04:23:18.7472695Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7473034Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7474578Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
____________ test_flash_decoding[False-False-5-True-4-16-16-32-16] _____________
2025-04-11T04:23:18.7474988Z
bsz = 16, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
kv_group_num = 4, same_context_len = True, q_len = 5, use_alibi_slopes = False
use_new_kcache_layout = False
2025-04-11T04:23:18.7475484Z
@pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
@pytest.mark.parametrize("bsz", [7, 16])
@pytest.mark.parametrize("block_size", [16, 32])
@pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
@pytest.mark.parametrize("num_attn_heads", [16])
@pytest.mark.parametrize("kv_group_num", [1, 4])
@pytest.mark.parametrize("same_context_len", [True, False])
@pytest.mark.parametrize("q_len", [1, 5])
@pytest.mark.parametrize("use_alibi_slopes", [True, False])
@pytest.mark.parametrize("use_new_kcache_layout", [True, False])
def test_flash_decoding(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_attn_heads: int,
kv_group_num: int,
same_context_len: bool,
q_len: int,
use_alibi_slopes: bool,
use_new_kcache_layout: bool,
):
if use_new_kcache_layout and use_alibi_slopes:
# TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
# the code (alibi kernel) will be refactored later to avoid code duplication, when
# the whole triton flow with new k cache layout has been supported and tested.
# And tests for the alibi kernel using new kcache layout will be added then.
pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.7478900Z
torch.manual_seed(123)
torch.cuda.empty_cache()
>       torch.cuda.synchronize()
2025-04-11T04:23:18.7479165Z
tests/test_infer/test_kernels/triton/test_decoding_attn.py:100:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.7479437Z
device = None
2025-04-11T04:23:18.7479516Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7479855Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7481497Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
_____________ test_flash_decoding[False-False-5-False-1-16-8-16-7] _____________
2025-04-11T04:23:18.7481903Z
bsz = 7, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
kv_group_num = 1, same_context_len = False, q_len = 5, use_alibi_slopes = False
use_new_kcache_layout = False
2025-04-11T04:23:18.7482309Z
@pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
@pytest.mark.parametrize("bsz", [7, 16])
@pytest.mark.parametrize("block_size", [16, 32])
@pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
@pytest.mark.parametrize("num_attn_heads", [16])
@pytest.mark.parametrize("kv_group_num", [1, 4])
@pytest.mark.parametrize("same_context_len", [True, False])
@pytest.mark.parametrize("q_len", [1, 5])
@pytest.mark.parametrize("use_alibi_slopes", [True, False])
@pytest.mark.parametrize("use_new_kcache_layout", [True, False])
def test_flash_decoding(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_attn_heads: int,
kv_group_num: int,
same_context_len: bool,
q_len: int,
use_alibi_slopes: bool,
use_new_kcache_layout: bool,
):
if use_new_kcache_layout and use_alibi_slopes:
# TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
# the code (alibi kernel) will be refactored later to avoid code duplication, when
# the whole triton flow with new k cache layout has been supported and tested.
# And tests for the alibi kernel using new kcache layout will be added then.
pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.7485702Z
torch.manual_seed(123)
torch.cuda.empty_cache()
>       torch.cuda.synchronize()
2025-04-11T04:23:18.7485979Z
tests/test_infer/test_kernels/triton/test_decoding_attn.py:100:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.7486257Z
device = None
2025-04-11T04:23:18.7486335Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7486677Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7488325Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
____________ test_flash_decoding[False-False-5-False-1-16-8-16-16] _____________
2025-04-11T04:23:18.7488739Z
bsz = 16, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
kv_group_num = 1, same_context_len = False, q_len = 5, use_alibi_slopes = False
use_new_kcache_layout = False
2025-04-11T04:23:18.7489206Z
@pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
@pytest.mark.parametrize("bsz", [7, 16])
@pytest.mark.parametrize("block_size", [16, 32])
@pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
@pytest.mark.parametrize("num_attn_heads", [16])
@pytest.mark.parametrize("kv_group_num", [1, 4])
@pytest.mark.parametrize("same_context_len", [True, False])
@pytest.mark.parametrize("q_len", [1, 5])
@pytest.mark.parametrize("use_alibi_slopes", [True, False])
@pytest.mark.parametrize("use_new_kcache_layout", [True, False])
def test_flash_decoding(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_attn_heads: int,
kv_group_num: int,
same_context_len: bool,
q_len: int,
use_alibi_slopes: bool,
use_new_kcache_layout: bool,
):
if use_new_kcache_layout and use_alibi_slopes:
# TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
# the code (alibi kernel) will be refactored later to avoid code duplication, when
# the whole triton flow with new k cache layout has been supported and tested.
# And tests for the alibi kernel using new kcache layout will be added then.
pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.7492533Z
torch.manual_seed(123)
torch.cuda.empty_cache()
>       torch.cuda.synchronize()
2025-04-11T04:23:18.7492797Z
tests/test_infer/test_kernels/triton/test_decoding_attn.py:100:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.7493069Z
device = None
2025-04-11T04:23:18.7493148Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7493578Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7495182Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
_____________ test_flash_decoding[False-False-5-False-1-16-8-32-7] _____________
2025-04-11T04:23:18.7495597Z
bsz = 7, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
kv_group_num = 1, same_context_len = False, q_len = 5, use_alibi_slopes = False
use_new_kcache_layout = False
2025-04-11T04:23:18.7496052Z
@pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
@pytest.mark.parametrize("bsz", [7, 16])
@pytest.mark.parametrize("block_size", [16, 32])
@pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
@pytest.mark.parametrize("num_attn_heads", [16])
@pytest.mark.parametrize("kv_group_num", [1, 4])
@pytest.mark.parametrize("same_context_len", [True, False])
@pytest.mark.parametrize("q_len", [1, 5])
@pytest.mark.parametrize("use_alibi_slopes", [True, False])
@pytest.mark.parametrize("use_new_kcache_layout", [True, False])
def test_flash_decoding(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_attn_heads: int,
kv_group_num: int,
same_context_len: bool,
q_len: int,
use_alibi_slopes: bool,
use_new_kcache_layout: bool,
):
if use_new_kcache_layout and use_alibi_slopes:
# TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
# the code (alibi kernel) will be refactored later to avoid code duplication, when
# the whole triton flow with new k cache layout has been supported and tested.
# And tests for the alibi kernel using new kcache layout will be added then.
pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.7499324Z
torch.manual_seed(123)
torch.cuda.empty_cache()
>       torch.cuda.synchronize()
2025-04-11T04:23:18.7499696Z
tests/test_infer/test_kernels/triton/test_decoding_attn.py:100:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.7499972Z
device = None
2025-04-11T04:23:18.7500051Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7500393Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7502083Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
____________ test_flash_decoding[False-False-5-False-1-16-8-32-16] _____________
2025-04-11T04:23:18.7502500Z
bsz = 16, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
kv_group_num = 1, same_context_len = False, q_len = 5, use_alibi_slopes = False
use_new_kcache_layout = False
2025-04-11T04:23:18.7502913Z
@pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
@pytest.mark.parametrize("bsz", [7, 16])
@pytest.mark.parametrize("block_size", [16, 32])
@pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
@pytest.mark.parametrize("num_attn_heads", [16])
@pytest.mark.parametrize("kv_group_num", [1, 4])
@pytest.mark.parametrize("same_context_len", [True, False])
@pytest.mark.parametrize("q_len", [1, 5])
@pytest.mark.parametrize("use_alibi_slopes", [True, False])
@pytest.mark.parametrize("use_new_kcache_layout", [True, False])
def test_flash_decoding(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_attn_heads: int,
kv_group_num: int,
same_context_len: bool,
q_len: int,
use_alibi_slopes: bool,
use_new_kcache_layout: bool,
):
if use_new_kcache_layout and use_alibi_slopes:
# TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
# the code (alibi kernel) will be refactored later to avoid code duplication, when
# the whole triton flow with new k cache layout has been supported and tested.
# And tests for the alibi kernel using new kcache layout will be added then.
pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.7506274Z
torch.manual_seed(123)
torch.cuda.empty_cache()
>       torch.cuda.synchronize()
2025-04-11T04:23:18.7506542Z
tests/test_infer/test_kernels/triton/test_decoding_attn.py:100:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.7506816Z
device = None
2025-04-11T04:23:18.7506893Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7507235Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7508940Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
____________ test_flash_decoding[False-False-5-False-1-16-16-16-7] _____________
2025-04-11T04:23:18.7509355Z
bsz = 7, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
kv_group_num = 1, same_context_len = False, q_len = 5, use_alibi_slopes = False
use_new_kcache_layout = False
2025-04-11T04:23:18.7509763Z
@pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
@pytest.mark.parametrize("bsz", [7, 16])
@pytest.mark.parametrize("block_size", [16, 32])
@pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
@pytest.mark.parametrize("num_attn_heads", [16])
@pytest.mark.parametrize("kv_group_num", [1, 4])
@pytest.mark.parametrize("same_context_len", [True, False])
@pytest.mark.parametrize("q_len", [1, 5])
@pytest.mark.parametrize("use_alibi_slopes", [True, False])
@pytest.mark.parametrize("use_new_kcache_layout", [True, False])
def test_flash_decoding(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_attn_heads: int,
kv_group_num: int,
same_context_len: bool,
q_len: int,
use_alibi_slopes: bool,
use_new_kcache_layout: bool,
):
if use_new_kcache_layout and use_alibi_slopes:
# TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
# the code (alibi kernel) will be refactored later to avoid code duplication, when
# the whole triton flow with new k cache layout has been supported and tested.
# And tests for the alibi kernel using new kcache layout will be added then.
pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.7513136Z
torch.manual_seed(123)
torch.cuda.empty_cache()
>       torch.cuda.synchronize()
2025-04-11T04:23:18.7513405Z
tests/test_infer/test_kernels/triton/test_decoding_attn.py:100:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.7513730Z
device = None
2025-04-11T04:23:18.7513809Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7514152Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7515740Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
____________ test_flash_decoding[False-False-5-False-1-16-16-16-16] ____________
2025-04-11T04:23:18.7516155Z
bsz = 16, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
kv_group_num = 1, same_context_len = False, q_len = 5, use_alibi_slopes = False
use_new_kcache_layout = False
2025-04-11T04:23:18.7516563Z
@pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
@pytest.mark.parametrize("bsz", [7, 16])
@pytest.mark.parametrize("block_size", [16, 32])
@pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
@pytest.mark.parametrize("num_attn_heads", [16])
@pytest.mark.parametrize("kv_group_num", [1, 4])
@pytest.mark.parametrize("same_context_len", [True, False])
@pytest.mark.parametrize("q_len", [1, 5])
@pytest.mark.parametrize("use_alibi_slopes", [True, False])
@pytest.mark.parametrize("use_new_kcache_layout", [True, False])
def test_flash_decoding(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_attn_heads: int,
kv_group_num: int,
same_context_len: bool,
q_len: int,
use_alibi_slopes: bool,
use_new_kcache_layout: bool,
):
if use_new_kcache_layout and use_alibi_slopes:
# TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
# the code (alibi kernel) will be refactored later to avoid code duplication, when
# the whole triton flow with new k cache layout has been supported and tested.
# And tests for the alibi kernel using new kcache layout will be added then.
pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.7519969Z
torch.manual_seed(123)
torch.cuda.empty_cache()
>       torch.cuda.synchronize()
2025-04-11T04:23:18.7520238Z
tests/test_infer/test_kernels/triton/test_decoding_attn.py:100:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.7520559Z
device = None
2025-04-11T04:23:18.7520645Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7520987Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7522535Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
____________ test_flash_decoding[False-False-5-False-1-16-16-32-7] _____________
2025-04-11T04:23:18.7522953Z
bsz = 7, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
kv_group_num = 1, same_context_len = False, q_len = 5, use_alibi_slopes = False
use_new_kcache_layout = False
2025-04-11T04:23:18.7523366Z
@pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
@pytest.mark.parametrize("bsz", [7, 16])
@pytest.mark.parametrize("block_size", [16, 32])
@pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
@pytest.mark.parametrize("num_attn_heads", [16])
@pytest.mark.parametrize("kv_group_num", [1, 4])
@pytest.mark.parametrize("same_context_len", [True, False])
@pytest.mark.parametrize("q_len", [1, 5])
@pytest.mark.parametrize("use_alibi_slopes", [True, False])
@pytest.mark.parametrize("use_new_kcache_layout", [True, False])
def test_flash_decoding(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_attn_heads: int,
kv_group_num: int,
same_context_len: bool,
q_len: int,
use_alibi_slopes: bool,
use_new_kcache_layout: bool,
):
if use_new_kcache_layout and use_alibi_slopes:
# TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
# the code (alibi kernel) will be refactored later to avoid code duplication, when
# the whole triton flow with new k cache layout has been supported and tested.
# And tests for the alibi kernel using new kcache layout will be added then.
pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.7526852Z
torch.manual_seed(123)
torch.cuda.empty_cache()
>       torch.cuda.synchronize()
2025-04-11T04:23:18.7527122Z
tests/test_infer/test_kernels/triton/test_decoding_attn.py:100:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.7527394Z
device = None
2025-04-11T04:23:18.7527475Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7527812Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7529361Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
____________ test_flash_decoding[False-False-5-False-1-16-16-32-16] ____________
2025-04-11T04:23:18.7529777Z
bsz = 16, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
kv_group_num = 1, same_context_len = False, q_len = 5, use_alibi_slopes = False
use_new_kcache_layout = False
2025-04-11T04:23:18.7530277Z
@pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
@pytest.mark.parametrize("bsz", [7, 16])
@pytest.mark.parametrize("block_size", [16, 32])
@pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
@pytest.mark.parametrize("num_attn_heads", [16])
@pytest.mark.parametrize("kv_group_num", [1, 4])
@pytest.mark.parametrize("same_context_len", [True, False])
@pytest.mark.parametrize("q_len", [1, 5])
@pytest.mark.parametrize("use_alibi_slopes", [True, False])
@pytest.mark.parametrize("use_new_kcache_layout", [True, False])
def test_flash_decoding(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_attn_heads: int,
kv_group_num: int,
same_context_len: bool,
q_len: int,
use_alibi_slopes: bool,
use_new_kcache_layout: bool,
):
if use_new_kcache_layout and use_alibi_slopes:
# TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
# the code (alibi kernel) will be refactored later to avoid code duplication, when
# the whole triton flow with new k cache layout has been supported and tested.
# And tests for the alibi kernel using new kcache layout will be added then.
pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.7533674Z
torch.manual_seed(123)
torch.cuda.empty_cache()
>       torch.cuda.synchronize()
2025-04-11T04:23:18.7533947Z
tests/test_infer/test_kernels/triton/test_decoding_attn.py:100:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.7534227Z
device = None
2025-04-11T04:23:18.7534309Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7534650Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7536280Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
_____________ test_flash_decoding[False-False-5-False-4-16-8-16-7] _____________
2025-04-11T04:23:18.7536696Z
bsz = 7, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
kv_group_num = 4, same_context_len = False, q_len = 5, use_alibi_slopes = False
use_new_kcache_layout = False
2025-04-11T04:23:18.7537106Z
@pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
@pytest.mark.parametrize("bsz", [7, 16])
@pytest.mark.parametrize("block_size", [16, 32])
@pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
@pytest.mark.parametrize("num_attn_heads", [16])
@pytest.mark.parametrize("kv_group_num", [1, 4])
@pytest.mark.parametrize("same_context_len", [True, False])
@pytest.mark.parametrize("q_len", [1, 5])
@pytest.mark.parametrize("use_alibi_slopes", [True, False])
@pytest.mark.parametrize("use_new_kcache_layout", [True, False])
def test_flash_decoding(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_attn_heads: int,
kv_group_num: int,
same_context_len: bool,
q_len: int,
use_alibi_slopes: bool,
use_new_kcache_layout: bool,
):
if use_new_kcache_layout and use_alibi_slopes:
# TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
# the code (alibi kernel) will be refactored later to avoid code duplication, when
# the whole triton flow with new k cache layout has been supported and tested.
# And tests for the alibi kernel using new kcache layout will be added then.
pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.7540570Z
torch.manual_seed(123)
torch.cuda.empty_cache()
>       torch.cuda.synchronize()
2025-04-11T04:23:18.7540843Z
tests/test_infer/test_kernels/triton/test_decoding_attn.py:100:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.7541116Z
device = None
2025-04-11T04:23:18.7541198Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7541533Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7543171Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
____________ test_flash_decoding[False-False-5-False-4-16-8-16-16] _____________
2025-04-11T04:23:18.7543587Z
bsz = 16, block_size = 16, max_num_blocks_per_seq = 8, num_attn_heads = 16
kv_group_num = 4, same_context_len = False, q_len = 5, use_alibi_slopes = False
use_new_kcache_layout = False
2025-04-11T04:23:18.7544046Z
@pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
@pytest.mark.parametrize("bsz", [7, 16])
@pytest.mark.parametrize("block_size", [16, 32])
@pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
@pytest.mark.parametrize("num_attn_heads", [16])
@pytest.mark.parametrize("kv_group_num", [1, 4])
@pytest.mark.parametrize("same_context_len", [True, False])
@pytest.mark.parametrize("q_len", [1, 5])
@pytest.mark.parametrize("use_alibi_slopes", [True, False])
@pytest.mark.parametrize("use_new_kcache_layout", [True, False])
def test_flash_decoding(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_attn_heads: int,
kv_group_num: int,
same_context_len: bool,
q_len: int,
use_alibi_slopes: bool,
use_new_kcache_layout: bool,
):
if use_new_kcache_layout and use_alibi_slopes:
# TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
# the code (alibi kernel) will be refactored later to avoid code duplication, when
# the whole triton flow with new k cache layout has been supported and tested.
# And tests for the alibi kernel using new kcache layout will be added then.
pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.7547407Z
torch.manual_seed(123)
torch.cuda.empty_cache()
>       torch.cuda.synchronize()
2025-04-11T04:23:18.7547678Z
tests/test_infer/test_kernels/triton/test_decoding_attn.py:100:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.7547952Z
device = None
2025-04-11T04:23:18.7548032Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7548498Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7550071Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
_____________ test_flash_decoding[False-False-5-False-4-16-8-32-7] _____________
2025-04-11T04:23:18.7550550Z
bsz = 7, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
kv_group_num = 4, same_context_len = False, q_len = 5, use_alibi_slopes = False
use_new_kcache_layout = False
2025-04-11T04:23:18.7551019Z
@pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
@pytest.mark.parametrize("bsz", [7, 16])
@pytest.mark.parametrize("block_size", [16, 32])
@pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
@pytest.mark.parametrize("num_attn_heads", [16])
@pytest.mark.parametrize("kv_group_num", [1, 4])
@pytest.mark.parametrize("same_context_len", [True, False])
@pytest.mark.parametrize("q_len", [1, 5])
@pytest.mark.parametrize("use_alibi_slopes", [True, False])
@pytest.mark.parametrize("use_new_kcache_layout", [True, False])
def test_flash_decoding(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_attn_heads: int,
kv_group_num: int,
same_context_len: bool,
q_len: int,
use_alibi_slopes: bool,
use_new_kcache_layout: bool,
):
if use_new_kcache_layout and use_alibi_slopes:
# TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
# the code (alibi kernel) will be refactored later to avoid code duplication, when
# the whole triton flow with new k cache layout has been supported and tested.
# And tests for the alibi kernel using new kcache layout will be added then.
pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.7554279Z
torch.manual_seed(123)
torch.cuda.empty_cache()
>       torch.cuda.synchronize()
2025-04-11T04:23:18.7554648Z
tests/test_infer/test_kernels/triton/test_decoding_attn.py:100:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.7554926Z
device = None
2025-04-11T04:23:18.7555009Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7555349Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7557004Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
____________ test_flash_decoding[False-False-5-False-4-16-8-32-16] _____________
2025-04-11T04:23:18.7557419Z
bsz = 16, block_size = 32, max_num_blocks_per_seq = 8, num_attn_heads = 16
kv_group_num = 4, same_context_len = False, q_len = 5, use_alibi_slopes = False
use_new_kcache_layout = False
2025-04-11T04:23:18.7557829Z
@pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
@pytest.mark.parametrize("bsz", [7, 16])
@pytest.mark.parametrize("block_size", [16, 32])
@pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
@pytest.mark.parametrize("num_attn_heads", [16])
@pytest.mark.parametrize("kv_group_num", [1, 4])
@pytest.mark.parametrize("same_context_len", [True, False])
@pytest.mark.parametrize("q_len", [1, 5])
@pytest.mark.parametrize("use_alibi_slopes", [True, False])
@pytest.mark.parametrize("use_new_kcache_layout", [True, False])
def test_flash_decoding(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_attn_heads: int,
kv_group_num: int,
same_context_len: bool,
q_len: int,
use_alibi_slopes: bool,
use_new_kcache_layout: bool,
):
if use_new_kcache_layout and use_alibi_slopes:
# TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
# the code (alibi kernel) will be refactored later to avoid code duplication, when
# the whole triton flow with new k cache layout has been supported and tested.
# And tests for the alibi kernel using new kcache layout will be added then.
pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.7561236Z
torch.manual_seed(123)
torch.cuda.empty_cache()
>       torch.cuda.synchronize()
2025-04-11T04:23:18.7561508Z
tests/test_infer/test_kernels/triton/test_decoding_attn.py:100:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.7561785Z
device = None
2025-04-11T04:23:18.7561866Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7562210Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7563866Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
____________ test_flash_decoding[False-False-5-False-4-16-16-16-7] _____________
2025-04-11T04:23:18.7564281Z
bsz = 7, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
kv_group_num = 4, same_context_len = False, q_len = 5, use_alibi_slopes = False
use_new_kcache_layout = False
2025-04-11T04:23:18.7564699Z
@pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
@pytest.mark.parametrize("bsz", [7, 16])
@pytest.mark.parametrize("block_size", [16, 32])
@pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
@pytest.mark.parametrize("num_attn_heads", [16])
@pytest.mark.parametrize("kv_group_num", [1, 4])
@pytest.mark.parametrize("same_context_len", [True, False])
@pytest.mark.parametrize("q_len", [1, 5])
@pytest.mark.parametrize("use_alibi_slopes", [True, False])
@pytest.mark.parametrize("use_new_kcache_layout", [True, False])
def test_flash_decoding(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_attn_heads: int,
kv_group_num: int,
same_context_len: bool,
q_len: int,
use_alibi_slopes: bool,
use_new_kcache_layout: bool,
):
if use_new_kcache_layout and use_alibi_slopes:
# TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
# the code (alibi kernel) will be refactored later to avoid code duplication, when
# the whole triton flow with new k cache layout has been supported and tested.
# And tests for the alibi kernel using new kcache layout will be added then.
pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.7568070Z
torch.manual_seed(123)
torch.cuda.empty_cache()
>       torch.cuda.synchronize()
2025-04-11T04:23:18.7568342Z
tests/test_infer/test_kernels/triton/test_decoding_attn.py:100:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.7568664Z
device = None
2025-04-11T04:23:18.7568747Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7569085Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7570703Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
____________ test_flash_decoding[False-False-5-False-4-16-16-16-16] ____________
2025-04-11T04:23:18.7571117Z
bsz = 16, block_size = 16, max_num_blocks_per_seq = 16, num_attn_heads = 16
kv_group_num = 4, same_context_len = False, q_len = 5, use_alibi_slopes = False
use_new_kcache_layout = False
2025-04-11T04:23:18.7571527Z
@pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
@pytest.mark.parametrize("bsz", [7, 16])
@pytest.mark.parametrize("block_size", [16, 32])
@pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
@pytest.mark.parametrize("num_attn_heads", [16])
@pytest.mark.parametrize("kv_group_num", [1, 4])
@pytest.mark.parametrize("same_context_len", [True, False])
@pytest.mark.parametrize("q_len", [1, 5])
@pytest.mark.parametrize("use_alibi_slopes", [True, False])
@pytest.mark.parametrize("use_new_kcache_layout", [True, False])
def test_flash_decoding(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_attn_heads: int,
kv_group_num: int,
same_context_len: bool,
q_len: int,
use_alibi_slopes: bool,
use_new_kcache_layout: bool,
):
if use_new_kcache_layout and use_alibi_slopes:
# TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
# the code (alibi kernel) will be refactored later to avoid code duplication, when
# the whole triton flow with new k cache layout has been supported and tested.
# And tests for the alibi kernel using new kcache layout will be added then.
pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.7574933Z
torch.manual_seed(123)
torch.cuda.empty_cache()
>       torch.cuda.synchronize()
2025-04-11T04:23:18.7575199Z
tests/test_infer/test_kernels/triton/test_decoding_attn.py:100:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.7575522Z
device = None
2025-04-11T04:23:18.7575605Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7575946Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7577485Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
____________ test_flash_decoding[False-False-5-False-4-16-16-32-7] _____________
2025-04-11T04:23:18.7577898Z
bsz = 7, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
kv_group_num = 4, same_context_len = False, q_len = 5, use_alibi_slopes = False
use_new_kcache_layout = False
2025-04-11T04:23:18.7578308Z
@pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
@pytest.mark.parametrize("bsz", [7, 16])
@pytest.mark.parametrize("block_size", [16, 32])
@pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
@pytest.mark.parametrize("num_attn_heads", [16])
@pytest.mark.parametrize("kv_group_num", [1, 4])
@pytest.mark.parametrize("same_context_len", [True, False])
@pytest.mark.parametrize("q_len", [1, 5])
@pytest.mark.parametrize("use_alibi_slopes", [True, False])
@pytest.mark.parametrize("use_new_kcache_layout", [True, False])
def test_flash_decoding(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_attn_heads: int,
kv_group_num: int,
same_context_len: bool,
q_len: int,
use_alibi_slopes: bool,
use_new_kcache_layout: bool,
):
if use_new_kcache_layout and use_alibi_slopes:
# TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
# the code (alibi kernel) will be refactored later to avoid code duplication, when
# the whole triton flow with new k cache layout has been supported and tested.
# And tests for the alibi kernel using new kcache layout will be added then.
pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.7581771Z
torch.manual_seed(123)
torch.cuda.empty_cache()
>       torch.cuda.synchronize()
2025-04-11T04:23:18.7582049Z
tests/test_infer/test_kernels/triton/test_decoding_attn.py:100:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.7582319Z
device = None
2025-04-11T04:23:18.7582402Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7582738Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7584287Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
____________ test_flash_decoding[False-False-5-False-4-16-16-32-16] ____________
2025-04-11T04:23:18.7584700Z
bsz = 16, block_size = 32, max_num_blocks_per_seq = 16, num_attn_heads = 16
kv_group_num = 4, same_context_len = False, q_len = 5, use_alibi_slopes = False
use_new_kcache_layout = False
2025-04-11T04:23:18.7585196Z
@pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
@pytest.mark.parametrize("bsz", [7, 16])
@pytest.mark.parametrize("block_size", [16, 32])
@pytest.mark.parametrize("max_num_blocks_per_seq", [8, 16])
@pytest.mark.parametrize("num_attn_heads", [16])
@pytest.mark.parametrize("kv_group_num", [1, 4])
@pytest.mark.parametrize("same_context_len", [True, False])
@pytest.mark.parametrize("q_len", [1, 5])
@pytest.mark.parametrize("use_alibi_slopes", [True, False])
@pytest.mark.parametrize("use_new_kcache_layout", [True, False])
def test_flash_decoding(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_attn_heads: int,
kv_group_num: int,
same_context_len: bool,
q_len: int,
use_alibi_slopes: bool,
use_new_kcache_layout: bool,
):
if use_new_kcache_layout and use_alibi_slopes:
# TODO(yuanheng-zhao): Since the alibi kernel is pretty similar to the original one,
# the code (alibi kernel) will be refactored later to avoid code duplication, when
# the whole triton flow with new k cache layout has been supported and tested.
# And tests for the alibi kernel using new kcache layout will be added then.
pytest.skip("Alibi kernel does not support new kcache layout yet.")
2025-04-11T04:23:18.7588607Z
torch.manual_seed(123)
torch.cuda.empty_cache()
>       torch.cuda.synchronize()
2025-04-11T04:23:18.7588882Z
tests/test_infer/test_kernels/triton/test_decoding_attn.py:100:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.7589152Z
device = None
2025-04-11T04:23:18.7589235Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7589576Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7591220Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
________________ test_copy_kv_to_caches[True-1-True-16-16-16-7] ________________
2025-04-11T04:23:18.7591623Z
bsz = 7, block_size = 16, max_num_blocks_per_seq = 16, num_kv_heads = 16
same_context_len = True, n_tokens = 1, use_new_kcache_layout = True
2025-04-11T04:23:18.7591936Z
@pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
@pytest.mark.parametrize("bsz", [7, 32])
@pytest.mark.parametrize("block_size", [16, 32, 64])
@pytest.mark.parametrize("max_num_blocks_per_seq", [16])
@pytest.mark.parametrize("num_kv_heads", [16])
@pytest.mark.parametrize("same_context_len", [True, False])
@pytest.mark.parametrize("n_tokens", [1, 5])
@pytest.mark.parametrize("use_new_kcache_layout", [True, False])
def test_copy_kv_to_caches(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_kv_heads: int,
same_context_len: bool,
n_tokens: int,
use_new_kcache_layout: bool,
):
torch.manual_seed(123)
torch.cuda.empty_cache()
>       torch.cuda.synchronize()
2025-04-11T04:23:18.7594188Z
tests/test_infer/test_kernels/triton/test_kvcache_copy.py:91:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.7594461Z
device = None
2025-04-11T04:23:18.7594543Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7594879Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7596422Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
_______________ test_copy_kv_to_caches[True-1-True-16-16-16-32] ________________
2025-04-11T04:23:18.7596821Z
bsz = 32, block_size = 16, max_num_blocks_per_seq = 16, num_kv_heads = 16
same_context_len = True, n_tokens = 1, use_new_kcache_layout = True
2025-04-11T04:23:18.7597215Z
@pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
@pytest.mark.parametrize("bsz", [7, 32])
@pytest.mark.parametrize("block_size", [16, 32, 64])
@pytest.mark.parametrize("max_num_blocks_per_seq", [16])
@pytest.mark.parametrize("num_kv_heads", [16])
@pytest.mark.parametrize("same_context_len", [True, False])
@pytest.mark.parametrize("n_tokens", [1, 5])
@pytest.mark.parametrize("use_new_kcache_layout", [True, False])
def test_copy_kv_to_caches(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_kv_heads: int,
same_context_len: bool,
n_tokens: int,
use_new_kcache_layout: bool,
):
torch.manual_seed(123)
torch.cuda.empty_cache()
>       torch.cuda.synchronize()
2025-04-11T04:23:18.7599378Z
tests/test_infer/test_kernels/triton/test_kvcache_copy.py:91:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.7599702Z
device = None
2025-04-11T04:23:18.7599787Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7600132Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7601680Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
________________ test_copy_kv_to_caches[True-1-True-16-16-32-7] ________________
2025-04-11T04:23:18.7602075Z
bsz = 7, block_size = 32, max_num_blocks_per_seq = 16, num_kv_heads = 16
same_context_len = True, n_tokens = 1, use_new_kcache_layout = True
2025-04-11T04:23:18.7602381Z
@pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
@pytest.mark.parametrize("bsz", [7, 32])
@pytest.mark.parametrize("block_size", [16, 32, 64])
@pytest.mark.parametrize("max_num_blocks_per_seq", [16])
@pytest.mark.parametrize("num_kv_heads", [16])
@pytest.mark.parametrize("same_context_len", [True, False])
@pytest.mark.parametrize("n_tokens", [1, 5])
@pytest.mark.parametrize("use_new_kcache_layout", [True, False])
def test_copy_kv_to_caches(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_kv_heads: int,
same_context_len: bool,
n_tokens: int,
use_new_kcache_layout: bool,
):
torch.manual_seed(123)
torch.cuda.empty_cache()
>       torch.cuda.synchronize()
2025-04-11T04:23:18.7604571Z
tests/test_infer/test_kernels/triton/test_kvcache_copy.py:91:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.7604843Z
device = None
2025-04-11T04:23:18.7604978Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7605322Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7606930Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
_______________ test_copy_kv_to_caches[True-1-True-16-16-32-32] ________________
2025-04-11T04:23:18.7607332Z
bsz = 32, block_size = 32, max_num_blocks_per_seq = 16, num_kv_heads = 16
same_context_len = True, n_tokens = 1, use_new_kcache_layout = True
2025-04-11T04:23:18.7607642Z
@pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
@pytest.mark.parametrize("bsz", [7, 32])
@pytest.mark.parametrize("block_size", [16, 32, 64])
@pytest.mark.parametrize("max_num_blocks_per_seq", [16])
@pytest.mark.parametrize("num_kv_heads", [16])
@pytest.mark.parametrize("same_context_len", [True, False])
@pytest.mark.parametrize("n_tokens", [1, 5])
@pytest.mark.parametrize("use_new_kcache_layout", [True, False])
def test_copy_kv_to_caches(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_kv_heads: int,
same_context_len: bool,
n_tokens: int,
use_new_kcache_layout: bool,
):
torch.manual_seed(123)
torch.cuda.empty_cache()
>       torch.cuda.synchronize()
2025-04-11T04:23:18.7609823Z
tests/test_infer/test_kernels/triton/test_kvcache_copy.py:91:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.7610092Z
device = None
2025-04-11T04:23:18.7610175Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7610513Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7612164Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
________________ test_copy_kv_to_caches[True-1-True-16-16-64-7] ________________
2025-04-11T04:23:18.7612562Z
bsz = 7, block_size = 64, max_num_blocks_per_seq = 16, num_kv_heads = 16
same_context_len = True, n_tokens = 1, use_new_kcache_layout = True
2025-04-11T04:23:18.7612869Z
@pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
@pytest.mark.parametrize("bsz", [7, 32])
@pytest.mark.parametrize("block_size", [16, 32, 64])
@pytest.mark.parametrize("max_num_blocks_per_seq", [16])
@pytest.mark.parametrize("num_kv_heads", [16])
@pytest.mark.parametrize("same_context_len", [True, False])
@pytest.mark.parametrize("n_tokens", [1, 5])
@pytest.mark.parametrize("use_new_kcache_layout", [True, False])
def test_copy_kv_to_caches(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_kv_heads: int,
same_context_len: bool,
n_tokens: int,
use_new_kcache_layout: bool,
):
torch.manual_seed(123)
torch.cuda.empty_cache()
>       torch.cuda.synchronize()
2025-04-11T04:23:18.7614950Z
tests/test_infer/test_kernels/triton/test_kvcache_copy.py:91:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.7615316Z
device = None
2025-04-11T04:23:18.7615402Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7615738Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7617328Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
_______________ test_copy_kv_to_caches[True-1-True-16-16-64-32] ________________
2025-04-11T04:23:18.7617790Z
bsz = 32, block_size = 64, max_num_blocks_per_seq = 16, num_kv_heads = 16
same_context_len = True, n_tokens = 1, use_new_kcache_layout = True
2025-04-11T04:23:18.7618098Z
@pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
@pytest.mark.parametrize("bsz", [7, 32])
@pytest.mark.parametrize("block_size", [16, 32, 64])
@pytest.mark.parametrize("max_num_blocks_per_seq", [16])
@pytest.mark.parametrize("num_kv_heads", [16])
@pytest.mark.parametrize("same_context_len", [True, False])
@pytest.mark.parametrize("n_tokens", [1, 5])
@pytest.mark.parametrize("use_new_kcache_layout", [True, False])
def test_copy_kv_to_caches(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_kv_heads: int,
same_context_len: bool,
n_tokens: int,
use_new_kcache_layout: bool,
):
torch.manual_seed(123)
torch.cuda.empty_cache()
>       torch.cuda.synchronize()
2025-04-11T04:23:18.7620173Z
tests/test_infer/test_kernels/triton/test_kvcache_copy.py:91:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.7620438Z
device = None
2025-04-11T04:23:18.7620515Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7620856Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7622477Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
_______________ test_copy_kv_to_caches[True-1-False-16-16-16-7] ________________
2025-04-11T04:23:18.7622874Z
bsz = 7, block_size = 16, max_num_blocks_per_seq = 16, num_kv_heads = 16
same_context_len = False, n_tokens = 1, use_new_kcache_layout = True
2025-04-11T04:23:18.7623232Z
@pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
@pytest.mark.parametrize("bsz", [7, 32])
@pytest.mark.parametrize("block_size", [16, 32, 64])
@pytest.mark.parametrize("max_num_blocks_per_seq", [16])
@pytest.mark.parametrize("num_kv_heads", [16])
@pytest.mark.parametrize("same_context_len", [True, False])
@pytest.mark.parametrize("n_tokens", [1, 5])
@pytest.mark.parametrize("use_new_kcache_layout", [True, False])
def test_copy_kv_to_caches(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_kv_heads: int,
same_context_len: bool,
n_tokens: int,
use_new_kcache_layout: bool,
):
torch.manual_seed(123)
torch.cuda.empty_cache()
>       torch.cuda.synchronize()
2025-04-11T04:23:18.7625347Z
tests/test_infer/test_kernels/triton/test_kvcache_copy.py:91:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.7625613Z
device = None
2025-04-11T04:23:18.7625694Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7626035Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7627663Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
_______________ test_copy_kv_to_caches[True-1-False-16-16-16-32] _______________
2025-04-11T04:23:18.7628058Z
bsz = 32, block_size = 16, max_num_blocks_per_seq = 16, num_kv_heads = 16
same_context_len = False, n_tokens = 1, use_new_kcache_layout = True
2025-04-11T04:23:18.7628367Z
@pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
@pytest.mark.parametrize("bsz", [7, 32])
@pytest.mark.parametrize("block_size", [16, 32, 64])
@pytest.mark.parametrize("max_num_blocks_per_seq", [16])
@pytest.mark.parametrize("num_kv_heads", [16])
@pytest.mark.parametrize("same_context_len", [True, False])
@pytest.mark.parametrize("n_tokens", [1, 5])
@pytest.mark.parametrize("use_new_kcache_layout", [True, False])
def test_copy_kv_to_caches(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_kv_heads: int,
same_context_len: bool,
n_tokens: int,
use_new_kcache_layout: bool,
):
torch.manual_seed(123)
torch.cuda.empty_cache()
>       torch.cuda.synchronize()
2025-04-11T04:23:18.7630599Z
tests/test_infer/test_kernels/triton/test_kvcache_copy.py:91:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.7630864Z
device = None
2025-04-11T04:23:18.7630942Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7631281Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7632820Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
_______________ test_copy_kv_to_caches[True-1-False-16-16-32-7] ________________
2025-04-11T04:23:18.7633304Z
bsz = 7, block_size = 32, max_num_blocks_per_seq = 16, num_kv_heads = 16
same_context_len = False, n_tokens = 1, use_new_kcache_layout = True
2025-04-11T04:23:18.7633617Z
@pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
@pytest.mark.parametrize("bsz", [7, 32])
@pytest.mark.parametrize("block_size", [16, 32, 64])
@pytest.mark.parametrize("max_num_blocks_per_seq", [16])
@pytest.mark.parametrize("num_kv_heads", [16])
@pytest.mark.parametrize("same_context_len", [True, False])
@pytest.mark.parametrize("n_tokens", [1, 5])
@pytest.mark.parametrize("use_new_kcache_layout", [True, False])
def test_copy_kv_to_caches(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_kv_heads: int,
same_context_len: bool,
n_tokens: int,
use_new_kcache_layout: bool,
):
torch.manual_seed(123)
torch.cuda.empty_cache()
>       torch.cuda.synchronize()
2025-04-11T04:23:18.7635851Z
tests/test_infer/test_kernels/triton/test_kvcache_copy.py:91:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.7636175Z
device = None
2025-04-11T04:23:18.7636253Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7636597Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7638162Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
_______________ test_copy_kv_to_caches[True-1-False-16-16-32-32] _______________
2025-04-11T04:23:18.7638562Z
bsz = 32, block_size = 32, max_num_blocks_per_seq = 16, num_kv_heads = 16
same_context_len = False, n_tokens = 1, use_new_kcache_layout = True
2025-04-11T04:23:18.7638866Z
@pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
@pytest.mark.parametrize("bsz", [7, 32])
@pytest.mark.parametrize("block_size", [16, 32, 64])
@pytest.mark.parametrize("max_num_blocks_per_seq", [16])
@pytest.mark.parametrize("num_kv_heads", [16])
@pytest.mark.parametrize("same_context_len", [True, False])
@pytest.mark.parametrize("n_tokens", [1, 5])
@pytest.mark.parametrize("use_new_kcache_layout", [True, False])
def test_copy_kv_to_caches(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_kv_heads: int,
same_context_len: bool,
n_tokens: int,
use_new_kcache_layout: bool,
):
torch.manual_seed(123)
torch.cuda.empty_cache()
>       torch.cuda.synchronize()
2025-04-11T04:23:18.7641042Z
tests/test_infer/test_kernels/triton/test_kvcache_copy.py:91:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.7641359Z
device = None
2025-04-11T04:23:18.7641438Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7649766Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7651546Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
_______________ test_copy_kv_to_caches[True-1-False-16-16-64-7] ________________
2025-04-11T04:23:18.7651951Z
bsz = 7, block_size = 64, max_num_blocks_per_seq = 16, num_kv_heads = 16
same_context_len = False, n_tokens = 1, use_new_kcache_layout = True
2025-04-11T04:23:18.7652265Z
@pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
@pytest.mark.parametrize("bsz", [7, 32])
@pytest.mark.parametrize("block_size", [16, 32, 64])
@pytest.mark.parametrize("max_num_blocks_per_seq", [16])
@pytest.mark.parametrize("num_kv_heads", [16])
@pytest.mark.parametrize("same_context_len", [True, False])
@pytest.mark.parametrize("n_tokens", [1, 5])
@pytest.mark.parametrize("use_new_kcache_layout", [True, False])
def test_copy_kv_to_caches(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_kv_heads: int,
same_context_len: bool,
n_tokens: int,
use_new_kcache_layout: bool,
):
torch.manual_seed(123)
torch.cuda.empty_cache()
>       torch.cuda.synchronize()
2025-04-11T04:23:18.7654470Z
tests/test_infer/test_kernels/triton/test_kvcache_copy.py:91:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.7654746Z
device = None
2025-04-11T04:23:18.7654826Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7655170Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7656846Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
_______________ test_copy_kv_to_caches[True-1-False-16-16-64-32] _______________
2025-04-11T04:23:18.7657248Z
bsz = 32, block_size = 64, max_num_blocks_per_seq = 16, num_kv_heads = 16
same_context_len = False, n_tokens = 1, use_new_kcache_layout = True
2025-04-11T04:23:18.7657552Z
@pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
@pytest.mark.parametrize("bsz", [7, 32])
@pytest.mark.parametrize("block_size", [16, 32, 64])
@pytest.mark.parametrize("max_num_blocks_per_seq", [16])
@pytest.mark.parametrize("num_kv_heads", [16])
@pytest.mark.parametrize("same_context_len", [True, False])
@pytest.mark.parametrize("n_tokens", [1, 5])
@pytest.mark.parametrize("use_new_kcache_layout", [True, False])
def test_copy_kv_to_caches(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_kv_heads: int,
same_context_len: bool,
n_tokens: int,
use_new_kcache_layout: bool,
):
torch.manual_seed(123)
torch.cuda.empty_cache()
>       torch.cuda.synchronize()
2025-04-11T04:23:18.7659645Z
tests/test_infer/test_kernels/triton/test_kvcache_copy.py:91:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.7659999Z
device = None
2025-04-11T04:23:18.7660078Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7660418Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7662024Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
________________ test_copy_kv_to_caches[True-5-True-16-16-16-7] ________________
2025-04-11T04:23:18.7662468Z
bsz = 7, block_size = 16, max_num_blocks_per_seq = 16, num_kv_heads = 16
same_context_len = True, n_tokens = 5, use_new_kcache_layout = True
2025-04-11T04:23:18.7662779Z
@pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
@pytest.mark.parametrize("bsz", [7, 32])
@pytest.mark.parametrize("block_size", [16, 32, 64])
@pytest.mark.parametrize("max_num_blocks_per_seq", [16])
@pytest.mark.parametrize("num_kv_heads", [16])
@pytest.mark.parametrize("same_context_len", [True, False])
@pytest.mark.parametrize("n_tokens", [1, 5])
@pytest.mark.parametrize("use_new_kcache_layout", [True, False])
def test_copy_kv_to_caches(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_kv_heads: int,
same_context_len: bool,
n_tokens: int,
use_new_kcache_layout: bool,
):
torch.manual_seed(123)
torch.cuda.empty_cache()
>       torch.cuda.synchronize()
2025-04-11T04:23:18.7664843Z
tests/test_infer/test_kernels/triton/test_kvcache_copy.py:91:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.7665108Z
device = None
2025-04-11T04:23:18.7665188Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7665526Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7667166Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
_______________ test_copy_kv_to_caches[True-5-True-16-16-16-32] ________________
2025-04-11T04:23:18.7667607Z
bsz = 32, block_size = 16, max_num_blocks_per_seq = 16, num_kv_heads = 16
same_context_len = True, n_tokens = 5, use_new_kcache_layout = True
2025-04-11T04:23:18.7667910Z
@pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
@pytest.mark.parametrize("bsz", [7, 32])
@pytest.mark.parametrize("block_size", [16, 32, 64])
@pytest.mark.parametrize("max_num_blocks_per_seq", [16])
@pytest.mark.parametrize("num_kv_heads", [16])
@pytest.mark.parametrize("same_context_len", [True, False])
@pytest.mark.parametrize("n_tokens", [1, 5])
@pytest.mark.parametrize("use_new_kcache_layout", [True, False])
def test_copy_kv_to_caches(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_kv_heads: int,
same_context_len: bool,
n_tokens: int,
use_new_kcache_layout: bool,
):
torch.manual_seed(123)
torch.cuda.empty_cache()
>       torch.cuda.synchronize()
2025-04-11T04:23:18.7670097Z
tests/test_infer/test_kernels/triton/test_kvcache_copy.py:91:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.7670366Z
device = None
2025-04-11T04:23:18.7670445Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7670781Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7672417Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
________________ test_copy_kv_to_caches[True-5-True-16-16-32-7] ________________
2025-04-11T04:23:18.7672817Z
bsz = 7, block_size = 32, max_num_blocks_per_seq = 16, num_kv_heads = 16
same_context_len = True, n_tokens = 5, use_new_kcache_layout = True
2025-04-11T04:23:18.7673122Z
@pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
@pytest.mark.parametrize("bsz", [7, 32])
@pytest.mark.parametrize("block_size", [16, 32, 64])
@pytest.mark.parametrize("max_num_blocks_per_seq", [16])
@pytest.mark.parametrize("num_kv_heads", [16])
@pytest.mark.parametrize("same_context_len", [True, False])
@pytest.mark.parametrize("n_tokens", [1, 5])
@pytest.mark.parametrize("use_new_kcache_layout", [True, False])
def test_copy_kv_to_caches(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_kv_heads: int,
same_context_len: bool,
n_tokens: int,
use_new_kcache_layout: bool,
):
torch.manual_seed(123)
torch.cuda.empty_cache()
>       torch.cuda.synchronize()
2025-04-11T04:23:18.7675323Z
tests/test_infer/test_kernels/triton/test_kvcache_copy.py:91:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.7675587Z
device = None
2025-04-11T04:23:18.7675666Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7676003Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7677549Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
_______________ test_copy_kv_to_caches[True-5-True-16-16-32-32] ________________
2025-04-11T04:23:18.7678027Z
bsz = 32, block_size = 32, max_num_blocks_per_seq = 16, num_kv_heads = 16
same_context_len = True, n_tokens = 5, use_new_kcache_layout = True
2025-04-11T04:23:18.7678335Z
@pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
@pytest.mark.parametrize("bsz", [7, 32])
@pytest.mark.parametrize("block_size", [16, 32, 64])
@pytest.mark.parametrize("max_num_blocks_per_seq", [16])
@pytest.mark.parametrize("num_kv_heads", [16])
@pytest.mark.parametrize("same_context_len", [True, False])
@pytest.mark.parametrize("n_tokens", [1, 5])
@pytest.mark.parametrize("use_new_kcache_layout", [True, False])
def test_copy_kv_to_caches(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_kv_heads: int,
same_context_len: bool,
n_tokens: int,
use_new_kcache_layout: bool,
):
torch.manual_seed(123)
torch.cuda.empty_cache()
>       torch.cuda.synchronize()
2025-04-11T04:23:18.7680525Z
tests/test_infer/test_kernels/triton/test_kvcache_copy.py:91:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.7680797Z
device = None
2025-04-11T04:23:18.7680875Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7681213Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7682755Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
________________ test_copy_kv_to_caches[True-5-True-16-16-64-7] ________________
2025-04-11T04:23:18.7683159Z
bsz = 7, block_size = 64, max_num_blocks_per_seq = 16, num_kv_heads = 16
same_context_len = True, n_tokens = 5, use_new_kcache_layout = True
2025-04-11T04:23:18.7683465Z
@pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
@pytest.mark.parametrize("bsz", [7, 32])
@pytest.mark.parametrize("block_size", [16, 32, 64])
@pytest.mark.parametrize("max_num_blocks_per_seq", [16])
@pytest.mark.parametrize("num_kv_heads", [16])
@pytest.mark.parametrize("same_context_len", [True, False])
@pytest.mark.parametrize("n_tokens", [1, 5])
@pytest.mark.parametrize("use_new_kcache_layout", [True, False])
def test_copy_kv_to_caches(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_kv_heads: int,
same_context_len: bool,
n_tokens: int,
use_new_kcache_layout: bool,
):
torch.manual_seed(123)
torch.cuda.empty_cache()
>       torch.cuda.synchronize()
2025-04-11T04:23:18.7685697Z
tests/test_infer/test_kernels/triton/test_kvcache_copy.py:91:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.7685965Z
device = None
2025-04-11T04:23:18.7686044Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7686436Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7687982Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
_______________ test_copy_kv_to_caches[True-5-True-16-16-64-32] ________________
2025-04-11T04:23:18.7688379Z
bsz = 32, block_size = 64, max_num_blocks_per_seq = 16, num_kv_heads = 16
same_context_len = True, n_tokens = 5, use_new_kcache_layout = True
2025-04-11T04:23:18.7688682Z
@pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
@pytest.mark.parametrize("bsz", [7, 32])
@pytest.mark.parametrize("block_size", [16, 32, 64])
@pytest.mark.parametrize("max_num_blocks_per_seq", [16])
@pytest.mark.parametrize("num_kv_heads", [16])
@pytest.mark.parametrize("same_context_len", [True, False])
@pytest.mark.parametrize("n_tokens", [1, 5])
@pytest.mark.parametrize("use_new_kcache_layout", [True, False])
def test_copy_kv_to_caches(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_kv_heads: int,
same_context_len: bool,
n_tokens: int,
use_new_kcache_layout: bool,
):
torch.manual_seed(123)
torch.cuda.empty_cache()
>       torch.cuda.synchronize()
2025-04-11T04:23:18.7690836Z
tests/test_infer/test_kernels/triton/test_kvcache_copy.py:91:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.7691099Z
device = None
2025-04-11T04:23:18.7691180Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7691518Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7693171Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
_______________ test_copy_kv_to_caches[True-5-False-16-16-16-7] ________________
2025-04-11T04:23:18.7693565Z
bsz = 7, block_size = 16, max_num_blocks_per_seq = 16, num_kv_heads = 16
same_context_len = False, n_tokens = 5, use_new_kcache_layout = True
2025-04-11T04:23:18.7693870Z
@pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
@pytest.mark.parametrize("bsz", [7, 32])
@pytest.mark.parametrize("block_size", [16, 32, 64])
@pytest.mark.parametrize("max_num_blocks_per_seq", [16])
@pytest.mark.parametrize("num_kv_heads", [16])
@pytest.mark.parametrize("same_context_len", [True, False])
@pytest.mark.parametrize("n_tokens", [1, 5])
@pytest.mark.parametrize("use_new_kcache_layout", [True, False])
def test_copy_kv_to_caches(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_kv_heads: int,
same_context_len: bool,
n_tokens: int,
use_new_kcache_layout: bool,
):
torch.manual_seed(123)
torch.cuda.empty_cache()
>       torch.cuda.synchronize()
2025-04-11T04:23:18.7696035Z
tests/test_infer/test_kernels/triton/test_kvcache_copy.py:91:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.7696301Z
device = None
2025-04-11T04:23:18.7696379Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7696714Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7698361Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
_______________ test_copy_kv_to_caches[True-5-False-16-16-16-32] _______________
2025-04-11T04:23:18.7698755Z
bsz = 32, block_size = 16, max_num_blocks_per_seq = 16, num_kv_heads = 16
same_context_len = False, n_tokens = 5, use_new_kcache_layout = True
2025-04-11T04:23:18.7699064Z
@pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
@pytest.mark.parametrize("bsz", [7, 32])
@pytest.mark.parametrize("block_size", [16, 32, 64])
@pytest.mark.parametrize("max_num_blocks_per_seq", [16])
@pytest.mark.parametrize("num_kv_heads", [16])
@pytest.mark.parametrize("same_context_len", [True, False])
@pytest.mark.parametrize("n_tokens", [1, 5])
@pytest.mark.parametrize("use_new_kcache_layout", [True, False])
def test_copy_kv_to_caches(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_kv_heads: int,
same_context_len: bool,
n_tokens: int,
use_new_kcache_layout: bool,
):
torch.manual_seed(123)
torch.cuda.empty_cache()
>       torch.cuda.synchronize()
2025-04-11T04:23:18.7701138Z
tests/test_infer/test_kernels/triton/test_kvcache_copy.py:91:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.7701402Z
device = None
2025-04-11T04:23:18.7701485Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7701910Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7703457Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
_______________ test_copy_kv_to_caches[True-5-False-16-16-32-7] ________________
2025-04-11T04:23:18.7703901Z
bsz = 7, block_size = 32, max_num_blocks_per_seq = 16, num_kv_heads = 16
same_context_len = False, n_tokens = 5, use_new_kcache_layout = True
2025-04-11T04:23:18.7704203Z
@pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
@pytest.mark.parametrize("bsz", [7, 32])
@pytest.mark.parametrize("block_size", [16, 32, 64])
@pytest.mark.parametrize("max_num_blocks_per_seq", [16])
@pytest.mark.parametrize("num_kv_heads", [16])
@pytest.mark.parametrize("same_context_len", [True, False])
@pytest.mark.parametrize("n_tokens", [1, 5])
@pytest.mark.parametrize("use_new_kcache_layout", [True, False])
def test_copy_kv_to_caches(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_kv_heads: int,
same_context_len: bool,
n_tokens: int,
use_new_kcache_layout: bool,
):
torch.manual_seed(123)
torch.cuda.empty_cache()
>       torch.cuda.synchronize()
2025-04-11T04:23:18.7706327Z
tests/test_infer/test_kernels/triton/test_kvcache_copy.py:91:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.7706596Z
device = None
2025-04-11T04:23:18.7706677Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7707008Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7708692Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
_______________ test_copy_kv_to_caches[True-5-False-16-16-32-32] _______________
2025-04-11T04:23:18.7709092Z
bsz = 32, block_size = 32, max_num_blocks_per_seq = 16, num_kv_heads = 16
same_context_len = False, n_tokens = 5, use_new_kcache_layout = True
2025-04-11T04:23:18.7709403Z
@pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
@pytest.mark.parametrize("bsz", [7, 32])
@pytest.mark.parametrize("block_size", [16, 32, 64])
@pytest.mark.parametrize("max_num_blocks_per_seq", [16])
@pytest.mark.parametrize("num_kv_heads", [16])
@pytest.mark.parametrize("same_context_len", [True, False])
@pytest.mark.parametrize("n_tokens", [1, 5])
@pytest.mark.parametrize("use_new_kcache_layout", [True, False])
def test_copy_kv_to_caches(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_kv_heads: int,
same_context_len: bool,
n_tokens: int,
use_new_kcache_layout: bool,
):
torch.manual_seed(123)
torch.cuda.empty_cache()
>       torch.cuda.synchronize()
2025-04-11T04:23:18.7711583Z
tests/test_infer/test_kernels/triton/test_kvcache_copy.py:91:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.7711849Z
device = None
2025-04-11T04:23:18.7711931Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7712263Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7713917Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
_______________ test_copy_kv_to_caches[True-5-False-16-16-64-7] ________________
2025-04-11T04:23:18.7714319Z
bsz = 7, block_size = 64, max_num_blocks_per_seq = 16, num_kv_heads = 16
same_context_len = False, n_tokens = 5, use_new_kcache_layout = True
2025-04-11T04:23:18.7714622Z
@pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
@pytest.mark.parametrize("bsz", [7, 32])
@pytest.mark.parametrize("block_size", [16, 32, 64])
@pytest.mark.parametrize("max_num_blocks_per_seq", [16])
@pytest.mark.parametrize("num_kv_heads", [16])
@pytest.mark.parametrize("same_context_len", [True, False])
@pytest.mark.parametrize("n_tokens", [1, 5])
@pytest.mark.parametrize("use_new_kcache_layout", [True, False])
def test_copy_kv_to_caches(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_kv_heads: int,
same_context_len: bool,
n_tokens: int,
use_new_kcache_layout: bool,
):
torch.manual_seed(123)
torch.cuda.empty_cache()
>       torch.cuda.synchronize()
2025-04-11T04:23:18.7716800Z
tests/test_infer/test_kernels/triton/test_kvcache_copy.py:91:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.7717071Z
device = None
2025-04-11T04:23:18.7717154Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7717488Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7719036Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
_______________ test_copy_kv_to_caches[True-5-False-16-16-64-32] _______________
2025-04-11T04:23:18.7719432Z
bsz = 32, block_size = 64, max_num_blocks_per_seq = 16, num_kv_heads = 16
same_context_len = False, n_tokens = 5, use_new_kcache_layout = True
2025-04-11T04:23:18.7719739Z
@pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
@pytest.mark.parametrize("bsz", [7, 32])
@pytest.mark.parametrize("block_size", [16, 32, 64])
@pytest.mark.parametrize("max_num_blocks_per_seq", [16])
@pytest.mark.parametrize("num_kv_heads", [16])
@pytest.mark.parametrize("same_context_len", [True, False])
@pytest.mark.parametrize("n_tokens", [1, 5])
@pytest.mark.parametrize("use_new_kcache_layout", [True, False])
def test_copy_kv_to_caches(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_kv_heads: int,
same_context_len: bool,
n_tokens: int,
use_new_kcache_layout: bool,
):
torch.manual_seed(123)
torch.cuda.empty_cache()
>       torch.cuda.synchronize()
2025-04-11T04:23:18.7721958Z
tests/test_infer/test_kernels/triton/test_kvcache_copy.py:91:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.7722223Z
device = None
2025-04-11T04:23:18.7722304Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7722677Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7724213Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
_______________ test_copy_kv_to_caches[False-1-True-16-16-16-7] ________________
2025-04-11T04:23:18.7724606Z
bsz = 7, block_size = 16, max_num_blocks_per_seq = 16, num_kv_heads = 16
same_context_len = True, n_tokens = 1, use_new_kcache_layout = False
2025-04-11T04:23:18.7724911Z
@pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
@pytest.mark.parametrize("bsz", [7, 32])
@pytest.mark.parametrize("block_size", [16, 32, 64])
@pytest.mark.parametrize("max_num_blocks_per_seq", [16])
@pytest.mark.parametrize("num_kv_heads", [16])
@pytest.mark.parametrize("same_context_len", [True, False])
@pytest.mark.parametrize("n_tokens", [1, 5])
@pytest.mark.parametrize("use_new_kcache_layout", [True, False])
def test_copy_kv_to_caches(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_kv_heads: int,
same_context_len: bool,
n_tokens: int,
use_new_kcache_layout: bool,
):
torch.manual_seed(123)
torch.cuda.empty_cache()
>       torch.cuda.synchronize()
2025-04-11T04:23:18.7727074Z
tests/test_infer/test_kernels/triton/test_kvcache_copy.py:91:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.7727341Z
device = None
2025-04-11T04:23:18.7727423Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7727807Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7729405Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
_______________ test_copy_kv_to_caches[False-1-True-16-16-16-32] _______________
2025-04-11T04:23:18.7729799Z
bsz = 32, block_size = 16, max_num_blocks_per_seq = 16, num_kv_heads = 16
same_context_len = True, n_tokens = 1, use_new_kcache_layout = False
2025-04-11T04:23:18.7730105Z
@pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
@pytest.mark.parametrize("bsz", [7, 32])
@pytest.mark.parametrize("block_size", [16, 32, 64])
@pytest.mark.parametrize("max_num_blocks_per_seq", [16])
@pytest.mark.parametrize("num_kv_heads", [16])
@pytest.mark.parametrize("same_context_len", [True, False])
@pytest.mark.parametrize("n_tokens", [1, 5])
@pytest.mark.parametrize("use_new_kcache_layout", [True, False])
def test_copy_kv_to_caches(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_kv_heads: int,
same_context_len: bool,
n_tokens: int,
use_new_kcache_layout: bool,
):
torch.manual_seed(123)
torch.cuda.empty_cache()
>       torch.cuda.synchronize()
2025-04-11T04:23:18.7732260Z
tests/test_infer/test_kernels/triton/test_kvcache_copy.py:91:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.7732526Z
device = None
2025-04-11T04:23:18.7732603Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7732942Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7734581Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
_______________ test_copy_kv_to_caches[False-1-True-16-16-32-7] ________________
2025-04-11T04:23:18.7734978Z
bsz = 7, block_size = 32, max_num_blocks_per_seq = 16, num_kv_heads = 16
same_context_len = True, n_tokens = 1, use_new_kcache_layout = False
2025-04-11T04:23:18.7735284Z
@pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
@pytest.mark.parametrize("bsz", [7, 32])
@pytest.mark.parametrize("block_size", [16, 32, 64])
@pytest.mark.parametrize("max_num_blocks_per_seq", [16])
@pytest.mark.parametrize("num_kv_heads", [16])
@pytest.mark.parametrize("same_context_len", [True, False])
@pytest.mark.parametrize("n_tokens", [1, 5])
@pytest.mark.parametrize("use_new_kcache_layout", [True, False])
def test_copy_kv_to_caches(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_kv_heads: int,
same_context_len: bool,
n_tokens: int,
use_new_kcache_layout: bool,
):
torch.manual_seed(123)
torch.cuda.empty_cache()
>       torch.cuda.synchronize()
2025-04-11T04:23:18.7737354Z
tests/test_infer/test_kernels/triton/test_kvcache_copy.py:91:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.7737619Z
device = None
2025-04-11T04:23:18.7737891Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7738240Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7739857Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
_______________ test_copy_kv_to_caches[False-1-True-16-16-32-32] _______________
2025-04-11T04:23:18.7740253Z
bsz = 32, block_size = 32, max_num_blocks_per_seq = 16, num_kv_heads = 16
same_context_len = True, n_tokens = 1, use_new_kcache_layout = False
2025-04-11T04:23:18.7740602Z
@pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
@pytest.mark.parametrize("bsz", [7, 32])
@pytest.mark.parametrize("block_size", [16, 32, 64])
@pytest.mark.parametrize("max_num_blocks_per_seq", [16])
@pytest.mark.parametrize("num_kv_heads", [16])
@pytest.mark.parametrize("same_context_len", [True, False])
@pytest.mark.parametrize("n_tokens", [1, 5])
@pytest.mark.parametrize("use_new_kcache_layout", [True, False])
def test_copy_kv_to_caches(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_kv_heads: int,
same_context_len: bool,
n_tokens: int,
use_new_kcache_layout: bool,
):
torch.manual_seed(123)
torch.cuda.empty_cache()
>       torch.cuda.synchronize()
2025-04-11T04:23:18.7742687Z
tests/test_infer/test_kernels/triton/test_kvcache_copy.py:91:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.7742953Z
device = None
2025-04-11T04:23:18.7743031Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7743371Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7745009Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
_______________ test_copy_kv_to_caches[False-1-True-16-16-64-7] ________________
2025-04-11T04:23:18.7745401Z
bsz = 7, block_size = 64, max_num_blocks_per_seq = 16, num_kv_heads = 16
same_context_len = True, n_tokens = 1, use_new_kcache_layout = False
2025-04-11T04:23:18.7745759Z
@pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
@pytest.mark.parametrize("bsz", [7, 32])
@pytest.mark.parametrize("block_size", [16, 32, 64])
@pytest.mark.parametrize("max_num_blocks_per_seq", [16])
@pytest.mark.parametrize("num_kv_heads", [16])
@pytest.mark.parametrize("same_context_len", [True, False])
@pytest.mark.parametrize("n_tokens", [1, 5])
@pytest.mark.parametrize("use_new_kcache_layout", [True, False])
def test_copy_kv_to_caches(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_kv_heads: int,
same_context_len: bool,
n_tokens: int,
use_new_kcache_layout: bool,
):
torch.manual_seed(123)
torch.cuda.empty_cache()
>       torch.cuda.synchronize()
2025-04-11T04:23:18.7747879Z
tests/test_infer/test_kernels/triton/test_kvcache_copy.py:91:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.7748147Z
device = None
2025-04-11T04:23:18.7748224Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7748618Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7750276Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
_______________ test_copy_kv_to_caches[False-1-True-16-16-64-32] _______________
2025-04-11T04:23:18.7750670Z
bsz = 32, block_size = 64, max_num_blocks_per_seq = 16, num_kv_heads = 16
same_context_len = True, n_tokens = 1, use_new_kcache_layout = False
2025-04-11T04:23:18.7750968Z
@pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
@pytest.mark.parametrize("bsz", [7, 32])
@pytest.mark.parametrize("block_size", [16, 32, 64])
@pytest.mark.parametrize("max_num_blocks_per_seq", [16])
@pytest.mark.parametrize("num_kv_heads", [16])
@pytest.mark.parametrize("same_context_len", [True, False])
@pytest.mark.parametrize("n_tokens", [1, 5])
@pytest.mark.parametrize("use_new_kcache_layout", [True, False])
def test_copy_kv_to_caches(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_kv_heads: int,
same_context_len: bool,
n_tokens: int,
use_new_kcache_layout: bool,
):
torch.manual_seed(123)
torch.cuda.empty_cache()
>       torch.cuda.synchronize()
2025-04-11T04:23:18.7753156Z
tests/test_infer/test_kernels/triton/test_kvcache_copy.py:91:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.7753423Z
device = None
2025-04-11T04:23:18.7753501Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7753831Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7755368Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
_______________ test_copy_kv_to_caches[False-1-False-16-16-16-7] _______________
2025-04-11T04:23:18.7755766Z
bsz = 7, block_size = 16, max_num_blocks_per_seq = 16, num_kv_heads = 16
same_context_len = False, n_tokens = 1, use_new_kcache_layout = False
2025-04-11T04:23:18.7756152Z
@pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
@pytest.mark.parametrize("bsz", [7, 32])
@pytest.mark.parametrize("block_size", [16, 32, 64])
@pytest.mark.parametrize("max_num_blocks_per_seq", [16])
@pytest.mark.parametrize("num_kv_heads", [16])
@pytest.mark.parametrize("same_context_len", [True, False])
@pytest.mark.parametrize("n_tokens", [1, 5])
@pytest.mark.parametrize("use_new_kcache_layout", [True, False])
def test_copy_kv_to_caches(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_kv_heads: int,
same_context_len: bool,
n_tokens: int,
use_new_kcache_layout: bool,
):
torch.manual_seed(123)
torch.cuda.empty_cache()
>       torch.cuda.synchronize()
2025-04-11T04:23:18.7758281Z
tests/test_infer/test_kernels/triton/test_kvcache_copy.py:91:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.7758605Z
device = None
2025-04-11T04:23:18.7758685Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7759025Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7760567Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
______________ test_copy_kv_to_caches[False-1-False-16-16-16-32] _______________
2025-04-11T04:23:18.7760958Z
bsz = 32, block_size = 16, max_num_blocks_per_seq = 16, num_kv_heads = 16
same_context_len = False, n_tokens = 1, use_new_kcache_layout = False
2025-04-11T04:23:18.7761268Z
@pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
@pytest.mark.parametrize("bsz", [7, 32])
@pytest.mark.parametrize("block_size", [16, 32, 64])
@pytest.mark.parametrize("max_num_blocks_per_seq", [16])
@pytest.mark.parametrize("num_kv_heads", [16])
@pytest.mark.parametrize("same_context_len", [True, False])
@pytest.mark.parametrize("n_tokens", [1, 5])
@pytest.mark.parametrize("use_new_kcache_layout", [True, False])
def test_copy_kv_to_caches(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_kv_heads: int,
same_context_len: bool,
n_tokens: int,
use_new_kcache_layout: bool,
):
torch.manual_seed(123)
torch.cuda.empty_cache()
>       torch.cuda.synchronize()
2025-04-11T04:23:18.7763465Z
tests/test_infer/test_kernels/triton/test_kvcache_copy.py:91:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.7763736Z
device = None
2025-04-11T04:23:18.7763867Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7764211Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7765801Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
_______________ test_copy_kv_to_caches[False-1-False-16-16-32-7] _______________
2025-04-11T04:23:18.7766200Z
bsz = 7, block_size = 32, max_num_blocks_per_seq = 16, num_kv_heads = 16
same_context_len = False, n_tokens = 1, use_new_kcache_layout = False
2025-04-11T04:23:18.7766508Z
@pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
@pytest.mark.parametrize("bsz", [7, 32])
@pytest.mark.parametrize("block_size", [16, 32, 64])
@pytest.mark.parametrize("max_num_blocks_per_seq", [16])
@pytest.mark.parametrize("num_kv_heads", [16])
@pytest.mark.parametrize("same_context_len", [True, False])
@pytest.mark.parametrize("n_tokens", [1, 5])
@pytest.mark.parametrize("use_new_kcache_layout", [True, False])
def test_copy_kv_to_caches(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_kv_heads: int,
same_context_len: bool,
n_tokens: int,
use_new_kcache_layout: bool,
):
torch.manual_seed(123)
torch.cuda.empty_cache()
>       torch.cuda.synchronize()
2025-04-11T04:23:18.7768677Z
tests/test_infer/test_kernels/triton/test_kvcache_copy.py:91:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.7768943Z
device = None
2025-04-11T04:23:18.7769024Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7769361Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7771033Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
______________ test_copy_kv_to_caches[False-1-False-16-16-32-32] _______________
2025-04-11T04:23:18.7771436Z
bsz = 32, block_size = 32, max_num_blocks_per_seq = 16, num_kv_heads = 16
same_context_len = False, n_tokens = 1, use_new_kcache_layout = False
2025-04-11T04:23:18.7771745Z
@pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
@pytest.mark.parametrize("bsz", [7, 32])
@pytest.mark.parametrize("block_size", [16, 32, 64])
@pytest.mark.parametrize("max_num_blocks_per_seq", [16])
@pytest.mark.parametrize("num_kv_heads", [16])
@pytest.mark.parametrize("same_context_len", [True, False])
@pytest.mark.parametrize("n_tokens", [1, 5])
@pytest.mark.parametrize("use_new_kcache_layout", [True, False])
def test_copy_kv_to_caches(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_kv_heads: int,
same_context_len: bool,
n_tokens: int,
use_new_kcache_layout: bool,
):
torch.manual_seed(123)
torch.cuda.empty_cache()
>       torch.cuda.synchronize()
2025-04-11T04:23:18.7773816Z
tests/test_infer/test_kernels/triton/test_kvcache_copy.py:91:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.7774164Z
device = None
2025-04-11T04:23:18.7774251Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7774592Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7776191Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
_______________ test_copy_kv_to_caches[False-1-False-16-16-64-7] _______________
2025-04-11T04:23:18.7776632Z
bsz = 7, block_size = 64, max_num_blocks_per_seq = 16, num_kv_heads = 16
same_context_len = False, n_tokens = 1, use_new_kcache_layout = False
2025-04-11T04:23:18.7776940Z
@pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
@pytest.mark.parametrize("bsz", [7, 32])
@pytest.mark.parametrize("block_size", [16, 32, 64])
@pytest.mark.parametrize("max_num_blocks_per_seq", [16])
@pytest.mark.parametrize("num_kv_heads", [16])
@pytest.mark.parametrize("same_context_len", [True, False])
@pytest.mark.parametrize("n_tokens", [1, 5])
@pytest.mark.parametrize("use_new_kcache_layout", [True, False])
def test_copy_kv_to_caches(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_kv_heads: int,
same_context_len: bool,
n_tokens: int,
use_new_kcache_layout: bool,
):
torch.manual_seed(123)
torch.cuda.empty_cache()
>       torch.cuda.synchronize()
2025-04-11T04:23:18.7779016Z
tests/test_infer/test_kernels/triton/test_kvcache_copy.py:91:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.7779279Z
device = None
2025-04-11T04:23:18.7779362Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7779698Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7781336Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
______________ test_copy_kv_to_caches[False-1-False-16-16-64-32] _______________
2025-04-11T04:23:18.7781738Z
bsz = 32, block_size = 64, max_num_blocks_per_seq = 16, num_kv_heads = 16
same_context_len = False, n_tokens = 1, use_new_kcache_layout = False
2025-04-11T04:23:18.7782117Z
@pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
@pytest.mark.parametrize("bsz", [7, 32])
@pytest.mark.parametrize("block_size", [16, 32, 64])
@pytest.mark.parametrize("max_num_blocks_per_seq", [16])
@pytest.mark.parametrize("num_kv_heads", [16])
@pytest.mark.parametrize("same_context_len", [True, False])
@pytest.mark.parametrize("n_tokens", [1, 5])
@pytest.mark.parametrize("use_new_kcache_layout", [True, False])
def test_copy_kv_to_caches(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_kv_heads: int,
same_context_len: bool,
n_tokens: int,
use_new_kcache_layout: bool,
):
torch.manual_seed(123)
torch.cuda.empty_cache()
>       torch.cuda.synchronize()
2025-04-11T04:23:18.7784237Z
tests/test_infer/test_kernels/triton/test_kvcache_copy.py:91:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.7784502Z
device = None
2025-04-11T04:23:18.7784585Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7784918Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7786546Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
_______________ test_copy_kv_to_caches[False-5-True-16-16-16-7] ________________
2025-04-11T04:23:18.7786938Z
bsz = 7, block_size = 16, max_num_blocks_per_seq = 16, num_kv_heads = 16
same_context_len = True, n_tokens = 5, use_new_kcache_layout = False
2025-04-11T04:23:18.7787244Z
@pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
@pytest.mark.parametrize("bsz", [7, 32])
@pytest.mark.parametrize("block_size", [16, 32, 64])
@pytest.mark.parametrize("max_num_blocks_per_seq", [16])
@pytest.mark.parametrize("num_kv_heads", [16])
@pytest.mark.parametrize("same_context_len", [True, False])
@pytest.mark.parametrize("n_tokens", [1, 5])
@pytest.mark.parametrize("use_new_kcache_layout", [True, False])
def test_copy_kv_to_caches(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_kv_heads: int,
same_context_len: bool,
n_tokens: int,
use_new_kcache_layout: bool,
):
torch.manual_seed(123)
torch.cuda.empty_cache()
>       torch.cuda.synchronize()
2025-04-11T04:23:18.7789466Z
tests/test_infer/test_kernels/triton/test_kvcache_copy.py:91:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.7789732Z
device = None
2025-04-11T04:23:18.7789812Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7790148Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7791691Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
_______________ test_copy_kv_to_caches[False-5-True-16-16-16-32] _______________
2025-04-11T04:23:18.7792176Z
bsz = 32, block_size = 16, max_num_blocks_per_seq = 16, num_kv_heads = 16
same_context_len = True, n_tokens = 5, use_new_kcache_layout = False
2025-04-11T04:23:18.7792486Z
@pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
@pytest.mark.parametrize("bsz", [7, 32])
@pytest.mark.parametrize("block_size", [16, 32, 64])
@pytest.mark.parametrize("max_num_blocks_per_seq", [16])
@pytest.mark.parametrize("num_kv_heads", [16])
@pytest.mark.parametrize("same_context_len", [True, False])
@pytest.mark.parametrize("n_tokens", [1, 5])
@pytest.mark.parametrize("use_new_kcache_layout", [True, False])
def test_copy_kv_to_caches(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_kv_heads: int,
same_context_len: bool,
n_tokens: int,
use_new_kcache_layout: bool,
):
torch.manual_seed(123)
torch.cuda.empty_cache()
>       torch.cuda.synchronize()
2025-04-11T04:23:18.7794601Z
tests/test_infer/test_kernels/triton/test_kvcache_copy.py:91:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.7794917Z
device = None
2025-04-11T04:23:18.7794997Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7795333Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7796874Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
_______________ test_copy_kv_to_caches[False-5-True-16-16-32-7] ________________
2025-04-11T04:23:18.7797267Z
bsz = 7, block_size = 32, max_num_blocks_per_seq = 16, num_kv_heads = 16
same_context_len = True, n_tokens = 5, use_new_kcache_layout = False
2025-04-11T04:23:18.7797566Z
@pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
@pytest.mark.parametrize("bsz", [7, 32])
@pytest.mark.parametrize("block_size", [16, 32, 64])
@pytest.mark.parametrize("max_num_blocks_per_seq", [16])
@pytest.mark.parametrize("num_kv_heads", [16])
@pytest.mark.parametrize("same_context_len", [True, False])
@pytest.mark.parametrize("n_tokens", [1, 5])
@pytest.mark.parametrize("use_new_kcache_layout", [True, False])
def test_copy_kv_to_caches(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_kv_heads: int,
same_context_len: bool,
n_tokens: int,
use_new_kcache_layout: bool,
):
torch.manual_seed(123)
torch.cuda.empty_cache()
>       torch.cuda.synchronize()
2025-04-11T04:23:18.7799705Z
tests/test_infer/test_kernels/triton/test_kvcache_copy.py:91:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.7800023Z
device = None
2025-04-11T04:23:18.7800105Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7800440Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7802031Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
_______________ test_copy_kv_to_caches[False-5-True-16-16-32-32] _______________
2025-04-11T04:23:18.7802432Z
bsz = 32, block_size = 32, max_num_blocks_per_seq = 16, num_kv_heads = 16
same_context_len = True, n_tokens = 5, use_new_kcache_layout = False
2025-04-11T04:23:18.7802739Z
@pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
@pytest.mark.parametrize("bsz", [7, 32])
@pytest.mark.parametrize("block_size", [16, 32, 64])
@pytest.mark.parametrize("max_num_blocks_per_seq", [16])
@pytest.mark.parametrize("num_kv_heads", [16])
@pytest.mark.parametrize("same_context_len", [True, False])
@pytest.mark.parametrize("n_tokens", [1, 5])
@pytest.mark.parametrize("use_new_kcache_layout", [True, False])
def test_copy_kv_to_caches(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_kv_heads: int,
same_context_len: bool,
n_tokens: int,
use_new_kcache_layout: bool,
):
torch.manual_seed(123)
torch.cuda.empty_cache()
>       torch.cuda.synchronize()
2025-04-11T04:23:18.7804945Z
tests/test_infer/test_kernels/triton/test_kvcache_copy.py:91:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.7805219Z
device = None
2025-04-11T04:23:18.7805298Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7805644Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7807290Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
_______________ test_copy_kv_to_caches[False-5-True-16-16-64-7] ________________
2025-04-11T04:23:18.7808941Z
bsz = 7, block_size = 64, max_num_blocks_per_seq = 16, num_kv_heads = 16
same_context_len = True, n_tokens = 5, use_new_kcache_layout = False
2025-04-11T04:23:18.7809252Z
@pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
@pytest.mark.parametrize("bsz", [7, 32])
@pytest.mark.parametrize("block_size", [16, 32, 64])
@pytest.mark.parametrize("max_num_blocks_per_seq", [16])
@pytest.mark.parametrize("num_kv_heads", [16])
@pytest.mark.parametrize("same_context_len", [True, False])
@pytest.mark.parametrize("n_tokens", [1, 5])
@pytest.mark.parametrize("use_new_kcache_layout", [True, False])
def test_copy_kv_to_caches(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_kv_heads: int,
same_context_len: bool,
n_tokens: int,
use_new_kcache_layout: bool,
):
torch.manual_seed(123)
torch.cuda.empty_cache()
>       torch.cuda.synchronize()
2025-04-11T04:23:18.7811402Z
tests/test_infer/test_kernels/triton/test_kvcache_copy.py:91:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.7811677Z
device = None
2025-04-11T04:23:18.7811759Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7812098Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7813693Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
_______________ test_copy_kv_to_caches[False-5-True-16-16-64-32] _______________
2025-04-11T04:23:18.7814141Z
bsz = 32, block_size = 64, max_num_blocks_per_seq = 16, num_kv_heads = 16
same_context_len = True, n_tokens = 5, use_new_kcache_layout = False
2025-04-11T04:23:18.7814445Z
@pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
@pytest.mark.parametrize("bsz", [7, 32])
@pytest.mark.parametrize("block_size", [16, 32, 64])
@pytest.mark.parametrize("max_num_blocks_per_seq", [16])
@pytest.mark.parametrize("num_kv_heads", [16])
@pytest.mark.parametrize("same_context_len", [True, False])
@pytest.mark.parametrize("n_tokens", [1, 5])
@pytest.mark.parametrize("use_new_kcache_layout", [True, False])
def test_copy_kv_to_caches(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_kv_heads: int,
same_context_len: bool,
n_tokens: int,
use_new_kcache_layout: bool,
):
torch.manual_seed(123)
torch.cuda.empty_cache()
>       torch.cuda.synchronize()
2025-04-11T04:23:18.7816591Z
tests/test_infer/test_kernels/triton/test_kvcache_copy.py:91:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.7816859Z
device = None
2025-04-11T04:23:18.7816939Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7817274Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7818873Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
_______________ test_copy_kv_to_caches[False-5-False-16-16-16-7] _______________
2025-04-11T04:23:18.7819322Z
bsz = 7, block_size = 16, max_num_blocks_per_seq = 16, num_kv_heads = 16
same_context_len = False, n_tokens = 5, use_new_kcache_layout = False
2025-04-11T04:23:18.7819634Z
@pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
@pytest.mark.parametrize("bsz", [7, 32])
@pytest.mark.parametrize("block_size", [16, 32, 64])
@pytest.mark.parametrize("max_num_blocks_per_seq", [16])
@pytest.mark.parametrize("num_kv_heads", [16])
@pytest.mark.parametrize("same_context_len", [True, False])
@pytest.mark.parametrize("n_tokens", [1, 5])
@pytest.mark.parametrize("use_new_kcache_layout", [True, False])
def test_copy_kv_to_caches(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_kv_heads: int,
same_context_len: bool,
n_tokens: int,
use_new_kcache_layout: bool,
):
torch.manual_seed(123)
torch.cuda.empty_cache()
>       torch.cuda.synchronize()
2025-04-11T04:23:18.7821809Z
tests/test_infer/test_kernels/triton/test_kvcache_copy.py:91:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.7822079Z
device = None
2025-04-11T04:23:18.7822164Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7822503Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7824100Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
______________ test_copy_kv_to_caches[False-5-False-16-16-16-32] _______________
2025-04-11T04:23:18.7824497Z
bsz = 32, block_size = 16, max_num_blocks_per_seq = 16, num_kv_heads = 16
same_context_len = False, n_tokens = 5, use_new_kcache_layout = False
2025-04-11T04:23:18.7824805Z
@pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
@pytest.mark.parametrize("bsz", [7, 32])
@pytest.mark.parametrize("block_size", [16, 32, 64])
@pytest.mark.parametrize("max_num_blocks_per_seq", [16])
@pytest.mark.parametrize("num_kv_heads", [16])
@pytest.mark.parametrize("same_context_len", [True, False])
@pytest.mark.parametrize("n_tokens", [1, 5])
@pytest.mark.parametrize("use_new_kcache_layout", [True, False])
def test_copy_kv_to_caches(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_kv_heads: int,
same_context_len: bool,
n_tokens: int,
use_new_kcache_layout: bool,
):
torch.manual_seed(123)
torch.cuda.empty_cache()
>       torch.cuda.synchronize()
2025-04-11T04:23:18.7827065Z
tests/test_infer/test_kernels/triton/test_kvcache_copy.py:91:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.7827337Z
device = None
2025-04-11T04:23:18.7827417Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7827762Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7829337Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
_______________ test_copy_kv_to_caches[False-5-False-16-16-32-7] _______________
2025-04-11T04:23:18.7829792Z
bsz = 7, block_size = 32, max_num_blocks_per_seq = 16, num_kv_heads = 16
same_context_len = False, n_tokens = 5, use_new_kcache_layout = False
2025-04-11T04:23:18.7830099Z
@pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
@pytest.mark.parametrize("bsz", [7, 32])
@pytest.mark.parametrize("block_size", [16, 32, 64])
@pytest.mark.parametrize("max_num_blocks_per_seq", [16])
@pytest.mark.parametrize("num_kv_heads", [16])
@pytest.mark.parametrize("same_context_len", [True, False])
@pytest.mark.parametrize("n_tokens", [1, 5])
@pytest.mark.parametrize("use_new_kcache_layout", [True, False])
def test_copy_kv_to_caches(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_kv_heads: int,
same_context_len: bool,
n_tokens: int,
use_new_kcache_layout: bool,
):
torch.manual_seed(123)
torch.cuda.empty_cache()
>       torch.cuda.synchronize()
2025-04-11T04:23:18.7832281Z
tests/test_infer/test_kernels/triton/test_kvcache_copy.py:91:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.7832542Z
device = None
2025-04-11T04:23:18.7832625Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7832966Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7834661Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
______________ test_copy_kv_to_caches[False-5-False-16-16-32-32] _______________
2025-04-11T04:23:18.7835058Z
bsz = 32, block_size = 32, max_num_blocks_per_seq = 16, num_kv_heads = 16
same_context_len = False, n_tokens = 5, use_new_kcache_layout = False
2025-04-11T04:23:18.7835368Z
@pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
@pytest.mark.parametrize("bsz", [7, 32])
@pytest.mark.parametrize("block_size", [16, 32, 64])
@pytest.mark.parametrize("max_num_blocks_per_seq", [16])
@pytest.mark.parametrize("num_kv_heads", [16])
@pytest.mark.parametrize("same_context_len", [True, False])
@pytest.mark.parametrize("n_tokens", [1, 5])
@pytest.mark.parametrize("use_new_kcache_layout", [True, False])
def test_copy_kv_to_caches(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_kv_heads: int,
same_context_len: bool,
n_tokens: int,
use_new_kcache_layout: bool,
):
torch.manual_seed(123)
torch.cuda.empty_cache()
>       torch.cuda.synchronize()
2025-04-11T04:23:18.7837551Z
tests/test_infer/test_kernels/triton/test_kvcache_copy.py:91:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.7837822Z
device = None
2025-04-11T04:23:18.7837906Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7838298Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7839896Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
_______________ test_copy_kv_to_caches[False-5-False-16-16-64-7] _______________
2025-04-11T04:23:18.7840293Z
bsz = 7, block_size = 64, max_num_blocks_per_seq = 16, num_kv_heads = 16
same_context_len = False, n_tokens = 5, use_new_kcache_layout = False
2025-04-11T04:23:18.7840603Z
@pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
@pytest.mark.parametrize("bsz", [7, 32])
@pytest.mark.parametrize("block_size", [16, 32, 64])
@pytest.mark.parametrize("max_num_blocks_per_seq", [16])
@pytest.mark.parametrize("num_kv_heads", [16])
@pytest.mark.parametrize("same_context_len", [True, False])
@pytest.mark.parametrize("n_tokens", [1, 5])
@pytest.mark.parametrize("use_new_kcache_layout", [True, False])
def test_copy_kv_to_caches(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_kv_heads: int,
same_context_len: bool,
n_tokens: int,
use_new_kcache_layout: bool,
):
torch.manual_seed(123)
torch.cuda.empty_cache()
>       torch.cuda.synchronize()
2025-04-11T04:23:18.7842745Z
tests/test_infer/test_kernels/triton/test_kvcache_copy.py:91:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.7843011Z
device = None
2025-04-11T04:23:18.7843095Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7843484Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7845096Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
______________ test_copy_kv_to_caches[False-5-False-16-16-64-32] _______________
2025-04-11T04:23:18.7845543Z
bsz = 32, block_size = 64, max_num_blocks_per_seq = 16, num_kv_heads = 16
same_context_len = False, n_tokens = 5, use_new_kcache_layout = False
2025-04-11T04:23:18.7845851Z
@pytest.mark.skipif(not (HAS_TRITON and TRITON_CUDA_SUPPORT), reason="requires triton")
@pytest.mark.parametrize("bsz", [7, 32])
@pytest.mark.parametrize("block_size", [16, 32, 64])
@pytest.mark.parametrize("max_num_blocks_per_seq", [16])
@pytest.mark.parametrize("num_kv_heads", [16])
@pytest.mark.parametrize("same_context_len", [True, False])
@pytest.mark.parametrize("n_tokens", [1, 5])
@pytest.mark.parametrize("use_new_kcache_layout", [True, False])
def test_copy_kv_to_caches(
bsz: int,
block_size: int,
max_num_blocks_per_seq: int,
num_kv_heads: int,
same_context_len: bool,
n_tokens: int,
use_new_kcache_layout: bool,
):
torch.manual_seed(123)
torch.cuda.empty_cache()
>       torch.cuda.synchronize()
2025-04-11T04:23:18.7847997Z
tests/test_infer/test_kernels/triton/test_kvcache_copy.py:91:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.7848268Z
device = None
2025-04-11T04:23:18.7848349Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.7848688Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7850363Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
_______________________________ test_layer_norm ________________________________
2025-04-11T04:23:18.7850741Z
kwargs = {}, val = 2, arg_map = {'M': 2}
partial_func = functools.partial(<function parameterize.<locals>._wrapper.<locals>._execute_function_by_param at 0x7fb5b8741750>, M=2)
2025-04-11T04:23:18.7851204Z
def _execute_function_by_param(**kwargs):
for val in values:
arg_map = {argument: val}
partial_func = partial(func, **arg_map)
>           partial_func(**kwargs)
2025-04-11T04:23:18.7851735Z
colossalai/testing/utils.py:64:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
colossalai/testing/utils.py:64: in _execute_function_by_param
partial_func(**kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.7852300Z
M = 2, N = 64
2025-04-11T04:23:18.7852377Z
@pytest.mark.skipif(
not TRITON_CUDA_SUPPORT or not HAS_TRITON, reason="triton requires cuda version to be higher than 11.4"
)
@parameterize("M", [2, 4, 8, 16])
@parameterize("N", [64, 128])
def test_layer_norm(M, N):
dtype = torch.float16
eps = 1e-5
x_shape = (M, N)
w_shape = (x_shape[-1],)
>       weight = torch.ones(w_shape, dtype=dtype, device="cuda")
E       RuntimeError: CUDA error: out of memory
E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7854295Z
tests/test_infer/test_kernels/triton/test_rmsnorm_triton.py:30: RuntimeError
___________________ test_rotary_emb[True-dtype0-64-32-64-4] ____________________
2025-04-11T04:23:18.7854637Z
BATCH_SIZE = 4, SEQ_LEN = 64, H = 32, D = 64, dtype = torch.float32
use_new_kcache_layout = True
2025-04-11T04:23:18.7854878Z
@pytest.mark.skipif(
not TRITON_CUDA_SUPPORT or not HAS_TRITON, reason="triton requires cuda version to be higher than 11.4"
)
@pytest.mark.parametrize("BATCH_SIZE", [4])
@pytest.mark.parametrize("SEQ_LEN", [64])
@pytest.mark.parametrize("H", [32])
@pytest.mark.parametrize("D", [64])
@pytest.mark.parametrize("dtype", [torch.float32])
@pytest.mark.parametrize("use_new_kcache_layout", [True, False])
def test_rotary_emb(BATCH_SIZE, SEQ_LEN, H, D, dtype, use_new_kcache_layout):
TOTAL_TOKENS = BATCH_SIZE * SEQ_LEN
# our crafted op equals to Transformers
x0 = torch.randn(BATCH_SIZE, H, SEQ_LEN, D, dtype=dtype)
x1 = torch.randn(BATCH_SIZE, H, SEQ_LEN, D, dtype=dtype)
emb = LlamaRotaryEmbedding(D)
position_ids = torch.arange(TOTAL_TOKENS).reshape((BATCH_SIZE, SEQ_LEN))
cos, sin = emb(x0, position_ids)
embd_x0, _ = apply_rotary_pos_emb(x0, x1, cos, sin)
cos = cos.reshape((TOTAL_TOKENS, -1))
sin = sin.reshape((TOTAL_TOKENS, -1))
cos_2 = cos[:, :32]
sin_2 = sin[:, :32]
x2 = x0.transpose(1, 2).reshape(TOTAL_TOKENS, H, D)
embd_stimulated_x = torch_rotary_emb(x2, cos_2, sin_2)
embd_stimulated_x = embd_stimulated_x.reshape((BATCH_SIZE, SEQ_LEN, H, D)).transpose(1, 2)
assert torch.allclose(embd_x0, embd_stimulated_x)
2025-04-11T04:23:18.7858298Z
# create data
block_size = 32
max_num_blocks_per_seq = 4
q_shape = (TOTAL_TOKENS, H, D)
>       q = -2.3 + 0.5 * torch.randn(q_shape, dtype=dtype, device="cuda")
E       RuntimeError: CUDA error: out of memory
E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7859490Z
tests/test_infer/test_kernels/triton/test_rotary_embdding_unpad.py:65: RuntimeError
___________________ test_rotary_emb[False-dtype0-64-32-64-4] ___________________
2025-04-11T04:23:18.7859857Z
BATCH_SIZE = 4, SEQ_LEN = 64, H = 32, D = 64, dtype = torch.float32
use_new_kcache_layout = False
2025-04-11T04:23:18.7860141Z
@pytest.mark.skipif(
not TRITON_CUDA_SUPPORT or not HAS_TRITON, reason="triton requires cuda version to be higher than 11.4"
)
@pytest.mark.parametrize("BATCH_SIZE", [4])
@pytest.mark.parametrize("SEQ_LEN", [64])
@pytest.mark.parametrize("H", [32])
@pytest.mark.parametrize("D", [64])
@pytest.mark.parametrize("dtype", [torch.float32])
@pytest.mark.parametrize("use_new_kcache_layout", [True, False])
def test_rotary_emb(BATCH_SIZE, SEQ_LEN, H, D, dtype, use_new_kcache_layout):
TOTAL_TOKENS = BATCH_SIZE * SEQ_LEN
# our crafted op equals to Transformers
x0 = torch.randn(BATCH_SIZE, H, SEQ_LEN, D, dtype=dtype)
x1 = torch.randn(BATCH_SIZE, H, SEQ_LEN, D, dtype=dtype)
emb = LlamaRotaryEmbedding(D)
position_ids = torch.arange(TOTAL_TOKENS).reshape((BATCH_SIZE, SEQ_LEN))
cos, sin = emb(x0, position_ids)
embd_x0, _ = apply_rotary_pos_emb(x0, x1, cos, sin)
cos = cos.reshape((TOTAL_TOKENS, -1))
sin = sin.reshape((TOTAL_TOKENS, -1))
cos_2 = cos[:, :32]
sin_2 = sin[:, :32]
x2 = x0.transpose(1, 2).reshape(TOTAL_TOKENS, H, D)
embd_stimulated_x = torch_rotary_emb(x2, cos_2, sin_2)
embd_stimulated_x = embd_stimulated_x.reshape((BATCH_SIZE, SEQ_LEN, H, D)).transpose(1, 2)
assert torch.allclose(embd_x0, embd_stimulated_x)
2025-04-11T04:23:18.7863469Z
# create data
block_size = 32
max_num_blocks_per_seq = 4
q_shape = (TOTAL_TOKENS, H, D)
>       q = -2.3 + 0.5 * torch.randn(q_shape, dtype=dtype, device="cuda")
E       RuntimeError: CUDA error: out of memory
E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7864693Z
tests/test_infer/test_kernels/triton/test_rotary_embdding_unpad.py:65: RuntimeError
_____________________ test_get_xine_cache[dtype0-64-64-4] ______________________
2025-04-11T04:23:18.7865051Z
BATCH_SIZE = 4, MAX_SEQ_LEN = 64, HEAD_DIM = 64, dtype = torch.float32
2025-04-11T04:23:18.7865202Z
@pytest.mark.skipif(
not TRITON_CUDA_SUPPORT or not HAS_TRITON, reason="triton requires cuda version to be higher than 11.4"
)
@pytest.mark.parametrize("BATCH_SIZE", [4])
@pytest.mark.parametrize("MAX_SEQ_LEN", [64])
@pytest.mark.parametrize("HEAD_DIM", [64])
@pytest.mark.parametrize("dtype", [torch.float32])
def test_get_xine_cache(BATCH_SIZE, MAX_SEQ_LEN, HEAD_DIM, dtype):
MAX_TOTAL_TOKENS = BATCH_SIZE * MAX_SEQ_LEN
>       cos_cache = torch.randn((MAX_TOTAL_TOKENS, HEAD_DIM), dtype=dtype, device="cuda")
E       RuntimeError: CUDA error: out of memory
E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7867227Z
tests/test_infer/test_kernels/triton/test_xine_copy.py:50: RuntimeError
_____________________ test_models_lazy_init[cuda-subset0] ______________________
2025-04-11T04:23:18.7867554Z
subset = ['custom_hanging_param_model', 'custom_nested_model', 'custom_repeated_computed_layers', 'custom_simple_net', 'diffusers_clip_text_model', 'diffusers_auto_encoder_kl', ...]
default_device = 'cuda'
2025-04-11T04:23:18.7868063Z
@pytest.mark.skipif(not SUPPORT_LAZY, reason="requires torch >= 1.12.0")
@pytest.mark.parametrize(
"subset",
(
[COMMON_MODELS]
if IS_FAST_TEST
else ["torchvision", "diffusers", "timm", "transformers", "torchaudio", "deepfm", "dlrm"]
),
)
@pytest.mark.parametrize("default_device", ["cpu", "cuda"])
def test_models_lazy_init(subset, default_device):
sub_model_zoo = model_zoo.get_sub_registry(subset, allow_empty=True)
for name, entry in sub_model_zoo.items():
# TODO(ver217): lazy init does not support weight norm, skip these models
if name in (
"torchaudio_wav2vec2_base",
"torchaudio_hubert_base",
"timm_beit",
"timm_vision_transformer",
"timm_deit",
"timm_beitv2",
"timm_deit3",
"timm_convit",
"timm_tnt_b_patch16_224",
) or name.startswith(("transformers_vit", "transformers_blip2", "transformers_whisper")):
continue
>           check_lazy_init(entry, verbose=True, default_device=default_device)
2025-04-11T04:23:18.7871226Z
tests/test_lazy/test_models.py:33:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
tests/test_lazy/lazy_init_utils.py:77: in check_lazy_init
model = model_fn()
tests/kit/model_zoo/custom/hanging_param_model.py:17: in __init__
self.proj1 = nn.Linear(4, 8)
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/linear.py:98: in __init__
self.weight = Parameter(torch.empty((out_features, in_features), **factory_kwargs))
colossalai/lazy/lazy_init.py:506: in wrapper
return self.tensor_cls(target, *args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.7872767Z
cls = <class 'colossalai.lazy.lazy_init._MyTensor'>
func = <built-in method empty of type object at 0x7fb8e3cd9840>
concrete_data = None, args = ((8, 4),)
kwargs = {'device': 'cuda', 'dtype': None}
2025-04-11T04:23:18.7873241Z
def __new__(cls, func, *args, concrete_data=None, **kwargs) -> "_MyTensor":
cls._pre_op_fn()
if concrete_data is not None:
# uniform api as LazyTensor
data = concrete_data
else:
kwargs["device"] = cls.default_device
>           data = func(*args, **kwargs)
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7874813Z
colossalai/lazy/lazy_init.py:93: RuntimeError
________________________________ test_lazy_ops _________________________________
2025-04-11T04:23:18.7875059Z
@pytest.mark.skipif(not SUPPORT_LAZY, reason="requires torch >= 1.12.0")
def test_lazy_ops():
with LazyInitContext():
x = torch.rand(2, 3)
assert tuple(x.shape) == (2, 3)
assert x.device.type == "cpu"
x.requires_grad is False
y = x.cuda()
assert tuple(y.shape) == (2, 3)
assert y.device.type == "cuda"
assert y.requires_grad is False
assert x.cpu() is x
p = Parameter(torch.empty(2, 3))
assert tuple(p.shape) == (2, 3)
assert p.device.type == "cpu"
assert p.requires_grad is True
assert isinstance(p, Parameter)
x.materialize()
assert tuple(x.shape) == (2, 3)
assert x.device.type == "cpu"
assert x.requires_grad is False
>       y.materialize()
2025-04-11T04:23:18.7877222Z
tests/test_lazy/test_ops.py:33:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
colossalai/lazy/lazy_init.py:217: in materialize
target = self._materialize_data()
colossalai/lazy/lazy_init.py:242: in _materialize_data
init_val = func(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.7877959Z
t = tensor([[0.8823, 0.9150, 0.3829],
[0.9593, 0.3904, 0.6009]])
kw = {'device': device(type='cuda')}
2025-04-11T04:23:18.7878224Z
def factory_fn(t: torch.Tensor, **kw):
>       return t.to(*args, **kwargs)
E       RuntimeError: CUDA error: out of memory
E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7879137Z
colossalai/lazy/lazy_init.py:380: RuntimeError
_____________________________ test_torch_ddp_lora ______________________________
2025-04-11T04:23:18.7879386Z
args = (), kwargs = {}, try_count = 1
error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T04:23:18.7880104Z
def _run_until_success(*args, **kwargs):
try_count = 0
assert max_try is None or isinstance(
max_try, int
), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T04:23:18.7880733Z
while max_try is None or try_count < max_try:
try:
try_count += 1
ret = func(*args, **kwargs)
return ret
except exception_type as e:
error_lines = str(e).split("\n")
if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
print("Exception is caught, retrying...")
# when pattern is not specified, we always skip the exception
# when pattern is specified, we only skip when pattern is matched
continue
else:
print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
>                   raise e
2025-04-11T04:23:18.7882551Z
colossalai/testing/utils.py:144:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
colossalai/testing/utils.py:133: in _run_until_success
ret = func(*args, **kwargs)
tests/test_lora/test_lora.py:108: in test_torch_ddp_lora
spawn(run_dist, 2)
colossalai/testing/utils.py:252: in spawn
mp.spawn(wrapped_func, nprocs=nprocs)
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
while not context.join():
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.7884330Z
self = <torch.multiprocessing.spawn.ProcessContext object at 0x7fb5b5e92b00>
timeout = None
2025-04-11T04:23:18.7884664Z
def join(self, timeout=None):
r"""Join one or more processes within spawn context.
2025-04-11T04:23:18.7884963Z
Attempt to join one or more processes in this spawn context.
If one of them exited with a non-zero exit status, this function
kills the remaining processes and raises an exception with the cause
of the first process exiting.
2025-04-11T04:23:18.7885587Z
Returns ``True`` if all processes have been joined successfully,
``False`` if there are more processes that need to be joined.
2025-04-11T04:23:18.7885941Z
Args:
timeout (float): Wait this long before giving up on waiting.
"""
# Ensure this function can be called even when we're done.
if len(self.sentinels) == 0:
return True
2025-04-11T04:23:18.7886660Z
# Wait for any process to fail or all of them to succeed.
ready = multiprocessing.connection.wait(
self.sentinels.keys(),
timeout=timeout,
)
2025-04-11T04:23:18.7887234Z
error_index = None
for sentinel in ready:
index = self.sentinels.pop(sentinel)
process = self.processes[index]
process.join()
if process.exitcode != 0:
error_index = index
break
2025-04-11T04:23:18.7888080Z
# Return if there was no error.
if error_index is None:
# Return whether or not all processes have been joined.
return len(self.sentinels) == 0
2025-04-11T04:23:18.7888612Z
# Assume failure. Terminate processes that are still alive.
for process in self.processes:
if process.is_alive():
process.terminate()
process.join()
2025-04-11T04:23:18.7889190Z
# There won't be an error on the queue if the process crashed.
failed_process = self.processes[error_index]
if self.error_queues[error_index].empty():
exitcode = self.processes[error_index].exitcode
if exitcode < 0:
name = signal.Signals(-exitcode).name
raise ProcessExitedException(
"process %d terminated with signal %s" % (error_index, name),
error_index=error_index,
error_pid=failed_process.pid,
exit_code=exitcode,
signal_name=name,
)
else:
raise ProcessExitedException(
"process %d terminated with exit code %d" % (error_index, exitcode),
error_index=error_index,
error_pid=failed_process.pid,
exit_code=exitcode,
)
2025-04-11T04:23:18.7891413Z
original_trace = self.error_queues[error_index].get()
msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
msg += original_trace
>       raise ProcessRaisedException(msg, error_index, failed_process.pid)
E       torch.multiprocessing.spawn.ProcessRaisedException:
E
E       -- Process 0 terminated with the following error:
E       Traceback (most recent call last):
E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
E           fn(i, *args)
E         File "/__w/ColossalAI/ColossalAI/tests/test_lora/test_lora.py", line 103, in run_dist
E           run_lora_test()
E         File "/__w/ColossalAI/ColossalAI/tests/test_lora/test_lora.py", line 98, in run_lora_test
E           check_fwd_bwd(model_fn, data_gen_fn, output_transform_fn, loss_fn, task_type)
E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 271, in _clear_cache
E           get_accelerator().synchronize()
E         File "/__w/ColossalAI/ColossalAI/colossalai/accelerator/cuda_accelerator.py", line 62, in synchronize
E           torch.cuda.synchronize(device)
E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py", line 801, in synchronize
E           return torch._C._cuda_synchronize()
E       RuntimeError: CUDA error: out of memory
E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7895439Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
----------------------------- Captured stdout call -----------------------------
[04/11/25 04:17:31] INFO     colossalai - colossalai - INFO:
/__w/ColossalAI/ColossalAI/colossalai/initialize.py
:75 launch
INFO     colossalai - colossalai - INFO: Distributed
environment is initialized, world size: 2
Maximum number of attempts is reached or pattern is not matched, no more retrying...
----------------------------- Captured stderr call -----------------------------
/opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
/opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
/opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
warnings.warn(
[W socket.cpp:697] [c10d] The client socket has failed to connect to [localhost]:61905 (errno: 99 - Cannot assign requested address).
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:356: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:356: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
warnings.warn(
_________________________ test_moe_kernel[data_type0] __________________________
2025-04-11T04:23:18.7909698Z
data_type = torch.float32
2025-04-11T04:23:18.7909849Z
@pytest.mark.parametrize("data_type", [torch.float32, torch.float16])
def test_moe_kernel(data_type):
torch.manual_seed(1024)
>       run_moe_cumsum()
2025-04-11T04:23:18.7910298Z
tests/test_moe/test_kernel.py:93:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.7910565Z
def run_moe_cumsum():
test_mask = torch.tensor(
[
[0, 1, 0, 0],
[1, 0, 0, 0],
[0, 1, 0, 0],
[1, 0, 0, 0],
],
dtype=torch.int32,
>       ).to("cuda")
E       RuntimeError: CUDA error: out of memory
E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7912113Z
tests/test_moe/test_kernel.py:29: RuntimeError
_________________________ test_moe_kernel[data_type1] __________________________
2025-04-11T04:23:18.7912372Z
data_type = torch.float16
2025-04-11T04:23:18.7912463Z
@pytest.mark.parametrize("data_type", [torch.float32, torch.float16])
def test_moe_kernel(data_type):
torch.manual_seed(1024)
>       run_moe_cumsum()
2025-04-11T04:23:18.7912901Z
tests/test_moe/test_kernel.py:93:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.7913109Z
def run_moe_cumsum():
test_mask = torch.tensor(
[
[0, 1, 0, 0],
[1, 0, 0, 0],
[0, 1, 0, 0],
[1, 0, 0, 0],
],
dtype=torch.int32,
>       ).to("cuda")
E       RuntimeError: CUDA error: out of memory
E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7914638Z
tests/test_moe/test_kernel.py:29: RuntimeError
__________________________ test_mixtral_moe_layer[4] ___________________________
2025-04-11T04:23:18.7914893Z
world_size = 4
2025-04-11T04:23:18.7914976Z
@pytest.mark.parametrize("world_size", [4])
def test_mixtral_moe_layer(world_size: int):
>       spawn(run_dist, world_size)
2025-04-11T04:23:18.7915292Z
tests/test_moe/test_moe_checkpoint.py:171:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
colossalai/testing/utils.py:252: in spawn
mp.spawn(wrapped_func, nprocs=nprocs)
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
while not context.join():
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.7916743Z
self = <torch.multiprocessing.spawn.ProcessContext object at 0x7fb5b5f0aa70>
timeout = None
2025-04-11T04:23:18.7917024Z
def join(self, timeout=None):
r"""Join one or more processes within spawn context.
2025-04-11T04:23:18.7917312Z
Attempt to join one or more processes in this spawn context.
If one of them exited with a non-zero exit status, this function
kills the remaining processes and raises an exception with the cause
of the first process exiting.
2025-04-11T04:23:18.7917976Z
Returns ``True`` if all processes have been joined successfully,
``False`` if there are more processes that need to be joined.
2025-04-11T04:23:18.7918337Z
Args:
timeout (float): Wait this long before giving up on waiting.
"""
# Ensure this function can be called even when we're done.
if len(self.sentinels) == 0:
return True
2025-04-11T04:23:18.7919000Z
# Wait for any process to fail or all of them to succeed.
ready = multiprocessing.connection.wait(
self.sentinels.keys(),
timeout=timeout,
)
2025-04-11T04:23:18.7919569Z
error_index = None
for sentinel in ready:
index = self.sentinels.pop(sentinel)
process = self.processes[index]
process.join()
if process.exitcode != 0:
error_index = index
break
2025-04-11T04:23:18.7920409Z
# Return if there was no error.
if error_index is None:
# Return whether or not all processes have been joined.
return len(self.sentinels) == 0
2025-04-11T04:23:18.7920886Z
# Assume failure. Terminate processes that are still alive.
for process in self.processes:
if process.is_alive():
process.terminate()
process.join()
2025-04-11T04:23:18.7921459Z
# There won't be an error on the queue if the process crashed.
failed_process = self.processes[error_index]
if self.error_queues[error_index].empty():
exitcode = self.processes[error_index].exitcode
if exitcode < 0:
name = signal.Signals(-exitcode).name
raise ProcessExitedException(
"process %d terminated with signal %s" % (error_index, name),
error_index=error_index,
error_pid=failed_process.pid,
exit_code=exitcode,
signal_name=name,
)
else:
raise ProcessExitedException(
"process %d terminated with exit code %d" % (error_index, exitcode),
error_index=error_index,
error_pid=failed_process.pid,
exit_code=exitcode,
)
2025-04-11T04:23:18.7923789Z
original_trace = self.error_queues[error_index].get()
msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
msg += original_trace
>       raise ProcessRaisedException(msg, error_index, failed_process.pid)
E       torch.multiprocessing.spawn.ProcessRaisedException:
E
E       -- Process 0 terminated with the following error:
E       Traceback (most recent call last):
E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
E           fn(i, *args)
E         File "/__w/ColossalAI/ColossalAI/tests/test_moe/test_moe_checkpoint.py", line 165, in run_dist
E           check_moe_checkpoint()
E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
E           partial_func(**kwargs)
E         File "/__w/ColossalAI/ColossalAI/tests/test_moe/test_moe_checkpoint.py", line 101, in check_moe_checkpoint
E           dist.broadcast_object_list(broadcast_objects, src=0)
E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/distributed/c10d_logger.py", line 72, in wrapper
E           return func(*args, **kwargs)
E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py", line 2405, in broadcast_object_list
E           tensor_list, size_list = zip(*[_object_to_tensor(obj, current_device) for obj in object_list])
E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py", line 2405, in <listcomp>
E           tensor_list, size_list = zip(*[_object_to_tensor(obj, current_device) for obj in object_list])
E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py", line 2115, in _object_to_tensor
E           byte_tensor = torch.ByteTensor(byte_storage).to(device)
E       RuntimeError: CUDA error: out of memory
E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7929122Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
----------------------------- Captured stdout call -----------------------------
[04/11/25 04:17:38] INFO     colossalai - colossalai - INFO:
/__w/ColossalAI/ColossalAI/colossalai/initialize.py
:75 launch
INFO     colossalai - colossalai - INFO: Distributed
environment is initialized, world size: 4
----------------------------- Captured stderr call -----------------------------
[W socket.cpp:697] [c10d] The client socket has failed to connect to [localhost]:27296 (errno: 99 - Cannot assign requested address).
[W socket.cpp:697] [c10d] The client socket has failed to connect to [localhost]:27296 (errno: 99 - Cannot assign requested address).
_____________ test_fused_adam_kernel[p_dtype0-g_dtype0-0.0-False] ______________
2025-04-11T04:23:18.7931250Z
adamw = False, weight_decay = 0.0, p_dtype = torch.float32
g_dtype = torch.float16
2025-04-11T04:23:18.7931482Z
@pytest.mark.parametrize("adamw", [False, True])
@pytest.mark.parametrize("weight_decay", [0.0, 0.1])
@pytest.mark.parametrize("p_dtype, g_dtype", _FUSED_ALLOWED_P_G_TYPES)
def test_fused_adam_kernel(adamw, weight_decay, p_dtype, g_dtype):
rtol, atol = 1e-5, 1e-8
if p_dtype is torch.float16 or g_dtype is torch.float16:
rtol, atol = 1e-3, 1e-3
if p_dtype is torch.bfloat16 or g_dtype is torch.bfloat16:
rtol, atol = 4e-3, 4e-3
>       check_adam_kernel(
FusedAdamKernel, adamw, weight_decay, p_dtype, g_dtype, get_accelerator().get_current_device(), 3, rtol, atol
)
2025-04-11T04:23:18.7933038Z
tests/test_optimizer/test_adam_kernel.py:159:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
tests/test_optimizer/test_adam_kernel.py:132: in check_adam_kernel
adam_kernel = kernel(lr, beta1, beta2, eps, weight_decay, adamw)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.7933745Z
self = <test_adam_kernel.FusedAdamKernel object at 0x7fb5b5c57c10>, lr = 0.001
beta1 = 0.9, beta2 = 0.999, eps = 1e-08, weight_decay = 0.0, use_adamw = False
2025-04-11T04:23:18.7934090Z
def __init__(self, lr: float, beta1: float, beta2: float, eps: float, weight_decay: float, use_adamw: bool) -> None:
super().__init__(lr, beta1, beta2, eps, weight_decay, use_adamw)
from colossalai.kernel.kernel_loader import FusedOptimizerLoader
2025-04-11T04:23:18.7934722Z
fused_optim = FusedOptimizerLoader().load()
self.fused_adam = fused_optim.multi_tensor_adam
>       self.dummy_overflow_buf = torch.tensor([0], dtype=torch.int, device=get_accelerator().get_current_device())
E       RuntimeError: CUDA error: out of memory
E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7936089Z
tests/test_optimizer/test_adam_kernel.py:72: RuntimeError
______________ test_fused_adam_kernel[p_dtype0-g_dtype0-0.0-True] ______________
2025-04-11T04:23:18.7936401Z
adamw = True, weight_decay = 0.0, p_dtype = torch.float32
g_dtype = torch.float16
2025-04-11T04:23:18.7936626Z
@pytest.mark.parametrize("adamw", [False, True])
@pytest.mark.parametrize("weight_decay", [0.0, 0.1])
@pytest.mark.parametrize("p_dtype, g_dtype", _FUSED_ALLOWED_P_G_TYPES)
def test_fused_adam_kernel(adamw, weight_decay, p_dtype, g_dtype):
rtol, atol = 1e-5, 1e-8
if p_dtype is torch.float16 or g_dtype is torch.float16:
rtol, atol = 1e-3, 1e-3
if p_dtype is torch.bfloat16 or g_dtype is torch.bfloat16:
rtol, atol = 4e-3, 4e-3
>       check_adam_kernel(
FusedAdamKernel, adamw, weight_decay, p_dtype, g_dtype, get_accelerator().get_current_device(), 3, rtol, atol
)
2025-04-11T04:23:18.7938209Z
tests/test_optimizer/test_adam_kernel.py:159:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
tests/test_optimizer/test_adam_kernel.py:132: in check_adam_kernel
adam_kernel = kernel(lr, beta1, beta2, eps, weight_decay, adamw)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.7938854Z
self = <test_adam_kernel.FusedAdamKernel object at 0x7fb5b5ad1e40>, lr = 0.001
beta1 = 0.9, beta2 = 0.999, eps = 1e-08, weight_decay = 0.0, use_adamw = True
2025-04-11T04:23:18.7939188Z
def __init__(self, lr: float, beta1: float, beta2: float, eps: float, weight_decay: float, use_adamw: bool) -> None:
super().__init__(lr, beta1, beta2, eps, weight_decay, use_adamw)
from colossalai.kernel.kernel_loader import FusedOptimizerLoader
2025-04-11T04:23:18.7939865Z
fused_optim = FusedOptimizerLoader().load()
self.fused_adam = fused_optim.multi_tensor_adam
>       self.dummy_overflow_buf = torch.tensor([0], dtype=torch.int, device=get_accelerator().get_current_device())
E       RuntimeError: CUDA error: out of memory
E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7941031Z
tests/test_optimizer/test_adam_kernel.py:72: RuntimeError
_____________ test_fused_adam_kernel[p_dtype0-g_dtype0-0.1-False] ______________
2025-04-11T04:23:18.7941388Z
adamw = False, weight_decay = 0.1, p_dtype = torch.float32
g_dtype = torch.float16
2025-04-11T04:23:18.7941612Z
@pytest.mark.parametrize("adamw", [False, True])
@pytest.mark.parametrize("weight_decay", [0.0, 0.1])
@pytest.mark.parametrize("p_dtype, g_dtype", _FUSED_ALLOWED_P_G_TYPES)
def test_fused_adam_kernel(adamw, weight_decay, p_dtype, g_dtype):
rtol, atol = 1e-5, 1e-8
if p_dtype is torch.float16 or g_dtype is torch.float16:
rtol, atol = 1e-3, 1e-3
if p_dtype is torch.bfloat16 or g_dtype is torch.bfloat16:
rtol, atol = 4e-3, 4e-3
>       check_adam_kernel(
FusedAdamKernel, adamw, weight_decay, p_dtype, g_dtype, get_accelerator().get_current_device(), 3, rtol, atol
)
2025-04-11T04:23:18.7943240Z
tests/test_optimizer/test_adam_kernel.py:159:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
tests/test_optimizer/test_adam_kernel.py:132: in check_adam_kernel
adam_kernel = kernel(lr, beta1, beta2, eps, weight_decay, adamw)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.7943893Z
self = <test_adam_kernel.FusedAdamKernel object at 0x7fb5b5a6fd00>, lr = 0.001
beta1 = 0.9, beta2 = 0.999, eps = 1e-08, weight_decay = 0.1, use_adamw = False
2025-04-11T04:23:18.7944229Z
def __init__(self, lr: float, beta1: float, beta2: float, eps: float, weight_decay: float, use_adamw: bool) -> None:
super().__init__(lr, beta1, beta2, eps, weight_decay, use_adamw)
from colossalai.kernel.kernel_loader import FusedOptimizerLoader
2025-04-11T04:23:18.7944852Z
fused_optim = FusedOptimizerLoader().load()
self.fused_adam = fused_optim.multi_tensor_adam
>       self.dummy_overflow_buf = torch.tensor([0], dtype=torch.int, device=get_accelerator().get_current_device())
E       RuntimeError: CUDA error: out of memory
E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7946073Z
tests/test_optimizer/test_adam_kernel.py:72: RuntimeError
______________ test_fused_adam_kernel[p_dtype0-g_dtype0-0.1-True] ______________
2025-04-11T04:23:18.7946375Z
adamw = True, weight_decay = 0.1, p_dtype = torch.float32
g_dtype = torch.float16
2025-04-11T04:23:18.7946593Z
@pytest.mark.parametrize("adamw", [False, True])
@pytest.mark.parametrize("weight_decay", [0.0, 0.1])
@pytest.mark.parametrize("p_dtype, g_dtype", _FUSED_ALLOWED_P_G_TYPES)
def test_fused_adam_kernel(adamw, weight_decay, p_dtype, g_dtype):
rtol, atol = 1e-5, 1e-8
if p_dtype is torch.float16 or g_dtype is torch.float16:
rtol, atol = 1e-3, 1e-3
if p_dtype is torch.bfloat16 or g_dtype is torch.bfloat16:
rtol, atol = 4e-3, 4e-3
>       check_adam_kernel(
FusedAdamKernel, adamw, weight_decay, p_dtype, g_dtype, get_accelerator().get_current_device(), 3, rtol, atol
)
2025-04-11T04:23:18.7948218Z
tests/test_optimizer/test_adam_kernel.py:159:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
tests/test_optimizer/test_adam_kernel.py:132: in check_adam_kernel
adam_kernel = kernel(lr, beta1, beta2, eps, weight_decay, adamw)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.7948903Z
self = <test_adam_kernel.FusedAdamKernel object at 0x7fb5b5c77370>, lr = 0.001
beta1 = 0.9, beta2 = 0.999, eps = 1e-08, weight_decay = 0.1, use_adamw = True
2025-04-11T04:23:18.7949290Z
def __init__(self, lr: float, beta1: float, beta2: float, eps: float, weight_decay: float, use_adamw: bool) -> None:
super().__init__(lr, beta1, beta2, eps, weight_decay, use_adamw)
from colossalai.kernel.kernel_loader import FusedOptimizerLoader
2025-04-11T04:23:18.7949901Z
fused_optim = FusedOptimizerLoader().load()
self.fused_adam = fused_optim.multi_tensor_adam
>       self.dummy_overflow_buf = torch.tensor([0], dtype=torch.int, device=get_accelerator().get_current_device())
E       RuntimeError: CUDA error: out of memory
E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7951057Z
tests/test_optimizer/test_adam_kernel.py:72: RuntimeError
_____________ test_fused_adam_kernel[p_dtype1-g_dtype1-0.0-False] ______________
2025-04-11T04:23:18.7951360Z
adamw = False, weight_decay = 0.0, p_dtype = torch.float32
g_dtype = torch.float32
2025-04-11T04:23:18.7951635Z
@pytest.mark.parametrize("adamw", [False, True])
@pytest.mark.parametrize("weight_decay", [0.0, 0.1])
@pytest.mark.parametrize("p_dtype, g_dtype", _FUSED_ALLOWED_P_G_TYPES)
def test_fused_adam_kernel(adamw, weight_decay, p_dtype, g_dtype):
rtol, atol = 1e-5, 1e-8
if p_dtype is torch.float16 or g_dtype is torch.float16:
rtol, atol = 1e-3, 1e-3
if p_dtype is torch.bfloat16 or g_dtype is torch.bfloat16:
rtol, atol = 4e-3, 4e-3
>       check_adam_kernel(
FusedAdamKernel, adamw, weight_decay, p_dtype, g_dtype, get_accelerator().get_current_device(), 3, rtol, atol
)
2025-04-11T04:23:18.7953148Z
tests/test_optimizer/test_adam_kernel.py:159:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
tests/test_optimizer/test_adam_kernel.py:132: in check_adam_kernel
adam_kernel = kernel(lr, beta1, beta2, eps, weight_decay, adamw)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.7953844Z
self = <test_adam_kernel.FusedAdamKernel object at 0x7fb5b5a84040>, lr = 0.001
beta1 = 0.9, beta2 = 0.999, eps = 1e-08, weight_decay = 0.0, use_adamw = False
2025-04-11T04:23:18.7954232Z
def __init__(self, lr: float, beta1: float, beta2: float, eps: float, weight_decay: float, use_adamw: bool) -> None:
super().__init__(lr, beta1, beta2, eps, weight_decay, use_adamw)
from colossalai.kernel.kernel_loader import FusedOptimizerLoader
2025-04-11T04:23:18.7954845Z
fused_optim = FusedOptimizerLoader().load()
self.fused_adam = fused_optim.multi_tensor_adam
>       self.dummy_overflow_buf = torch.tensor([0], dtype=torch.int, device=get_accelerator().get_current_device())
E       RuntimeError: CUDA error: out of memory
E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7956054Z
tests/test_optimizer/test_adam_kernel.py:72: RuntimeError
______________ test_fused_adam_kernel[p_dtype1-g_dtype1-0.0-True] ______________
2025-04-11T04:23:18.7956365Z
adamw = True, weight_decay = 0.0, p_dtype = torch.float32
g_dtype = torch.float32
2025-04-11T04:23:18.7956590Z
@pytest.mark.parametrize("adamw", [False, True])
@pytest.mark.parametrize("weight_decay", [0.0, 0.1])
@pytest.mark.parametrize("p_dtype, g_dtype", _FUSED_ALLOWED_P_G_TYPES)
def test_fused_adam_kernel(adamw, weight_decay, p_dtype, g_dtype):
rtol, atol = 1e-5, 1e-8
if p_dtype is torch.float16 or g_dtype is torch.float16:
rtol, atol = 1e-3, 1e-3
if p_dtype is torch.bfloat16 or g_dtype is torch.bfloat16:
rtol, atol = 4e-3, 4e-3
>       check_adam_kernel(
FusedAdamKernel, adamw, weight_decay, p_dtype, g_dtype, get_accelerator().get_current_device(), 3, rtol, atol
)
2025-04-11T04:23:18.7958169Z
tests/test_optimizer/test_adam_kernel.py:159:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
tests/test_optimizer/test_adam_kernel.py:132: in check_adam_kernel
adam_kernel = kernel(lr, beta1, beta2, eps, weight_decay, adamw)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.7958803Z
self = <test_adam_kernel.FusedAdamKernel object at 0x7fb5b5b3f790>, lr = 0.001
beta1 = 0.9, beta2 = 0.999, eps = 1e-08, weight_decay = 0.0, use_adamw = True
2025-04-11T04:23:18.7959132Z
def __init__(self, lr: float, beta1: float, beta2: float, eps: float, weight_decay: float, use_adamw: bool) -> None:
super().__init__(lr, beta1, beta2, eps, weight_decay, use_adamw)
from colossalai.kernel.kernel_loader import FusedOptimizerLoader
2025-04-11T04:23:18.7959794Z
fused_optim = FusedOptimizerLoader().load()
self.fused_adam = fused_optim.multi_tensor_adam
>       self.dummy_overflow_buf = torch.tensor([0], dtype=torch.int, device=get_accelerator().get_current_device())
E       RuntimeError: CUDA error: out of memory
E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7960992Z
tests/test_optimizer/test_adam_kernel.py:72: RuntimeError
_____________ test_fused_adam_kernel[p_dtype1-g_dtype1-0.1-False] ______________
2025-04-11T04:23:18.7961345Z
adamw = False, weight_decay = 0.1, p_dtype = torch.float32
g_dtype = torch.float32
2025-04-11T04:23:18.7961564Z
@pytest.mark.parametrize("adamw", [False, True])
@pytest.mark.parametrize("weight_decay", [0.0, 0.1])
@pytest.mark.parametrize("p_dtype, g_dtype", _FUSED_ALLOWED_P_G_TYPES)
def test_fused_adam_kernel(adamw, weight_decay, p_dtype, g_dtype):
rtol, atol = 1e-5, 1e-8
if p_dtype is torch.float16 or g_dtype is torch.float16:
rtol, atol = 1e-3, 1e-3
if p_dtype is torch.bfloat16 or g_dtype is torch.bfloat16:
rtol, atol = 4e-3, 4e-3
>       check_adam_kernel(
FusedAdamKernel, adamw, weight_decay, p_dtype, g_dtype, get_accelerator().get_current_device(), 3, rtol, atol
)
2025-04-11T04:23:18.7963062Z
tests/test_optimizer/test_adam_kernel.py:159:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
tests/test_optimizer/test_adam_kernel.py:132: in check_adam_kernel
adam_kernel = kernel(lr, beta1, beta2, eps, weight_decay, adamw)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.7963746Z
self = <test_adam_kernel.FusedAdamKernel object at 0x7fb5b5a854b0>, lr = 0.001
beta1 = 0.9, beta2 = 0.999, eps = 1e-08, weight_decay = 0.1, use_adamw = False
2025-04-11T04:23:18.7964081Z
def __init__(self, lr: float, beta1: float, beta2: float, eps: float, weight_decay: float, use_adamw: bool) -> None:
super().__init__(lr, beta1, beta2, eps, weight_decay, use_adamw)
from colossalai.kernel.kernel_loader import FusedOptimizerLoader
2025-04-11T04:23:18.7964696Z
fused_optim = FusedOptimizerLoader().load()
self.fused_adam = fused_optim.multi_tensor_adam
>       self.dummy_overflow_buf = torch.tensor([0], dtype=torch.int, device=get_accelerator().get_current_device())
E       RuntimeError: CUDA error: out of memory
E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7965908Z
tests/test_optimizer/test_adam_kernel.py:72: RuntimeError
______________ test_fused_adam_kernel[p_dtype1-g_dtype1-0.1-True] ______________
2025-04-11T04:23:18.7966245Z
adamw = True, weight_decay = 0.1, p_dtype = torch.float32
g_dtype = torch.float32
2025-04-11T04:23:18.7966465Z
@pytest.mark.parametrize("adamw", [False, True])
@pytest.mark.parametrize("weight_decay", [0.0, 0.1])
@pytest.mark.parametrize("p_dtype, g_dtype", _FUSED_ALLOWED_P_G_TYPES)
def test_fused_adam_kernel(adamw, weight_decay, p_dtype, g_dtype):
rtol, atol = 1e-5, 1e-8
if p_dtype is torch.float16 or g_dtype is torch.float16:
rtol, atol = 1e-3, 1e-3
if p_dtype is torch.bfloat16 or g_dtype is torch.bfloat16:
rtol, atol = 4e-3, 4e-3
>       check_adam_kernel(
FusedAdamKernel, adamw, weight_decay, p_dtype, g_dtype, get_accelerator().get_current_device(), 3, rtol, atol
)
2025-04-11T04:23:18.7968014Z
tests/test_optimizer/test_adam_kernel.py:159:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
tests/test_optimizer/test_adam_kernel.py:132: in check_adam_kernel
adam_kernel = kernel(lr, beta1, beta2, eps, weight_decay, adamw)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.7968661Z
self = <test_adam_kernel.FusedAdamKernel object at 0x7fb5b5b3fbb0>, lr = 0.001
beta1 = 0.9, beta2 = 0.999, eps = 1e-08, weight_decay = 0.1, use_adamw = True
2025-04-11T04:23:18.7968992Z
def __init__(self, lr: float, beta1: float, beta2: float, eps: float, weight_decay: float, use_adamw: bool) -> None:
super().__init__(lr, beta1, beta2, eps, weight_decay, use_adamw)
from colossalai.kernel.kernel_loader import FusedOptimizerLoader
2025-04-11T04:23:18.7969600Z
fused_optim = FusedOptimizerLoader().load()
self.fused_adam = fused_optim.multi_tensor_adam
>       self.dummy_overflow_buf = torch.tensor([0], dtype=torch.int, device=get_accelerator().get_current_device())
E       RuntimeError: CUDA error: out of memory
E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7970815Z
tests/test_optimizer/test_adam_kernel.py:72: RuntimeError
_____________ test_fused_adam_kernel[p_dtype2-g_dtype2-0.0-False] ______________
2025-04-11T04:23:18.7971113Z
adamw = False, weight_decay = 0.0, p_dtype = torch.float16
g_dtype = torch.float16
2025-04-11T04:23:18.7971336Z
@pytest.mark.parametrize("adamw", [False, True])
@pytest.mark.parametrize("weight_decay", [0.0, 0.1])
@pytest.mark.parametrize("p_dtype, g_dtype", _FUSED_ALLOWED_P_G_TYPES)
def test_fused_adam_kernel(adamw, weight_decay, p_dtype, g_dtype):
rtol, atol = 1e-5, 1e-8
if p_dtype is torch.float16 or g_dtype is torch.float16:
rtol, atol = 1e-3, 1e-3
if p_dtype is torch.bfloat16 or g_dtype is torch.bfloat16:
rtol, atol = 4e-3, 4e-3
>       check_adam_kernel(
FusedAdamKernel, adamw, weight_decay, p_dtype, g_dtype, get_accelerator().get_current_device(), 3, rtol, atol
)
2025-04-11T04:23:18.7972951Z
tests/test_optimizer/test_adam_kernel.py:159:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
tests/test_optimizer/test_adam_kernel.py:132: in check_adam_kernel
adam_kernel = kernel(lr, beta1, beta2, eps, weight_decay, adamw)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.7973639Z
self = <test_adam_kernel.FusedAdamKernel object at 0x7fb5b5a85150>, lr = 0.001
beta1 = 0.9, beta2 = 0.999, eps = 1e-08, weight_decay = 0.0, use_adamw = False
2025-04-11T04:23:18.7973972Z
def __init__(self, lr: float, beta1: float, beta2: float, eps: float, weight_decay: float, use_adamw: bool) -> None:
super().__init__(lr, beta1, beta2, eps, weight_decay, use_adamw)
from colossalai.kernel.kernel_loader import FusedOptimizerLoader
2025-04-11T04:23:18.7974584Z
fused_optim = FusedOptimizerLoader().load()
self.fused_adam = fused_optim.multi_tensor_adam
>       self.dummy_overflow_buf = torch.tensor([0], dtype=torch.int, device=get_accelerator().get_current_device())
E       RuntimeError: CUDA error: out of memory
E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7975799Z
tests/test_optimizer/test_adam_kernel.py:72: RuntimeError
______________ test_fused_adam_kernel[p_dtype2-g_dtype2-0.0-True] ______________
2025-04-11T04:23:18.7976106Z
adamw = True, weight_decay = 0.0, p_dtype = torch.float16
g_dtype = torch.float16
2025-04-11T04:23:18.7976326Z
@pytest.mark.parametrize("adamw", [False, True])
@pytest.mark.parametrize("weight_decay", [0.0, 0.1])
@pytest.mark.parametrize("p_dtype, g_dtype", _FUSED_ALLOWED_P_G_TYPES)
def test_fused_adam_kernel(adamw, weight_decay, p_dtype, g_dtype):
rtol, atol = 1e-5, 1e-8
if p_dtype is torch.float16 or g_dtype is torch.float16:
rtol, atol = 1e-3, 1e-3
if p_dtype is torch.bfloat16 or g_dtype is torch.bfloat16:
rtol, atol = 4e-3, 4e-3
>       check_adam_kernel(
FusedAdamKernel, adamw, weight_decay, p_dtype, g_dtype, get_accelerator().get_current_device(), 3, rtol, atol
)
2025-04-11T04:23:18.7977883Z
tests/test_optimizer/test_adam_kernel.py:159:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
tests/test_optimizer/test_adam_kernel.py:132: in check_adam_kernel
adam_kernel = kernel(lr, beta1, beta2, eps, weight_decay, adamw)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.7978574Z
self = <test_adam_kernel.FusedAdamKernel object at 0x7fb5819268c0>, lr = 0.001
beta1 = 0.9, beta2 = 0.999, eps = 1e-08, weight_decay = 0.0, use_adamw = True
2025-04-11T04:23:18.7978906Z
def __init__(self, lr: float, beta1: float, beta2: float, eps: float, weight_decay: float, use_adamw: bool) -> None:
super().__init__(lr, beta1, beta2, eps, weight_decay, use_adamw)
from colossalai.kernel.kernel_loader import FusedOptimizerLoader
2025-04-11T04:23:18.7979572Z
fused_optim = FusedOptimizerLoader().load()
self.fused_adam = fused_optim.multi_tensor_adam
>       self.dummy_overflow_buf = torch.tensor([0], dtype=torch.int, device=get_accelerator().get_current_device())
E       RuntimeError: CUDA error: out of memory
E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7980722Z
tests/test_optimizer/test_adam_kernel.py:72: RuntimeError
_____________ test_fused_adam_kernel[p_dtype2-g_dtype2-0.1-False] ______________
2025-04-11T04:23:18.7981026Z
adamw = False, weight_decay = 0.1, p_dtype = torch.float16
g_dtype = torch.float16
2025-04-11T04:23:18.7981247Z
@pytest.mark.parametrize("adamw", [False, True])
@pytest.mark.parametrize("weight_decay", [0.0, 0.1])
@pytest.mark.parametrize("p_dtype, g_dtype", _FUSED_ALLOWED_P_G_TYPES)
def test_fused_adam_kernel(adamw, weight_decay, p_dtype, g_dtype):
rtol, atol = 1e-5, 1e-8
if p_dtype is torch.float16 or g_dtype is torch.float16:
rtol, atol = 1e-3, 1e-3
if p_dtype is torch.bfloat16 or g_dtype is torch.bfloat16:
rtol, atol = 4e-3, 4e-3
>       check_adam_kernel(
FusedAdamKernel, adamw, weight_decay, p_dtype, g_dtype, get_accelerator().get_current_device(), 3, rtol, atol
)
2025-04-11T04:23:18.7982790Z
tests/test_optimizer/test_adam_kernel.py:159:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
tests/test_optimizer/test_adam_kernel.py:132: in check_adam_kernel
adam_kernel = kernel(lr, beta1, beta2, eps, weight_decay, adamw)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.7983426Z
self = <test_adam_kernel.FusedAdamKernel object at 0x7fb581d7a710>, lr = 0.001
beta1 = 0.9, beta2 = 0.999, eps = 1e-08, weight_decay = 0.1, use_adamw = False
2025-04-11T04:23:18.7983808Z
def __init__(self, lr: float, beta1: float, beta2: float, eps: float, weight_decay: float, use_adamw: bool) -> None:
super().__init__(lr, beta1, beta2, eps, weight_decay, use_adamw)
from colossalai.kernel.kernel_loader import FusedOptimizerLoader
2025-04-11T04:23:18.7984468Z
fused_optim = FusedOptimizerLoader().load()
self.fused_adam = fused_optim.multi_tensor_adam
>       self.dummy_overflow_buf = torch.tensor([0], dtype=torch.int, device=get_accelerator().get_current_device())
E       RuntimeError: CUDA error: out of memory
E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7985666Z
tests/test_optimizer/test_adam_kernel.py:72: RuntimeError
______________ test_fused_adam_kernel[p_dtype2-g_dtype2-0.1-True] ______________
2025-04-11T04:23:18.7985970Z
adamw = True, weight_decay = 0.1, p_dtype = torch.float16
g_dtype = torch.float16
2025-04-11T04:23:18.7986186Z
@pytest.mark.parametrize("adamw", [False, True])
@pytest.mark.parametrize("weight_decay", [0.0, 0.1])
@pytest.mark.parametrize("p_dtype, g_dtype", _FUSED_ALLOWED_P_G_TYPES)
def test_fused_adam_kernel(adamw, weight_decay, p_dtype, g_dtype):
rtol, atol = 1e-5, 1e-8
if p_dtype is torch.float16 or g_dtype is torch.float16:
rtol, atol = 1e-3, 1e-3
if p_dtype is torch.bfloat16 or g_dtype is torch.bfloat16:
rtol, atol = 4e-3, 4e-3
>       check_adam_kernel(
FusedAdamKernel, adamw, weight_decay, p_dtype, g_dtype, get_accelerator().get_current_device(), 3, rtol, atol
)
2025-04-11T04:23:18.7987672Z
tests/test_optimizer/test_adam_kernel.py:159:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
tests/test_optimizer/test_adam_kernel.py:132: in check_adam_kernel
adam_kernel = kernel(lr, beta1, beta2, eps, weight_decay, adamw)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.7988377Z
self = <test_adam_kernel.FusedAdamKernel object at 0x7fb5b5e929e0>, lr = 0.001
beta1 = 0.9, beta2 = 0.999, eps = 1e-08, weight_decay = 0.1, use_adamw = True
2025-04-11T04:23:18.7988752Z
def __init__(self, lr: float, beta1: float, beta2: float, eps: float, weight_decay: float, use_adamw: bool) -> None:
super().__init__(lr, beta1, beta2, eps, weight_decay, use_adamw)
from colossalai.kernel.kernel_loader import FusedOptimizerLoader
2025-04-11T04:23:18.7989365Z
fused_optim = FusedOptimizerLoader().load()
self.fused_adam = fused_optim.multi_tensor_adam
>       self.dummy_overflow_buf = torch.tensor([0], dtype=torch.int, device=get_accelerator().get_current_device())
E       RuntimeError: CUDA error: out of memory
E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7990651Z
tests/test_optimizer/test_adam_kernel.py:72: RuntimeError
_____________ test_fused_adam_kernel[p_dtype3-g_dtype3-0.0-False] ______________
2025-04-11T04:23:18.7990953Z
adamw = False, weight_decay = 0.0, p_dtype = torch.float32
g_dtype = torch.bfloat16
2025-04-11T04:23:18.7991175Z
@pytest.mark.parametrize("adamw", [False, True])
@pytest.mark.parametrize("weight_decay", [0.0, 0.1])
@pytest.mark.parametrize("p_dtype, g_dtype", _FUSED_ALLOWED_P_G_TYPES)
def test_fused_adam_kernel(adamw, weight_decay, p_dtype, g_dtype):
rtol, atol = 1e-5, 1e-8
if p_dtype is torch.float16 or g_dtype is torch.float16:
rtol, atol = 1e-3, 1e-3
if p_dtype is torch.bfloat16 or g_dtype is torch.bfloat16:
rtol, atol = 4e-3, 4e-3
>       check_adam_kernel(
FusedAdamKernel, adamw, weight_decay, p_dtype, g_dtype, get_accelerator().get_current_device(), 3, rtol, atol
)
2025-04-11T04:23:18.7992752Z
tests/test_optimizer/test_adam_kernel.py:159:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
tests/test_optimizer/test_adam_kernel.py:132: in check_adam_kernel
adam_kernel = kernel(lr, beta1, beta2, eps, weight_decay, adamw)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.7993392Z
self = <test_adam_kernel.FusedAdamKernel object at 0x7fb5b5c54cd0>, lr = 0.001
beta1 = 0.9, beta2 = 0.999, eps = 1e-08, weight_decay = 0.0, use_adamw = False
2025-04-11T04:23:18.7993720Z
def __init__(self, lr: float, beta1: float, beta2: float, eps: float, weight_decay: float, use_adamw: bool) -> None:
super().__init__(lr, beta1, beta2, eps, weight_decay, use_adamw)
from colossalai.kernel.kernel_loader import FusedOptimizerLoader
2025-04-11T04:23:18.7994406Z
fused_optim = FusedOptimizerLoader().load()
self.fused_adam = fused_optim.multi_tensor_adam
>       self.dummy_overflow_buf = torch.tensor([0], dtype=torch.int, device=get_accelerator().get_current_device())
E       RuntimeError: CUDA error: out of memory
E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.7995566Z
tests/test_optimizer/test_adam_kernel.py:72: RuntimeError
______________ test_fused_adam_kernel[p_dtype3-g_dtype3-0.0-True] ______________
2025-04-11T04:23:18.7995923Z
adamw = True, weight_decay = 0.0, p_dtype = torch.float32
g_dtype = torch.bfloat16
2025-04-11T04:23:18.7996148Z
@pytest.mark.parametrize("adamw", [False, True])
@pytest.mark.parametrize("weight_decay", [0.0, 0.1])
@pytest.mark.parametrize("p_dtype, g_dtype", _FUSED_ALLOWED_P_G_TYPES)
def test_fused_adam_kernel(adamw, weight_decay, p_dtype, g_dtype):
rtol, atol = 1e-5, 1e-8
if p_dtype is torch.float16 or g_dtype is torch.float16:
rtol, atol = 1e-3, 1e-3
if p_dtype is torch.bfloat16 or g_dtype is torch.bfloat16:
rtol, atol = 4e-3, 4e-3
>       check_adam_kernel(
FusedAdamKernel, adamw, weight_decay, p_dtype, g_dtype, get_accelerator().get_current_device(), 3, rtol, atol
)
2025-04-11T04:23:18.7997772Z
tests/test_optimizer/test_adam_kernel.py:159:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
tests/test_optimizer/test_adam_kernel.py:132: in check_adam_kernel
adam_kernel = kernel(lr, beta1, beta2, eps, weight_decay, adamw)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.7998409Z
self = <test_adam_kernel.FusedAdamKernel object at 0x7fb5b5c74c70>, lr = 0.001
beta1 = 0.9, beta2 = 0.999, eps = 1e-08, weight_decay = 0.0, use_adamw = True
2025-04-11T04:23:18.7998733Z
def __init__(self, lr: float, beta1: float, beta2: float, eps: float, weight_decay: float, use_adamw: bool) -> None:
super().__init__(lr, beta1, beta2, eps, weight_decay, use_adamw)
from colossalai.kernel.kernel_loader import FusedOptimizerLoader
2025-04-11T04:23:18.7999346Z
fused_optim = FusedOptimizerLoader().load()
self.fused_adam = fused_optim.multi_tensor_adam
>       self.dummy_overflow_buf = torch.tensor([0], dtype=torch.int, device=get_accelerator().get_current_device())
E       RuntimeError: CUDA error: out of memory
E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.8000555Z
tests/test_optimizer/test_adam_kernel.py:72: RuntimeError
_____________ test_fused_adam_kernel[p_dtype3-g_dtype3-0.1-False] ______________
2025-04-11T04:23:18.8000852Z
adamw = False, weight_decay = 0.1, p_dtype = torch.float32
g_dtype = torch.bfloat16
2025-04-11T04:23:18.8001077Z
@pytest.mark.parametrize("adamw", [False, True])
@pytest.mark.parametrize("weight_decay", [0.0, 0.1])
@pytest.mark.parametrize("p_dtype, g_dtype", _FUSED_ALLOWED_P_G_TYPES)
def test_fused_adam_kernel(adamw, weight_decay, p_dtype, g_dtype):
rtol, atol = 1e-5, 1e-8
if p_dtype is torch.float16 or g_dtype is torch.float16:
rtol, atol = 1e-3, 1e-3
if p_dtype is torch.bfloat16 or g_dtype is torch.bfloat16:
rtol, atol = 4e-3, 4e-3
>       check_adam_kernel(
FusedAdamKernel, adamw, weight_decay, p_dtype, g_dtype, get_accelerator().get_current_device(), 3, rtol, atol
)
2025-04-11T04:23:18.8002681Z
tests/test_optimizer/test_adam_kernel.py:159:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
tests/test_optimizer/test_adam_kernel.py:132: in check_adam_kernel
adam_kernel = kernel(lr, beta1, beta2, eps, weight_decay, adamw)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.8003338Z
self = <test_adam_kernel.FusedAdamKernel object at 0x7fb5b5a89ed0>, lr = 0.001
beta1 = 0.9, beta2 = 0.999, eps = 1e-08, weight_decay = 0.1, use_adamw = False
2025-04-11T04:23:18.8003721Z
def __init__(self, lr: float, beta1: float, beta2: float, eps: float, weight_decay: float, use_adamw: bool) -> None:
super().__init__(lr, beta1, beta2, eps, weight_decay, use_adamw)
from colossalai.kernel.kernel_loader import FusedOptimizerLoader
2025-04-11T04:23:18.8004334Z
fused_optim = FusedOptimizerLoader().load()
self.fused_adam = fused_optim.multi_tensor_adam
>       self.dummy_overflow_buf = torch.tensor([0], dtype=torch.int, device=get_accelerator().get_current_device())
E       RuntimeError: CUDA error: out of memory
E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.8005496Z
tests/test_optimizer/test_adam_kernel.py:72: RuntimeError
______________ test_fused_adam_kernel[p_dtype3-g_dtype3-0.1-True] ______________
2025-04-11T04:23:18.8005798Z
adamw = True, weight_decay = 0.1, p_dtype = torch.float32
g_dtype = torch.bfloat16
2025-04-11T04:23:18.8006072Z
@pytest.mark.parametrize("adamw", [False, True])
@pytest.mark.parametrize("weight_decay", [0.0, 0.1])
@pytest.mark.parametrize("p_dtype, g_dtype", _FUSED_ALLOWED_P_G_TYPES)
def test_fused_adam_kernel(adamw, weight_decay, p_dtype, g_dtype):
rtol, atol = 1e-5, 1e-8
if p_dtype is torch.float16 or g_dtype is torch.float16:
rtol, atol = 1e-3, 1e-3
if p_dtype is torch.bfloat16 or g_dtype is torch.bfloat16:
rtol, atol = 4e-3, 4e-3
>       check_adam_kernel(
FusedAdamKernel, adamw, weight_decay, p_dtype, g_dtype, get_accelerator().get_current_device(), 3, rtol, atol
)
2025-04-11T04:23:18.8007573Z
tests/test_optimizer/test_adam_kernel.py:159:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
tests/test_optimizer/test_adam_kernel.py:132: in check_adam_kernel
adam_kernel = kernel(lr, beta1, beta2, eps, weight_decay, adamw)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.8008259Z
self = <test_adam_kernel.FusedAdamKernel object at 0x7fb5b5b3c2b0>, lr = 0.001
beta1 = 0.9, beta2 = 0.999, eps = 1e-08, weight_decay = 0.1, use_adamw = True
2025-04-11T04:23:18.8008626Z
def __init__(self, lr: float, beta1: float, beta2: float, eps: float, weight_decay: float, use_adamw: bool) -> None:
super().__init__(lr, beta1, beta2, eps, weight_decay, use_adamw)
from colossalai.kernel.kernel_loader import FusedOptimizerLoader
2025-04-11T04:23:18.8009234Z
fused_optim = FusedOptimizerLoader().load()
self.fused_adam = fused_optim.multi_tensor_adam
>       self.dummy_overflow_buf = torch.tensor([0], dtype=torch.int, device=get_accelerator().get_current_device())
E       RuntimeError: CUDA error: out of memory
E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.8010427Z
tests/test_optimizer/test_adam_kernel.py:72: RuntimeError
_____________ test_fused_adam_kernel[p_dtype4-g_dtype4-0.0-False] ______________
2025-04-11T04:23:18.8010728Z
adamw = False, weight_decay = 0.0, p_dtype = torch.bfloat16
g_dtype = torch.bfloat16
2025-04-11T04:23:18.8010957Z
@pytest.mark.parametrize("adamw", [False, True])
@pytest.mark.parametrize("weight_decay", [0.0, 0.1])
@pytest.mark.parametrize("p_dtype, g_dtype", _FUSED_ALLOWED_P_G_TYPES)
def test_fused_adam_kernel(adamw, weight_decay, p_dtype, g_dtype):
rtol, atol = 1e-5, 1e-8
if p_dtype is torch.float16 or g_dtype is torch.float16:
rtol, atol = 1e-3, 1e-3
if p_dtype is torch.bfloat16 or g_dtype is torch.bfloat16:
rtol, atol = 4e-3, 4e-3
>       check_adam_kernel(
FusedAdamKernel, adamw, weight_decay, p_dtype, g_dtype, get_accelerator().get_current_device(), 3, rtol, atol
)
2025-04-11T04:23:18.8012507Z
tests/test_optimizer/test_adam_kernel.py:159:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
tests/test_optimizer/test_adam_kernel.py:132: in check_adam_kernel
adam_kernel = kernel(lr, beta1, beta2, eps, weight_decay, adamw)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.8013144Z
self = <test_adam_kernel.FusedAdamKernel object at 0x7fb5b5a88880>, lr = 0.001
beta1 = 0.9, beta2 = 0.999, eps = 1e-08, weight_decay = 0.0, use_adamw = False
2025-04-11T04:23:18.8013473Z
def __init__(self, lr: float, beta1: float, beta2: float, eps: float, weight_decay: float, use_adamw: bool) -> None:
super().__init__(lr, beta1, beta2, eps, weight_decay, use_adamw)
from colossalai.kernel.kernel_loader import FusedOptimizerLoader
2025-04-11T04:23:18.8014135Z
fused_optim = FusedOptimizerLoader().load()
self.fused_adam = fused_optim.multi_tensor_adam
>       self.dummy_overflow_buf = torch.tensor([0], dtype=torch.int, device=get_accelerator().get_current_device())
E       RuntimeError: CUDA error: out of memory
E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.8015343Z
tests/test_optimizer/test_adam_kernel.py:72: RuntimeError
______________ test_fused_adam_kernel[p_dtype4-g_dtype4-0.0-True] ______________
2025-04-11T04:23:18.8015706Z
adamw = True, weight_decay = 0.0, p_dtype = torch.bfloat16
g_dtype = torch.bfloat16
2025-04-11T04:23:18.8015928Z
@pytest.mark.parametrize("adamw", [False, True])
@pytest.mark.parametrize("weight_decay", [0.0, 0.1])
@pytest.mark.parametrize("p_dtype, g_dtype", _FUSED_ALLOWED_P_G_TYPES)
def test_fused_adam_kernel(adamw, weight_decay, p_dtype, g_dtype):
rtol, atol = 1e-5, 1e-8
if p_dtype is torch.float16 or g_dtype is torch.float16:
rtol, atol = 1e-3, 1e-3
if p_dtype is torch.bfloat16 or g_dtype is torch.bfloat16:
rtol, atol = 4e-3, 4e-3
>       check_adam_kernel(
FusedAdamKernel, adamw, weight_decay, p_dtype, g_dtype, get_accelerator().get_current_device(), 3, rtol, atol
)
2025-04-11T04:23:18.8017438Z
tests/test_optimizer/test_adam_kernel.py:159:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
tests/test_optimizer/test_adam_kernel.py:132: in check_adam_kernel
adam_kernel = kernel(lr, beta1, beta2, eps, weight_decay, adamw)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.8018126Z
self = <test_adam_kernel.FusedAdamKernel object at 0x7fb5b5b3fdf0>, lr = 0.001
beta1 = 0.9, beta2 = 0.999, eps = 1e-08, weight_decay = 0.0, use_adamw = True
2025-04-11T04:23:18.8018462Z
def __init__(self, lr: float, beta1: float, beta2: float, eps: float, weight_decay: float, use_adamw: bool) -> None:
super().__init__(lr, beta1, beta2, eps, weight_decay, use_adamw)
from colossalai.kernel.kernel_loader import FusedOptimizerLoader
2025-04-11T04:23:18.8019072Z
fused_optim = FusedOptimizerLoader().load()
self.fused_adam = fused_optim.multi_tensor_adam
>       self.dummy_overflow_buf = torch.tensor([0], dtype=torch.int, device=get_accelerator().get_current_device())
E       RuntimeError: CUDA error: out of memory
E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.8020279Z
tests/test_optimizer/test_adam_kernel.py:72: RuntimeError
_____________ test_fused_adam_kernel[p_dtype4-g_dtype4-0.1-False] ______________
2025-04-11T04:23:18.8020614Z
adamw = False, weight_decay = 0.1, p_dtype = torch.bfloat16
g_dtype = torch.bfloat16
2025-04-11T04:23:18.8020842Z
@pytest.mark.parametrize("adamw", [False, True])
@pytest.mark.parametrize("weight_decay", [0.0, 0.1])
@pytest.mark.parametrize("p_dtype, g_dtype", _FUSED_ALLOWED_P_G_TYPES)
def test_fused_adam_kernel(adamw, weight_decay, p_dtype, g_dtype):
rtol, atol = 1e-5, 1e-8
if p_dtype is torch.float16 or g_dtype is torch.float16:
rtol, atol = 1e-3, 1e-3
if p_dtype is torch.bfloat16 or g_dtype is torch.bfloat16:
rtol, atol = 4e-3, 4e-3
>       check_adam_kernel(
FusedAdamKernel, adamw, weight_decay, p_dtype, g_dtype, get_accelerator().get_current_device(), 3, rtol, atol
)
2025-04-11T04:23:18.8022390Z
tests/test_optimizer/test_adam_kernel.py:159:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
tests/test_optimizer/test_adam_kernel.py:132: in check_adam_kernel
adam_kernel = kernel(lr, beta1, beta2, eps, weight_decay, adamw)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.8023032Z
self = <test_adam_kernel.FusedAdamKernel object at 0x7fb5b5a885b0>, lr = 0.001
beta1 = 0.9, beta2 = 0.999, eps = 1e-08, weight_decay = 0.1, use_adamw = False
2025-04-11T04:23:18.8023360Z
def __init__(self, lr: float, beta1: float, beta2: float, eps: float, weight_decay: float, use_adamw: bool) -> None:
super().__init__(lr, beta1, beta2, eps, weight_decay, use_adamw)
from colossalai.kernel.kernel_loader import FusedOptimizerLoader
2025-04-11T04:23:18.8024019Z
fused_optim = FusedOptimizerLoader().load()
self.fused_adam = fused_optim.multi_tensor_adam
>       self.dummy_overflow_buf = torch.tensor([0], dtype=torch.int, device=get_accelerator().get_current_device())
E       RuntimeError: CUDA error: out of memory
E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.8025175Z
tests/test_optimizer/test_adam_kernel.py:72: RuntimeError
______________ test_fused_adam_kernel[p_dtype4-g_dtype4-0.1-True] ______________
2025-04-11T04:23:18.8025475Z
adamw = True, weight_decay = 0.1, p_dtype = torch.bfloat16
g_dtype = torch.bfloat16
2025-04-11T04:23:18.8025699Z
@pytest.mark.parametrize("adamw", [False, True])
@pytest.mark.parametrize("weight_decay", [0.0, 0.1])
@pytest.mark.parametrize("p_dtype, g_dtype", _FUSED_ALLOWED_P_G_TYPES)
def test_fused_adam_kernel(adamw, weight_decay, p_dtype, g_dtype):
rtol, atol = 1e-5, 1e-8
if p_dtype is torch.float16 or g_dtype is torch.float16:
rtol, atol = 1e-3, 1e-3
if p_dtype is torch.bfloat16 or g_dtype is torch.bfloat16:
rtol, atol = 4e-3, 4e-3
>       check_adam_kernel(
FusedAdamKernel, adamw, weight_decay, p_dtype, g_dtype, get_accelerator().get_current_device(), 3, rtol, atol
)
2025-04-11T04:23:18.8027308Z
tests/test_optimizer/test_adam_kernel.py:159:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
tests/test_optimizer/test_adam_kernel.py:132: in check_adam_kernel
adam_kernel = kernel(lr, beta1, beta2, eps, weight_decay, adamw)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.8027991Z
self = <test_adam_kernel.FusedAdamKernel object at 0x7fb5b5b3e350>, lr = 0.001
beta1 = 0.9, beta2 = 0.999, eps = 1e-08, weight_decay = 0.1, use_adamw = True
2025-04-11T04:23:18.8028323Z
def __init__(self, lr: float, beta1: float, beta2: float, eps: float, weight_decay: float, use_adamw: bool) -> None:
super().__init__(lr, beta1, beta2, eps, weight_decay, use_adamw)
from colossalai.kernel.kernel_loader import FusedOptimizerLoader
2025-04-11T04:23:18.8028974Z
fused_optim = FusedOptimizerLoader().load()
self.fused_adam = fused_optim.multi_tensor_adam
>       self.dummy_overflow_buf = torch.tensor([0], dtype=torch.int, device=get_accelerator().get_current_device())
E       RuntimeError: CUDA error: out of memory
E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.8030192Z
tests/test_optimizer/test_adam_kernel.py:72: RuntimeError
______ test_adam_optim_on_bert[p_dtype0-g_dtype0-False-FusedAdam-device0] ______
2025-04-11T04:23:18.8030520Z
optim_cls = <class 'colossalai.nn.optimizer.fused_adam.FusedAdam'>
device = device(type='cuda', index=0), adamw = False, p_dtype = torch.float32
g_dtype = torch.float32
2025-04-11T04:23:18.8030940Z
@pytest.mark.parametrize("optim_cls, device", _ALLOWED_OPTIM_DEVICES)
@pytest.mark.parametrize("adamw", [False, True])
@pytest.mark.parametrize("p_dtype, g_dtype", _ALLOWED_P_G_TYPES)
def test_adam_optim_on_bert(
optim_cls: Union[Type[FusedAdam], Type[CPUAdam], Type[HybridAdam]],
device: torch.device,
adamw: bool,
p_dtype: torch.dtype,
g_dtype: torch.dtype,
) -> None:
model_fn, *_ = next(iter(model_zoo.get_sub_registry("transformers_bert_for_sequence_classification").values()))
>       torch_model = model_fn().to(device)
2025-04-11T04:23:18.8032578Z
tests/test_optimizer/test_adam_optim.py:48:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
/opt/conda/envs/pytorch/lib/python3.10/site-packages/transformers/modeling_utils.py:2576: in to
return super().to(*args, **kwargs)
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:1152: in to
return self._apply(convert)
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:802: in _apply
module._apply(fn)
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:802: in _apply
module._apply(fn)
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:802: in _apply
module._apply(fn)
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:825: in _apply
param_applied = fn(param)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.8035012Z
t = Parameter containing:
tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
[ 0.0028, -0.0014,...0,  0.0213, -0.0091],
[-0.0226, -0.0230, -0.0057,  ..., -0.0094, -0.0239, -0.0399]],
requires_grad=True)
2025-04-11T04:23:18.8035558Z
def convert(t):
if convert_to_format is not None and t.dim() in (4, 5):
return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None,
non_blocking, memory_format=convert_to_format)
>       return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
E       RuntimeError: CUDA error: out of memory
E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.8037032Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:1150: RuntimeError
_______ test_adam_optim_on_bert[p_dtype0-g_dtype0-False-CPUAdam-device2] _______
2025-04-11T04:23:18.8037469Z
optim_cls = <class 'colossalai.nn.optimizer.cpu_adam.CPUAdam'>
device = device(type='cuda', index=0), adamw = False, p_dtype = torch.float32
g_dtype = torch.float32
2025-04-11T04:23:18.8037877Z
@pytest.mark.parametrize("optim_cls, device", _ALLOWED_OPTIM_DEVICES)
@pytest.mark.parametrize("adamw", [False, True])
@pytest.mark.parametrize("p_dtype, g_dtype", _ALLOWED_P_G_TYPES)
def test_adam_optim_on_bert(
optim_cls: Union[Type[FusedAdam], Type[CPUAdam], Type[HybridAdam]],
device: torch.device,
adamw: bool,
p_dtype: torch.dtype,
g_dtype: torch.dtype,
) -> None:
model_fn, *_ = next(iter(model_zoo.get_sub_registry("transformers_bert_for_sequence_classification").values()))
>       torch_model = model_fn().to(device)
2025-04-11T04:23:18.8039454Z
tests/test_optimizer/test_adam_optim.py:48:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
/opt/conda/envs/pytorch/lib/python3.10/site-packages/transformers/modeling_utils.py:2576: in to
return super().to(*args, **kwargs)
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:1152: in to
return self._apply(convert)
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:802: in _apply
module._apply(fn)
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:802: in _apply
module._apply(fn)
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:802: in _apply
module._apply(fn)
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:825: in _apply
param_applied = fn(param)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.8041752Z
t = Parameter containing:
tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
[-0.0048,  0.0237,...2, -0.0204,  0.0268],
[ 0.0211,  0.0139,  0.0082,  ...,  0.0303, -0.0201, -0.0544]],
requires_grad=True)
2025-04-11T04:23:18.8042284Z
def convert(t):
if convert_to_format is not None and t.dim() in (4, 5):
return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None,
non_blocking, memory_format=convert_to_format)
>       return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
E       RuntimeError: CUDA error: out of memory
E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.8043753Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:1150: RuntimeError
_____ test_adam_optim_on_bert[p_dtype0-g_dtype0-False-HybridAdam-device3] ______
2025-04-11T04:23:18.8044194Z
optim_cls = <class 'colossalai.nn.optimizer.hybrid_adam.HybridAdam'>
device = device(type='cpu'), adamw = False, p_dtype = torch.float32
g_dtype = torch.float32
2025-04-11T04:23:18.8044604Z
@pytest.mark.parametrize("optim_cls, device", _ALLOWED_OPTIM_DEVICES)
@pytest.mark.parametrize("adamw", [False, True])
@pytest.mark.parametrize("p_dtype, g_dtype", _ALLOWED_P_G_TYPES)
def test_adam_optim_on_bert(
optim_cls: Union[Type[FusedAdam], Type[CPUAdam], Type[HybridAdam]],
device: torch.device,
adamw: bool,
p_dtype: torch.dtype,
g_dtype: torch.dtype,
) -> None:
model_fn, *_ = next(iter(model_zoo.get_sub_registry("transformers_bert_for_sequence_classification").values()))
torch_model = model_fn().to(device)
model = deepcopy(torch_model).to(p_dtype)
lr = 1e-3
beta1, beta2 = 0.9, 0.999
eps = 1e-8
torch_optim_cls = AdamW if adamw else Adam
torch_optim = torch_optim_cls(setup_param_groups(torch_model), lr=lr, betas=(beta1, beta2), eps=eps)
>       optim = optim_cls(setup_param_groups(model), lr=lr, betas=(beta1, beta2), eps=eps, adamw_mode=adamw)
2025-04-11T04:23:18.8047145Z
tests/test_optimizer/test_adam_optim.py:55:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.8047379Z
self = HybridAdam (
Parameter Group 0
betas: (0.9, 0.999)
bias_correction: True
eps: 1e-08
lr: 0.001
weig...arameter Group 1
betas: (0.9, 0.999)
bias_correction: True
eps: 1e-08
lr: 0.001
weight_decay: 0.0
)
model_params = [{'betas': (0.9, 0.999), 'bias_correction': True, 'eps': 1e-08, 'lr': 0.001, ...}, {'betas': (0.9, 0.999), 'bias_correction': True, 'eps': 1e-08, 'lr': 0.001, ...}]
lr = 0.001, bias_correction = True, betas = (0.9, 0.999), eps = 1e-08
weight_decay = 0, adamw_mode = False, nvme_offload_fraction = 0.0
nvme_offload_dir = None, defaults = {}
fused_optim = <module 'colossalai._C.fused_optim_cuda' from '/__w/ColossalAI/ColossalAI/colossalai/_C/fused_optim_cuda.cpython-310-x86_64-linux-gnu.so'>
2025-04-11T04:23:18.8049529Z
def __init__(
self,
model_params,
lr=1e-3,
bias_correction=True,
betas=(0.9, 0.999),
eps=1e-8,
weight_decay=0,
adamw_mode=True,
nvme_offload_fraction: float = 0.0,
nvme_offload_dir: Optional[str] = None,
**defaults: Any,
):
super().__init__(
model_params,
lr,
bias_correction,
betas,
eps,
weight_decay,
adamw_mode,
nvme_offload_fraction,
nvme_offload_dir,
)
if torch.cuda.is_available():
fused_optim = FusedOptimizerLoader().load()
self.gpu_adam_op = fused_optim.multi_tensor_adam
>           self._dummy_overflow_buf = torch.tensor([0], dtype=torch.int, device=get_current_device())
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.8052872Z
colossalai/nn/optimizer/hybrid_adam.py:90: RuntimeError
_____ test_adam_optim_on_bert[p_dtype0-g_dtype0-False-HybridAdam-device4] ______
2025-04-11T04:23:18.8053200Z
optim_cls = <class 'colossalai.nn.optimizer.hybrid_adam.HybridAdam'>
device = device(type='cuda', index=0), adamw = False, p_dtype = torch.float32
g_dtype = torch.float32
2025-04-11T04:23:18.8053679Z
@pytest.mark.parametrize("optim_cls, device", _ALLOWED_OPTIM_DEVICES)
@pytest.mark.parametrize("adamw", [False, True])
@pytest.mark.parametrize("p_dtype, g_dtype", _ALLOWED_P_G_TYPES)
def test_adam_optim_on_bert(
optim_cls: Union[Type[FusedAdam], Type[CPUAdam], Type[HybridAdam]],
device: torch.device,
adamw: bool,
p_dtype: torch.dtype,
g_dtype: torch.dtype,
) -> None:
model_fn, *_ = next(iter(model_zoo.get_sub_registry("transformers_bert_for_sequence_classification").values()))
>       torch_model = model_fn().to(device)
2025-04-11T04:23:18.8055137Z
tests/test_optimizer/test_adam_optim.py:48:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
/opt/conda/envs/pytorch/lib/python3.10/site-packages/transformers/modeling_utils.py:2576: in to
return super().to(*args, **kwargs)
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:1152: in to
return self._apply(convert)
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:802: in _apply
module._apply(fn)
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:802: in _apply
module._apply(fn)
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:802: in _apply
module._apply(fn)
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:825: in _apply
param_applied = fn(param)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.8057426Z
t = Parameter containing:
tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
[ 0.0210, -0.0131,...8, -0.0156, -0.0054],
[ 0.0148,  0.0292,  0.0008,  ...,  0.0355, -0.0048, -0.0186]],
requires_grad=True)
2025-04-11T04:23:18.8058006Z
def convert(t):
if convert_to_format is not None and t.dim() in (4, 5):
return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None,
non_blocking, memory_format=convert_to_format)
>       return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
E       RuntimeError: CUDA error: out of memory
E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.8059479Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:1150: RuntimeError
______ test_adam_optim_on_bert[p_dtype0-g_dtype0-True-FusedAdam-device0] _______
2025-04-11T04:23:18.8059965Z
optim_cls = <class 'colossalai.nn.optimizer.fused_adam.FusedAdam'>
device = device(type='cuda', index=0), adamw = True, p_dtype = torch.float32
g_dtype = torch.float32
2025-04-11T04:23:18.8060393Z
@pytest.mark.parametrize("optim_cls, device", _ALLOWED_OPTIM_DEVICES)
@pytest.mark.parametrize("adamw", [False, True])
@pytest.mark.parametrize("p_dtype, g_dtype", _ALLOWED_P_G_TYPES)
def test_adam_optim_on_bert(
optim_cls: Union[Type[FusedAdam], Type[CPUAdam], Type[HybridAdam]],
device: torch.device,
adamw: bool,
p_dtype: torch.dtype,
g_dtype: torch.dtype,
) -> None:
model_fn, *_ = next(iter(model_zoo.get_sub_registry("transformers_bert_for_sequence_classification").values()))
>       torch_model = model_fn().to(device)
2025-04-11T04:23:18.8061841Z
tests/test_optimizer/test_adam_optim.py:48:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
/opt/conda/envs/pytorch/lib/python3.10/site-packages/transformers/modeling_utils.py:2576: in to
return super().to(*args, **kwargs)
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:1152: in to
return self._apply(convert)
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:802: in _apply
module._apply(fn)
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:802: in _apply
module._apply(fn)
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:802: in _apply
module._apply(fn)
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:825: in _apply
param_applied = fn(param)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.8064108Z
t = Parameter containing:
tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
[ 0.0168, -0.0116,...1,  0.0247, -0.0168],
[ 0.0078, -0.0201,  0.0158,  ..., -0.0204,  0.0234,  0.0068]],
requires_grad=True)
2025-04-11T04:23:18.8064700Z
def convert(t):
if convert_to_format is not None and t.dim() in (4, 5):
return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None,
non_blocking, memory_format=convert_to_format)
>       return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
E       RuntimeError: CUDA error: out of memory
E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.8066201Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:1150: RuntimeError
_______ test_adam_optim_on_bert[p_dtype0-g_dtype0-True-CPUAdam-device2] ________
2025-04-11T04:23:18.8066638Z
optim_cls = <class 'colossalai.nn.optimizer.cpu_adam.CPUAdam'>
device = device(type='cuda', index=0), adamw = True, p_dtype = torch.float32
g_dtype = torch.float32
2025-04-11T04:23:18.8067046Z
@pytest.mark.parametrize("optim_cls, device", _ALLOWED_OPTIM_DEVICES)
@pytest.mark.parametrize("adamw", [False, True])
@pytest.mark.parametrize("p_dtype, g_dtype", _ALLOWED_P_G_TYPES)
def test_adam_optim_on_bert(
optim_cls: Union[Type[FusedAdam], Type[CPUAdam], Type[HybridAdam]],
device: torch.device,
adamw: bool,
p_dtype: torch.dtype,
g_dtype: torch.dtype,
) -> None:
model_fn, *_ = next(iter(model_zoo.get_sub_registry("transformers_bert_for_sequence_classification").values()))
>       torch_model = model_fn().to(device)
2025-04-11T04:23:18.8068540Z
tests/test_optimizer/test_adam_optim.py:48:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
/opt/conda/envs/pytorch/lib/python3.10/site-packages/transformers/modeling_utils.py:2576: in to
return super().to(*args, **kwargs)
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:1152: in to
return self._apply(convert)
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:802: in _apply
module._apply(fn)
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:802: in _apply
module._apply(fn)
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:802: in _apply
module._apply(fn)
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:825: in _apply
param_applied = fn(param)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.8070872Z
t = Parameter containing:
tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
[ 0.0171, -0.0037,...8, -0.0068,  0.0037],
[ 0.0260, -0.0271, -0.0247,  ...,  0.0262,  0.0078,  0.0236]],
requires_grad=True)
2025-04-11T04:23:18.8071435Z
def convert(t):
if convert_to_format is not None and t.dim() in (4, 5):
return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None,
non_blocking, memory_format=convert_to_format)
>       return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
E       RuntimeError: CUDA error: out of memory
E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.8072891Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:1150: RuntimeError
______ test_adam_optim_on_bert[p_dtype0-g_dtype0-True-HybridAdam-device3] ______
2025-04-11T04:23:18.8073333Z
optim_cls = <class 'colossalai.nn.optimizer.hybrid_adam.HybridAdam'>
device = device(type='cpu'), adamw = True, p_dtype = torch.float32
g_dtype = torch.float32
2025-04-11T04:23:18.8073743Z
@pytest.mark.parametrize("optim_cls, device", _ALLOWED_OPTIM_DEVICES)
@pytest.mark.parametrize("adamw", [False, True])
@pytest.mark.parametrize("p_dtype, g_dtype", _ALLOWED_P_G_TYPES)
def test_adam_optim_on_bert(
optim_cls: Union[Type[FusedAdam], Type[CPUAdam], Type[HybridAdam]],
device: torch.device,
adamw: bool,
p_dtype: torch.dtype,
g_dtype: torch.dtype,
) -> None:
model_fn, *_ = next(iter(model_zoo.get_sub_registry("transformers_bert_for_sequence_classification").values()))
torch_model = model_fn().to(device)
model = deepcopy(torch_model).to(p_dtype)
lr = 1e-3
beta1, beta2 = 0.9, 0.999
eps = 1e-8
torch_optim_cls = AdamW if adamw else Adam
torch_optim = torch_optim_cls(setup_param_groups(torch_model), lr=lr, betas=(beta1, beta2), eps=eps)
>       optim = optim_cls(setup_param_groups(model), lr=lr, betas=(beta1, beta2), eps=eps, adamw_mode=adamw)
2025-04-11T04:23:18.8076147Z
tests/test_optimizer/test_adam_optim.py:55:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.8076371Z
self = HybridAdam (
Parameter Group 0
betas: (0.9, 0.999)
bias_correction: True
eps: 1e-08
lr: 0.001
weig...arameter Group 1
betas: (0.9, 0.999)
bias_correction: True
eps: 1e-08
lr: 0.001
weight_decay: 0.0
)
model_params = [{'betas': (0.9, 0.999), 'bias_correction': True, 'eps': 1e-08, 'lr': 0.001, ...}, {'betas': (0.9, 0.999), 'bias_correction': True, 'eps': 1e-08, 'lr': 0.001, ...}]
lr = 0.001, bias_correction = True, betas = (0.9, 0.999), eps = 1e-08
weight_decay = 0, adamw_mode = True, nvme_offload_fraction = 0.0
nvme_offload_dir = None, defaults = {}
fused_optim = <module 'colossalai._C.fused_optim_cuda' from '/__w/ColossalAI/ColossalAI/colossalai/_C/fused_optim_cuda.cpython-310-x86_64-linux-gnu.so'>
2025-04-11T04:23:18.8078602Z
def __init__(
self,
model_params,
lr=1e-3,
bias_correction=True,
betas=(0.9, 0.999),
eps=1e-8,
weight_decay=0,
adamw_mode=True,
nvme_offload_fraction: float = 0.0,
nvme_offload_dir: Optional[str] = None,
**defaults: Any,
):
super().__init__(
model_params,
lr,
bias_correction,
betas,
eps,
weight_decay,
adamw_mode,
nvme_offload_fraction,
nvme_offload_dir,
)
if torch.cuda.is_available():
fused_optim = FusedOptimizerLoader().load()
self.gpu_adam_op = fused_optim.multi_tensor_adam
>           self._dummy_overflow_buf = torch.tensor([0], dtype=torch.int, device=get_current_device())
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.8081908Z
colossalai/nn/optimizer/hybrid_adam.py:90: RuntimeError
______ test_adam_optim_on_bert[p_dtype0-g_dtype0-True-HybridAdam-device4] ______
2025-04-11T04:23:18.8082233Z
optim_cls = <class 'colossalai.nn.optimizer.hybrid_adam.HybridAdam'>
device = device(type='cuda', index=0), adamw = True, p_dtype = torch.float32
g_dtype = torch.float32
2025-04-11T04:23:18.8082655Z
@pytest.mark.parametrize("optim_cls, device", _ALLOWED_OPTIM_DEVICES)
@pytest.mark.parametrize("adamw", [False, True])
@pytest.mark.parametrize("p_dtype, g_dtype", _ALLOWED_P_G_TYPES)
def test_adam_optim_on_bert(
optim_cls: Union[Type[FusedAdam], Type[CPUAdam], Type[HybridAdam]],
device: torch.device,
adamw: bool,
p_dtype: torch.dtype,
g_dtype: torch.dtype,
) -> None:
model_fn, *_ = next(iter(model_zoo.get_sub_registry("transformers_bert_for_sequence_classification").values()))
>       torch_model = model_fn().to(device)
2025-04-11T04:23:18.8084225Z
tests/test_optimizer/test_adam_optim.py:48:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
/opt/conda/envs/pytorch/lib/python3.10/site-packages/transformers/modeling_utils.py:2576: in to
return super().to(*args, **kwargs)
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:1152: in to
return self._apply(convert)
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:802: in _apply
module._apply(fn)
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:802: in _apply
module._apply(fn)
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:802: in _apply
module._apply(fn)
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:825: in _apply
param_applied = fn(param)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.8086549Z
t = Parameter containing:
tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
[-0.0301, -0.0063,...5, -0.0105,  0.0078],
[-0.0225,  0.0108,  0.0321,  ..., -0.0056, -0.0089, -0.0360]],
requires_grad=True)
2025-04-11T04:23:18.8087077Z
def convert(t):
if convert_to_format is not None and t.dim() in (4, 5):
return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None,
non_blocking, memory_format=convert_to_format)
>       return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
E       RuntimeError: CUDA error: out of memory
E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.8088589Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:1150: RuntimeError
______ test_adam_optim_on_bert[p_dtype1-g_dtype1-False-FusedAdam-device0] ______
2025-04-11T04:23:18.8089025Z
optim_cls = <class 'colossalai.nn.optimizer.fused_adam.FusedAdam'>
device = device(type='cuda', index=0), adamw = False, p_dtype = torch.float32
g_dtype = torch.float16
2025-04-11T04:23:18.8089450Z
@pytest.mark.parametrize("optim_cls, device", _ALLOWED_OPTIM_DEVICES)
@pytest.mark.parametrize("adamw", [False, True])
@pytest.mark.parametrize("p_dtype, g_dtype", _ALLOWED_P_G_TYPES)
def test_adam_optim_on_bert(
optim_cls: Union[Type[FusedAdam], Type[CPUAdam], Type[HybridAdam]],
device: torch.device,
adamw: bool,
p_dtype: torch.dtype,
g_dtype: torch.dtype,
) -> None:
model_fn, *_ = next(iter(model_zoo.get_sub_registry("transformers_bert_for_sequence_classification").values()))
>       torch_model = model_fn().to(device)
2025-04-11T04:23:18.8091012Z
tests/test_optimizer/test_adam_optim.py:48:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
/opt/conda/envs/pytorch/lib/python3.10/site-packages/transformers/modeling_utils.py:2576: in to
return super().to(*args, **kwargs)
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:1152: in to
return self._apply(convert)
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:802: in _apply
module._apply(fn)
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:802: in _apply
module._apply(fn)
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:802: in _apply
module._apply(fn)
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:825: in _apply
param_applied = fn(param)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.8093339Z
t = Parameter containing:
tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
[-0.0063,  0.0127,...8,  0.0139, -0.0372],
[-0.0001,  0.0211,  0.0425,  ..., -0.0074,  0.0182,  0.0033]],
requires_grad=True)
2025-04-11T04:23:18.8093864Z
def convert(t):
if convert_to_format is not None and t.dim() in (4, 5):
return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None,
non_blocking, memory_format=convert_to_format)
>       return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
E       RuntimeError: CUDA error: out of memory
E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.8095320Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:1150: RuntimeError
_______ test_adam_optim_on_bert[p_dtype1-g_dtype1-False-CPUAdam-device2] _______
2025-04-11T04:23:18.8095759Z
optim_cls = <class 'colossalai.nn.optimizer.cpu_adam.CPUAdam'>
device = device(type='cuda', index=0), adamw = False, p_dtype = torch.float32
g_dtype = torch.float16
2025-04-11T04:23:18.8096168Z
@pytest.mark.parametrize("optim_cls, device", _ALLOWED_OPTIM_DEVICES)
@pytest.mark.parametrize("adamw", [False, True])
@pytest.mark.parametrize("p_dtype, g_dtype", _ALLOWED_P_G_TYPES)
def test_adam_optim_on_bert(
optim_cls: Union[Type[FusedAdam], Type[CPUAdam], Type[HybridAdam]],
device: torch.device,
adamw: bool,
p_dtype: torch.dtype,
g_dtype: torch.dtype,
) -> None:
model_fn, *_ = next(iter(model_zoo.get_sub_registry("transformers_bert_for_sequence_classification").values()))
>       torch_model = model_fn().to(device)
2025-04-11T04:23:18.8097718Z
tests/test_optimizer/test_adam_optim.py:48:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
/opt/conda/envs/pytorch/lib/python3.10/site-packages/transformers/modeling_utils.py:2576: in to
return super().to(*args, **kwargs)
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:1152: in to
return self._apply(convert)
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:802: in _apply
module._apply(fn)
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:802: in _apply
module._apply(fn)
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:802: in _apply
module._apply(fn)
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:825: in _apply
param_applied = fn(param)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.8100005Z
t = Parameter containing:
tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
[ 0.0058,  0.0119,...4, -0.0198,  0.0151],
[-0.0479,  0.0136, -0.0425,  ..., -0.0021, -0.0081,  0.0171]],
requires_grad=True)
2025-04-11T04:23:18.8100532Z
def convert(t):
if convert_to_format is not None and t.dim() in (4, 5):
return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None,
non_blocking, memory_format=convert_to_format)
>       return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
E       RuntimeError: CUDA error: out of memory
E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.8101985Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:1150: RuntimeError
_____ test_adam_optim_on_bert[p_dtype1-g_dtype1-False-HybridAdam-device3] ______
2025-04-11T04:23:18.8102425Z
optim_cls = <class 'colossalai.nn.optimizer.hybrid_adam.HybridAdam'>
device = device(type='cpu'), adamw = False, p_dtype = torch.float32
g_dtype = torch.float16
2025-04-11T04:23:18.8102885Z
@pytest.mark.parametrize("optim_cls, device", _ALLOWED_OPTIM_DEVICES)
@pytest.mark.parametrize("adamw", [False, True])
@pytest.mark.parametrize("p_dtype, g_dtype", _ALLOWED_P_G_TYPES)
def test_adam_optim_on_bert(
optim_cls: Union[Type[FusedAdam], Type[CPUAdam], Type[HybridAdam]],
device: torch.device,
adamw: bool,
p_dtype: torch.dtype,
g_dtype: torch.dtype,
) -> None:
model_fn, *_ = next(iter(model_zoo.get_sub_registry("transformers_bert_for_sequence_classification").values()))
torch_model = model_fn().to(device)
model = deepcopy(torch_model).to(p_dtype)
lr = 1e-3
beta1, beta2 = 0.9, 0.999
eps = 1e-8
torch_optim_cls = AdamW if adamw else Adam
torch_optim = torch_optim_cls(setup_param_groups(torch_model), lr=lr, betas=(beta1, beta2), eps=eps)
>       optim = optim_cls(setup_param_groups(model), lr=lr, betas=(beta1, beta2), eps=eps, adamw_mode=adamw)
2025-04-11T04:23:18.8105346Z
tests/test_optimizer/test_adam_optim.py:55:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.8105574Z
self = HybridAdam (
Parameter Group 0
betas: (0.9, 0.999)
bias_correction: True
eps: 1e-08
lr: 0.001
weig...arameter Group 1
betas: (0.9, 0.999)
bias_correction: True
eps: 1e-08
lr: 0.001
weight_decay: 0.0
)
model_params = [{'betas': (0.9, 0.999), 'bias_correction': True, 'eps': 1e-08, 'lr': 0.001, ...}, {'betas': (0.9, 0.999), 'bias_correction': True, 'eps': 1e-08, 'lr': 0.001, ...}]
lr = 0.001, bias_correction = True, betas = (0.9, 0.999), eps = 1e-08
weight_decay = 0, adamw_mode = False, nvme_offload_fraction = 0.0
nvme_offload_dir = None, defaults = {}
fused_optim = <module 'colossalai._C.fused_optim_cuda' from '/__w/ColossalAI/ColossalAI/colossalai/_C/fused_optim_cuda.cpython-310-x86_64-linux-gnu.so'>
2025-04-11T04:23:18.8107766Z
def __init__(
self,
model_params,
lr=1e-3,
bias_correction=True,
betas=(0.9, 0.999),
eps=1e-8,
weight_decay=0,
adamw_mode=True,
nvme_offload_fraction: float = 0.0,
nvme_offload_dir: Optional[str] = None,
**defaults: Any,
):
super().__init__(
model_params,
lr,
bias_correction,
betas,
eps,
weight_decay,
adamw_mode,
nvme_offload_fraction,
nvme_offload_dir,
)
if torch.cuda.is_available():
fused_optim = FusedOptimizerLoader().load()
self.gpu_adam_op = fused_optim.multi_tensor_adam
>           self._dummy_overflow_buf = torch.tensor([0], dtype=torch.int, device=get_current_device())
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.8111174Z
colossalai/nn/optimizer/hybrid_adam.py:90: RuntimeError
_____ test_adam_optim_on_bert[p_dtype1-g_dtype1-False-HybridAdam-device4] ______
2025-04-11T04:23:18.8111496Z
optim_cls = <class 'colossalai.nn.optimizer.hybrid_adam.HybridAdam'>
device = device(type='cuda', index=0), adamw = False, p_dtype = torch.float32
g_dtype = torch.float16
2025-04-11T04:23:18.8111918Z
@pytest.mark.parametrize("optim_cls, device", _ALLOWED_OPTIM_DEVICES)
@pytest.mark.parametrize("adamw", [False, True])
@pytest.mark.parametrize("p_dtype, g_dtype", _ALLOWED_P_G_TYPES)
def test_adam_optim_on_bert(
optim_cls: Union[Type[FusedAdam], Type[CPUAdam], Type[HybridAdam]],
device: torch.device,
adamw: bool,
p_dtype: torch.dtype,
g_dtype: torch.dtype,
) -> None:
model_fn, *_ = next(iter(model_zoo.get_sub_registry("transformers_bert_for_sequence_classification").values()))
>       torch_model = model_fn().to(device)
2025-04-11T04:23:18.8113371Z
tests/test_optimizer/test_adam_optim.py:48:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
/opt/conda/envs/pytorch/lib/python3.10/site-packages/transformers/modeling_utils.py:2576: in to
return super().to(*args, **kwargs)
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:1152: in to
return self._apply(convert)
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:802: in _apply
module._apply(fn)
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:802: in _apply
module._apply(fn)
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:802: in _apply
module._apply(fn)
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:825: in _apply
param_applied = fn(param)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.8115732Z
t = Parameter containing:
tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
[ 0.0127, -0.0053,...6, -0.0203,  0.0294],
[ 0.0315,  0.0270, -0.0379,  ...,  0.0044, -0.0077,  0.0209]],
requires_grad=True)
2025-04-11T04:23:18.8116264Z
def convert(t):
if convert_to_format is not None and t.dim() in (4, 5):
return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None,
non_blocking, memory_format=convert_to_format)
>       return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
E       RuntimeError: CUDA error: out of memory
E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.8117756Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:1150: RuntimeError
______ test_adam_optim_on_bert[p_dtype1-g_dtype1-True-FusedAdam-device0] _______
2025-04-11T04:23:18.8118187Z
optim_cls = <class 'colossalai.nn.optimizer.fused_adam.FusedAdam'>
device = device(type='cuda', index=0), adamw = True, p_dtype = torch.float32
g_dtype = torch.float16
2025-04-11T04:23:18.8118607Z
@pytest.mark.parametrize("optim_cls, device", _ALLOWED_OPTIM_DEVICES)
@pytest.mark.parametrize("adamw", [False, True])
@pytest.mark.parametrize("p_dtype, g_dtype", _ALLOWED_P_G_TYPES)
def test_adam_optim_on_bert(
optim_cls: Union[Type[FusedAdam], Type[CPUAdam], Type[HybridAdam]],
device: torch.device,
adamw: bool,
p_dtype: torch.dtype,
g_dtype: torch.dtype,
) -> None:
model_fn, *_ = next(iter(model_zoo.get_sub_registry("transformers_bert_for_sequence_classification").values()))
>       torch_model = model_fn().to(device)
2025-04-11T04:23:18.8120102Z
tests/test_optimizer/test_adam_optim.py:48:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
/opt/conda/envs/pytorch/lib/python3.10/site-packages/transformers/modeling_utils.py:2576: in to
return super().to(*args, **kwargs)
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:1152: in to
return self._apply(convert)
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:802: in _apply
module._apply(fn)
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:802: in _apply
module._apply(fn)
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:802: in _apply
module._apply(fn)
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:825: in _apply
param_applied = fn(param)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.8122454Z
t = Parameter containing:
tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
[ 0.0126,  0.0307,...5,  0.0153,  0.0116],
[-0.0007,  0.0044, -0.0020,  ..., -0.0033,  0.0164, -0.0073]],
requires_grad=True)
2025-04-11T04:23:18.8123018Z
def convert(t):
if convert_to_format is not None and t.dim() in (4, 5):
return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None,
non_blocking, memory_format=convert_to_format)
>       return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
E       RuntimeError: CUDA error: out of memory
E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.8124465Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:1150: RuntimeError
_______ test_adam_optim_on_bert[p_dtype1-g_dtype1-True-CPUAdam-device2] ________
2025-04-11T04:23:18.8124904Z
optim_cls = <class 'colossalai.nn.optimizer.cpu_adam.CPUAdam'>
device = device(type='cuda', index=0), adamw = True, p_dtype = torch.float32
g_dtype = torch.float16
2025-04-11T04:23:18.8125317Z
@pytest.mark.parametrize("optim_cls, device", _ALLOWED_OPTIM_DEVICES)
@pytest.mark.parametrize("adamw", [False, True])
@pytest.mark.parametrize("p_dtype, g_dtype", _ALLOWED_P_G_TYPES)
def test_adam_optim_on_bert(
optim_cls: Union[Type[FusedAdam], Type[CPUAdam], Type[HybridAdam]],
device: torch.device,
adamw: bool,
p_dtype: torch.dtype,
g_dtype: torch.dtype,
) -> None:
model_fn, *_ = next(iter(model_zoo.get_sub_registry("transformers_bert_for_sequence_classification").values()))
>       torch_model = model_fn().to(device)
2025-04-11T04:23:18.8126846Z
tests/test_optimizer/test_adam_optim.py:48:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
/opt/conda/envs/pytorch/lib/python3.10/site-packages/transformers/modeling_utils.py:2576: in to
return super().to(*args, **kwargs)
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:1152: in to
return self._apply(convert)
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:802: in _apply
module._apply(fn)
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:802: in _apply
module._apply(fn)
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:802: in _apply
module._apply(fn)
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:825: in _apply
param_applied = fn(param)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.8129157Z
t = Parameter containing:
tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
[ 0.0062,  0.0098,...3, -0.0036,  0.0170],
[ 0.0053,  0.0281, -0.0163,  ..., -0.0098, -0.0364,  0.0040]],
requires_grad=True)
2025-04-11T04:23:18.8129737Z
def convert(t):
if convert_to_format is not None and t.dim() in (4, 5):
return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None,
non_blocking, memory_format=convert_to_format)
>       return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
E       RuntimeError: CUDA error: out of memory
E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.8131195Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:1150: RuntimeError
______ test_adam_optim_on_bert[p_dtype1-g_dtype1-True-HybridAdam-device3] ______
2025-04-11T04:23:18.8131634Z
optim_cls = <class 'colossalai.nn.optimizer.hybrid_adam.HybridAdam'>
device = device(type='cpu'), adamw = True, p_dtype = torch.float32
g_dtype = torch.float16
2025-04-11T04:23:18.8132045Z
@pytest.mark.parametrize("optim_cls, device", _ALLOWED_OPTIM_DEVICES)
@pytest.mark.parametrize("adamw", [False, True])
@pytest.mark.parametrize("p_dtype, g_dtype", _ALLOWED_P_G_TYPES)
def test_adam_optim_on_bert(
optim_cls: Union[Type[FusedAdam], Type[CPUAdam], Type[HybridAdam]],
device: torch.device,
adamw: bool,
p_dtype: torch.dtype,
g_dtype: torch.dtype,
) -> None:
model_fn, *_ = next(iter(model_zoo.get_sub_registry("transformers_bert_for_sequence_classification").values()))
torch_model = model_fn().to(device)
model = deepcopy(torch_model).to(p_dtype)
lr = 1e-3
beta1, beta2 = 0.9, 0.999
eps = 1e-8
torch_optim_cls = AdamW if adamw else Adam
torch_optim = torch_optim_cls(setup_param_groups(torch_model), lr=lr, betas=(beta1, beta2), eps=eps)
>       optim = optim_cls(setup_param_groups(model), lr=lr, betas=(beta1, beta2), eps=eps, adamw_mode=adamw)
2025-04-11T04:23:18.8134555Z
tests/test_optimizer/test_adam_optim.py:55:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.8134832Z
self = HybridAdam (
Parameter Group 0
betas: (0.9, 0.999)
bias_correction: True
eps: 1e-08
lr: 0.001
weig...arameter Group 1
betas: (0.9, 0.999)
bias_correction: True
eps: 1e-08
lr: 0.001
weight_decay: 0.0
)
model_params = [{'betas': (0.9, 0.999), 'bias_correction': True, 'eps': 1e-08, 'lr': 0.001, ...}, {'betas': (0.9, 0.999), 'bias_correction': True, 'eps': 1e-08, 'lr': 0.001, ...}]
lr = 0.001, bias_correction = True, betas = (0.9, 0.999), eps = 1e-08
weight_decay = 0, adamw_mode = True, nvme_offload_fraction = 0.0
nvme_offload_dir = None, defaults = {}
fused_optim = <module 'colossalai._C.fused_optim_cuda' from '/__w/ColossalAI/ColossalAI/colossalai/_C/fused_optim_cuda.cpython-310-x86_64-linux-gnu.so'>
2025-04-11T04:23:18.8137070Z
def __init__(
self,
model_params,
lr=1e-3,
bias_correction=True,
betas=(0.9, 0.999),
eps=1e-8,
weight_decay=0,
adamw_mode=True,
nvme_offload_fraction: float = 0.0,
nvme_offload_dir: Optional[str] = None,
**defaults: Any,
):
super().__init__(
model_params,
lr,
bias_correction,
betas,
eps,
weight_decay,
adamw_mode,
nvme_offload_fraction,
nvme_offload_dir,
)
if torch.cuda.is_available():
fused_optim = FusedOptimizerLoader().load()
self.gpu_adam_op = fused_optim.multi_tensor_adam
>           self._dummy_overflow_buf = torch.tensor([0], dtype=torch.int, device=get_current_device())
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.8140290Z
colossalai/nn/optimizer/hybrid_adam.py:90: RuntimeError
______ test_adam_optim_on_bert[p_dtype1-g_dtype1-True-HybridAdam-device4] ______
2025-04-11T04:23:18.8140610Z
optim_cls = <class 'colossalai.nn.optimizer.hybrid_adam.HybridAdam'>
device = device(type='cuda', index=0), adamw = True, p_dtype = torch.float32
g_dtype = torch.float16
2025-04-11T04:23:18.8141032Z
@pytest.mark.parametrize("optim_cls, device", _ALLOWED_OPTIM_DEVICES)
@pytest.mark.parametrize("adamw", [False, True])
@pytest.mark.parametrize("p_dtype, g_dtype", _ALLOWED_P_G_TYPES)
def test_adam_optim_on_bert(
optim_cls: Union[Type[FusedAdam], Type[CPUAdam], Type[HybridAdam]],
device: torch.device,
adamw: bool,
p_dtype: torch.dtype,
g_dtype: torch.dtype,
) -> None:
model_fn, *_ = next(iter(model_zoo.get_sub_registry("transformers_bert_for_sequence_classification").values()))
>       torch_model = model_fn().to(device)
2025-04-11T04:23:18.8142585Z
tests/test_optimizer/test_adam_optim.py:48:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
/opt/conda/envs/pytorch/lib/python3.10/site-packages/transformers/modeling_utils.py:2576: in to
return super().to(*args, **kwargs)
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:1152: in to
return self._apply(convert)
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:802: in _apply
module._apply(fn)
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:802: in _apply
module._apply(fn)
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:802: in _apply
module._apply(fn)
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:825: in _apply
param_applied = fn(param)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.8144888Z
t = Parameter containing:
tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
[ 0.0145, -0.0268,...4,  0.0235, -0.0067],
[-0.0276, -0.0061,  0.0080,  ...,  0.0096,  0.0016, -0.0028]],
requires_grad=True)
2025-04-11T04:23:18.8145415Z
def convert(t):
if convert_to_format is not None and t.dim() in (4, 5):
return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None,
non_blocking, memory_format=convert_to_format)
>       return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
E       RuntimeError: CUDA error: out of memory
E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.8146886Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:1150: RuntimeError
______ test_adam_optim_on_bert[p_dtype2-g_dtype2-False-FusedAdam-device0] ______
2025-04-11T04:23:18.8147325Z
optim_cls = <class 'colossalai.nn.optimizer.fused_adam.FusedAdam'>
device = device(type='cuda', index=0), adamw = False, p_dtype = torch.float32
g_dtype = torch.bfloat16
2025-04-11T04:23:18.8147801Z
@pytest.mark.parametrize("optim_cls, device", _ALLOWED_OPTIM_DEVICES)
@pytest.mark.parametrize("adamw", [False, True])
@pytest.mark.parametrize("p_dtype, g_dtype", _ALLOWED_P_G_TYPES)
def test_adam_optim_on_bert(
optim_cls: Union[Type[FusedAdam], Type[CPUAdam], Type[HybridAdam]],
device: torch.device,
adamw: bool,
p_dtype: torch.dtype,
g_dtype: torch.dtype,
) -> None:
model_fn, *_ = next(iter(model_zoo.get_sub_registry("transformers_bert_for_sequence_classification").values()))
>       torch_model = model_fn().to(device)
2025-04-11T04:23:18.8149370Z
tests/test_optimizer/test_adam_optim.py:48:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
/opt/conda/envs/pytorch/lib/python3.10/site-packages/transformers/modeling_utils.py:2576: in to
return super().to(*args, **kwargs)
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:1152: in to
return self._apply(convert)
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:802: in _apply
module._apply(fn)
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:802: in _apply
module._apply(fn)
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:802: in _apply
module._apply(fn)
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:825: in _apply
param_applied = fn(param)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.8151684Z
t = Parameter containing:
tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
[ 0.0360, -0.0060,...2,  0.0336, -0.0315],
[ 0.0418,  0.0034,  0.0053,  ...,  0.0279, -0.0100,  0.0020]],
requires_grad=True)
2025-04-11T04:23:18.8152260Z
def convert(t):
if convert_to_format is not None and t.dim() in (4, 5):
return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None,
non_blocking, memory_format=convert_to_format)
>       return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
E       RuntimeError: CUDA error: out of memory
E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.8153671Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:1150: RuntimeError
_______ test_adam_optim_on_bert[p_dtype2-g_dtype2-False-CPUAdam-device2] _______
2025-04-11T04:23:18.8154155Z
optim_cls = <class 'colossalai.nn.optimizer.cpu_adam.CPUAdam'>
device = device(type='cuda', index=0), adamw = False, p_dtype = torch.float32
g_dtype = torch.bfloat16
2025-04-11T04:23:18.8154573Z
@pytest.mark.parametrize("optim_cls, device", _ALLOWED_OPTIM_DEVICES)
@pytest.mark.parametrize("adamw", [False, True])
@pytest.mark.parametrize("p_dtype, g_dtype", _ALLOWED_P_G_TYPES)
def test_adam_optim_on_bert(
optim_cls: Union[Type[FusedAdam], Type[CPUAdam], Type[HybridAdam]],
device: torch.device,
adamw: bool,
p_dtype: torch.dtype,
g_dtype: torch.dtype,
) -> None:
model_fn, *_ = next(iter(model_zoo.get_sub_registry("transformers_bert_for_sequence_classification").values()))
>       torch_model = model_fn().to(device)
2025-04-11T04:23:18.8156123Z
tests/test_optimizer/test_adam_optim.py:48:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
/opt/conda/envs/pytorch/lib/python3.10/site-packages/transformers/modeling_utils.py:2576: in to
return super().to(*args, **kwargs)
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:1152: in to
return self._apply(convert)
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:802: in _apply
module._apply(fn)
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:802: in _apply
module._apply(fn)
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:802: in _apply
module._apply(fn)
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:825: in _apply
param_applied = fn(param)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.8158349Z
t = Parameter containing:
tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
[-0.0029, -0.0003,...8,  0.0132,  0.0134],
[-0.0017, -0.0011, -0.0088,  ...,  0.0178,  0.0258,  0.0116]],
requires_grad=True)
2025-04-11T04:23:18.8158926Z
def convert(t):
if convert_to_format is not None and t.dim() in (4, 5):
return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None,
non_blocking, memory_format=convert_to_format)
>       return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
E       RuntimeError: CUDA error: out of memory
E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.8160385Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:1150: RuntimeError
_____ test_adam_optim_on_bert[p_dtype2-g_dtype2-False-HybridAdam-device3] ______
2025-04-11T04:23:18.8160829Z
optim_cls = <class 'colossalai.nn.optimizer.hybrid_adam.HybridAdam'>
device = device(type='cpu'), adamw = False, p_dtype = torch.float32
g_dtype = torch.bfloat16
2025-04-11T04:23:18.8161285Z
@pytest.mark.parametrize("optim_cls, device", _ALLOWED_OPTIM_DEVICES)
@pytest.mark.parametrize("adamw", [False, True])
@pytest.mark.parametrize("p_dtype, g_dtype", _ALLOWED_P_G_TYPES)
def test_adam_optim_on_bert(
optim_cls: Union[Type[FusedAdam], Type[CPUAdam], Type[HybridAdam]],
device: torch.device,
adamw: bool,
p_dtype: torch.dtype,
g_dtype: torch.dtype,
) -> None:
model_fn, *_ = next(iter(model_zoo.get_sub_registry("transformers_bert_for_sequence_classification").values()))
torch_model = model_fn().to(device)
model = deepcopy(torch_model).to(p_dtype)
lr = 1e-3
beta1, beta2 = 0.9, 0.999
eps = 1e-8
torch_optim_cls = AdamW if adamw else Adam
torch_optim = torch_optim_cls(setup_param_groups(torch_model), lr=lr, betas=(beta1, beta2), eps=eps)
>       optim = optim_cls(setup_param_groups(model), lr=lr, betas=(beta1, beta2), eps=eps, adamw_mode=adamw)
2025-04-11T04:23:18.8163709Z
tests/test_optimizer/test_adam_optim.py:55:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.8163938Z
self = HybridAdam (
Parameter Group 0
betas: (0.9, 0.999)
bias_correction: True
eps: 1e-08
lr: 0.001
weig...arameter Group 1
betas: (0.9, 0.999)
bias_correction: True
eps: 1e-08
lr: 0.001
weight_decay: 0.0
)
model_params = [{'betas': (0.9, 0.999), 'bias_correction': True, 'eps': 1e-08, 'lr': 0.001, ...}, {'betas': (0.9, 0.999), 'bias_correction': True, 'eps': 1e-08, 'lr': 0.001, ...}]
lr = 0.001, bias_correction = True, betas = (0.9, 0.999), eps = 1e-08
weight_decay = 0, adamw_mode = False, nvme_offload_fraction = 0.0
nvme_offload_dir = None, defaults = {}
fused_optim = <module 'colossalai._C.fused_optim_cuda' from '/__w/ColossalAI/ColossalAI/colossalai/_C/fused_optim_cuda.cpython-310-x86_64-linux-gnu.so'>
2025-04-11T04:23:18.8166122Z
def __init__(
self,
model_params,
lr=1e-3,
bias_correction=True,
betas=(0.9, 0.999),
eps=1e-8,
weight_decay=0,
adamw_mode=True,
nvme_offload_fraction: float = 0.0,
nvme_offload_dir: Optional[str] = None,
**defaults: Any,
):
super().__init__(
model_params,
lr,
bias_correction,
betas,
eps,
weight_decay,
adamw_mode,
nvme_offload_fraction,
nvme_offload_dir,
)
if torch.cuda.is_available():
fused_optim = FusedOptimizerLoader().load()
self.gpu_adam_op = fused_optim.multi_tensor_adam
>           self._dummy_overflow_buf = torch.tensor([0], dtype=torch.int, device=get_current_device())
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.8169441Z
colossalai/nn/optimizer/hybrid_adam.py:90: RuntimeError
_____ test_adam_optim_on_bert[p_dtype2-g_dtype2-False-HybridAdam-device4] ______
2025-04-11T04:23:18.8169763Z
optim_cls = <class 'colossalai.nn.optimizer.hybrid_adam.HybridAdam'>
device = device(type='cuda', index=0), adamw = False, p_dtype = torch.float32
g_dtype = torch.bfloat16
2025-04-11T04:23:18.8170185Z
@pytest.mark.parametrize("optim_cls, device", _ALLOWED_OPTIM_DEVICES)
@pytest.mark.parametrize("adamw", [False, True])
@pytest.mark.parametrize("p_dtype, g_dtype", _ALLOWED_P_G_TYPES)
def test_adam_optim_on_bert(
optim_cls: Union[Type[FusedAdam], Type[CPUAdam], Type[HybridAdam]],
device: torch.device,
adamw: bool,
p_dtype: torch.dtype,
g_dtype: torch.dtype,
) -> None:
model_fn, *_ = next(iter(model_zoo.get_sub_registry("transformers_bert_for_sequence_classification").values()))
>       torch_model = model_fn().to(device)
2025-04-11T04:23:18.8171694Z
tests/test_optimizer/test_adam_optim.py:48:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
/opt/conda/envs/pytorch/lib/python3.10/site-packages/transformers/modeling_utils.py:2576: in to
return super().to(*args, **kwargs)
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:1152: in to
return self._apply(convert)
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:802: in _apply
module._apply(fn)
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:802: in _apply
module._apply(fn)
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:802: in _apply
module._apply(fn)
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:825: in _apply
param_applied = fn(param)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.8174008Z
t = Parameter containing:
tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
[ 0.0281,  0.0026,...4, -0.0037,  0.0294],
[ 0.0003,  0.0104, -0.0075,  ...,  0.0078,  0.0005, -0.0179]],
requires_grad=True)
2025-04-11T04:23:18.8174585Z
def convert(t):
if convert_to_format is not None and t.dim() in (4, 5):
return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None,
non_blocking, memory_format=convert_to_format)
>       return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
E       RuntimeError: CUDA error: out of memory
E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.8176048Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:1150: RuntimeError
______ test_adam_optim_on_bert[p_dtype2-g_dtype2-True-FusedAdam-device0] _______
2025-04-11T04:23:18.8176481Z
optim_cls = <class 'colossalai.nn.optimizer.fused_adam.FusedAdam'>
device = device(type='cuda', index=0), adamw = True, p_dtype = torch.float32
g_dtype = torch.bfloat16
2025-04-11T04:23:18.8176907Z
@pytest.mark.parametrize("optim_cls, device", _ALLOWED_OPTIM_DEVICES)
@pytest.mark.parametrize("adamw", [False, True])
@pytest.mark.parametrize("p_dtype, g_dtype", _ALLOWED_P_G_TYPES)
def test_adam_optim_on_bert(
optim_cls: Union[Type[FusedAdam], Type[CPUAdam], Type[HybridAdam]],
device: torch.device,
adamw: bool,
p_dtype: torch.dtype,
g_dtype: torch.dtype,
) -> None:
model_fn, *_ = next(iter(model_zoo.get_sub_registry("transformers_bert_for_sequence_classification").values()))
>       torch_model = model_fn().to(device)
2025-04-11T04:23:18.8178417Z
tests/test_optimizer/test_adam_optim.py:48:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
/opt/conda/envs/pytorch/lib/python3.10/site-packages/transformers/modeling_utils.py:2576: in to
return super().to(*args, **kwargs)
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:1152: in to
return self._apply(convert)
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:802: in _apply
module._apply(fn)
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:802: in _apply
module._apply(fn)
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:802: in _apply
module._apply(fn)
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:825: in _apply
param_applied = fn(param)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.8180747Z
t = Parameter containing:
tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
[ 0.0240, -0.0152,...6, -0.0175, -0.0244],
[-0.0064, -0.0248,  0.0195,  ..., -0.0030, -0.0263,  0.0248]],
requires_grad=True)
2025-04-11T04:23:18.8181277Z
def convert(t):
if convert_to_format is not None and t.dim() in (4, 5):
return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None,
non_blocking, memory_format=convert_to_format)
>       return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
E       RuntimeError: CUDA error: out of memory
E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.8182731Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:1150: RuntimeError
_______ test_adam_optim_on_bert[p_dtype2-g_dtype2-True-CPUAdam-device2] ________
2025-04-11T04:23:18.8183169Z
optim_cls = <class 'colossalai.nn.optimizer.cpu_adam.CPUAdam'>
device = device(type='cuda', index=0), adamw = True, p_dtype = torch.float32
g_dtype = torch.bfloat16
2025-04-11T04:23:18.8183580Z
@pytest.mark.parametrize("optim_cls, device", _ALLOWED_OPTIM_DEVICES)
@pytest.mark.parametrize("adamw", [False, True])
@pytest.mark.parametrize("p_dtype, g_dtype", _ALLOWED_P_G_TYPES)
def test_adam_optim_on_bert(
optim_cls: Union[Type[FusedAdam], Type[CPUAdam], Type[HybridAdam]],
device: torch.device,
adamw: bool,
p_dtype: torch.dtype,
g_dtype: torch.dtype,
) -> None:
model_fn, *_ = next(iter(model_zoo.get_sub_registry("transformers_bert_for_sequence_classification").values()))
>       torch_model = model_fn().to(device)
2025-04-11T04:23:18.8185098Z
tests/test_optimizer/test_adam_optim.py:48:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
/opt/conda/envs/pytorch/lib/python3.10/site-packages/transformers/modeling_utils.py:2576: in to
return super().to(*args, **kwargs)
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:1152: in to
return self._apply(convert)
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:802: in _apply
module._apply(fn)
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:802: in _apply
module._apply(fn)
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:802: in _apply
module._apply(fn)
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:825: in _apply
param_applied = fn(param)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.8187438Z
t = Parameter containing:
tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
[ 0.0105,  0.0235,...3,  0.0204, -0.0137],
[ 0.0001, -0.0009, -0.0197,  ...,  0.0352, -0.0017,  0.0075]],
requires_grad=True)
2025-04-11T04:23:18.8188020Z
def convert(t):
if convert_to_format is not None and t.dim() in (4, 5):
return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None,
non_blocking, memory_format=convert_to_format)
>       return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
E       RuntimeError: CUDA error: out of memory
E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.8189455Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:1150: RuntimeError
______ test_adam_optim_on_bert[p_dtype2-g_dtype2-True-HybridAdam-device3] ______
2025-04-11T04:23:18.8189895Z
optim_cls = <class 'colossalai.nn.optimizer.hybrid_adam.HybridAdam'>
device = device(type='cpu'), adamw = True, p_dtype = torch.float32
g_dtype = torch.bfloat16
2025-04-11T04:23:18.8190308Z
@pytest.mark.parametrize("optim_cls, device", _ALLOWED_OPTIM_DEVICES)
@pytest.mark.parametrize("adamw", [False, True])
@pytest.mark.parametrize("p_dtype, g_dtype", _ALLOWED_P_G_TYPES)
def test_adam_optim_on_bert(
optim_cls: Union[Type[FusedAdam], Type[CPUAdam], Type[HybridAdam]],
device: torch.device,
adamw: bool,
p_dtype: torch.dtype,
g_dtype: torch.dtype,
) -> None:
model_fn, *_ = next(iter(model_zoo.get_sub_registry("transformers_bert_for_sequence_classification").values()))
torch_model = model_fn().to(device)
model = deepcopy(torch_model).to(p_dtype)
lr = 1e-3
beta1, beta2 = 0.9, 0.999
eps = 1e-8
torch_optim_cls = AdamW if adamw else Adam
torch_optim = torch_optim_cls(setup_param_groups(torch_model), lr=lr, betas=(beta1, beta2), eps=eps)
>       optim = optim_cls(setup_param_groups(model), lr=lr, betas=(beta1, beta2), eps=eps, adamw_mode=adamw)
2025-04-11T04:23:18.8192773Z
tests/test_optimizer/test_adam_optim.py:55:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.8193052Z
self = HybridAdam (
Parameter Group 0
betas: (0.9, 0.999)
bias_correction: True
eps: 1e-08
lr: 0.001
weig...arameter Group 1
betas: (0.9, 0.999)
bias_correction: True
eps: 1e-08
lr: 0.001
weight_decay: 0.0
)
model_params = [{'betas': (0.9, 0.999), 'bias_correction': True, 'eps': 1e-08, 'lr': 0.001, ...}, {'betas': (0.9, 0.999), 'bias_correction': True, 'eps': 1e-08, 'lr': 0.001, ...}]
lr = 0.001, bias_correction = True, betas = (0.9, 0.999), eps = 1e-08
weight_decay = 0, adamw_mode = True, nvme_offload_fraction = 0.0
nvme_offload_dir = None, defaults = {}
fused_optim = <module 'colossalai._C.fused_optim_cuda' from '/__w/ColossalAI/ColossalAI/colossalai/_C/fused_optim_cuda.cpython-310-x86_64-linux-gnu.so'>
2025-04-11T04:23:18.8195224Z
def __init__(
self,
model_params,
lr=1e-3,
bias_correction=True,
betas=(0.9, 0.999),
eps=1e-8,
weight_decay=0,
adamw_mode=True,
nvme_offload_fraction: float = 0.0,
nvme_offload_dir: Optional[str] = None,
**defaults: Any,
):
super().__init__(
model_params,
lr,
bias_correction,
betas,
eps,
weight_decay,
adamw_mode,
nvme_offload_fraction,
nvme_offload_dir,
)
if torch.cuda.is_available():
fused_optim = FusedOptimizerLoader().load()
self.gpu_adam_op = fused_optim.multi_tensor_adam
>           self._dummy_overflow_buf = torch.tensor([0], dtype=torch.int, device=get_current_device())
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.8198494Z
colossalai/nn/optimizer/hybrid_adam.py:90: RuntimeError
______ test_adam_optim_on_bert[p_dtype2-g_dtype2-True-HybridAdam-device4] ______
2025-04-11T04:23:18.8198870Z
optim_cls = <class 'colossalai.nn.optimizer.hybrid_adam.HybridAdam'>
device = device(type='cuda', index=0), adamw = True, p_dtype = torch.float32
g_dtype = torch.bfloat16
2025-04-11T04:23:18.8199302Z
@pytest.mark.parametrize("optim_cls, device", _ALLOWED_OPTIM_DEVICES)
@pytest.mark.parametrize("adamw", [False, True])
@pytest.mark.parametrize("p_dtype, g_dtype", _ALLOWED_P_G_TYPES)
def test_adam_optim_on_bert(
optim_cls: Union[Type[FusedAdam], Type[CPUAdam], Type[HybridAdam]],
device: torch.device,
adamw: bool,
p_dtype: torch.dtype,
g_dtype: torch.dtype,
) -> None:
model_fn, *_ = next(iter(model_zoo.get_sub_registry("transformers_bert_for_sequence_classification").values()))
>       torch_model = model_fn().to(device)
2025-04-11T04:23:18.8200867Z
tests/test_optimizer/test_adam_optim.py:48:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
/opt/conda/envs/pytorch/lib/python3.10/site-packages/transformers/modeling_utils.py:2576: in to
return super().to(*args, **kwargs)
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:1152: in to
return self._apply(convert)
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:802: in _apply
module._apply(fn)
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:802: in _apply
module._apply(fn)
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:802: in _apply
module._apply(fn)
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:825: in _apply
param_applied = fn(param)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.8203124Z
t = Parameter containing:
tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
[ 0.0140, -0.0115,...1,  0.0094,  0.0310],
[ 0.0050,  0.0139, -0.0004,  ...,  0.0203, -0.0216, -0.0075]],
requires_grad=True)
2025-04-11T04:23:18.8203715Z
def convert(t):
if convert_to_format is not None and t.dim() in (4, 5):
return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None,
non_blocking, memory_format=convert_to_format)
>       return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
E       RuntimeError: CUDA error: out of memory
E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.8205193Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:1150: RuntimeError
_____________________________ test_dist_adafactor ______________________________
2025-04-11T04:23:18.8205593Z
args = (), kwargs = {}, try_count = 1
error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T04:23:18.8206356Z
def _run_until_success(*args, **kwargs):
try_count = 0
assert max_try is None or isinstance(
max_try, int
), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T04:23:18.8206948Z
while max_try is None or try_count < max_try:
try:
try_count += 1
ret = func(*args, **kwargs)
return ret
except exception_type as e:
error_lines = str(e).split("\n")
if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
print("Exception is caught, retrying...")
# when pattern is not specified, we always skip the exception
# when pattern is specified, we only skip when pattern is matched
continue
else:
print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
>                   raise e
2025-04-11T04:23:18.8208736Z
colossalai/testing/utils.py:144:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
colossalai/testing/utils.py:133: in _run_until_success
ret = func(*args, **kwargs)
tests/test_optimizer/test_dist_adafactor.py:468: in test_dist_adafactor
spawn(run_dist, nprocs=4)
colossalai/testing/utils.py:252: in spawn
mp.spawn(wrapped_func, nprocs=nprocs)
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
while not context.join():
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.8210614Z
self = <torch.multiprocessing.spawn.ProcessContext object at 0x7fb5b5a8aa10>
timeout = None
2025-04-11T04:23:18.8210919Z
def join(self, timeout=None):
r"""Join one or more processes within spawn context.
2025-04-11T04:23:18.8211210Z
Attempt to join one or more processes in this spawn context.
If one of them exited with a non-zero exit status, this function
kills the remaining processes and raises an exception with the cause
of the first process exiting.
2025-04-11T04:23:18.8211886Z
Returns ``True`` if all processes have been joined successfully,
``False`` if there are more processes that need to be joined.
2025-04-11T04:23:18.8212242Z
Args:
timeout (float): Wait this long before giving up on waiting.
"""
# Ensure this function can be called even when we're done.
if len(self.sentinels) == 0:
return True
2025-04-11T04:23:18.8212975Z
# Wait for any process to fail or all of them to succeed.
ready = multiprocessing.connection.wait(
self.sentinels.keys(),
timeout=timeout,
)
2025-04-11T04:23:18.8213604Z
error_index = None
for sentinel in ready:
index = self.sentinels.pop(sentinel)
process = self.processes[index]
process.join()
if process.exitcode != 0:
error_index = index
break
2025-04-11T04:23:18.8214397Z
# Return if there was no error.
if error_index is None:
# Return whether or not all processes have been joined.
return len(self.sentinels) == 0
2025-04-11T04:23:18.8214887Z
# Assume failure. Terminate processes that are still alive.
for process in self.processes:
if process.is_alive():
process.terminate()
process.join()
2025-04-11T04:23:18.8215473Z
# There won't be an error on the queue if the process crashed.
failed_process = self.processes[error_index]
if self.error_queues[error_index].empty():
exitcode = self.processes[error_index].exitcode
if exitcode < 0:
name = signal.Signals(-exitcode).name
raise ProcessExitedException(
"process %d terminated with signal %s" % (error_index, name),
error_index=error_index,
error_pid=failed_process.pid,
exit_code=exitcode,
signal_name=name,
)
else:
raise ProcessExitedException(
"process %d terminated with exit code %d" % (error_index, exitcode),
error_index=error_index,
error_pid=failed_process.pid,
exit_code=exitcode,
)
2025-04-11T04:23:18.8217693Z
original_trace = self.error_queues[error_index].get()
msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
msg += original_trace
>       raise ProcessRaisedException(msg, error_index, failed_process.pid)
E       torch.multiprocessing.spawn.ProcessRaisedException:
E
E       -- Process 0 terminated with the following error:
E       Traceback (most recent call last):
E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
E           fn(i, *args)
E         File "/__w/ColossalAI/ColossalAI/tests/test_optimizer/test_dist_adafactor.py", line 459, in run_dist
E           exam_dist_adafactor_base()
E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
E           partial_func(**kwargs)
E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
E           partial_func(**kwargs)
E         File "/__w/ColossalAI/ColossalAI/tests/test_optimizer/test_dist_adafactor.py", line 111, in exam_dist_adafactor_base
E           model_col = nn.Linear(H, W).to(local_rank)  # Col parallel weight
E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1152, in to
E           return self._apply(convert)
E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
E           param_applied = fn(param)
E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1150, in convert
E           return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
E       RuntimeError: CUDA error: out of memory
E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.8222671Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
----------------------------- Captured stdout call -----------------------------
[04/11/25 04:18:01] INFO     colossalai - colossalai - INFO:
/__w/ColossalAI/ColossalAI/colossalai/initialize.py
:75 launch
INFO     colossalai - colossalai - INFO: Distributed
environment is initialized, world size: 4
Base Test Passed
Base Test Passed
Base Test Passed
Maximum number of attempts is reached or pattern is not matched, no more retrying...
----------------------------- Captured stderr call -----------------------------
/opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
/opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
/opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
/opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
/opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
warnings.warn(
[W socket.cpp:697] [c10d] The client socket has failed to connect to [localhost]:21639 (errno: 99 - Cannot assign requested address).
/opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
warnings.warn(
[W socket.cpp:697] [c10d] The client socket has failed to connect to [localhost]:21639 (errno: 99 - Cannot assign requested address).
________________________________ test_dist_came ________________________________
2025-04-11T04:23:18.8244580Z
args = (), kwargs = {}, try_count = 1
error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T04:23:18.8245342Z
def _run_until_success(*args, **kwargs):
try_count = 0
assert max_try is None or isinstance(
max_try, int
), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T04:23:18.8245982Z
while max_try is None or try_count < max_try:
try:
try_count += 1
ret = func(*args, **kwargs)
return ret
except exception_type as e:
error_lines = str(e).split("\n")
if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
print("Exception is caught, retrying...")
# when pattern is not specified, we always skip the exception
# when pattern is specified, we only skip when pattern is matched
continue
else:
print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
>                   raise e
2025-04-11T04:23:18.8247706Z
colossalai/testing/utils.py:144:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
colossalai/testing/utils.py:133: in _run_until_success
ret = func(*args, **kwargs)
tests/test_optimizer/test_dist_came.py:357: in test_dist_came
spawn(run_dist, nprocs=4)
colossalai/testing/utils.py:252: in spawn
mp.spawn(wrapped_func, nprocs=nprocs)
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
while not context.join():
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.8249557Z
self = <torch.multiprocessing.spawn.ProcessContext object at 0x7fb5b8692950>
timeout = None
2025-04-11T04:23:18.8249836Z
def join(self, timeout=None):
r"""Join one or more processes within spawn context.
2025-04-11T04:23:18.8250120Z
Attempt to join one or more processes in this spawn context.
If one of them exited with a non-zero exit status, this function
kills the remaining processes and raises an exception with the cause
of the first process exiting.
2025-04-11T04:23:18.8250786Z
Returns ``True`` if all processes have been joined successfully,
``False`` if there are more processes that need to be joined.
2025-04-11T04:23:18.8251192Z
Args:
timeout (float): Wait this long before giving up on waiting.
"""
# Ensure this function can be called even when we're done.
if len(self.sentinels) == 0:
return True
2025-04-11T04:23:18.8251862Z
# Wait for any process to fail or all of them to succeed.
ready = multiprocessing.connection.wait(
self.sentinels.keys(),
timeout=timeout,
)
2025-04-11T04:23:18.8252481Z
error_index = None
for sentinel in ready:
index = self.sentinels.pop(sentinel)
process = self.processes[index]
process.join()
if process.exitcode != 0:
error_index = index
break
2025-04-11T04:23:18.8253263Z
# Return if there was no error.
if error_index is None:
# Return whether or not all processes have been joined.
return len(self.sentinels) == 0
2025-04-11T04:23:18.8253744Z
# Assume failure. Terminate processes that are still alive.
for process in self.processes:
if process.is_alive():
process.terminate()
process.join()
2025-04-11T04:23:18.8254315Z
# There won't be an error on the queue if the process crashed.
failed_process = self.processes[error_index]
if self.error_queues[error_index].empty():
exitcode = self.processes[error_index].exitcode
if exitcode < 0:
name = signal.Signals(-exitcode).name
raise ProcessExitedException(
"process %d terminated with signal %s" % (error_index, name),
error_index=error_index,
error_pid=failed_process.pid,
exit_code=exitcode,
signal_name=name,
)
else:
raise ProcessExitedException(
"process %d terminated with exit code %d" % (error_index, exitcode),
error_index=error_index,
error_pid=failed_process.pid,
exit_code=exitcode,
)
2025-04-11T04:23:18.8256557Z
original_trace = self.error_queues[error_index].get()
msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
msg += original_trace
>       raise ProcessRaisedException(msg, error_index, failed_process.pid)
E       torch.multiprocessing.spawn.ProcessRaisedException:
E
E       -- Process 0 terminated with the following error:
E       Traceback (most recent call last):
E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
E           fn(i, *args)
E         File "/__w/ColossalAI/ColossalAI/tests/test_optimizer/test_dist_came.py", line 349, in run_dist
E           exam_bert_test_on_lowlevelzero_plugin()  # err in TODO layer
E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
E           partial_func(**kwargs)
E         File "/__w/ColossalAI/ColossalAI/tests/test_optimizer/test_dist_came.py", line 206, in exam_bert_test_on_lowlevelzero_plugin
E           ) = build_model_from_low_level_zero_plugin(model_fn, loss_fn, test_config, CAME, DistributedCAME)
E         File "/__w/ColossalAI/ColossalAI/tests/test_shardformer/test_model/_utils.py", line 188, in build_model_from_low_level_zero_plugin
E           org_model = org_model.cuda()
E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2548, in cuda
E           return super().cuda(*args, **kwargs)
E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in cuda
E           return self._apply(lambda t: t.cuda(device))
E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
E           module._apply(fn)
E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
E           module._apply(fn)
E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
E           param_applied = fn(param)
E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in <lambda>
E           return self._apply(lambda t: t.cuda(device))
E       RuntimeError: CUDA error: out of memory
E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.8262763Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
----------------------------- Captured stdout call -----------------------------
[04/11/25 04:18:11] INFO     colossalai - colossalai - INFO:
/__w/ColossalAI/ColossalAI/colossalai/initialize.py
:75 launch
INFO     colossalai - colossalai - INFO: Distributed
environment is initialized, world size: 4
Maximum number of attempts is reached or pattern is not matched, no more retrying...
----------------------------- Captured stderr call -----------------------------
/opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
/opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
/opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
/opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
/opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
warnings.warn(
[W socket.cpp:697] [c10d] The client socket has failed to connect to [localhost]:38024 (errno: 99 - Cannot assign requested address).
[W socket.cpp:697] [c10d] The client socket has failed to connect to [localhost]:38024 (errno: 99 - Cannot assign requested address).
[W socket.cpp:697] [c10d] The client socket has failed to connect to [localhost]:38024 (errno: 99 - Cannot assign requested address).
_______________________________ test_dist_galore _______________________________
2025-04-11T04:23:18.8284550Z
args = (), kwargs = {}, try_count = 1
error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T04:23:18.8285251Z
def _run_until_success(*args, **kwargs):
try_count = 0
assert max_try is None or isinstance(
max_try, int
), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T04:23:18.8285839Z
while max_try is None or try_count < max_try:
try:
try_count += 1
ret = func(*args, **kwargs)
return ret
except exception_type as e:
error_lines = str(e).split("\n")
if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
print("Exception is caught, retrying...")
# when pattern is not specified, we always skip the exception
# when pattern is specified, we only skip when pattern is matched
continue
else:
print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
>                   raise e
2025-04-11T04:23:18.8287616Z
colossalai/testing/utils.py:144:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
colossalai/testing/utils.py:133: in _run_until_success
ret = func(*args, **kwargs)
tests/test_optimizer/test_dist_galore.py:298: in test_dist_galore
spawn(check_dist_galore, nprocs=4)
colossalai/testing/utils.py:252: in spawn
mp.spawn(wrapped_func, nprocs=nprocs)
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
while not context.join():
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.8289479Z
self = <torch.multiprocessing.spawn.ProcessContext object at 0x7fb5b5a8b2e0>
timeout = None
2025-04-11T04:23:18.8289811Z
def join(self, timeout=None):
r"""Join one or more processes within spawn context.
2025-04-11T04:23:18.8290100Z
Attempt to join one or more processes in this spawn context.
If one of them exited with a non-zero exit status, this function
kills the remaining processes and raises an exception with the cause
of the first process exiting.
2025-04-11T04:23:18.8290781Z
Returns ``True`` if all processes have been joined successfully,
``False`` if there are more processes that need to be joined.
2025-04-11T04:23:18.8291141Z
Args:
timeout (float): Wait this long before giving up on waiting.
"""
# Ensure this function can be called even when we're done.
if len(self.sentinels) == 0:
return True
2025-04-11T04:23:18.8291812Z
# Wait for any process to fail or all of them to succeed.
ready = multiprocessing.connection.wait(
self.sentinels.keys(),
timeout=timeout,
)
2025-04-11T04:23:18.8292377Z
error_index = None
for sentinel in ready:
index = self.sentinels.pop(sentinel)
process = self.processes[index]
process.join()
if process.exitcode != 0:
error_index = index
break
2025-04-11T04:23:18.8293158Z
# Return if there was no error.
if error_index is None:
# Return whether or not all processes have been joined.
return len(self.sentinels) == 0
2025-04-11T04:23:18.8293702Z
# Assume failure. Terminate processes that are still alive.
for process in self.processes:
if process.is_alive():
process.terminate()
process.join()
2025-04-11T04:23:18.8294268Z
# There won't be an error on the queue if the process crashed.
failed_process = self.processes[error_index]
if self.error_queues[error_index].empty():
exitcode = self.processes[error_index].exitcode
if exitcode < 0:
name = signal.Signals(-exitcode).name
raise ProcessExitedException(
"process %d terminated with signal %s" % (error_index, name),
error_index=error_index,
error_pid=failed_process.pid,
exit_code=exitcode,
signal_name=name,
)
else:
raise ProcessExitedException(
"process %d terminated with exit code %d" % (error_index, exitcode),
error_index=error_index,
error_pid=failed_process.pid,
exit_code=exitcode,
)
2025-04-11T04:23:18.8296506Z
original_trace = self.error_queues[error_index].get()
msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
msg += original_trace
>       raise ProcessRaisedException(msg, error_index, failed_process.pid)
E       torch.multiprocessing.spawn.ProcessRaisedException:
E
E       -- Process 0 terminated with the following error:
E       Traceback (most recent call last):
E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
E           fn(i, *args)
E         File "/__w/ColossalAI/ColossalAI/tests/test_optimizer/test_dist_galore.py", line 291, in check_dist_galore
E           dist.barrier()
E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/distributed/c10d_logger.py", line 72, in wrapper
E           return func(*args, **kwargs)
E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py", line 3439, in barrier
E           work = default_pg.barrier(opts=opts)
E       RuntimeError: CUDA error: out of memory
E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.8299870Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
----------------------------- Captured stdout call -----------------------------
[04/11/25 04:18:19] INFO     colossalai - colossalai - INFO:
/__w/ColossalAI/ColossalAI/colossalai/initialize.py
:75 launch
INFO     colossalai - colossalai - INFO: Distributed
environment is initialized, world size: 4
Skipping forward-backward tests due to SVD instability
Running bert tests, which are expected to produce minor errors due to instability in SVD convergence.             For example, a 1e-9 grad diff causes drastic difference in SVD output.
CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.8302260Z
CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.8302953Z
CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.8303646Z
[3] is setting up NCCL communicator and retrieving ncclUniqueId from [0] via c10d key-value store by key '0', but store->get('0') got error: Connection reset by peer
Exception raised from recvBytes at /opt/conda/conda-bld/pytorch_1711403380909/work/torch/csrc/distributed/c10d/Utils.hpp:670 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x57 (0x7f10aa7d3d87 in /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x5522c2e (0x7f10ef0bcc2e in /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
frame #2: c10d::TCPStore::doWait(c10::ArrayRef<std::string>, std::chrono::duration<long, std::ratio<1l, 1000l> >) + 0x360 (0x7f10ef0b7440 in /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
frame #3: c10d::TCPStore::doGet(std::string const&) + 0x32 (0x7f10ef0b7782 in /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
frame #4: c10d::TCPStore::get(std::string const&) + 0xa1 (0x7f10ef0b85b1 in /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
frame #5: c10d::PrefixStore::get(std::string const&) + 0x31 (0x7f10ef06da21 in /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
frame #6: c10d::PrefixStore::get(std::string const&) + 0x31 (0x7f10ef06da21 in /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
frame #7: c10d::PrefixStore::get(std::string const&) + 0x31 (0x7f10ef06da21 in /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
frame #8: c10d::PrefixStore::get(std::string const&) + 0x31 (0x7f10ef06da21 in /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
frame #9: c10d::ProcessGroupNCCL::broadcastUniqueNCCLID(ncclUniqueId*, bool, std::string const&, int) + 0xa9 (0x7f10ab998a59 in /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #10: c10d::ProcessGroupNCCL::getNCCLComm(std::string const&, std::vector<c10::Device, std::allocator<c10::Device> > const&, c10d::OpType, int, bool) + 0x22b (0x7f10ab99fa4b in /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #11: c10d::ProcessGroupNCCL::allgather(std::vector<std::vector<at::Tensor, std::allocator<at::Tensor> >, std::allocator<std::vector<at::Tensor, std::allocator<at::Tensor> > > >&, std::vector<at::Tensor, std::allocator<at::Tensor> >&, c10d::AllgatherOptions const&) + 0xb5c (0x7f10ab9b5e4c in /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #12: <unknown function> + 0x54c7dbd (0x7f10ef061dbd in /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
frame #13: <unknown function> + 0x54d1cb8 (0x7f10ef06bcb8 in /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
frame #14: <unknown function> + 0x4b16e6c (0x7f10ee6b0e6c in /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
frame #15: <unknown function> + 0x1696528 (0x7f10eb230528 in /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
frame #16: <unknown function> + 0x54d94d3 (0x7f10ef0734d3 in /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
frame #17: <unknown function> + 0x54e48bf (0x7f10ef07e8bf in /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
frame #18: c10d::verify_params_across_processes(c10::intrusive_ptr<c10d::ProcessGroup, c10::detail::intrusive_target_default_null_type<c10d::ProcessGroup> > const&, std::vector<at::Tensor, std::allocator<at::Tensor> > const&, std::optional<std::weak_ptr<c10d::Logger> > const&) + 0x26f (0x7f10ef0e4f2f in /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
frame #19: <unknown function> + 0xc55ad1 (0x7f10f6c8fad1 in /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib/libtorch_python.so)
frame #20: <unknown function> + 0x413ea4 (0x7f10f644dea4 in /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib/libtorch_python.so)
frame #21: /opt/conda/envs/pytorch/bin/python() [0x4fdc87]
frame #22: _PyObject_MakeTpCall + 0x25b (0x4f741b in /opt/conda/envs/pytorch/bin/python)
frame #23: _PyEval_EvalFrameDefault + 0x53d6 (0x4f34c6 in /opt/conda/envs/pytorch/bin/python)
frame #24: _PyFunction_Vectorcall + 0x6f (0x4fe0cf in /opt/conda/envs/pytorch/bin/python)
frame #25: _PyEval_EvalFrameDefault + 0x31f (0x4ee40f in /opt/conda/envs/pytorch/bin/python)
frame #26: _PyFunction_Vectorcall + 0x6f (0x4fe0cf in /opt/conda/envs/pytorch/bin/python)
frame #27: _PyObject_FastCallDictTstate + 0x17d (0x4f681d in /opt/conda/envs/pytorch/bin/python)
frame #28: /opt/conda/envs/pytorch/bin/python() [0x507588]
frame #29: /opt/conda/envs/pytorch/bin/python() [0x4f7786]
frame #30: PyObject_Call + 0x209 (0x50a659 in /opt/conda/envs/pytorch/bin/python)
frame #31: _PyEval_EvalFrameDefault + 0x2b79 (0x4f0c69 in /opt/conda/envs/pytorch/bin/python)
frame #32: _PyFunction_Vectorcall + 0x6f (0x4fe0cf in /opt/conda/envs/pytorch/bin/python)
frame #33: _PyObject_FastCallDictTstate + 0x17d (0x4f681d in /opt/conda/envs/pytorch/bin/python)
frame #34: /opt/conda/envs/pytorch/bin/python() [0x507588]
frame #35: _PyObject_MakeTpCall + 0x2ab (0x4f746b in /opt/conda/envs/pytorch/bin/python)
frame #36: _PyEval_EvalFrameDefault + 0x5757 (0x4f3847 in /opt/conda/envs/pytorch/bin/python)
frame #37: _PyFunction_Vectorcall + 0x6f (0x4fe0cf in /opt/conda/envs/pytorch/bin/python)
frame #38: _PyEval_EvalFrameDefault + 0x731 (0x4ee821 in /opt/conda/envs/pytorch/bin/python)
frame #39: _PyFunction_Vectorcall + 0x6f (0x4fe0cf in /opt/conda/envs/pytorch/bin/python)
frame #40: _PyEval_EvalFrameDefault + 0x731 (0x4ee821 in /opt/conda/envs/pytorch/bin/python)
frame #41: _PyFunction_Vectorcall + 0x6f (0x4fe0cf in /opt/conda/envs/pytorch/bin/python)
frame #42: _PyEval_EvalFrameDefault + 0x31f (0x4ee40f in /opt/conda/envs/pytorch/bin/python)
frame #43: _PyFunction_Vectorcall + 0x6f (0x4fe0cf in /opt/conda/envs/pytorch/bin/python)
frame #44: _PyEval_EvalFrameDefault + 0x31f (0x4ee40f in /opt/conda/envs/pytorch/bin/python)
frame #45: _PyFunction_Vectorcall + 0x6f (0x4fe0cf in /opt/conda/envs/pytorch/bin/python)
frame #46: PyObject_Call + 0xb8 (0x50a508 in /opt/conda/envs/pytorch/bin/python)
frame #47: /opt/conda/envs/pytorch/bin/python() [0x5c8798]
frame #48: _PyObject_MakeTpCall + 0x25b (0x4f741b in /opt/conda/envs/pytorch/bin/python)
frame #49: PyObject_Call + 0xb8 (0x50a508 in /opt/conda/envs/pytorch/bin/python)
frame #50: _PyEval_EvalFrameDefault + 0x2b79 (0x4f0c69 in /opt/conda/envs/pytorch/bin/python)
frame #51: _PyFunction_Vectorcall + 0x6f (0x4fe0cf in /opt/conda/envs/pytorch/bin/python)
frame #52: _PyEval_EvalFrameDefault + 0x13b3 (0x4ef4a3 in /opt/conda/envs/pytorch/bin/python)
frame #53: _PyFunction_Vectorcall + 0x6f (0x4fe0cf in /opt/conda/envs/pytorch/bin/python)
frame #54: PyObject_Call + 0xb8 (0x50a508 in /opt/conda/envs/pytorch/bin/python)
frame #55: /opt/conda/envs/pytorch/bin/python() [0x5c8798]
frame #56: _PyObject_MakeTpCall + 0x25b (0x4f741b in /opt/conda/envs/pytorch/bin/python)
frame #57: _PyEval_EvalFrameDefault + 0x2b79 (0x4f0c69 in /opt/conda/envs/pytorch/bin/python)
frame #58: _PyFunction_Vectorcall + 0x6f (0x4fe0cf in /opt/conda/envs/pytorch/bin/python)
frame #59: _PyEval_EvalFrameDefault + 0x2b79 (0x4f0c69 in /opt/conda/envs/pytorch/bin/python)
frame #60: _PyFunction_Vectorcall + 0x6f (0x4fe0cf in /opt/conda/envs/pytorch/bin/python)
frame #61: _PyEval_EvalFrameDefault + 0x731 (0x4ee821 in /opt/conda/envs/pytorch/bin/python)
frame #62: _PyFunction_Vectorcall + 0x6f (0x4fe0cf in /opt/conda/envs/pytorch/bin/python)
frame #63: _PyEval_EvalFrameDefault + 0x731 (0x4ee821 in /opt/conda/envs/pytorch/bin/python)
. This may indicate a possible application crash on rank 0 or a network set up issue.
[1] is setting up NCCL communicator and retrieving ncclUniqueId from [0] via c10d key-value store by key '0', but store->get('0') got error: Connection reset by peer
Exception raised from recvBytes at /opt/conda/conda-bld/pytorch_1711403380909/work/torch/csrc/distributed/c10d/Utils.hpp:670 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x57 (0x7f6608535d87 in /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x5522c2e (0x7f664ce1ec2e in /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
frame #2: c10d::TCPStore::doWait(c10::ArrayRef<std::string>, std::chrono::duration<long, std::ratio<1l, 1000l> >) + 0x360 (0x7f664ce19440 in /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
frame #3: c10d::TCPStore::doGet(std::string const&) + 0x32 (0x7f664ce19782 in /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
frame #4: c10d::TCPStore::get(std::string const&) + 0xa1 (0x7f664ce1a5b1 in /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
frame #5: c10d::PrefixStore::get(std::string const&) + 0x31 (0x7f664cdcfa21 in /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
frame #6: c10d::PrefixStore::get(std::string const&) + 0x31 (0x7f664cdcfa21 in /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
frame #7: c10d::PrefixStore::get(std::string const&) + 0x31 (0x7f664cdcfa21 in /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
frame #8: c10d::PrefixStore::get(std::string const&) + 0x31 (0x7f664cdcfa21 in /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
frame #9: c10d::ProcessGroupNCCL::broadcastUniqueNCCLID(ncclUniqueId*, bool, std::string const&, int) + 0xa9 (0x7f66096faa59 in /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #10: c10d::ProcessGroupNCCL::getNCCLComm(std::string const&, std::vector<c10::Device, std::allocator<c10::Device> > const&, c10d::OpType, int, bool) + 0x22b (0x7f6609701a4b in /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #11: c10d::ProcessGroupNCCL::allgather(std::vector<std::vector<at::Tensor, std::allocator<at::Tensor> >, std::allocator<std::vector<at::Tensor, std::allocator<at::Tensor> > > >&, std::vector<at::Tensor, std::allocator<at::Tensor> >&, c10d::AllgatherOptions const&) + 0xb5c (0x7f6609717e4c in /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #12: <unknown function> + 0x54c7dbd (0x7f664cdc3dbd in /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
frame #13: <unknown function> + 0x54d1cb8 (0x7f664cdcdcb8 in /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
frame #14: <unknown function> + 0x4b16e6c (0x7f664c412e6c in /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
frame #15: <unknown function> + 0x1696528 (0x7f6648f92528 in /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
frame #16: <unknown function> + 0x54d94d3 (0x7f664cdd54d3 in /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
frame #17: <unknown function> + 0x54e48bf (0x7f664cde08bf in /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
frame #18: c10d::verify_params_across_processes(c10::intrusive_ptr<c10d::ProcessGroup, c10::detail::intrusive_target_default_null_type<c10d::ProcessGroup> > const&, std::vector<at::Tensor, std::allocator<at::Tensor> > const&, std::optional<std::weak_ptr<c10d::Logger> > const&) + 0x26f (0x7f664ce46f2f in /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
frame #19: <unknown function> + 0xc55ad1 (0x7f66549f1ad1 in /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib/libtorch_python.so)
frame #20: <unknown function> + 0x413ea4 (0x7f66541afea4 in /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib/libtorch_python.so)
frame #21: /opt/conda/envs/pytorch/bin/python() [0x4fdc87]
frame #22: _PyObject_MakeTpCall + 0x25b (0x4f741b in /opt/conda/envs/pytorch/bin/python)
frame #23: _PyEval_EvalFrameDefault + 0x53d6 (0x4f34c6 in /opt/conda/envs/pytorch/bin/python)
frame #24: _PyFunction_Vectorcall + 0x6f (0x4fe0cf in /opt/conda/envs/pytorch/bin/python)
frame #25: _PyEval_EvalFrameDefault + 0x31f (0x4ee40f in /opt/conda/envs/pytorch/bin/python)
frame #26: _PyFunction_Vectorcall + 0x6f (0x4fe0cf in /opt/conda/envs/pytorch/bin/python)
frame #27: _PyObject_FastCallDictTstate + 0x17d (0x4f681d in /opt/conda/envs/pytorch/bin/python)
frame #28: /opt/conda/envs/pytorch/bin/python() [0x507588]
frame #29: /opt/conda/envs/pytorch/bin/python() [0x4f7786]
frame #30: PyObject_Call + 0x209 (0x50a659 in /opt/conda/envs/pytorch/bin/python)
frame #31: _PyEval_EvalFrameDefault + 0x2b79 (0x4f0c69 in /opt/conda/envs/pytorch/bin/python)
frame #32: _PyFunction_Vectorcall + 0x6f (0x4fe0cf in /opt/conda/envs/pytorch/bin/python)
frame #33: _PyObject_FastCallDictTstate + 0x17d (0x4f681d in /opt/conda/envs/pytorch/bin/python)
frame #34: /opt/conda/envs/pytorch/bin/python() [0x507588]
frame #35: _PyObject_MakeTpCall + 0x2ab (0x4f746b in /opt/conda/envs/pytorch/bin/python)
frame #36: _PyEval_EvalFrameDefault + 0x5757 (0x4f3847 in /opt/conda/envs/pytorch/bin/python)
frame #37: _PyFunction_Vectorcall + 0x6f (0x4fe0cf in /opt/conda/envs/pytorch/bin/python)
frame #38: _PyEval_EvalFrameDefault + 0x731 (0x4ee821 in /opt/conda/envs/pytorch/bin/python)
frame #39: _PyFunction_Vectorcall + 0x6f (0x4fe0cf in /opt/conda/envs/pytorch/bin/python)
frame #40: _PyEval_EvalFrameDefault + 0x731 (0x4ee821 in /opt/conda/envs/pytorch/bin/python)
frame #41: _PyFunction_Vectorcall + 0x6f (0x4fe0cf in /opt/conda/envs/pytorch/bin/python)
frame #42: _PyEval_EvalFrameDefault + 0x31f (0x4ee40f in /opt/conda/envs/pytorch/bin/python)
frame #43: _PyFunction_Vectorcall + 0x6f (0x4fe0cf in /opt/conda/envs/pytorch/bin/python)
frame #44: _PyEval_EvalFrameDefault + 0x31f (0x4ee40f in /opt/conda/envs/pytorch/bin/python)
frame #45: _PyFunction_Vectorcall + 0x6f (0x4fe0cf in /opt/conda/envs/pytorch/bin/python)
frame #46: PyObject_Call + 0xb8 (0x50a508 in /opt/conda/envs/pytorch/bin/python)
frame #47: /opt/conda/envs/pytorch/bin/python() [0x5c8798]
frame #48: _PyObject_MakeTpCall + 0x25b (0x4f741b in /opt/conda/envs/pytorch/bin/python)
frame #49: PyObject_Call + 0xb8 (0x50a508 in /opt/conda/envs/pytorch/bin/python)
frame #50: _PyEval_EvalFrameDefault + 0x2b79 (0x4f0c69 in /opt/conda/envs/pytorch/bin/python)
frame #51: _PyFunction_Vectorcall + 0x6f (0x4fe0cf in /opt/conda/envs/pytorch/bin/python)
frame #52: _PyEval_EvalFrameDefault + 0x13b3 (0x4ef4a3 in /opt/conda/envs/pytorch/bin/python)
frame #53: _PyFunction_Vectorcall + 0x6f (0x4fe0cf in /opt/conda/envs/pytorch/bin/python)
frame #54: PyObject_Call + 0xb8 (0x50a508 in /opt/conda/envs/pytorch/bin/python)
frame #55: /opt/conda/envs/pytorch/bin/python() [0x5c8798]
frame #56: _PyObject_MakeTpCall + 0x25b (0x4f741b in /opt/conda/envs/pytorch/bin/python)
frame #57: _PyEval_EvalFrameDefault + 0x2b79 (0x4f0c69 in /opt/conda/envs/pytorch/bin/python)
frame #58: _PyFunction_Vectorcall + 0x6f (0x4fe0cf in /opt/conda/envs/pytorch/bin/python)
frame #59: _PyEval_EvalFrameDefault + 0x2b79 (0x4f0c69 in /opt/conda/envs/pytorch/bin/python)
frame #60: _PyFunction_Vectorcall + 0x6f (0x4fe0cf in /opt/conda/envs/pytorch/bin/python)
frame #61: _PyEval_EvalFrameDefault + 0x731 (0x4ee821 in /opt/conda/envs/pytorch/bin/python)
frame #62: _PyFunction_Vectorcall + 0x6f (0x4fe0cf in /opt/conda/envs/pytorch/bin/python)
frame #63: _PyEval_EvalFrameDefault + 0x731 (0x4ee821 in /opt/conda/envs/pytorch/bin/python)
. This may indicate a possible application crash on rank 0 or a network set up issue.
Failed to replace attention.self.query of type Linear with Linear1D_Col with the exception: [1] is setting up NCCL communicator and retrieving ncclUniqueId from [0] via c10d key-value store by key '0', but store->get('0') got error: Connection reset by peer
Exception raised from recvBytes at /opt/conda/conda-bld/pytorch_1711403380909/work/torch/csrc/distributed/c10d/Utils.hpp:670 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x57 (0x7f10aa7d3d87 in /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x5522c2e (0x7f10ef0bcc2e in /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
frame #2: c10d::TCPStore::doWait(c10::ArrayRef<std::string>, std::chrono::duration<long, std::ratio<1l, 1000l> >) + 0x360 (0x7f10ef0b7440 in /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
frame #3: c10d::TCPStore::doGet(std::string const&) + 0x32 (0x7f10ef0b7782 in /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
frame #4: c10d::TCPStore::get(std::string const&) + 0xa1 (0x7f10ef0b85b1 in /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
frame #5: c10d::PrefixStore::get(std::string const&) + 0x31 (0x7f10ef06da21 in /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
frame #6: c10d::PrefixStore::get(std::string const&) + 0x31 (0x7f10ef06da21 in /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
frame #7: c10d::PrefixStore::get(std::string const&) + 0x31 (0x7f10ef06da21 in /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
frame #8: c10d::PrefixStore::get(std::string const&) + 0x31 (0x7f10ef06da21 in /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
frame #9: c10d::ProcessGroupNCCL::broadcastUniqueNCCLID(ncclUniqueId*, bool, std::string const&, int) + 0xa9 (0x7f10ab998a59 in /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #10: c10d::ProcessGroupNCCL::getNCCLComm(std::string const&, std::vector<c10::Device, std::allocator<c10::Device> > const&, c10d::OpType, int, bool) + 0x22b (0x7f10ab99fa4b in /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #11: c10d::ProcessGroupNCCL::allgather(std::vector<std::vector<at::Tensor, std::allocator<at::Tensor> >, std::allocator<std::vector<at::Tensor, std::allocator<at::Tensor> > > >&, std::vector<at::Tensor, std::allocator<at::Tensor> >&, c10d::AllgatherOptions const&) + 0xb5c (0x7f10ab9b5e4c in /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #12: <unknown function> + 0x54c7dbd (0x7f10ef061dbd in /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
frame #13: <unknown function> + 0x54d1cb8 (0x7f10ef06bcb8 in /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
frame #14: <unknown function> + 0x4b16e6c (0x7f10ee6b0e6c in /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
frame #15: <unknown function> + 0x1696528 (0x7f10eb230528 in /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
frame #16: <unknown function> + 0x54d94d3 (0x7f10ef0734d3 in /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
frame #17: <unknown function> + 0x54e48bf (0x7f10ef07e8bf in /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)
frame #18: <unknown function> + 0xca3fae (0x7f10f6cddfae in /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib/libtorch_python.so)
frame #19: <unknown function> + 0x413ea4 (0x7f10f644dea4 in /opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/lib/libtorch_python.so)
frame #20: /opt/conda/envs/pytorch/bin/python() [0x4fdc87]
frame #21: _PyObject_MakeTpCall + 0x25b (0x4f741b in /opt/conda/envs/pytorch/bin/python)
frame #22: /opt/conda/envs/pytorch/bin/python() [0x509cbf]
frame #23: _PyEval_EvalFrameDefault + 0x4b26 (0x4f2c16 in /opt/conda/envs/pytorch/bin/python)
frame #24: _PyFunction_Vectorcall + 0x6f (0x4fe0cf in /opt/conda/envs/pytorch/bin/python)
frame #25: _PyEval_EvalFrameDefault + 0x2b79 (0x4f0c69 in /opt/conda/envs/pytorch/bin/python)
frame #26: _PyFunction_Vectorcall + 0x6f (0x4fe0cf in /opt/conda/envs/pytorch/bin/python)
frame #27: _PyEval_EvalFrameDefault + 0x4b26 (0x4f2c16 in /opt/conda/envs/pytorch/bin/python)
frame #28: _PyFunction_Vectorcall + 0x6f (0x4fe0cf in /opt/conda/envs/pytorch/bin/python)
frame #29: _PyEval_EvalFrameDefault + 0x4b26 (0x4f2c16 in /opt/conda/envs/pytorch/bin/python)
frame #30: _PyFunction_Vectorcall + 0x6f (0x4fe0cf in /opt/conda/envs/pytorch/bin/python)
frame #31: _PyEval_EvalFrameDefault + 0x13b3 (0x4ef4a3 in /opt/conda/envs/pytorch/bin/python)
frame #32: _PyFunction_Vectorcall + 0x6f (0x4fe0cf in /opt/conda/envs/pytorch/bin/python)
frame #33: _PyObject_FastCallDictTstate + 0x17d (0x4f681d in /opt/conda/envs/pytorch/bin/python)
frame #34: /opt/conda/envs/pytorch/bin/python() [0x507588]
frame #35: /opt/conda/envs/pytorch/bin/python() [0x4f7786]
frame #36: PyObject_Call + 0x209 (0x50a659 in /opt/conda/envs/pytorch/bin/python)
frame #37: _PyEval_EvalFrameDefault + 0x2b79 (0x4f0c69 in /opt/conda/envs/pytorch/bin/python)
frame #38: _PyFunction_Vectorcall + 0x6f (0x4fe0cf in /opt/conda/envs/pytorch/bin/python)
frame #39: PyObject_Call + 0xb8 (0x50a508 in /opt/conda/envs/pytorch/bin/python)
frame #40: _PyEval_EvalFrameDefault + 0x2b79 (0x4f0c69 in /opt/conda/envs/pytorch/bin/python)
frame #41: _PyFunction_Vectorcall + 0x6f (0x4fe0cf in /opt/conda/envs/pytorch/bin/python)
frame #42: _PyEval_EvalFrameDefault + 0x731 (0x4ee821 in /opt/conda/envs/pytorch/bin/python)
frame #43: /opt/conda/envs/pytorch/bin/python() [0x5099ce]
frame #44: _PyEval_EvalFrameDefault + 0x13b3 (0x4ef4a3 in /opt/conda/envs/pytorch/bin/python)
frame #45: /opt/conda/envs/pytorch/bin/python() [0x5099ce]
frame #46: _PyEval_EvalFrameDefault + 0x13b3 (0x4ef4a3 in /opt/conda/envs/pytorch/bin/python)
frame #47: /opt/conda/envs/pytorch/bin/python() [0x5099ce]
frame #48: _PyEval_EvalFrameDefault + 0x13b3 (0x4ef4a3 in /opt/conda/envs/pytorch/bin/python)
frame #49: /opt/conda/envs/pytorch/bin/python() [0x5099ce]
frame #50: _PyEval_EvalFrameDefault + 0x13b3 (0x4ef4a3 in /opt/conda/envs/pytorch/bin/python)
frame #51: /opt/conda/envs/pytorch/bin/python() [0x5099ce]
frame #52: _PyEval_EvalFrameDefault + 0x13b3 (0x4ef4a3 in /opt/conda/envs/pytorch/bin/python)
frame #53: _PyFunction_Vectorcall + 0x6f (0x4fe0cf in /opt/conda/envs/pytorch/bin/python)
frame #54: _PyEval_EvalFrameDefault + 0x731 (0x4ee821 in /opt/conda/envs/pytorch/bin/python)
frame #55: /opt/conda/envs/pytorch/bin/python() [0x5099ce]
frame #56: _PyEval_EvalFrameDefault + 0x13b3 (0x4ef4a3 in /opt/conda/envs/pytorch/bin/python)
frame #57: _PyFunction_Vectorcall + 0x6f (0x4fe0cf in /opt/conda/envs/pytorch/bin/python)
frame #58: _PyObject_FastCallDictTstate + 0x17d (0x4f681d in /opt/conda/envs/pytorch/bin/python)
frame #59: /opt/conda/envs/pytorch/bin/python() [0x507588]
frame #60: _PyObject_MakeTpCall + 0x2ab (0x4f746b in /opt/conda/envs/pytorch/bin/python)
frame #61: _PyEval_EvalFrameDefault + 0x5757 (0x4f3847 in /opt/conda/envs/pytorch/bin/python)
frame #62: _PyFunction_Vectorcall + 0x6f (0x4fe0cf in /opt/conda/envs/pytorch/bin/python)
frame #63: _PyEval_EvalFrameDefault + 0x731 (0x4ee821 in /opt/conda/envs/pytorch/bin/python)
. This may indicate a possible application crash on rank 0 or a network set up issue.. Please check your model configuration or sharding policy, you can set up an issue for us to help you as well.
Maximum number of attempts is reached or pattern is not matched, no more retrying...
----------------------------- Captured stderr call -----------------------------
/opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
/opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
/opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
/opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
/opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
warnings.warn(
[W socket.cpp:697] [c10d] The client socket has failed to connect to [localhost]:36049 (errno: 99 - Cannot assign requested address).
[W socket.cpp:697] [c10d] The client socket has failed to connect to [localhost]:36049 (errno: 99 - Cannot assign requested address).
[W socket.cpp:697] [c10d] The client socket has failed to connect to [localhost]:36049 (errno: 99 - Cannot assign requested address).
________________________________ test_dist_lamb ________________________________
2025-04-11T04:23:18.8398719Z
args = (), kwargs = {}, try_count = 1
error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T04:23:18.8399436Z
def _run_until_success(*args, **kwargs):
try_count = 0
assert max_try is None or isinstance(
max_try, int
), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T04:23:18.8400039Z
while max_try is None or try_count < max_try:
try:
try_count += 1
ret = func(*args, **kwargs)
return ret
except exception_type as e:
error_lines = str(e).split("\n")
if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
print("Exception is caught, retrying...")
# when pattern is not specified, we always skip the exception
# when pattern is specified, we only skip when pattern is matched
continue
else:
print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
>                   raise e
2025-04-11T04:23:18.8401904Z
colossalai/testing/utils.py:144:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
colossalai/testing/utils.py:133: in _run_until_success
ret = func(*args, **kwargs)
tests/test_optimizer/test_dist_lamb.py:276: in test_dist_lamb
spawn(check_dist_lamb, nprocs=4)
colossalai/testing/utils.py:252: in spawn
mp.spawn(wrapped_func, nprocs=nprocs)
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
while not context.join():
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.8403862Z
self = <torch.multiprocessing.spawn.ProcessContext object at 0x7fb5b8690c40>
timeout = None
2025-04-11T04:23:18.8404152Z
def join(self, timeout=None):
r"""Join one or more processes within spawn context.
2025-04-11T04:23:18.8404442Z
Attempt to join one or more processes in this spawn context.
If one of them exited with a non-zero exit status, this function
kills the remaining processes and raises an exception with the cause
of the first process exiting.
2025-04-11T04:23:18.8405062Z
Returns ``True`` if all processes have been joined successfully,
``False`` if there are more processes that need to be joined.
2025-04-11T04:23:18.8405469Z
Args:
timeout (float): Wait this long before giving up on waiting.
"""
# Ensure this function can be called even when we're done.
if len(self.sentinels) == 0:
return True
2025-04-11T04:23:18.8406136Z
# Wait for any process to fail or all of them to succeed.
ready = multiprocessing.connection.wait(
self.sentinels.keys(),
timeout=timeout,
)
2025-04-11T04:23:18.8406706Z
error_index = None
for sentinel in ready:
index = self.sentinels.pop(sentinel)
process = self.processes[index]
process.join()
if process.exitcode != 0:
error_index = index
break
2025-04-11T04:23:18.8407543Z
# Return if there was no error.
if error_index is None:
# Return whether or not all processes have been joined.
return len(self.sentinels) == 0
2025-04-11T04:23:18.8408091Z
# Assume failure. Terminate processes that are still alive.
for process in self.processes:
if process.is_alive():
process.terminate()
process.join()
2025-04-11T04:23:18.8408658Z
# There won't be an error on the queue if the process crashed.
failed_process = self.processes[error_index]
if self.error_queues[error_index].empty():
exitcode = self.processes[error_index].exitcode
if exitcode < 0:
name = signal.Signals(-exitcode).name
raise ProcessExitedException(
"process %d terminated with signal %s" % (error_index, name),
error_index=error_index,
error_pid=failed_process.pid,
exit_code=exitcode,
signal_name=name,
)
else:
raise ProcessExitedException(
"process %d terminated with exit code %d" % (error_index, exitcode),
error_index=error_index,
error_pid=failed_process.pid,
exit_code=exitcode,
)
2025-04-11T04:23:18.8410871Z
original_trace = self.error_queues[error_index].get()
msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
msg += original_trace
>       raise ProcessRaisedException(msg, error_index, failed_process.pid)
E       torch.multiprocessing.spawn.ProcessRaisedException:
E
E       -- Process 0 terminated with the following error:
E       Traceback (most recent call last):
E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
E           fn(i, *args)
E         File "/__w/ColossalAI/ColossalAI/tests/test_optimizer/test_dist_lamb.py", line 263, in check_dist_lamb
E           run_dist_lamb_basic()
E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
E           partial_func(**kwargs)
E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
E           partial_func(**kwargs)
E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
E           partial_func(**kwargs)
E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 271, in _clear_cache
E           get_accelerator().synchronize()
E         File "/__w/ColossalAI/ColossalAI/colossalai/accelerator/cuda_accelerator.py", line 62, in synchronize
E           torch.cuda.synchronize(device)
E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py", line 801, in synchronize
E           return torch._C._cuda_synchronize()
E       RuntimeError: CUDA error: out of memory
E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.8415628Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
----------------------------- Captured stdout call -----------------------------
[04/11/25 04:18:28] INFO     colossalai - colossalai - INFO:
/__w/ColossalAI/ColossalAI/colossalai/initialize.py
:75 launch
INFO     colossalai - colossalai - INFO: Distributed
environment is initialized, world size: 4
Maximum number of attempts is reached or pattern is not matched, no more retrying...
----------------------------- Captured stderr call -----------------------------
/opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
/opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
/opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
/opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
/opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
warnings.warn(
[W socket.cpp:697] [c10d] The client socket has failed to connect to [localhost]:34628 (errno: 99 - Cannot assign requested address).
[W socket.cpp:697] [c10d] The client socket has failed to connect to [localhost]:34628 (errno: 99 - Cannot assign requested address).
[W socket.cpp:697] [c10d] The client socket has failed to connect to [localhost]:34628 (errno: 99 - Cannot assign requested address).
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:356: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:356: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:356: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:356: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
warnings.warn(
______________________________ test_pipeline_p2p _______________________________
2025-04-11T04:23:18.8442581Z
args = (), kwargs = {}, try_count = 1
error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T04:23:18.8443291Z
def _run_until_success(*args, **kwargs):
try_count = 0
assert max_try is None or isinstance(
max_try, int
), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T04:23:18.8443883Z
while max_try is None or try_count < max_try:
try:
try_count += 1
ret = func(*args, **kwargs)
return ret
except exception_type as e:
error_lines = str(e).split("\n")
if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
print("Exception is caught, retrying...")
# when pattern is not specified, we always skip the exception
# when pattern is specified, we only skip when pattern is matched
continue
else:
print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
>                   raise e
2025-04-11T04:23:18.8445657Z
colossalai/testing/utils.py:144:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
colossalai/testing/utils.py:133: in _run_until_success
ret = func(*args, **kwargs)
tests/test_pipeline/test_p2p_communication.py:79: in test_pipeline_p2p
spawn(run_dist, WORLD_SIZE)
colossalai/testing/utils.py:252: in spawn
mp.spawn(wrapped_func, nprocs=nprocs)
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
while not context.join():
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.8447540Z
self = <torch.multiprocessing.spawn.ProcessContext object at 0x7fb5b5f139a0>
timeout = None
2025-04-11T04:23:18.8447869Z
def join(self, timeout=None):
r"""Join one or more processes within spawn context.
2025-04-11T04:23:18.8448157Z
Attempt to join one or more processes in this spawn context.
If one of them exited with a non-zero exit status, this function
kills the remaining processes and raises an exception with the cause
of the first process exiting.
2025-04-11T04:23:18.8448770Z
Returns ``True`` if all processes have been joined successfully,
``False`` if there are more processes that need to be joined.
2025-04-11T04:23:18.8449170Z
Args:
timeout (float): Wait this long before giving up on waiting.
"""
# Ensure this function can be called even when we're done.
if len(self.sentinels) == 0:
return True
2025-04-11T04:23:18.8449832Z
# Wait for any process to fail or all of them to succeed.
ready = multiprocessing.connection.wait(
self.sentinels.keys(),
timeout=timeout,
)
2025-04-11T04:23:18.8450390Z
error_index = None
for sentinel in ready:
index = self.sentinels.pop(sentinel)
process = self.processes[index]
process.join()
if process.exitcode != 0:
error_index = index
break
2025-04-11T04:23:18.8451172Z
# Return if there was no error.
if error_index is None:
# Return whether or not all processes have been joined.
return len(self.sentinels) == 0
2025-04-11T04:23:18.8451698Z
# Assume failure. Terminate processes that are still alive.
for process in self.processes:
if process.is_alive():
process.terminate()
process.join()
2025-04-11T04:23:18.8452264Z
# There won't be an error on the queue if the process crashed.
failed_process = self.processes[error_index]
if self.error_queues[error_index].empty():
exitcode = self.processes[error_index].exitcode
if exitcode < 0:
name = signal.Signals(-exitcode).name
raise ProcessExitedException(
"process %d terminated with signal %s" % (error_index, name),
error_index=error_index,
error_pid=failed_process.pid,
exit_code=exitcode,
signal_name=name,
)
else:
raise ProcessExitedException(
"process %d terminated with exit code %d" % (error_index, exitcode),
error_index=error_index,
error_pid=failed_process.pid,
exit_code=exitcode,
)
2025-04-11T04:23:18.8454505Z
original_trace = self.error_queues[error_index].get()
msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
msg += original_trace
>       raise ProcessRaisedException(msg, error_index, failed_process.pid)
E       torch.multiprocessing.spawn.ProcessRaisedException:
E
E       -- Process 0 terminated with the following error:
E       Traceback (most recent call last):
E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
E           fn(i, *args)
E         File "/__w/ColossalAI/ColossalAI/tests/test_pipeline/test_p2p_communication.py", line 73, in run_dist
E           check_p2p_communication()
E         File "/__w/ColossalAI/ColossalAI/tests/test_pipeline/test_p2p_communication.py", line 21, in check_p2p_communication
E           tensor = torch.ones(1, device=get_accelerator().get_current_device())
E       RuntimeError: CUDA error: out of memory
E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.8457484Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
----------------------------- Captured stdout call -----------------------------
[04/11/25 04:18:33] INFO     colossalai - colossalai - INFO:
/__w/ColossalAI/ColossalAI/colossalai/initialize.py
:75 launch
INFO     colossalai - colossalai - INFO: Distributed
environment is initialized, world size: 2
Maximum number of attempts is reached or pattern is not matched, no more retrying...
_________________________ test_pipeline_stage_manager __________________________
2025-04-11T04:23:18.8459036Z
args = (), kwargs = {}, try_count = 1
error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T04:23:18.8459792Z
def _run_until_success(*args, **kwargs):
try_count = 0
assert max_try is None or isinstance(
max_try, int
), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T04:23:18.8460373Z
while max_try is None or try_count < max_try:
try:
try_count += 1
ret = func(*args, **kwargs)
return ret
except exception_type as e:
error_lines = str(e).split("\n")
if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
print("Exception is caught, retrying...")
# when pattern is not specified, we always skip the exception
# when pattern is specified, we only skip when pattern is matched
continue
else:
print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
>                   raise e
2025-04-11T04:23:18.8462193Z
colossalai/testing/utils.py:144:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
colossalai/testing/utils.py:133: in _run_until_success
ret = func(*args, **kwargs)
tests/test_pipeline/test_stage_manager.py:74: in test_pipeline_stage_manager
spawn(run_dist, 4)
colossalai/testing/utils.py:252: in spawn
mp.spawn(wrapped_func, nprocs=nprocs)
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
while not context.join():
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.8464012Z
self = <torch.multiprocessing.spawn.ProcessContext object at 0x7fb5b5f09de0>
timeout = None
2025-04-11T04:23:18.8464347Z
def join(self, timeout=None):
r"""Join one or more processes within spawn context.
2025-04-11T04:23:18.8464634Z
Attempt to join one or more processes in this spawn context.
If one of them exited with a non-zero exit status, this function
kills the remaining processes and raises an exception with the cause
of the first process exiting.
2025-04-11T04:23:18.8465251Z
Returns ``True`` if all processes have been joined successfully,
``False`` if there are more processes that need to be joined.
2025-04-11T04:23:18.8465602Z
Args:
timeout (float): Wait this long before giving up on waiting.
"""
# Ensure this function can be called even when we're done.
if len(self.sentinels) == 0:
return True
2025-04-11T04:23:18.8466324Z
# Wait for any process to fail or all of them to succeed.
ready = multiprocessing.connection.wait(
self.sentinels.keys(),
timeout=timeout,
)
2025-04-11T04:23:18.8466938Z
error_index = None
for sentinel in ready:
index = self.sentinels.pop(sentinel)
process = self.processes[index]
process.join()
if process.exitcode != 0:
error_index = index
break
2025-04-11T04:23:18.8467722Z
# Return if there was no error.
if error_index is None:
# Return whether or not all processes have been joined.
return len(self.sentinels) == 0
2025-04-11T04:23:18.8468245Z
# Assume failure. Terminate processes that are still alive.
for process in self.processes:
if process.is_alive():
process.terminate()
process.join()
2025-04-11T04:23:18.8468859Z
# There won't be an error on the queue if the process crashed.
failed_process = self.processes[error_index]
if self.error_queues[error_index].empty():
exitcode = self.processes[error_index].exitcode
if exitcode < 0:
name = signal.Signals(-exitcode).name
raise ProcessExitedException(
"process %d terminated with signal %s" % (error_index, name),
error_index=error_index,
error_pid=failed_process.pid,
exit_code=exitcode,
signal_name=name,
)
else:
raise ProcessExitedException(
"process %d terminated with exit code %d" % (error_index, exitcode),
error_index=error_index,
error_pid=failed_process.pid,
exit_code=exitcode,
)
2025-04-11T04:23:18.8471054Z
original_trace = self.error_queues[error_index].get()
msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
msg += original_trace
>       raise ProcessRaisedException(msg, error_index, failed_process.pid)
E       torch.multiprocessing.spawn.ProcessRaisedException:
E
E       -- Process 0 terminated with the following error:
E       Traceback (most recent call last):
E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
E           fn(i, *args)
E         File "/__w/ColossalAI/ColossalAI/tests/test_pipeline/test_stage_manager.py", line 68, in run_dist
E           check_stage_manager()
E         File "/__w/ColossalAI/ColossalAI/tests/test_pipeline/test_stage_manager.py", line 56, in check_stage_manager
E           dist.barrier(group=group)
E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/distributed/c10d_logger.py", line 72, in wrapper
E           return func(*args, **kwargs)
E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py", line 3441, in barrier
E           work = group.barrier(opts=opts)
E       RuntimeError: CUDA error: out of memory
E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.8474845Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
----------------------------- Captured stdout call -----------------------------
[04/11/25 04:18:39] INFO     colossalai - colossalai - INFO:
/__w/ColossalAI/ColossalAI/colossalai/initialize.py
:75 launch
INFO     colossalai - colossalai - INFO: Distributed
environment is initialized, world size: 4
Maximum number of attempts is reached or pattern is not matched, no more retrying...
----------------------------- Captured stderr call -----------------------------
[W socket.cpp:697] [c10d] The client socket has failed to connect to [localhost]:26717 (errno: 99 - Cannot assign requested address).
[W socket.cpp:697] [c10d] The client socket has failed to connect to [localhost]:26717 (errno: 99 - Cannot assign requested address).
_______________________________ test_pp[2-12-4] ________________________________
2025-04-11T04:23:18.8477055Z
args = ()
kwargs = {'batch_size': 12, 'num_microbatch': 4, 'num_model_chunk': 2}
try_count = 1
error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T04:23:18.8477998Z
def _run_until_success(*args, **kwargs):
try_count = 0
assert max_try is None or isinstance(
max_try, int
), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T04:23:18.8478577Z
while max_try is None or try_count < max_try:
try:
try_count += 1
ret = func(*args, **kwargs)
return ret
except exception_type as e:
error_lines = str(e).split("\n")
if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
print("Exception is caught, retrying...")
# when pattern is not specified, we always skip the exception
# when pattern is specified, we only skip when pattern is matched
continue
else:
print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
>                   raise e
2025-04-11T04:23:18.8480431Z
colossalai/testing/utils.py:144:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
colossalai/testing/utils.py:133: in _run_until_success
ret = func(*args, **kwargs)
tests/test_pipeline/test_schedule/test_interleaved.py:156: in test_pp
spawn(
colossalai/testing/utils.py:252: in spawn
mp.spawn(wrapped_func, nprocs=nprocs)
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
while not context.join():
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.8482286Z
self = <torch.multiprocessing.spawn.ProcessContext object at 0x7fb5b5f09cf0>
timeout = None
2025-04-11T04:23:18.8482566Z
def join(self, timeout=None):
r"""Join one or more processes within spawn context.
2025-04-11T04:23:18.8482851Z
Attempt to join one or more processes in this spawn context.
If one of them exited with a non-zero exit status, this function
kills the remaining processes and raises an exception with the cause
of the first process exiting.
2025-04-11T04:23:18.8483516Z
Returns ``True`` if all processes have been joined successfully,
``False`` if there are more processes that need to be joined.
2025-04-11T04:23:18.8483873Z
Args:
timeout (float): Wait this long before giving up on waiting.
"""
# Ensure this function can be called even when we're done.
if len(self.sentinels) == 0:
return True
2025-04-11T04:23:18.8484536Z
# Wait for any process to fail or all of them to succeed.
ready = multiprocessing.connection.wait(
self.sentinels.keys(),
timeout=timeout,
)
2025-04-11T04:23:18.8485093Z
error_index = None
for sentinel in ready:
index = self.sentinels.pop(sentinel)
process = self.processes[index]
process.join()
if process.exitcode != 0:
error_index = index
break
2025-04-11T04:23:18.8485928Z
# Return if there was no error.
if error_index is None:
# Return whether or not all processes have been joined.
return len(self.sentinels) == 0
2025-04-11T04:23:18.8486448Z
# Assume failure. Terminate processes that are still alive.
for process in self.processes:
if process.is_alive():
process.terminate()
process.join()
2025-04-11T04:23:18.8487012Z
# There won't be an error on the queue if the process crashed.
failed_process = self.processes[error_index]
if self.error_queues[error_index].empty():
exitcode = self.processes[error_index].exitcode
if exitcode < 0:
name = signal.Signals(-exitcode).name
raise ProcessExitedException(
"process %d terminated with signal %s" % (error_index, name),
error_index=error_index,
error_pid=failed_process.pid,
exit_code=exitcode,
signal_name=name,
)
else:
raise ProcessExitedException(
"process %d terminated with exit code %d" % (error_index, exitcode),
error_index=error_index,
error_pid=failed_process.pid,
exit_code=exitcode,
)
2025-04-11T04:23:18.8489202Z
original_trace = self.error_queues[error_index].get()
msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
msg += original_trace
>       raise ProcessRaisedException(msg, error_index, failed_process.pid)
E       torch.multiprocessing.spawn.ProcessRaisedException:
E
E       -- Process 0 terminated with the following error:
E       Traceback (most recent call last):
E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
E           fn(i, *args)
E         File "/__w/ColossalAI/ColossalAI/tests/test_pipeline/test_schedule/test_interleaved.py", line 66, in run_pp
E           torch_model = MlpModel().cuda()
E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in cuda
E           return self._apply(lambda t: t.cuda(device))
E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
E           module._apply(fn)
E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
E           module._apply(fn)
E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
E           param_applied = fn(param)
E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in <lambda>
E           return self._apply(lambda t: t.cuda(device))
E       RuntimeError: CUDA error: out of memory
E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.8493705Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
----------------------------- Captured stdout call -----------------------------
[04/11/25 04:18:45] INFO     colossalai - colossalai - INFO:
/__w/ColossalAI/ColossalAI/colossalai/initialize.py
:75 launch
INFO     colossalai - colossalai - INFO: Distributed
environment is initialized, world size: 4
Maximum number of attempts is reached or pattern is not matched, no more retrying...
_______________________________ test_pp[2-12-12] _______________________________
2025-04-11T04:23:18.8495147Z
args = ()
kwargs = {'batch_size': 12, 'num_microbatch': 12, 'num_model_chunk': 2}
try_count = 1
error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T04:23:18.8496055Z
def _run_until_success(*args, **kwargs):
try_count = 0
assert max_try is None or isinstance(
max_try, int
), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T04:23:18.8496687Z
while max_try is None or try_count < max_try:
try:
try_count += 1
ret = func(*args, **kwargs)
return ret
except exception_type as e:
error_lines = str(e).split("\n")
if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
print("Exception is caught, retrying...")
# when pattern is not specified, we always skip the exception
# when pattern is specified, we only skip when pattern is matched
continue
else:
print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
>                   raise e
2025-04-11T04:23:18.8498439Z
colossalai/testing/utils.py:144:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
colossalai/testing/utils.py:133: in _run_until_success
ret = func(*args, **kwargs)
tests/test_pipeline/test_schedule/test_interleaved.py:156: in test_pp
spawn(
colossalai/testing/utils.py:252: in spawn
mp.spawn(wrapped_func, nprocs=nprocs)
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
while not context.join():
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.8500320Z
self = <torch.multiprocessing.spawn.ProcessContext object at 0x7fb5b5ad28f0>
timeout = None
2025-04-11T04:23:18.8500594Z
def join(self, timeout=None):
r"""Join one or more processes within spawn context.
2025-04-11T04:23:18.8500881Z
Attempt to join one or more processes in this spawn context.
If one of them exited with a non-zero exit status, this function
kills the remaining processes and raises an exception with the cause
of the first process exiting.
2025-04-11T04:23:18.8501493Z
Returns ``True`` if all processes have been joined successfully,
``False`` if there are more processes that need to be joined.
2025-04-11T04:23:18.8501843Z
Args:
timeout (float): Wait this long before giving up on waiting.
"""
# Ensure this function can be called even when we're done.
if len(self.sentinels) == 0:
return True
2025-04-11T04:23:18.8502556Z
# Wait for any process to fail or all of them to succeed.
ready = multiprocessing.connection.wait(
self.sentinels.keys(),
timeout=timeout,
)
2025-04-11T04:23:18.8503123Z
error_index = None
for sentinel in ready:
index = self.sentinels.pop(sentinel)
process = self.processes[index]
process.join()
if process.exitcode != 0:
error_index = index
break
2025-04-11T04:23:18.8503901Z
# Return if there was no error.
if error_index is None:
# Return whether or not all processes have been joined.
return len(self.sentinels) == 0
2025-04-11T04:23:18.8504429Z
# Assume failure. Terminate processes that are still alive.
for process in self.processes:
if process.is_alive():
process.terminate()
process.join()
2025-04-11T04:23:18.8504993Z
# There won't be an error on the queue if the process crashed.
failed_process = self.processes[error_index]
if self.error_queues[error_index].empty():
exitcode = self.processes[error_index].exitcode
if exitcode < 0:
name = signal.Signals(-exitcode).name
raise ProcessExitedException(
"process %d terminated with signal %s" % (error_index, name),
error_index=error_index,
error_pid=failed_process.pid,
exit_code=exitcode,
signal_name=name,
)
else:
raise ProcessExitedException(
"process %d terminated with exit code %d" % (error_index, exitcode),
error_index=error_index,
error_pid=failed_process.pid,
exit_code=exitcode,
)
2025-04-11T04:23:18.8507220Z
original_trace = self.error_queues[error_index].get()
msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
msg += original_trace
>       raise ProcessRaisedException(msg, error_index, failed_process.pid)
E       torch.multiprocessing.spawn.ProcessRaisedException:
E
E       -- Process 0 terminated with the following error:
E       Traceback (most recent call last):
E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
E           fn(i, *args)
E         File "/__w/ColossalAI/ColossalAI/tests/test_pipeline/test_schedule/test_interleaved.py", line 66, in run_pp
E           torch_model = MlpModel().cuda()
E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in cuda
E           return self._apply(lambda t: t.cuda(device))
E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
E           module._apply(fn)
E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
E           module._apply(fn)
E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
E           param_applied = fn(param)
E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in <lambda>
E           return self._apply(lambda t: t.cuda(device))
E       RuntimeError: CUDA error: out of memory
E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.8511884Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
----------------------------- Captured stdout call -----------------------------
[04/11/25 04:18:50] INFO     colossalai - colossalai - INFO:
/__w/ColossalAI/ColossalAI/colossalai/initialize.py
:75 launch
INFO     colossalai - colossalai - INFO: Distributed
environment is initialized, world size: 4
Maximum number of attempts is reached or pattern is not matched, no more retrying...
_______________________________ test_pp[4-12-4] ________________________________
2025-04-11T04:23:18.8513402Z
args = ()
kwargs = {'batch_size': 12, 'num_microbatch': 4, 'num_model_chunk': 4}
try_count = 1
error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T04:23:18.8514322Z
def _run_until_success(*args, **kwargs):
try_count = 0
assert max_try is None or isinstance(
max_try, int
), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T04:23:18.8514910Z
while max_try is None or try_count < max_try:
try:
try_count += 1
ret = func(*args, **kwargs)
return ret
except exception_type as e:
error_lines = str(e).split("\n")
if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
print("Exception is caught, retrying...")
# when pattern is not specified, we always skip the exception
# when pattern is specified, we only skip when pattern is matched
continue
else:
print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
>                   raise e
2025-04-11T04:23:18.8516676Z
colossalai/testing/utils.py:144:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
colossalai/testing/utils.py:133: in _run_until_success
ret = func(*args, **kwargs)
tests/test_pipeline/test_schedule/test_interleaved.py:156: in test_pp
spawn(
colossalai/testing/utils.py:252: in spawn
mp.spawn(wrapped_func, nprocs=nprocs)
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
while not context.join():
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.8518577Z
self = <torch.multiprocessing.spawn.ProcessContext object at 0x7fb5b5b3d810>
timeout = None
2025-04-11T04:23:18.8518860Z
def join(self, timeout=None):
r"""Join one or more processes within spawn context.
2025-04-11T04:23:18.8519190Z
Attempt to join one or more processes in this spawn context.
If one of them exited with a non-zero exit status, this function
kills the remaining processes and raises an exception with the cause
of the first process exiting.
2025-04-11T04:23:18.8519814Z
Returns ``True`` if all processes have been joined successfully,
``False`` if there are more processes that need to be joined.
2025-04-11T04:23:18.8520171Z
Args:
timeout (float): Wait this long before giving up on waiting.
"""
# Ensure this function can be called even when we're done.
if len(self.sentinels) == 0:
return True
2025-04-11T04:23:18.8520839Z
# Wait for any process to fail or all of them to succeed.
ready = multiprocessing.connection.wait(
self.sentinels.keys(),
timeout=timeout,
)
2025-04-11T04:23:18.8521414Z
error_index = None
for sentinel in ready:
index = self.sentinels.pop(sentinel)
process = self.processes[index]
process.join()
if process.exitcode != 0:
error_index = index
break
2025-04-11T04:23:18.8522254Z
# Return if there was no error.
if error_index is None:
# Return whether or not all processes have been joined.
return len(self.sentinels) == 0
2025-04-11T04:23:18.8522737Z
# Assume failure. Terminate processes that are still alive.
for process in self.processes:
if process.is_alive():
process.terminate()
process.join()
2025-04-11T04:23:18.8523307Z
# There won't be an error on the queue if the process crashed.
failed_process = self.processes[error_index]
if self.error_queues[error_index].empty():
exitcode = self.processes[error_index].exitcode
if exitcode < 0:
name = signal.Signals(-exitcode).name
raise ProcessExitedException(
"process %d terminated with signal %s" % (error_index, name),
error_index=error_index,
error_pid=failed_process.pid,
exit_code=exitcode,
signal_name=name,
)
else:
raise ProcessExitedException(
"process %d terminated with exit code %d" % (error_index, exitcode),
error_index=error_index,
error_pid=failed_process.pid,
exit_code=exitcode,
)
2025-04-11T04:23:18.8525601Z
original_trace = self.error_queues[error_index].get()
msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
msg += original_trace
>       raise ProcessRaisedException(msg, error_index, failed_process.pid)
E       torch.multiprocessing.spawn.ProcessRaisedException:
E
E       -- Process 0 terminated with the following error:
E       Traceback (most recent call last):
E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
E           fn(i, *args)
E         File "/__w/ColossalAI/ColossalAI/tests/test_pipeline/test_schedule/test_interleaved.py", line 66, in run_pp
E           torch_model = MlpModel().cuda()
E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in cuda
E           return self._apply(lambda t: t.cuda(device))
E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
E           module._apply(fn)
E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
E           module._apply(fn)
E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
E           param_applied = fn(param)
E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in <lambda>
E           return self._apply(lambda t: t.cuda(device))
E       RuntimeError: CUDA error: out of memory
E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.8529998Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
----------------------------- Captured stdout call -----------------------------
[04/11/25 04:18:55] INFO     colossalai - colossalai - INFO:
/__w/ColossalAI/ColossalAI/colossalai/initialize.py
:75 launch
INFO     colossalai - colossalai - INFO: Distributed
environment is initialized, world size: 2
Maximum number of attempts is reached or pattern is not matched, no more retrying...
_______________________________ test_pp[4-12-12] _______________________________
2025-04-11T04:23:18.8531535Z
args = ()
kwargs = {'batch_size': 12, 'num_microbatch': 12, 'num_model_chunk': 4}
try_count = 1
error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T04:23:18.8532500Z
def _run_until_success(*args, **kwargs):
try_count = 0
assert max_try is None or isinstance(
max_try, int
), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T04:23:18.8533081Z
while max_try is None or try_count < max_try:
try:
try_count += 1
ret = func(*args, **kwargs)
return ret
except exception_type as e:
error_lines = str(e).split("\n")
if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
print("Exception is caught, retrying...")
# when pattern is not specified, we always skip the exception
# when pattern is specified, we only skip when pattern is matched
continue
else:
print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
>                   raise e
2025-04-11T04:23:18.8534832Z
colossalai/testing/utils.py:144:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
colossalai/testing/utils.py:133: in _run_until_success
ret = func(*args, **kwargs)
tests/test_pipeline/test_schedule/test_interleaved.py:156: in test_pp
spawn(
colossalai/testing/utils.py:252: in spawn
mp.spawn(wrapped_func, nprocs=nprocs)
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
while not context.join():
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.8536670Z
self = <torch.multiprocessing.spawn.ProcessContext object at 0x7fb5b42475e0>
timeout = None
2025-04-11T04:23:18.8536954Z
def join(self, timeout=None):
r"""Join one or more processes within spawn context.
2025-04-11T04:23:18.8537286Z
Attempt to join one or more processes in this spawn context.
If one of them exited with a non-zero exit status, this function
kills the remaining processes and raises an exception with the cause
of the first process exiting.
2025-04-11T04:23:18.8537900Z
Returns ``True`` if all processes have been joined successfully,
``False`` if there are more processes that need to be joined.
2025-04-11T04:23:18.8538303Z
Args:
timeout (float): Wait this long before giving up on waiting.
"""
# Ensure this function can be called even when we're done.
if len(self.sentinels) == 0:
return True
2025-04-11T04:23:18.8538967Z
# Wait for any process to fail or all of them to succeed.
ready = multiprocessing.connection.wait(
self.sentinels.keys(),
timeout=timeout,
)
2025-04-11T04:23:18.8539524Z
error_index = None
for sentinel in ready:
index = self.sentinels.pop(sentinel)
process = self.processes[index]
process.join()
if process.exitcode != 0:
error_index = index
break
2025-04-11T04:23:18.8540297Z
# Return if there was no error.
if error_index is None:
# Return whether or not all processes have been joined.
return len(self.sentinels) == 0
2025-04-11T04:23:18.8540830Z
# Assume failure. Terminate processes that are still alive.
for process in self.processes:
if process.is_alive():
process.terminate()
process.join()
2025-04-11T04:23:18.8541397Z
# There won't be an error on the queue if the process crashed.
failed_process = self.processes[error_index]
if self.error_queues[error_index].empty():
exitcode = self.processes[error_index].exitcode
if exitcode < 0:
name = signal.Signals(-exitcode).name
raise ProcessExitedException(
"process %d terminated with signal %s" % (error_index, name),
error_index=error_index,
error_pid=failed_process.pid,
exit_code=exitcode,
signal_name=name,
)
else:
raise ProcessExitedException(
"process %d terminated with exit code %d" % (error_index, exitcode),
error_index=error_index,
error_pid=failed_process.pid,
exit_code=exitcode,
)
2025-04-11T04:23:18.8543620Z
original_trace = self.error_queues[error_index].get()
msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
msg += original_trace
>       raise ProcessRaisedException(msg, error_index, failed_process.pid)
E       torch.multiprocessing.spawn.ProcessRaisedException:
E
E       -- Process 0 terminated with the following error:
E       Traceback (most recent call last):
E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
E           fn(i, *args)
E         File "/__w/ColossalAI/ColossalAI/tests/test_pipeline/test_schedule/test_interleaved.py", line 66, in run_pp
E           torch_model = MlpModel().cuda()
E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in cuda
E           return self._apply(lambda t: t.cuda(device))
E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
E           module._apply(fn)
E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
E           module._apply(fn)
E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
E           param_applied = fn(param)
E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in <lambda>
E           return self._apply(lambda t: t.cuda(device))
E       RuntimeError: CUDA error: out of memory
E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.8548015Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
----------------------------- Captured stdout call -----------------------------
[04/11/25 04:19:00] INFO     colossalai - colossalai - INFO:
/__w/ColossalAI/ColossalAI/colossalai/initialize.py
:75 launch
INFO     colossalai - colossalai - INFO: Distributed
environment is initialized, world size: 2
Maximum number of attempts is reached or pattern is not matched, no more retrying...
----------------------------- Captured stderr call -----------------------------
[W socket.cpp:697] [c10d] The client socket has failed to connect to [localhost]:47696 (errno: 99 - Cannot assign requested address).
_______________________________ test_pp[2-12-4] ________________________________
2025-04-11T04:23:18.8550069Z
args = (), kwargs = {'batch_size': 12, 'num_microbatch': 4, 'world_size': 2}
try_count = 1
error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T04:23:18.8550916Z
def _run_until_success(*args, **kwargs):
try_count = 0
assert max_try is None or isinstance(
max_try, int
), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T04:23:18.8551543Z
while max_try is None or try_count < max_try:
try:
try_count += 1
ret = func(*args, **kwargs)
return ret
except exception_type as e:
error_lines = str(e).split("\n")
if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
print("Exception is caught, retrying...")
# when pattern is not specified, we always skip the exception
# when pattern is specified, we only skip when pattern is matched
continue
else:
print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
>                   raise e
2025-04-11T04:23:18.8553244Z
colossalai/testing/utils.py:144:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
colossalai/testing/utils.py:133: in _run_until_success
ret = func(*args, **kwargs)
tests/test_pipeline/test_schedule/test_oneF_oneB.py:163: in test_pp
spawn(
colossalai/testing/utils.py:252: in spawn
mp.spawn(wrapped_func, nprocs=nprocs)
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
while not context.join():
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.8555086Z
self = <torch.multiprocessing.spawn.ProcessContext object at 0x7fb5b5b3e200>
timeout = None
2025-04-11T04:23:18.8555362Z
def join(self, timeout=None):
r"""Join one or more processes within spawn context.
2025-04-11T04:23:18.8555695Z
Attempt to join one or more processes in this spawn context.
If one of them exited with a non-zero exit status, this function
kills the remaining processes and raises an exception with the cause
of the first process exiting.
2025-04-11T04:23:18.8556355Z
Returns ``True`` if all processes have been joined successfully,
``False`` if there are more processes that need to be joined.
2025-04-11T04:23:18.8556713Z
Args:
timeout (float): Wait this long before giving up on waiting.
"""
# Ensure this function can be called even when we're done.
if len(self.sentinels) == 0:
return True
2025-04-11T04:23:18.8557424Z
# Wait for any process to fail or all of them to succeed.
ready = multiprocessing.connection.wait(
self.sentinels.keys(),
timeout=timeout,
)
2025-04-11T04:23:18.8557990Z
error_index = None
for sentinel in ready:
index = self.sentinels.pop(sentinel)
process = self.processes[index]
process.join()
if process.exitcode != 0:
error_index = index
break
2025-04-11T04:23:18.8558766Z
# Return if there was no error.
if error_index is None:
# Return whether or not all processes have been joined.
return len(self.sentinels) == 0
2025-04-11T04:23:18.8559237Z
# Assume failure. Terminate processes that are still alive.
for process in self.processes:
if process.is_alive():
process.terminate()
process.join()
2025-04-11T04:23:18.8559794Z
# There won't be an error on the queue if the process crashed.
failed_process = self.processes[error_index]
if self.error_queues[error_index].empty():
exitcode = self.processes[error_index].exitcode
if exitcode < 0:
name = signal.Signals(-exitcode).name
raise ProcessExitedException(
"process %d terminated with signal %s" % (error_index, name),
error_index=error_index,
error_pid=failed_process.pid,
exit_code=exitcode,
signal_name=name,
)
else:
raise ProcessExitedException(
"process %d terminated with exit code %d" % (error_index, exitcode),
error_index=error_index,
error_pid=failed_process.pid,
exit_code=exitcode,
)
2025-04-11T04:23:18.8562030Z
original_trace = self.error_queues[error_index].get()
msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
msg += original_trace
>       raise ProcessRaisedException(msg, error_index, failed_process.pid)
E       torch.multiprocessing.spawn.ProcessRaisedException:
E
E       -- Process 0 terminated with the following error:
E       Traceback (most recent call last):
E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
E           fn(i, *args)
E         File "/__w/ColossalAI/ColossalAI/tests/test_pipeline/test_schedule/test_oneF_oneB.py", line 153, in run_dist
E           examine_pp(num_microbatch, batch_size)
E         File "/__w/ColossalAI/ColossalAI/tests/test_pipeline/test_schedule/test_oneF_oneB.py", line 58, in examine_pp
E           torch_model = MlpModel().cuda()
E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in cuda
E           return self._apply(lambda t: t.cuda(device))
E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
E           module._apply(fn)
E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
E           module._apply(fn)
E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
E           param_applied = fn(param)
E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in <lambda>
E           return self._apply(lambda t: t.cuda(device))
E       RuntimeError: CUDA error: out of memory
E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.8566848Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
----------------------------- Captured stdout call -----------------------------
[04/11/25 04:19:05] INFO     colossalai - colossalai - INFO:
/__w/ColossalAI/ColossalAI/colossalai/initialize.py
:75 launch
INFO     colossalai - colossalai - INFO: Distributed
environment is initialized, world size: 2
Maximum number of attempts is reached or pattern is not matched, no more retrying...
----------------------------- Captured stderr call -----------------------------
[W socket.cpp:697] [c10d] The client socket has failed to connect to [localhost]:42298 (errno: 99 - Cannot assign requested address).
_______________________________ test_pp[2-12-6] ________________________________
2025-04-11T04:23:18.8568790Z
args = (), kwargs = {'batch_size': 12, 'num_microbatch': 6, 'world_size': 2}
try_count = 1
error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T04:23:18.8569683Z
def _run_until_success(*args, **kwargs):
try_count = 0
assert max_try is None or isinstance(
max_try, int
), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T04:23:18.8570305Z
while max_try is None or try_count < max_try:
try:
try_count += 1
ret = func(*args, **kwargs)
return ret
except exception_type as e:
error_lines = str(e).split("\n")
if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
print("Exception is caught, retrying...")
# when pattern is not specified, we always skip the exception
# when pattern is specified, we only skip when pattern is matched
continue
else:
print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
>                   raise e
2025-04-11T04:23:18.8572011Z
colossalai/testing/utils.py:144:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
colossalai/testing/utils.py:133: in _run_until_success
ret = func(*args, **kwargs)
tests/test_pipeline/test_schedule/test_oneF_oneB.py:163: in test_pp
spawn(
colossalai/testing/utils.py:252: in spawn
mp.spawn(wrapped_func, nprocs=nprocs)
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
while not context.join():
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.8573854Z
self = <torch.multiprocessing.spawn.ProcessContext object at 0x7fb5b4245e40>
timeout = None
2025-04-11T04:23:18.8574134Z
def join(self, timeout=None):
r"""Join one or more processes within spawn context.
2025-04-11T04:23:18.8574419Z
Attempt to join one or more processes in this spawn context.
If one of them exited with a non-zero exit status, this function
kills the remaining processes and raises an exception with the cause
of the first process exiting.
2025-04-11T04:23:18.8575088Z
Returns ``True`` if all processes have been joined successfully,
``False`` if there are more processes that need to be joined.
2025-04-11T04:23:18.8575490Z
Args:
timeout (float): Wait this long before giving up on waiting.
"""
# Ensure this function can be called even when we're done.
if len(self.sentinels) == 0:
return True
2025-04-11T04:23:18.8576153Z
# Wait for any process to fail or all of them to succeed.
ready = multiprocessing.connection.wait(
self.sentinels.keys(),
timeout=timeout,
)
2025-04-11T04:23:18.8576780Z
error_index = None
for sentinel in ready:
index = self.sentinels.pop(sentinel)
process = self.processes[index]
process.join()
if process.exitcode != 0:
error_index = index
break
2025-04-11T04:23:18.8577557Z
# Return if there was no error.
if error_index is None:
# Return whether or not all processes have been joined.
return len(self.sentinels) == 0
2025-04-11T04:23:18.8578029Z
# Assume failure. Terminate processes that are still alive.
for process in self.processes:
if process.is_alive():
process.terminate()
process.join()
2025-04-11T04:23:18.8578596Z
# There won't be an error on the queue if the process crashed.
failed_process = self.processes[error_index]
if self.error_queues[error_index].empty():
exitcode = self.processes[error_index].exitcode
if exitcode < 0:
name = signal.Signals(-exitcode).name
raise ProcessExitedException(
"process %d terminated with signal %s" % (error_index, name),
error_index=error_index,
error_pid=failed_process.pid,
exit_code=exitcode,
signal_name=name,
)
else:
raise ProcessExitedException(
"process %d terminated with exit code %d" % (error_index, exitcode),
error_index=error_index,
error_pid=failed_process.pid,
exit_code=exitcode,
)
2025-04-11T04:23:18.8580810Z
original_trace = self.error_queues[error_index].get()
msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
msg += original_trace
>       raise ProcessRaisedException(msg, error_index, failed_process.pid)
E       torch.multiprocessing.spawn.ProcessRaisedException:
E
E       -- Process 0 terminated with the following error:
E       Traceback (most recent call last):
E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
E           fn(i, *args)
E         File "/__w/ColossalAI/ColossalAI/tests/test_pipeline/test_schedule/test_oneF_oneB.py", line 153, in run_dist
E           examine_pp(num_microbatch, batch_size)
E         File "/__w/ColossalAI/ColossalAI/tests/test_pipeline/test_schedule/test_oneF_oneB.py", line 58, in examine_pp
E           torch_model = MlpModel().cuda()
E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in cuda
E           return self._apply(lambda t: t.cuda(device))
E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
E           module._apply(fn)
E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
E           module._apply(fn)
E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
E           param_applied = fn(param)
E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in <lambda>
E           return self._apply(lambda t: t.cuda(device))
E       RuntimeError: CUDA error: out of memory
E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.8585611Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
----------------------------- Captured stdout call -----------------------------
[04/11/25 04:19:09] INFO     colossalai - colossalai - INFO:
/__w/ColossalAI/ColossalAI/colossalai/initialize.py
:75 launch
INFO     colossalai - colossalai - INFO: Distributed
environment is initialized, world size: 2
Maximum number of attempts is reached or pattern is not matched, no more retrying...
_______________________________ test_pp[4-12-4] ________________________________
2025-04-11T04:23:18.8587061Z
args = (), kwargs = {'batch_size': 12, 'num_microbatch': 4, 'world_size': 4}
try_count = 1
error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T04:23:18.8587952Z
def _run_until_success(*args, **kwargs):
try_count = 0
assert max_try is None or isinstance(
max_try, int
), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T04:23:18.8588605Z
while max_try is None or try_count < max_try:
try:
try_count += 1
ret = func(*args, **kwargs)
return ret
except exception_type as e:
error_lines = str(e).split("\n")
if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
print("Exception is caught, retrying...")
# when pattern is not specified, we always skip the exception
# when pattern is specified, we only skip when pattern is matched
continue
else:
print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
>                   raise e
2025-04-11T04:23:18.8590360Z
colossalai/testing/utils.py:144:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
colossalai/testing/utils.py:133: in _run_until_success
ret = func(*args, **kwargs)
tests/test_pipeline/test_schedule/test_oneF_oneB.py:163: in test_pp
spawn(
colossalai/testing/utils.py:252: in spawn
mp.spawn(wrapped_func, nprocs=nprocs)
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
while not context.join():
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.8592205Z
self = <torch.multiprocessing.spawn.ProcessContext object at 0x7fb5b5b3f640>
timeout = None
2025-04-11T04:23:18.8592482Z
def join(self, timeout=None):
r"""Join one or more processes within spawn context.
2025-04-11T04:23:18.8592769Z
Attempt to join one or more processes in this spawn context.
If one of them exited with a non-zero exit status, this function
kills the remaining processes and raises an exception with the cause
of the first process exiting.
2025-04-11T04:23:18.8593382Z
Returns ``True`` if all processes have been joined successfully,
``False`` if there are more processes that need to be joined.
2025-04-11T04:23:18.8593785Z
Args:
timeout (float): Wait this long before giving up on waiting.
"""
# Ensure this function can be called even when we're done.
if len(self.sentinels) == 0:
return True
2025-04-11T04:23:18.8594506Z
# Wait for any process to fail or all of them to succeed.
ready = multiprocessing.connection.wait(
self.sentinels.keys(),
timeout=timeout,
)
2025-04-11T04:23:18.8595069Z
error_index = None
for sentinel in ready:
index = self.sentinels.pop(sentinel)
process = self.processes[index]
process.join()
if process.exitcode != 0:
error_index = index
break
2025-04-11T04:23:18.8595894Z
# Return if there was no error.
if error_index is None:
# Return whether or not all processes have been joined.
return len(self.sentinels) == 0
2025-04-11T04:23:18.8596365Z
# Assume failure. Terminate processes that are still alive.
for process in self.processes:
if process.is_alive():
process.terminate()
process.join()
2025-04-11T04:23:18.8596934Z
# There won't be an error on the queue if the process crashed.
failed_process = self.processes[error_index]
if self.error_queues[error_index].empty():
exitcode = self.processes[error_index].exitcode
if exitcode < 0:
name = signal.Signals(-exitcode).name
raise ProcessExitedException(
"process %d terminated with signal %s" % (error_index, name),
error_index=error_index,
error_pid=failed_process.pid,
exit_code=exitcode,
signal_name=name,
)
else:
raise ProcessExitedException(
"process %d terminated with exit code %d" % (error_index, exitcode),
error_index=error_index,
error_pid=failed_process.pid,
exit_code=exitcode,
)
2025-04-11T04:23:18.8599116Z
original_trace = self.error_queues[error_index].get()
msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
msg += original_trace
>       raise ProcessRaisedException(msg, error_index, failed_process.pid)
E       torch.multiprocessing.spawn.ProcessRaisedException:
E
E       -- Process 0 terminated with the following error:
E       Traceback (most recent call last):
E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
E           fn(i, *args)
E         File "/__w/ColossalAI/ColossalAI/tests/test_pipeline/test_schedule/test_oneF_oneB.py", line 153, in run_dist
E           examine_pp(num_microbatch, batch_size)
E         File "/__w/ColossalAI/ColossalAI/tests/test_pipeline/test_schedule/test_oneF_oneB.py", line 58, in examine_pp
E           torch_model = MlpModel().cuda()
E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in cuda
E           return self._apply(lambda t: t.cuda(device))
E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
E           module._apply(fn)
E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
E           module._apply(fn)
E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
E           param_applied = fn(param)
E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in <lambda>
E           return self._apply(lambda t: t.cuda(device))
E       RuntimeError: CUDA error: out of memory
E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.8603978Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
----------------------------- Captured stdout call -----------------------------
[04/11/25 04:19:15] INFO     colossalai - colossalai - INFO:
/__w/ColossalAI/ColossalAI/colossalai/initialize.py
:75 launch
INFO     colossalai - colossalai - INFO: Distributed
environment is initialized, world size: 4
Maximum number of attempts is reached or pattern is not matched, no more retrying...
----------------------------- Captured stderr call -----------------------------
[W socket.cpp:697] [c10d] The client socket has failed to connect to [localhost]:30189 (errno: 99 - Cannot assign requested address).
_______________________________ test_pp[4-12-6] ________________________________
2025-04-11T04:23:18.8605912Z
args = (), kwargs = {'batch_size': 12, 'num_microbatch': 6, 'world_size': 4}
try_count = 1
error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T04:23:18.8606806Z
def _run_until_success(*args, **kwargs):
try_count = 0
assert max_try is None or isinstance(
max_try, int
), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T04:23:18.8607416Z
while max_try is None or try_count < max_try:
try:
try_count += 1
ret = func(*args, **kwargs)
return ret
except exception_type as e:
error_lines = str(e).split("\n")
if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
print("Exception is caught, retrying...")
# when pattern is not specified, we always skip the exception
# when pattern is specified, we only skip when pattern is matched
continue
else:
print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
>                   raise e
2025-04-11T04:23:18.8609184Z
colossalai/testing/utils.py:144:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
colossalai/testing/utils.py:133: in _run_until_success
ret = func(*args, **kwargs)
tests/test_pipeline/test_schedule/test_oneF_oneB.py:163: in test_pp
spawn(
colossalai/testing/utils.py:252: in spawn
mp.spawn(wrapped_func, nprocs=nprocs)
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
while not context.join():
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.8611009Z
self = <torch.multiprocessing.spawn.ProcessContext object at 0x7fb5b86920e0>
timeout = None
2025-04-11T04:23:18.8611290Z
def join(self, timeout=None):
r"""Join one or more processes within spawn context.
2025-04-11T04:23:18.8611571Z
Attempt to join one or more processes in this spawn context.
If one of them exited with a non-zero exit status, this function
kills the remaining processes and raises an exception with the cause
of the first process exiting.
2025-04-11T04:23:18.8612180Z
Returns ``True`` if all processes have been joined successfully,
``False`` if there are more processes that need to be joined.
2025-04-11T04:23:18.8612526Z
Args:
timeout (float): Wait this long before giving up on waiting.
"""
# Ensure this function can be called even when we're done.
if len(self.sentinels) == 0:
return True
2025-04-11T04:23:18.8613333Z
# Wait for any process to fail or all of them to succeed.
ready = multiprocessing.connection.wait(
self.sentinels.keys(),
timeout=timeout,
)
2025-04-11T04:23:18.8613945Z
error_index = None
for sentinel in ready:
index = self.sentinels.pop(sentinel)
process = self.processes[index]
process.join()
if process.exitcode != 0:
error_index = index
break
2025-04-11T04:23:18.8614764Z
# Return if there was no error.
if error_index is None:
# Return whether or not all processes have been joined.
return len(self.sentinels) == 0
2025-04-11T04:23:18.8615232Z
# Assume failure. Terminate processes that are still alive.
for process in self.processes:
if process.is_alive():
process.terminate()
process.join()
2025-04-11T04:23:18.8615789Z
# There won't be an error on the queue if the process crashed.
failed_process = self.processes[error_index]
if self.error_queues[error_index].empty():
exitcode = self.processes[error_index].exitcode
if exitcode < 0:
name = signal.Signals(-exitcode).name
raise ProcessExitedException(
"process %d terminated with signal %s" % (error_index, name),
error_index=error_index,
error_pid=failed_process.pid,
exit_code=exitcode,
signal_name=name,
)
else:
raise ProcessExitedException(
"process %d terminated with exit code %d" % (error_index, exitcode),
error_index=error_index,
error_pid=failed_process.pid,
exit_code=exitcode,
)
2025-04-11T04:23:18.8617952Z
original_trace = self.error_queues[error_index].get()
msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
msg += original_trace
>       raise ProcessRaisedException(msg, error_index, failed_process.pid)
E       torch.multiprocessing.spawn.ProcessRaisedException:
E
E       -- Process 0 terminated with the following error:
E       Traceback (most recent call last):
E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
E           fn(i, *args)
E         File "/__w/ColossalAI/ColossalAI/tests/test_pipeline/test_schedule/test_oneF_oneB.py", line 153, in run_dist
E           examine_pp(num_microbatch, batch_size)
E         File "/__w/ColossalAI/ColossalAI/tests/test_pipeline/test_schedule/test_oneF_oneB.py", line 58, in examine_pp
E           torch_model = MlpModel().cuda()
E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in cuda
E           return self._apply(lambda t: t.cuda(device))
E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
E           module._apply(fn)
E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
E           module._apply(fn)
E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
E           param_applied = fn(param)
E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in <lambda>
E           return self._apply(lambda t: t.cuda(device))
E       RuntimeError: CUDA error: out of memory
E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.8622769Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
----------------------------- Captured stdout call -----------------------------
[04/11/25 04:19:22] INFO     colossalai - colossalai - INFO:
/__w/ColossalAI/ColossalAI/colossalai/initialize.py
:75 launch
INFO     colossalai - colossalai - INFO: Distributed
environment is initialized, world size: 4
Maximum number of attempts is reached or pattern is not matched, no more retrying...
----------------------------- Captured stderr call -----------------------------
[W socket.cpp:697] [c10d] The client socket has failed to connect to [localhost]:34110 (errno: 99 - Cannot assign requested address).
[W socket.cpp:697] [c10d] The client socket has failed to connect to [localhost]:34110 (errno: 99 - Cannot assign requested address).
[W socket.cpp:697] [c10d] The client socket has failed to connect to [localhost]:34110 (errno: 99 - Cannot assign requested address).
___________________________________ test_pp ____________________________________
2025-04-11T04:23:18.8625251Z
args = (), kwargs = {}, try_count = 1
error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T04:23:18.8626005Z
def _run_until_success(*args, **kwargs):
try_count = 0
assert max_try is None or isinstance(
max_try, int
), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T04:23:18.8626615Z
while max_try is None or try_count < max_try:
try:
try_count += 1
ret = func(*args, **kwargs)
return ret
except exception_type as e:
error_lines = str(e).split("\n")
if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
print("Exception is caught, retrying...")
# when pattern is not specified, we always skip the exception
# when pattern is specified, we only skip when pattern is matched
continue
else:
print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
>                   raise e
2025-04-11T04:23:18.8628359Z
colossalai/testing/utils.py:144:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
colossalai/testing/utils.py:133: in _run_until_success
ret = func(*args, **kwargs)
tests/test_pipeline/test_schedule/test_zerobubble_pp.py:1077: in test_pp
spawn(
colossalai/testing/utils.py:252: in spawn
mp.spawn(wrapped_func, nprocs=nprocs)
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
while not context.join():
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.8630259Z
self = <torch.multiprocessing.spawn.ProcessContext object at 0x7fb5b4246350>
timeout = None
2025-04-11T04:23:18.8630540Z
def join(self, timeout=None):
r"""Join one or more processes within spawn context.
2025-04-11T04:23:18.8630829Z
Attempt to join one or more processes in this spawn context.
If one of them exited with a non-zero exit status, this function
kills the remaining processes and raises an exception with the cause
of the first process exiting.
2025-04-11T04:23:18.8631443Z
Returns ``True`` if all processes have been joined successfully,
``False`` if there are more processes that need to be joined.
2025-04-11T04:23:18.8631796Z
Args:
timeout (float): Wait this long before giving up on waiting.
"""
# Ensure this function can be called even when we're done.
if len(self.sentinels) == 0:
return True
2025-04-11T04:23:18.8632518Z
# Wait for any process to fail or all of them to succeed.
ready = multiprocessing.connection.wait(
self.sentinels.keys(),
timeout=timeout,
)
2025-04-11T04:23:18.8633135Z
error_index = None
for sentinel in ready:
index = self.sentinels.pop(sentinel)
process = self.processes[index]
process.join()
if process.exitcode != 0:
error_index = index
break
2025-04-11T04:23:18.8633966Z
# Return if there was no error.
if error_index is None:
# Return whether or not all processes have been joined.
return len(self.sentinels) == 0
2025-04-11T04:23:18.8634438Z
# Assume failure. Terminate processes that are still alive.
for process in self.processes:
if process.is_alive():
process.terminate()
process.join()
2025-04-11T04:23:18.8634999Z
# There won't be an error on the queue if the process crashed.
failed_process = self.processes[error_index]
if self.error_queues[error_index].empty():
exitcode = self.processes[error_index].exitcode
if exitcode < 0:
name = signal.Signals(-exitcode).name
raise ProcessExitedException(
"process %d terminated with signal %s" % (error_index, name),
error_index=error_index,
error_pid=failed_process.pid,
exit_code=exitcode,
signal_name=name,
)
else:
raise ProcessExitedException(
"process %d terminated with exit code %d" % (error_index, exitcode),
error_index=error_index,
error_pid=failed_process.pid,
exit_code=exitcode,
)
2025-04-11T04:23:18.8637166Z
original_trace = self.error_queues[error_index].get()
msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
msg += original_trace
>       raise ProcessRaisedException(msg, error_index, failed_process.pid)
E       torch.multiprocessing.spawn.ProcessRaisedException:
E
E       -- Process 0 terminated with the following error:
E       Traceback (most recent call last):
E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
E           fn(i, *args)
E         File "/__w/ColossalAI/ColossalAI/tests/test_pipeline/test_schedule/test_zerobubble_pp.py", line 1070, in run_dist
E           run_with_booster_moehybridplugin()
E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
E           partial_func(**kwargs)
E         File "/__w/ColossalAI/ColossalAI/tests/test_pipeline/test_schedule/test_zerobubble_pp.py", line 788, in run_with_booster_moehybridplugin
E           torch_model = MixtralModel(config).to(dtype).cuda()
E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2548, in cuda
E           return super().cuda(*args, **kwargs)
E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in cuda
E           return self._apply(lambda t: t.cuda(device))
E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
E           module._apply(fn)
E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
E           param_applied = fn(param)
E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in <lambda>
E           return self._apply(lambda t: t.cuda(device))
E       RuntimeError: CUDA error: out of memory
E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.8642567Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
----------------------------- Captured stdout call -----------------------------
[04/11/25 04:19:27] INFO     colossalai - colossalai - INFO:
/__w/ColossalAI/ColossalAI/colossalai/initialize.py
:75 launch
INFO     colossalai - colossalai - INFO: Distributed
environment is initialized, world size: 4
Maximum number of attempts is reached or pattern is not matched, no more retrying...
----------------------------- Captured stderr call -----------------------------
Flash Attention 2.0 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in MixtralModel is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `torch_dtype` argument. Example: `model = AutoModel.from_pretrained("openai/whisper-tiny", attn_implementation="flash_attention_2", torch_dtype=torch.float16)`
Flash Attention 2.0 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in MixtralModel is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `torch_dtype` argument. Example: `model = AutoModel.from_pretrained("openai/whisper-tiny", attn_implementation="flash_attention_2", torch_dtype=torch.float16)`
Flash Attention 2.0 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in MixtralModel is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `torch_dtype` argument. Example: `model = AutoModel.from_pretrained("openai/whisper-tiny", attn_implementation="flash_attention_2", torch_dtype=torch.float16)`
Flash Attention 2.0 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in MixtralModel is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `torch_dtype` argument. Example: `model = AutoModel.from_pretrained("openai/whisper-tiny", attn_implementation="flash_attention_2", torch_dtype=torch.float16)`
_____________________________ test_flash_attn_func _____________________________
2025-04-11T04:23:18.8648788Z
args = (), kwargs = {}
2025-04-11T04:23:18.8648877Z
def _clear_cache(*args, **kwargs):
get_accelerator().empty_cache()
get_accelerator().reset_peak_memory_stats()
get_accelerator().reset_max_memory_allocated()
get_accelerator().reset_max_memory_cached()
>       get_accelerator().synchronize()
2025-04-11T04:23:18.8649494Z
colossalai/testing/utils.py:271:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
colossalai/accelerator/cuda_accelerator.py:62: in synchronize
torch.cuda.synchronize(device)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.8650069Z
device = None
2025-04-11T04:23:18.8650149Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.8650562Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.8652138Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
______________________________ test_release_layer ______________________________
2025-04-11T04:23:18.8652570Z
def test_release_layer():
orig_cuda_allocated = torch.cuda.memory_allocated()
>       model = Net().cuda()
2025-04-11T04:23:18.8652875Z
tests/test_shardformer/test_shard_utils.py:16:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:911: in cuda
return self._apply(lambda t: t.cuda(device))
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:802: in _apply
module._apply(fn)
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:802: in _apply
module._apply(fn)
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:825: in _apply
param_applied = fn(param)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.8654587Z
t = Parameter containing:
tensor([[-0.8151],
[ 0.1839]], requires_grad=True)
2025-04-11T04:23:18.8654850Z
>   return self._apply(lambda t: t.cuda(device))
E   RuntimeError: CUDA error: out of memory
E   CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E   For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E   Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.8655644Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:911: RuntimeError
__________________________________ test_gpt2 ___________________________________
2025-04-11T04:23:18.8656023Z
args = (), kwargs = {}, try_count = 1
2025-04-11T04:23:18.8656120Z
def _run_until_success(*args, **kwargs):
try_count = 0
assert max_try is None or isinstance(
max_try, int
), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T04:23:18.8656740Z
while max_try is None or try_count < max_try:
try:
try_count += 1
>               ret = func(*args, **kwargs)
2025-04-11T04:23:18.8657104Z
colossalai/testing/utils.py:133:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
colossalai/testing/utils.py:271: in _clear_cache
get_accelerator().synchronize()
colossalai/accelerator/cuda_accelerator.py:62: in synchronize
torch.cuda.synchronize(device)
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:800: in synchronize
with torch.cuda.device(device):
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.8658211Z
self = <torch.cuda.device object at 0x7fb581d6d5d0>
2025-04-11T04:23:18.8658384Z
def __enter__(self):
>       self.prev_idx = torch.cuda._exchange_device(self.idx)
E       RuntimeError: CUDA error: out of memory
E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.8659344Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:374: RuntimeError
_____________________________ test_grad_clip_norm ______________________________
2025-04-11T04:23:18.8659713Z
args = (), kwargs = {}, try_count = 1
2025-04-11T04:23:18.8659812Z
def _run_until_success(*args, **kwargs):
try_count = 0
assert max_try is None or isinstance(
max_try, int
), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T04:23:18.8660428Z
while max_try is None or try_count < max_try:
try:
try_count += 1
>               ret = func(*args, **kwargs)
2025-04-11T04:23:18.8660790Z
colossalai/testing/utils.py:133:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
colossalai/testing/utils.py:271: in _clear_cache
get_accelerator().synchronize()
colossalai/accelerator/cuda_accelerator.py:62: in synchronize
torch.cuda.synchronize(device)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.8661552Z
device = None
2025-04-11T04:23:18.8661631Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.8661977Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.8663593Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
_____________________________ test_grad_clip_norm ______________________________
2025-04-11T04:23:18.8663967Z
args = (), kwargs = {}, try_count = 1
2025-04-11T04:23:18.8664061Z
def _run_until_success(*args, **kwargs):
try_count = 0
assert max_try is None or isinstance(
max_try, int
), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T04:23:18.8664680Z
while max_try is None or try_count < max_try:
try:
try_count += 1
>               ret = func(*args, **kwargs)
2025-04-11T04:23:18.8665101Z
colossalai/testing/utils.py:133:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
colossalai/testing/utils.py:271: in _clear_cache
get_accelerator().synchronize()
colossalai/accelerator/cuda_accelerator.py:62: in synchronize
torch.cuda.synchronize(device)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.8665917Z
device = None
2025-04-11T04:23:18.8666002Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.8666346Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.8667914Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
_____________________________ test_grad_clip_norm ______________________________
2025-04-11T04:23:18.8668340Z
args = (), kwargs = {}, try_count = 1
2025-04-11T04:23:18.8668466Z
def _run_until_success(*args, **kwargs):
try_count = 0
assert max_try is None or isinstance(
max_try, int
), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T04:23:18.8669041Z
while max_try is None or try_count < max_try:
try:
try_count += 1
>               ret = func(*args, **kwargs)
2025-04-11T04:23:18.8669403Z
colossalai/testing/utils.py:133:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
colossalai/testing/utils.py:271: in _clear_cache
get_accelerator().synchronize()
colossalai/accelerator/cuda_accelerator.py:62: in synchronize
torch.cuda.synchronize(device)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.8670239Z
device = None
2025-04-11T04:23:18.8670318Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.8670703Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.8672337Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
____________________________ test_dist_crossentropy ____________________________
2025-04-11T04:23:18.8672714Z
args = (), kwargs = {}, try_count = 1
error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T04:23:18.8673437Z
def _run_until_success(*args, **kwargs):
try_count = 0
assert max_try is None or isinstance(
max_try, int
), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T04:23:18.8674009Z
while max_try is None or try_count < max_try:
try:
try_count += 1
ret = func(*args, **kwargs)
return ret
except exception_type as e:
error_lines = str(e).split("\n")
if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
print("Exception is caught, retrying...")
# when pattern is not specified, we always skip the exception
# when pattern is specified, we only skip when pattern is matched
continue
else:
print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
>                   raise e
2025-04-11T04:23:18.8675753Z
colossalai/testing/utils.py:144:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
colossalai/testing/utils.py:133: in _run_until_success
ret = func(*args, **kwargs)
tests/test_shardformer/test_layer/test_dist_crossentropy.py:51: in test_dist_crossentropy
spawn(check_dist_crossentropy, 2, ignore_index=ignore_index)
colossalai/testing/utils.py:252: in spawn
mp.spawn(wrapped_func, nprocs=nprocs)
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
while not context.join():
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.8677758Z
self = <torch.multiprocessing.spawn.ProcessContext object at 0x7fb5b5b3e410>
timeout = None
2025-04-11T04:23:18.8678110Z
def join(self, timeout=None):
r"""Join one or more processes within spawn context.
2025-04-11T04:23:18.8678396Z
Attempt to join one or more processes in this spawn context.
If one of them exited with a non-zero exit status, this function
kills the remaining processes and raises an exception with the cause
of the first process exiting.
2025-04-11T04:23:18.8679013Z
Returns ``True`` if all processes have been joined successfully,
``False`` if there are more processes that need to be joined.
2025-04-11T04:23:18.8679366Z
Args:
timeout (float): Wait this long before giving up on waiting.
"""
# Ensure this function can be called even when we're done.
if len(self.sentinels) == 0:
return True
2025-04-11T04:23:18.8680035Z
# Wait for any process to fail or all of them to succeed.
ready = multiprocessing.connection.wait(
self.sentinels.keys(),
timeout=timeout,
)
2025-04-11T04:23:18.8680604Z
error_index = None
for sentinel in ready:
index = self.sentinels.pop(sentinel)
process = self.processes[index]
process.join()
if process.exitcode != 0:
error_index = index
break
2025-04-11T04:23:18.8681449Z
# Return if there was no error.
if error_index is None:
# Return whether or not all processes have been joined.
return len(self.sentinels) == 0
2025-04-11T04:23:18.8681926Z
# Assume failure. Terminate processes that are still alive.
for process in self.processes:
if process.is_alive():
process.terminate()
process.join()
2025-04-11T04:23:18.8682499Z
# There won't be an error on the queue if the process crashed.
failed_process = self.processes[error_index]
if self.error_queues[error_index].empty():
exitcode = self.processes[error_index].exitcode
if exitcode < 0:
name = signal.Signals(-exitcode).name
raise ProcessExitedException(
"process %d terminated with signal %s" % (error_index, name),
error_index=error_index,
error_pid=failed_process.pid,
exit_code=exitcode,
signal_name=name,
)
else:
raise ProcessExitedException(
"process %d terminated with exit code %d" % (error_index, exitcode),
error_index=error_index,
error_pid=failed_process.pid,
exit_code=exitcode,
)
2025-04-11T04:23:18.8684798Z
original_trace = self.error_queues[error_index].get()
msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
msg += original_trace
>       raise ProcessRaisedException(msg, error_index, failed_process.pid)
E       torch.multiprocessing.spawn.ProcessRaisedException:
E
E       -- Process 0 terminated with the following error:
E       Traceback (most recent call last):
E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
E           fn(i, *args)
E         File "/__w/ColossalAI/ColossalAI/tests/test_shardformer/test_layer/test_dist_crossentropy.py", line 20, in check_dist_crossentropy
E           pred = torch.randn(2, 4, 8, requires_grad=True).cuda()
E       RuntimeError: CUDA error: out of memory
E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.8687417Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
----------------------------- Captured stdout call -----------------------------
[04/11/25 04:19:34] INFO     colossalai - colossalai - INFO:
/__w/ColossalAI/ColossalAI/colossalai/initialize.py
:75 launch
INFO     colossalai - colossalai - INFO: Distributed
environment is initialized, world size: 2
Maximum number of attempts is reached or pattern is not matched, no more retrying...
----------------------------- Captured stderr call -----------------------------
[W socket.cpp:697] [c10d] The client socket has failed to connect to [localhost]:26698 (errno: 99 - Cannot assign requested address).
_________________________________ test_dropout _________________________________
2025-04-11T04:23:18.8689352Z
args = (), kwargs = {}, try_count = 1
error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T04:23:18.8690085Z
def _run_until_success(*args, **kwargs):
try_count = 0
assert max_try is None or isinstance(
max_try, int
), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T04:23:18.8690665Z
while max_try is None or try_count < max_try:
try:
try_count += 1
ret = func(*args, **kwargs)
return ret
except exception_type as e:
error_lines = str(e).split("\n")
if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
print("Exception is caught, retrying...")
# when pattern is not specified, we always skip the exception
# when pattern is specified, we only skip when pattern is matched
continue
else:
print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
>                   raise e
2025-04-11T04:23:18.8692405Z
colossalai/testing/utils.py:144:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
colossalai/testing/utils.py:133: in _run_until_success
ret = func(*args, **kwargs)
tests/test_shardformer/test_layer/test_dropout.py:66: in test_dropout
spawn(run_dist, nprocs=2)
colossalai/testing/utils.py:252: in spawn
mp.spawn(wrapped_func, nprocs=nprocs)
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
while not context.join():
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.8694244Z
self = <torch.multiprocessing.spawn.ProcessContext object at 0x7fb5b5a898d0>
timeout = None
2025-04-11T04:23:18.8694521Z
def join(self, timeout=None):
r"""Join one or more processes within spawn context.
2025-04-11T04:23:18.8694802Z
Attempt to join one or more processes in this spawn context.
If one of them exited with a non-zero exit status, this function
kills the remaining processes and raises an exception with the cause
of the first process exiting.
2025-04-11T04:23:18.8695466Z
Returns ``True`` if all processes have been joined successfully,
``False`` if there are more processes that need to be joined.
2025-04-11T04:23:18.8695812Z
Args:
timeout (float): Wait this long before giving up on waiting.
"""
# Ensure this function can be called even when we're done.
if len(self.sentinels) == 0:
return True
2025-04-11T04:23:18.8696537Z
# Wait for any process to fail or all of them to succeed.
ready = multiprocessing.connection.wait(
self.sentinels.keys(),
timeout=timeout,
)
2025-04-11T04:23:18.8697151Z
error_index = None
for sentinel in ready:
index = self.sentinels.pop(sentinel)
process = self.processes[index]
process.join()
if process.exitcode != 0:
error_index = index
break
2025-04-11T04:23:18.8697922Z
# Return if there was no error.
if error_index is None:
# Return whether or not all processes have been joined.
return len(self.sentinels) == 0
2025-04-11T04:23:18.8698394Z
# Assume failure. Terminate processes that are still alive.
for process in self.processes:
if process.is_alive():
process.terminate()
process.join()
2025-04-11T04:23:18.8698954Z
# There won't be an error on the queue if the process crashed.
failed_process = self.processes[error_index]
if self.error_queues[error_index].empty():
exitcode = self.processes[error_index].exitcode
if exitcode < 0:
name = signal.Signals(-exitcode).name
raise ProcessExitedException(
"process %d terminated with signal %s" % (error_index, name),
error_index=error_index,
error_pid=failed_process.pid,
exit_code=exitcode,
signal_name=name,
)
else:
raise ProcessExitedException(
"process %d terminated with exit code %d" % (error_index, exitcode),
error_index=error_index,
error_pid=failed_process.pid,
exit_code=exitcode,
)
2025-04-11T04:23:18.8701169Z
original_trace = self.error_queues[error_index].get()
msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
msg += original_trace
>       raise ProcessRaisedException(msg, error_index, failed_process.pid)
E       torch.multiprocessing.spawn.ProcessRaisedException:
E
E       -- Process 0 terminated with the following error:
E       Traceback (most recent call last):
E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
E           fn(i, *args)
E         File "/__w/ColossalAI/ColossalAI/tests/test_shardformer/test_layer/test_dropout.py", line 60, in run_dist
E           check_dropout_parallel_input()
E         File "/__w/ColossalAI/ColossalAI/tests/test_shardformer/test_layer/test_dropout.py", line 12, in check_dropout_parallel_input
E           dropout_1d = DropoutForParallelInput.from_native_module(dropout, process_group=None)
E         File "/__w/ColossalAI/ColossalAI/colossalai/shardformer/layer/dropout.py", line 42, in from_native_module
E           return DropoutForParallelInput(p=p, inplace=inplace, process_group=process_group)
E         File "/__w/ColossalAI/ColossalAI/colossalai/shardformer/layer/dropout.py", line 31, in __init__
E           self.randomizer = create_randomizer_with_offset(seed, process_group=process_group)
E         File "/__w/ColossalAI/ColossalAI/colossalai/shardformer/layer/utils.py", line 318, in create_randomizer_with_offset
E           is_synchronized = Randomizer.is_randomizer_index_synchronized(process_group)
E         File "/__w/ColossalAI/ColossalAI/colossalai/shardformer/layer/utils.py", line 258, in is_randomizer_index_synchronized
E           index_tensor = torch.tensor(index, dtype=torch.int32, device=get_accelerator().get_current_device())
E       RuntimeError: CUDA error: out of memory
E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.8706302Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
----------------------------- Captured stdout call -----------------------------
[04/11/25 04:19:38] INFO     colossalai - colossalai - INFO:
/__w/ColossalAI/ColossalAI/colossalai/initialize.py
:75 launch
INFO     colossalai - colossalai - INFO: Distributed
environment is initialized, world size: 2
Maximum number of attempts is reached or pattern is not matched, no more retrying...
______________________________ test_embedding_1d _______________________________
2025-04-11T04:23:18.8707755Z
args = (), kwargs = {}, try_count = 1
error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T04:23:18.8708544Z
def _run_until_success(*args, **kwargs):
try_count = 0
assert max_try is None or isinstance(
max_try, int
), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T04:23:18.8709167Z
while max_try is None or try_count < max_try:
try:
try_count += 1
ret = func(*args, **kwargs)
return ret
except exception_type as e:
error_lines = str(e).split("\n")
if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
print("Exception is caught, retrying...")
# when pattern is not specified, we always skip the exception
# when pattern is specified, we only skip when pattern is matched
continue
else:
print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
>                   raise e
2025-04-11T04:23:18.8710907Z
colossalai/testing/utils.py:144:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
colossalai/testing/utils.py:133: in _run_until_success
ret = func(*args, **kwargs)
tests/test_shardformer/test_layer/test_embedding.py:52: in test_embedding_1d
spawn(run_dist, nprocs=2)
colossalai/testing/utils.py:252: in spawn
mp.spawn(wrapped_func, nprocs=nprocs)
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
while not context.join():
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.8712861Z
self = <torch.multiprocessing.spawn.ProcessContext object at 0x7fb5b5b3fe80>
timeout = None
2025-04-11T04:23:18.8713137Z
def join(self, timeout=None):
r"""Join one or more processes within spawn context.
2025-04-11T04:23:18.8713422Z
Attempt to join one or more processes in this spawn context.
If one of them exited with a non-zero exit status, this function
kills the remaining processes and raises an exception with the cause
of the first process exiting.
2025-04-11T04:23:18.8714030Z
Returns ``True`` if all processes have been joined successfully,
``False`` if there are more processes that need to be joined.
2025-04-11T04:23:18.8714428Z
Args:
timeout (float): Wait this long before giving up on waiting.
"""
# Ensure this function can be called even when we're done.
if len(self.sentinels) == 0:
return True
2025-04-11T04:23:18.8715092Z
# Wait for any process to fail or all of them to succeed.
ready = multiprocessing.connection.wait(
self.sentinels.keys(),
timeout=timeout,
)
2025-04-11T04:23:18.8715706Z
error_index = None
for sentinel in ready:
index = self.sentinels.pop(sentinel)
process = self.processes[index]
process.join()
if process.exitcode != 0:
error_index = index
break
2025-04-11T04:23:18.8716539Z
# Return if there was no error.
if error_index is None:
# Return whether or not all processes have been joined.
return len(self.sentinels) == 0
2025-04-11T04:23:18.8717016Z
# Assume failure. Terminate processes that are still alive.
for process in self.processes:
if process.is_alive():
process.terminate()
process.join()
2025-04-11T04:23:18.8717580Z
# There won't be an error on the queue if the process crashed.
failed_process = self.processes[error_index]
if self.error_queues[error_index].empty():
exitcode = self.processes[error_index].exitcode
if exitcode < 0:
name = signal.Signals(-exitcode).name
raise ProcessExitedException(
"process %d terminated with signal %s" % (error_index, name),
error_index=error_index,
error_pid=failed_process.pid,
exit_code=exitcode,
signal_name=name,
)
else:
raise ProcessExitedException(
"process %d terminated with exit code %d" % (error_index, exitcode),
error_index=error_index,
error_pid=failed_process.pid,
exit_code=exitcode,
)
2025-04-11T04:23:18.8719753Z
original_trace = self.error_queues[error_index].get()
msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
msg += original_trace
>       raise ProcessRaisedException(msg, error_index, failed_process.pid)
E       torch.multiprocessing.spawn.ProcessRaisedException:
E
E       -- Process 0 terminated with the following error:
E       Traceback (most recent call last):
E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
E           fn(i, *args)
E         File "/__w/ColossalAI/ColossalAI/tests/test_shardformer/test_layer/test_embedding.py", line 47, in run_dist
E           check_embedding_1d()
E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
E           partial_func(**kwargs)
E         File "/__w/ColossalAI/ColossalAI/tests/test_shardformer/test_layer/test_embedding.py", line 18, in check_embedding_1d
E           embedding = nn.Embedding(32, 128).cuda()
E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in cuda
E           return self._apply(lambda t: t.cuda(device))
E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
E           param_applied = fn(param)
E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in <lambda>
E           return self._apply(lambda t: t.cuda(device))
E       RuntimeError: CUDA error: out of memory
E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.8724275Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
----------------------------- Captured stdout call -----------------------------
[04/11/25 04:19:43] INFO     colossalai - colossalai - INFO:
/__w/ColossalAI/ColossalAI/colossalai/initialize.py
:75 launch
INFO     colossalai - colossalai - INFO: Distributed
environment is initialized, world size: 2
Maximum number of attempts is reached or pattern is not matched, no more retrying...
_______________________________ test_linearconv ________________________________
2025-04-11T04:23:18.8725765Z
args = (), kwargs = {}, try_count = 1
error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T04:23:18.8726471Z
def _run_until_success(*args, **kwargs):
try_count = 0
assert max_try is None or isinstance(
max_try, int
), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T04:23:18.8727051Z
while max_try is None or try_count < max_try:
try:
try_count += 1
ret = func(*args, **kwargs)
return ret
except exception_type as e:
error_lines = str(e).split("\n")
if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
print("Exception is caught, retrying...")
# when pattern is not specified, we always skip the exception
# when pattern is specified, we only skip when pattern is matched
continue
else:
print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
>                   raise e
2025-04-11T04:23:18.8728877Z
colossalai/testing/utils.py:144:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
colossalai/testing/utils.py:133: in _run_until_success
ret = func(*args, **kwargs)
tests/test_shardformer/test_layer/test_gpt2_qkv_fused_linear_1d.py:209: in test_linearconv
spawn(run_dist, nprocs=2)
colossalai/testing/utils.py:252: in spawn
mp.spawn(wrapped_func, nprocs=nprocs)
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
while not context.join():
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.8730763Z
self = <torch.multiprocessing.spawn.ProcessContext object at 0x7fb5b5b3c910>
timeout = None
2025-04-11T04:23:18.8731043Z
def join(self, timeout=None):
r"""Join one or more processes within spawn context.
2025-04-11T04:23:18.8731325Z
Attempt to join one or more processes in this spawn context.
If one of them exited with a non-zero exit status, this function
kills the remaining processes and raises an exception with the cause
of the first process exiting.
2025-04-11T04:23:18.8731997Z
Returns ``True`` if all processes have been joined successfully,
``False`` if there are more processes that need to be joined.
2025-04-11T04:23:18.8732367Z
Args:
timeout (float): Wait this long before giving up on waiting.
"""
# Ensure this function can be called even when we're done.
if len(self.sentinels) == 0:
return True
2025-04-11T04:23:18.8733053Z
# Wait for any process to fail or all of them to succeed.
ready = multiprocessing.connection.wait(
self.sentinels.keys(),
timeout=timeout,
)
2025-04-11T04:23:18.8733687Z
error_index = None
for sentinel in ready:
index = self.sentinels.pop(sentinel)
process = self.processes[index]
process.join()
if process.exitcode != 0:
error_index = index
break
2025-04-11T04:23:18.8734514Z
# Return if there was no error.
if error_index is None:
# Return whether or not all processes have been joined.
return len(self.sentinels) == 0
2025-04-11T04:23:18.8734985Z
# Assume failure. Terminate processes that are still alive.
for process in self.processes:
if process.is_alive():
process.terminate()
process.join()
2025-04-11T04:23:18.8735596Z
# There won't be an error on the queue if the process crashed.
failed_process = self.processes[error_index]
if self.error_queues[error_index].empty():
exitcode = self.processes[error_index].exitcode
if exitcode < 0:
name = signal.Signals(-exitcode).name
raise ProcessExitedException(
"process %d terminated with signal %s" % (error_index, name),
error_index=error_index,
error_pid=failed_process.pid,
exit_code=exitcode,
signal_name=name,
)
else:
raise ProcessExitedException(
"process %d terminated with exit code %d" % (error_index, exitcode),
error_index=error_index,
error_pid=failed_process.pid,
exit_code=exitcode,
)
2025-04-11T04:23:18.8737763Z
original_trace = self.error_queues[error_index].get()
msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
msg += original_trace
>       raise ProcessRaisedException(msg, error_index, failed_process.pid)
E       torch.multiprocessing.spawn.ProcessRaisedException:
E
E       -- Process 0 terminated with the following error:
E       Traceback (most recent call last):
E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
E           fn(i, *args)
E         File "/__w/ColossalAI/ColossalAI/tests/test_shardformer/test_layer/test_gpt2_qkv_fused_linear_1d.py", line 204, in run_dist
E           check_gpt2_qkv_fused_linear_1d()
E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
E           partial_func(**kwargs)
E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
E           partial_func(**kwargs)
E         File "/__w/ColossalAI/ColossalAI/tests/test_shardformer/test_layer/test_gpt2_qkv_fused_linear_1d.py", line 194, in check_gpt2_qkv_fused_linear_1d
E           check_linear_conv_1d_col(lazy_init, seq_parallel_mode)
E         File "/__w/ColossalAI/ColossalAI/tests/test_shardformer/test_layer/test_gpt2_qkv_fused_linear_1d.py", line 47, in check_linear_conv_1d_col
E           linear = Conv1D(192, 48).cuda()
E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in cuda
E           return self._apply(lambda t: t.cuda(device))
E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
E           param_applied = fn(param)
E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in <lambda>
E           return self._apply(lambda t: t.cuda(device))
E       RuntimeError: CUDA error: out of memory
E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.8743186Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
----------------------------- Captured stdout call -----------------------------
[04/11/25 04:19:48] INFO     colossalai - colossalai - INFO:
/__w/ColossalAI/ColossalAI/colossalai/initialize.py
:75 launch
INFO     colossalai - colossalai - INFO: Distributed
environment is initialized, world size: 2
Maximum number of attempts is reached or pattern is not matched, no more retrying...
----------------------------- Captured stderr call -----------------------------
[W socket.cpp:697] [c10d] The client socket has failed to connect to [localhost]:26338 (errno: 99 - Cannot assign requested address).
________________________________ test_layernorm ________________________________
2025-04-11T04:23:18.8745136Z
args = (), kwargs = {}, try_count = 1
error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T04:23:18.8745827Z
def _run_until_success(*args, **kwargs):
try_count = 0
assert max_try is None or isinstance(
max_try, int
), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T04:23:18.8746446Z
while max_try is None or try_count < max_try:
try:
try_count += 1
ret = func(*args, **kwargs)
return ret
except exception_type as e:
error_lines = str(e).split("\n")
if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
print("Exception is caught, retrying...")
# when pattern is not specified, we always skip the exception
# when pattern is specified, we only skip when pattern is matched
continue
else:
print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
>                   raise e
2025-04-11T04:23:18.8748256Z
colossalai/testing/utils.py:144:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
colossalai/testing/utils.py:133: in _run_until_success
ret = func(*args, **kwargs)
tests/test_shardformer/test_layer/test_layernorm.py:50: in test_layernorm
spawn(run_dist, nprocs=2)
colossalai/testing/utils.py:252: in spawn
mp.spawn(wrapped_func, nprocs=nprocs)
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
while not context.join():
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.8750088Z
self = <torch.multiprocessing.spawn.ProcessContext object at 0x7fb5b5b3e9e0>
timeout = None
2025-04-11T04:23:18.8750374Z
def join(self, timeout=None):
r"""Join one or more processes within spawn context.
2025-04-11T04:23:18.8750655Z
Attempt to join one or more processes in this spawn context.
If one of them exited with a non-zero exit status, this function
kills the remaining processes and raises an exception with the cause
of the first process exiting.
2025-04-11T04:23:18.8751331Z
Returns ``True`` if all processes have been joined successfully,
``False`` if there are more processes that need to be joined.
2025-04-11T04:23:18.8751687Z
Args:
timeout (float): Wait this long before giving up on waiting.
"""
# Ensure this function can be called even when we're done.
if len(self.sentinels) == 0:
return True
2025-04-11T04:23:18.8752352Z
# Wait for any process to fail or all of them to succeed.
ready = multiprocessing.connection.wait(
self.sentinels.keys(),
timeout=timeout,
)
2025-04-11T04:23:18.8752970Z
error_index = None
for sentinel in ready:
index = self.sentinels.pop(sentinel)
process = self.processes[index]
process.join()
if process.exitcode != 0:
error_index = index
break
2025-04-11T04:23:18.8753798Z
# Return if there was no error.
if error_index is None:
# Return whether or not all processes have been joined.
return len(self.sentinels) == 0
2025-04-11T04:23:18.8754270Z
# Assume failure. Terminate processes that are still alive.
for process in self.processes:
if process.is_alive():
process.terminate()
process.join()
2025-04-11T04:23:18.8754885Z
# There won't be an error on the queue if the process crashed.
failed_process = self.processes[error_index]
if self.error_queues[error_index].empty():
exitcode = self.processes[error_index].exitcode
if exitcode < 0:
name = signal.Signals(-exitcode).name
raise ProcessExitedException(
"process %d terminated with signal %s" % (error_index, name),
error_index=error_index,
error_pid=failed_process.pid,
exit_code=exitcode,
signal_name=name,
)
else:
raise ProcessExitedException(
"process %d terminated with exit code %d" % (error_index, exitcode),
error_index=error_index,
error_pid=failed_process.pid,
exit_code=exitcode,
)
2025-04-11T04:23:18.8757052Z
original_trace = self.error_queues[error_index].get()
msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
msg += original_trace
>       raise ProcessRaisedException(msg, error_index, failed_process.pid)
E       torch.multiprocessing.spawn.ProcessRaisedException:
E
E       -- Process 0 terminated with the following error:
E       Traceback (most recent call last):
E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
E           fn(i, *args)
E         File "/__w/ColossalAI/ColossalAI/tests/test_shardformer/test_layer/test_layernorm.py", line 45, in run_dist
E           check_layernorm()
E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
E           partial_func(**kwargs)
E         File "/__w/ColossalAI/ColossalAI/tests/test_shardformer/test_layer/test_layernorm.py", line 17, in check_layernorm
E           norm = nn.LayerNorm(128, 0.00001).cuda()
E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in cuda
E           return self._apply(lambda t: t.cuda(device))
E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
E           param_applied = fn(param)
E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in <lambda>
E           return self._apply(lambda t: t.cuda(device))
E       RuntimeError: CUDA error: out of memory
E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.8761529Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
----------------------------- Captured stdout call -----------------------------
[04/11/25 04:19:52] INFO     colossalai - colossalai - INFO:
/__w/ColossalAI/ColossalAI/colossalai/initialize.py
:75 launch
INFO     colossalai - colossalai - INFO: Distributed
environment is initialized, world size: 2
Maximum number of attempts is reached or pattern is not matched, no more retrying...
_________________________________ test_linear __________________________________
2025-04-11T04:23:18.8762979Z
args = (), kwargs = {}, try_count = 1
error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T04:23:18.8763740Z
def _run_until_success(*args, **kwargs):
try_count = 0
assert max_try is None or isinstance(
max_try, int
), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T04:23:18.8764323Z
while max_try is None or try_count < max_try:
try:
try_count += 1
ret = func(*args, **kwargs)
return ret
except exception_type as e:
error_lines = str(e).split("\n")
if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
print("Exception is caught, retrying...")
# when pattern is not specified, we always skip the exception
# when pattern is specified, we only skip when pattern is matched
continue
else:
print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
>                   raise e
2025-04-11T04:23:18.8766132Z
colossalai/testing/utils.py:144:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
colossalai/testing/utils.py:133: in _run_until_success
ret = func(*args, **kwargs)
tests/test_shardformer/test_layer/test_linear_1d.py:284: in test_linear
spawn(check_dist_linear, nprocs=2)
colossalai/testing/utils.py:252: in spawn
mp.spawn(wrapped_func, nprocs=nprocs)
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
while not context.join():
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.8768019Z
self = <torch.multiprocessing.spawn.ProcessContext object at 0x7fb5b8693070>
timeout = None
2025-04-11T04:23:18.8768316Z
def join(self, timeout=None):
r"""Join one or more processes within spawn context.
2025-04-11T04:23:18.8768602Z
Attempt to join one or more processes in this spawn context.
If one of them exited with a non-zero exit status, this function
kills the remaining processes and raises an exception with the cause
of the first process exiting.
2025-04-11T04:23:18.8769218Z
Returns ``True`` if all processes have been joined successfully,
``False`` if there are more processes that need to be joined.
2025-04-11T04:23:18.8769571Z
Args:
timeout (float): Wait this long before giving up on waiting.
"""
# Ensure this function can be called even when we're done.
if len(self.sentinels) == 0:
return True
2025-04-11T04:23:18.8770287Z
# Wait for any process to fail or all of them to succeed.
ready = multiprocessing.connection.wait(
self.sentinels.keys(),
timeout=timeout,
)
2025-04-11T04:23:18.8770857Z
error_index = None
for sentinel in ready:
index = self.sentinels.pop(sentinel)
process = self.processes[index]
process.join()
if process.exitcode != 0:
error_index = index
break
2025-04-11T04:23:18.8771630Z
# Return if there was no error.
if error_index is None:
# Return whether or not all processes have been joined.
return len(self.sentinels) == 0
2025-04-11T04:23:18.8772169Z
# Assume failure. Terminate processes that are still alive.
for process in self.processes:
if process.is_alive():
process.terminate()
process.join()
2025-04-11T04:23:18.8772783Z
# There won't be an error on the queue if the process crashed.
failed_process = self.processes[error_index]
if self.error_queues[error_index].empty():
exitcode = self.processes[error_index].exitcode
if exitcode < 0:
name = signal.Signals(-exitcode).name
raise ProcessExitedException(
"process %d terminated with signal %s" % (error_index, name),
error_index=error_index,
error_pid=failed_process.pid,
exit_code=exitcode,
signal_name=name,
)
else:
raise ProcessExitedException(
"process %d terminated with exit code %d" % (error_index, exitcode),
error_index=error_index,
error_pid=failed_process.pid,
exit_code=exitcode,
)
2025-04-11T04:23:18.8774970Z
original_trace = self.error_queues[error_index].get()
msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
msg += original_trace
>       raise ProcessRaisedException(msg, error_index, failed_process.pid)
E       torch.multiprocessing.spawn.ProcessRaisedException:
E
E       -- Process 0 terminated with the following error:
E       Traceback (most recent call last):
E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
E           fn(i, *args)
E         File "/__w/ColossalAI/ColossalAI/tests/test_shardformer/test_layer/test_linear_1d.py", line 279, in check_dist_linear
E           run_dist_linear_test()
E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
E           partial_func(**kwargs)
E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
E           partial_func(**kwargs)
E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
E           partial_func(**kwargs)
E         File "/__w/ColossalAI/ColossalAI/tests/test_shardformer/test_layer/test_linear_1d.py", line 270, in run_dist_linear_test
E           check_linear_1d_col(lazy_init, seq_parallel_mode, overlap)
E         File "/__w/ColossalAI/ColossalAI/tests/test_shardformer/test_layer/test_linear_1d.py", line 21, in check_linear_1d_col
E           linear = nn.Linear(32, 128).cuda()
E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in cuda
E           return self._apply(lambda t: t.cuda(device))
E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
E           param_applied = fn(param)
E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in <lambda>
E           return self._apply(lambda t: t.cuda(device))
E       RuntimeError: CUDA error: out of memory
E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.8780670Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
----------------------------- Captured stdout call -----------------------------
[04/11/25 04:19:56] INFO     colossalai - colossalai - INFO:
/__w/ColossalAI/ColossalAI/colossalai/initialize.py
:75 launch
INFO     colossalai - colossalai - INFO: Distributed
environment is initialized, world size: 2
Maximum number of attempts is reached or pattern is not matched, no more retrying...
_______________________________ test_linearconv ________________________________
2025-04-11T04:23:18.8782127Z
args = (), kwargs = {}, try_count = 1
error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T04:23:18.8782872Z
def _run_until_success(*args, **kwargs):
try_count = 0
assert max_try is None or isinstance(
max_try, int
), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T04:23:18.8783454Z
while max_try is None or try_count < max_try:
try:
try_count += 1
ret = func(*args, **kwargs)
return ret
except exception_type as e:
error_lines = str(e).split("\n")
if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
print("Exception is caught, retrying...")
# when pattern is not specified, we always skip the exception
# when pattern is specified, we only skip when pattern is matched
continue
else:
print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
>                   raise e
2025-04-11T04:23:18.8785200Z
colossalai/testing/utils.py:144:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
colossalai/testing/utils.py:133: in _run_until_success
ret = func(*args, **kwargs)
tests/test_shardformer/test_layer/test_qkv_fused_linear_1d.py:166: in test_linearconv
spawn(run_dist, nprocs=2)
colossalai/testing/utils.py:252: in spawn
mp.spawn(wrapped_func, nprocs=nprocs)
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
while not context.join():
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.8787140Z
self = <torch.multiprocessing.spawn.ProcessContext object at 0x7fb5b5b3cfd0>
timeout = None
2025-04-11T04:23:18.8787424Z
def join(self, timeout=None):
r"""Join one or more processes within spawn context.
2025-04-11T04:23:18.8787706Z
Attempt to join one or more processes in this spawn context.
If one of them exited with a non-zero exit status, this function
kills the remaining processes and raises an exception with the cause
of the first process exiting.
2025-04-11T04:23:18.8788319Z
Returns ``True`` if all processes have been joined successfully,
``False`` if there are more processes that need to be joined.
2025-04-11T04:23:18.8788712Z
Args:
timeout (float): Wait this long before giving up on waiting.
"""
# Ensure this function can be called even when we're done.
if len(self.sentinels) == 0:
return True
2025-04-11T04:23:18.8789450Z
# Wait for any process to fail or all of them to succeed.
ready = multiprocessing.connection.wait(
self.sentinels.keys(),
timeout=timeout,
)
2025-04-11T04:23:18.8790017Z
error_index = None
for sentinel in ready:
index = self.sentinels.pop(sentinel)
process = self.processes[index]
process.join()
if process.exitcode != 0:
error_index = index
break
2025-04-11T04:23:18.8790797Z
# Return if there was no error.
if error_index is None:
# Return whether or not all processes have been joined.
return len(self.sentinels) == 0
2025-04-11T04:23:18.8791327Z
# Assume failure. Terminate processes that are still alive.
for process in self.processes:
if process.is_alive():
process.terminate()
process.join()
2025-04-11T04:23:18.8791955Z
# There won't be an error on the queue if the process crashed.
failed_process = self.processes[error_index]
if self.error_queues[error_index].empty():
exitcode = self.processes[error_index].exitcode
if exitcode < 0:
name = signal.Signals(-exitcode).name
raise ProcessExitedException(
"process %d terminated with signal %s" % (error_index, name),
error_index=error_index,
error_pid=failed_process.pid,
exit_code=exitcode,
signal_name=name,
)
else:
raise ProcessExitedException(
"process %d terminated with exit code %d" % (error_index, exitcode),
error_index=error_index,
error_pid=failed_process.pid,
exit_code=exitcode,
)
2025-04-11T04:23:18.8794134Z
original_trace = self.error_queues[error_index].get()
msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
msg += original_trace
>       raise ProcessRaisedException(msg, error_index, failed_process.pid)
E       torch.multiprocessing.spawn.ProcessRaisedException:
E
E       -- Process 0 terminated with the following error:
E       Traceback (most recent call last):
E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
E           fn(i, *args)
E         File "/__w/ColossalAI/ColossalAI/tests/test_shardformer/test_layer/test_qkv_fused_linear_1d.py", line 158, in run_dist
E           check_linear_1d_col()
E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
E           partial_func(**kwargs)
E         File "/__w/ColossalAI/ColossalAI/tests/test_shardformer/test_layer/test_qkv_fused_linear_1d.py", line 21, in check_linear_1d_col
E           linear = nn.Linear(8, 80).cuda()
E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in cuda
E           return self._apply(lambda t: t.cuda(device))
E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
E           param_applied = fn(param)
E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in <lambda>
E           return self._apply(lambda t: t.cuda(device))
E       RuntimeError: CUDA error: out of memory
E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.8798669Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
----------------------------- Captured stdout call -----------------------------
[04/11/25 04:20:01] INFO     colossalai - colossalai - INFO:
/__w/ColossalAI/ColossalAI/colossalai/initialize.py
:75 launch
INFO     colossalai - colossalai - INFO: Distributed
environment is initialized, world size: 2
Maximum number of attempts is reached or pattern is not matched, no more retrying...
________________________________ test_ring_attn ________________________________
2025-04-11T04:23:18.8800171Z
args = (), kwargs = {}, try_count = 1
error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T04:23:18.8800882Z
def _run_until_success(*args, **kwargs):
try_count = 0
assert max_try is None or isinstance(
max_try, int
), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T04:23:18.8801458Z
while max_try is None or try_count < max_try:
try:
try_count += 1
ret = func(*args, **kwargs)
return ret
except exception_type as e:
error_lines = str(e).split("\n")
if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
print("Exception is caught, retrying...")
# when pattern is not specified, we always skip the exception
# when pattern is specified, we only skip when pattern is matched
continue
else:
print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
>                   raise e
2025-04-11T04:23:18.8803210Z
colossalai/testing/utils.py:144:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
colossalai/testing/utils.py:133: in _run_until_success
ret = func(*args, **kwargs)
colossalai/testing/utils.py:64: in _execute_function_by_param
partial_func(**kwargs)
tests/test_shardformer/test_layer/test_ring_attn.py:181: in test_ring_attn
spawn(launch_single_ring, nprocs=world_size)
colossalai/testing/utils.py:252: in spawn
mp.spawn(wrapped_func, nprocs=nprocs)
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
while not context.join():
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.8805389Z
self = <torch.multiprocessing.spawn.ProcessContext object at 0x7fb5b8692200>
timeout = None
2025-04-11T04:23:18.8805721Z
def join(self, timeout=None):
r"""Join one or more processes within spawn context.
2025-04-11T04:23:18.8806004Z
Attempt to join one or more processes in this spawn context.
If one of them exited with a non-zero exit status, this function
kills the remaining processes and raises an exception with the cause
of the first process exiting.
2025-04-11T04:23:18.8806620Z
Returns ``True`` if all processes have been joined successfully,
``False`` if there are more processes that need to be joined.
2025-04-11T04:23:18.8806972Z
Args:
timeout (float): Wait this long before giving up on waiting.
"""
# Ensure this function can be called even when we're done.
if len(self.sentinels) == 0:
return True
2025-04-11T04:23:18.8807639Z
# Wait for any process to fail or all of them to succeed.
ready = multiprocessing.connection.wait(
self.sentinels.keys(),
timeout=timeout,
)
2025-04-11T04:23:18.8808254Z
error_index = None
for sentinel in ready:
index = self.sentinels.pop(sentinel)
process = self.processes[index]
process.join()
if process.exitcode != 0:
error_index = index
break
2025-04-11T04:23:18.8809046Z
# Return if there was no error.
if error_index is None:
# Return whether or not all processes have been joined.
return len(self.sentinels) == 0
2025-04-11T04:23:18.8809518Z
# Assume failure. Terminate processes that are still alive.
for process in self.processes:
if process.is_alive():
process.terminate()
process.join()
2025-04-11T04:23:18.8810138Z
# There won't be an error on the queue if the process crashed.
failed_process = self.processes[error_index]
if self.error_queues[error_index].empty():
exitcode = self.processes[error_index].exitcode
if exitcode < 0:
name = signal.Signals(-exitcode).name
raise ProcessExitedException(
"process %d terminated with signal %s" % (error_index, name),
error_index=error_index,
error_pid=failed_process.pid,
exit_code=exitcode,
signal_name=name,
)
else:
raise ProcessExitedException(
"process %d terminated with exit code %d" % (error_index, exitcode),
error_index=error_index,
error_pid=failed_process.pid,
exit_code=exitcode,
)
2025-04-11T04:23:18.8812464Z
original_trace = self.error_queues[error_index].get()
msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
msg += original_trace
>       raise ProcessRaisedException(msg, error_index, failed_process.pid)
E       torch.multiprocessing.spawn.ProcessRaisedException:
E
E       -- Process 0 terminated with the following error:
E       Traceback (most recent call last):
E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
E           fn(i, *args)
E         File "/__w/ColossalAI/ColossalAI/tests/test_shardformer/test_layer/test_ring_attn.py", line 169, in launch_single_ring
E           check_packed_seq()
E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
E           partial_func(**kwargs)
E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
E           partial_func(**kwargs)
E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
E           partial_func(**kwargs)
E         [Previous line repeated 2 more times]
E         File "/__w/ColossalAI/ColossalAI/tests/test_shardformer/test_layer/test_ring_attn.py", line 94, in check_packed_seq
E           padding_mask = torch.ones((bs, seqlen), dtype=torch.int, device=device)
E       RuntimeError: CUDA error: out of memory
E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.8816623Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
----------------------------- Captured stdout call -----------------------------
[04/11/25 04:20:05] INFO     colossalai - colossalai - INFO:
/__w/ColossalAI/ColossalAI/colossalai/initialize.py
:75 launch
INFO     colossalai - colossalai - INFO: Distributed
environment is initialized, world size: 2
Maximum number of attempts is reached or pattern is not matched, no more retrying...
_______________________________ test_double_ring _______________________________
2025-04-11T04:23:18.8818133Z
args = (), kwargs = {}, try_count = 1
error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T04:23:18.8818890Z
def _run_until_success(*args, **kwargs):
try_count = 0
assert max_try is None or isinstance(
max_try, int
), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T04:23:18.8819470Z
while max_try is None or try_count < max_try:
try:
try_count += 1
ret = func(*args, **kwargs)
return ret
except exception_type as e:
error_lines = str(e).split("\n")
if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
print("Exception is caught, retrying...")
# when pattern is not specified, we always skip the exception
# when pattern is specified, we only skip when pattern is matched
continue
else:
print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
>                   raise e
2025-04-11T04:23:18.8821225Z
colossalai/testing/utils.py:144:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
colossalai/testing/utils.py:133: in _run_until_success
ret = func(*args, **kwargs)
colossalai/testing/utils.py:64: in _execute_function_by_param
partial_func(**kwargs)
tests/test_shardformer/test_layer/test_ring_attn.py:187: in test_double_ring
spawn(launch_double_ring, nprocs=world_size)
colossalai/testing/utils.py:252: in spawn
mp.spawn(wrapped_func, nprocs=nprocs)
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
while not context.join():
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.8823352Z
self = <torch.multiprocessing.spawn.ProcessContext object at 0x7fb5b5a88c40>
timeout = None
2025-04-11T04:23:18.8823686Z
def join(self, timeout=None):
r"""Join one or more processes within spawn context.
2025-04-11T04:23:18.8823969Z
Attempt to join one or more processes in this spawn context.
If one of them exited with a non-zero exit status, this function
kills the remaining processes and raises an exception with the cause
of the first process exiting.
2025-04-11T04:23:18.8824583Z
Returns ``True`` if all processes have been joined successfully,
``False`` if there are more processes that need to be joined.
2025-04-11T04:23:18.8824982Z
Args:
timeout (float): Wait this long before giving up on waiting.
"""
# Ensure this function can be called even when we're done.
if len(self.sentinels) == 0:
return True
2025-04-11T04:23:18.8825645Z
# Wait for any process to fail or all of them to succeed.
ready = multiprocessing.connection.wait(
self.sentinels.keys(),
timeout=timeout,
)
2025-04-11T04:23:18.8826209Z
error_index = None
for sentinel in ready:
index = self.sentinels.pop(sentinel)
process = self.processes[index]
process.join()
if process.exitcode != 0:
error_index = index
break
2025-04-11T04:23:18.8826982Z
# Return if there was no error.
if error_index is None:
# Return whether or not all processes have been joined.
return len(self.sentinels) == 0
2025-04-11T04:23:18.8827507Z
# Assume failure. Terminate processes that are still alive.
for process in self.processes:
if process.is_alive():
process.terminate()
process.join()
2025-04-11T04:23:18.8828068Z
# There won't be an error on the queue if the process crashed.
failed_process = self.processes[error_index]
if self.error_queues[error_index].empty():
exitcode = self.processes[error_index].exitcode
if exitcode < 0:
name = signal.Signals(-exitcode).name
raise ProcessExitedException(
"process %d terminated with signal %s" % (error_index, name),
error_index=error_index,
error_pid=failed_process.pid,
exit_code=exitcode,
signal_name=name,
)
else:
raise ProcessExitedException(
"process %d terminated with exit code %d" % (error_index, exitcode),
error_index=error_index,
error_pid=failed_process.pid,
exit_code=exitcode,
)
2025-04-11T04:23:18.8830345Z
original_trace = self.error_queues[error_index].get()
msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
msg += original_trace
>       raise ProcessRaisedException(msg, error_index, failed_process.pid)
E       torch.multiprocessing.spawn.ProcessRaisedException:
E
E       -- Process 0 terminated with the following error:
E       Traceback (most recent call last):
E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
E           fn(i, *args)
E         File "/__w/ColossalAI/ColossalAI/tests/test_shardformer/test_layer/test_ring_attn.py", line 175, in launch_double_ring
E           check_ring_attn(inner_ring_size=2)
E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
E           partial_func(**kwargs)
E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
E           partial_func(**kwargs)
E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
E           partial_func(**kwargs)
E         [Previous line repeated 2 more times]
E         File "/__w/ColossalAI/ColossalAI/tests/test_shardformer/test_layer/test_ring_attn.py", line 36, in check_ring_attn
E           qkv = torch.randn(bs, seq_len, 3, nheads, d, device=device, dtype=dtype, requires_grad=True)
E       RuntimeError: CUDA error: out of memory
E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.8834571Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
----------------------------- Captured stdout call -----------------------------
[04/11/25 04:20:10] INFO     colossalai - colossalai - INFO:
/__w/ColossalAI/ColossalAI/colossalai/initialize.py
:75 launch
INFO     colossalai - colossalai - INFO: Distributed
environment is initialized, world size: 4
Maximum number of attempts is reached or pattern is not matched, no more retrying...
__________________________ test_all_to_all_attention ___________________________
2025-04-11T04:23:18.8836077Z
args = (), kwargs = {}, try_count = 1
error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T04:23:18.8836805Z
def _run_until_success(*args, **kwargs):
try_count = 0
assert max_try is None or isinstance(
max_try, int
), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T04:23:18.8837437Z
while max_try is None or try_count < max_try:
try:
try_count += 1
ret = func(*args, **kwargs)
return ret
except exception_type as e:
error_lines = str(e).split("\n")
if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
print("Exception is caught, retrying...")
# when pattern is not specified, we always skip the exception
# when pattern is specified, we only skip when pattern is matched
continue
else:
print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
>                   raise e
2025-04-11T04:23:18.8839979Z
colossalai/testing/utils.py:144:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
colossalai/testing/utils.py:133: in _run_until_success
ret = func(*args, **kwargs)
tests/test_shardformer/test_layer/test_sequence_parallel.py:174: in test_all_to_all_attention
spawn(check_all2all_attn, nprocs=4)
colossalai/testing/utils.py:252: in spawn
mp.spawn(wrapped_func, nprocs=nprocs)
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
while not context.join():
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.8841926Z
self = <torch.multiprocessing.spawn.ProcessContext object at 0x7fb5b5b3c4c0>
timeout = None
2025-04-11T04:23:18.8842207Z
def join(self, timeout=None):
r"""Join one or more processes within spawn context.
2025-04-11T04:23:18.8842493Z
Attempt to join one or more processes in this spawn context.
If one of them exited with a non-zero exit status, this function
kills the remaining processes and raises an exception with the cause
of the first process exiting.
2025-04-11T04:23:18.8843160Z
Returns ``True`` if all processes have been joined successfully,
``False`` if there are more processes that need to be joined.
2025-04-11T04:23:18.8843565Z
Args:
timeout (float): Wait this long before giving up on waiting.
"""
# Ensure this function can be called even when we're done.
if len(self.sentinels) == 0:
return True
2025-04-11T04:23:18.8844227Z
# Wait for any process to fail or all of them to succeed.
ready = multiprocessing.connection.wait(
self.sentinels.keys(),
timeout=timeout,
)
2025-04-11T04:23:18.8844836Z
error_index = None
for sentinel in ready:
index = self.sentinels.pop(sentinel)
process = self.processes[index]
process.join()
if process.exitcode != 0:
error_index = index
break
2025-04-11T04:23:18.8845611Z
# Return if there was no error.
if error_index is None:
# Return whether or not all processes have been joined.
return len(self.sentinels) == 0
2025-04-11T04:23:18.8846081Z
# Assume failure. Terminate processes that are still alive.
for process in self.processes:
if process.is_alive():
process.terminate()
process.join()
2025-04-11T04:23:18.8846643Z
# There won't be an error on the queue if the process crashed.
failed_process = self.processes[error_index]
if self.error_queues[error_index].empty():
exitcode = self.processes[error_index].exitcode
if exitcode < 0:
name = signal.Signals(-exitcode).name
raise ProcessExitedException(
"process %d terminated with signal %s" % (error_index, name),
error_index=error_index,
error_pid=failed_process.pid,
exit_code=exitcode,
signal_name=name,
)
else:
raise ProcessExitedException(
"process %d terminated with exit code %d" % (error_index, exitcode),
error_index=error_index,
error_pid=failed_process.pid,
exit_code=exitcode,
)
2025-04-11T04:23:18.8848850Z
original_trace = self.error_queues[error_index].get()
msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
msg += original_trace
>       raise ProcessRaisedException(msg, error_index, failed_process.pid)
E       torch.multiprocessing.spawn.ProcessRaisedException:
E
E       -- Process 0 terminated with the following error:
E       Traceback (most recent call last):
E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
E           fn(i, *args)
E         File "/__w/ColossalAI/ColossalAI/tests/test_shardformer/test_layer/test_sequence_parallel.py", line 169, in check_all2all_attn
E           run_seq_parallel_attn()
E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
E           partial_func(**kwargs)
E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
E           partial_func(**kwargs)
E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
E           partial_func(**kwargs)
E         [Previous line repeated 1 more time]
E         File "/__w/ColossalAI/ColossalAI/tests/test_shardformer/test_layer/test_sequence_parallel.py", line 164, in run_seq_parallel_attn
E           seq_parallel_attn(seq_len, hidden_dim, head_num, batch_size)
E         File "/__w/ColossalAI/ColossalAI/tests/test_shardformer/test_layer/test_sequence_parallel.py", line 101, in seq_parallel_attn
E           x = torch.randn(batch_size, seq_len, hidden_dim).cuda()
E       RuntimeError: CUDA error: out of memory
E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.8853568Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
----------------------------- Captured stdout call -----------------------------
[04/11/25 04:20:15] INFO     colossalai - colossalai - INFO:
/__w/ColossalAI/ColossalAI/colossalai/initialize.py
:75 launch
INFO     colossalai - colossalai - INFO: Distributed
environment is initialized, world size: 4
Maximum number of attempts is reached or pattern is not matched, no more retrying...
_____________________________ test_vocab_embedding _____________________________
2025-04-11T04:23:18.8855022Z
args = (), kwargs = {}, try_count = 1
error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T04:23:18.8855773Z
def _run_until_success(*args, **kwargs):
try_count = 0
assert max_try is None or isinstance(
max_try, int
), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T04:23:18.8856399Z
while max_try is None or try_count < max_try:
try:
try_count += 1
ret = func(*args, **kwargs)
return ret
except exception_type as e:
error_lines = str(e).split("\n")
if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
print("Exception is caught, retrying...")
# when pattern is not specified, we always skip the exception
# when pattern is specified, we only skip when pattern is matched
continue
else:
print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
>                   raise e
2025-04-11T04:23:18.8858168Z
colossalai/testing/utils.py:144:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
colossalai/testing/utils.py:133: in _run_until_success
ret = func(*args, **kwargs)
tests/test_shardformer/test_layer/test_vocab_parallel_embedding_1d.py:54: in test_vocab_embedding
spawn(run_dist, nprocs=2)
colossalai/testing/utils.py:252: in spawn
mp.spawn(wrapped_func, nprocs=nprocs)
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
while not context.join():
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.8860097Z
self = <torch.multiprocessing.spawn.ProcessContext object at 0x7fb581d79540>
timeout = None
2025-04-11T04:23:18.8860382Z
def join(self, timeout=None):
r"""Join one or more processes within spawn context.
2025-04-11T04:23:18.8860669Z
Attempt to join one or more processes in this spawn context.
If one of them exited with a non-zero exit status, this function
kills the remaining processes and raises an exception with the cause
of the first process exiting.
2025-04-11T04:23:18.8861277Z
Returns ``True`` if all processes have been joined successfully,
``False`` if there are more processes that need to be joined.
2025-04-11T04:23:18.8861624Z
Args:
timeout (float): Wait this long before giving up on waiting.
"""
# Ensure this function can be called even when we're done.
if len(self.sentinels) == 0:
return True
2025-04-11T04:23:18.8862351Z
# Wait for any process to fail or all of them to succeed.
ready = multiprocessing.connection.wait(
self.sentinels.keys(),
timeout=timeout,
)
2025-04-11T04:23:18.8862966Z
error_index = None
for sentinel in ready:
index = self.sentinels.pop(sentinel)
process = self.processes[index]
process.join()
if process.exitcode != 0:
error_index = index
break
2025-04-11T04:23:18.8863823Z
# Return if there was no error.
if error_index is None:
# Return whether or not all processes have been joined.
return len(self.sentinels) == 0
2025-04-11T04:23:18.8864301Z
# Assume failure. Terminate processes that are still alive.
for process in self.processes:
if process.is_alive():
process.terminate()
process.join()
2025-04-11T04:23:18.8864876Z
# There won't be an error on the queue if the process crashed.
failed_process = self.processes[error_index]
if self.error_queues[error_index].empty():
exitcode = self.processes[error_index].exitcode
if exitcode < 0:
name = signal.Signals(-exitcode).name
raise ProcessExitedException(
"process %d terminated with signal %s" % (error_index, name),
error_index=error_index,
error_pid=failed_process.pid,
exit_code=exitcode,
signal_name=name,
)
else:
raise ProcessExitedException(
"process %d terminated with exit code %d" % (error_index, exitcode),
error_index=error_index,
error_pid=failed_process.pid,
exit_code=exitcode,
)
2025-04-11T04:23:18.8867079Z
original_trace = self.error_queues[error_index].get()
msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
msg += original_trace
>       raise ProcessRaisedException(msg, error_index, failed_process.pid)
E       torch.multiprocessing.spawn.ProcessRaisedException:
E
E       -- Process 0 terminated with the following error:
E       Traceback (most recent call last):
E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
E           fn(i, *args)
E         File "/__w/ColossalAI/ColossalAI/tests/test_shardformer/test_layer/test_vocab_parallel_embedding_1d.py", line 49, in run_dist
E           check_vocab_embedding_1d()
E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
E           partial_func(**kwargs)
E         File "/__w/ColossalAI/ColossalAI/tests/test_shardformer/test_layer/test_vocab_parallel_embedding_1d.py", line 18, in check_vocab_embedding_1d
E           embedding = nn.Embedding(128, 32).to("cuda")
E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1152, in to
E           return self._apply(convert)
E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
E           param_applied = fn(param)
E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1150, in convert
E           return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
E       RuntimeError: CUDA error: out of memory
E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.8871897Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
----------------------------- Captured stdout call -----------------------------
[04/11/25 04:20:20] INFO     colossalai - colossalai - INFO:
/__w/ColossalAI/ColossalAI/colossalai/initialize.py
:75 launch
INFO     colossalai - colossalai - INFO: Distributed
environment is initialized, world size: 2
Maximum number of attempts is reached or pattern is not matched, no more retrying...
__________________________________ test_bert ___________________________________
2025-04-11T04:23:18.8873399Z
args = (), kwargs = {}, try_count = 1
2025-04-11T04:23:18.8873493Z
def _run_until_success(*args, **kwargs):
try_count = 0
assert max_try is None or isinstance(
max_try, int
), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T04:23:18.8874068Z
while max_try is None or try_count < max_try:
try:
try_count += 1
>               ret = func(*args, **kwargs)
2025-04-11T04:23:18.8874432Z
colossalai/testing/utils.py:133:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
colossalai/testing/utils.py:271: in _clear_cache
get_accelerator().synchronize()
colossalai/accelerator/cuda_accelerator.py:62: in synchronize
torch.cuda.synchronize(device)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.8875316Z
device = None
2025-04-11T04:23:18.8875399Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.8875744Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.8877381Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
__________________________________ test_blip2 __________________________________
2025-04-11T04:23:18.8877762Z
args = (), kwargs = {}, try_count = 1
2025-04-11T04:23:18.8877854Z
def _run_until_success(*args, **kwargs):
try_count = 0
assert max_try is None or isinstance(
max_try, int
), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T04:23:18.8878429Z
while max_try is None or try_count < max_try:
try:
try_count += 1
>               ret = func(*args, **kwargs)
2025-04-11T04:23:18.8878843Z
colossalai/testing/utils.py:133:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
colossalai/testing/utils.py:271: in _clear_cache
get_accelerator().synchronize()
colossalai/accelerator/cuda_accelerator.py:62: in synchronize
torch.cuda.synchronize(device)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.8879618Z
device = None
2025-04-11T04:23:18.8879696Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.8880038Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.8881700Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
__________________________________ test_bloom __________________________________
2025-04-11T04:23:18.8882071Z
args = (), kwargs = {}, try_count = 1
2025-04-11T04:23:18.8882217Z
def _run_until_success(*args, **kwargs):
try_count = 0
assert max_try is None or isinstance(
max_try, int
), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T04:23:18.8882787Z
while max_try is None or try_count < max_try:
try:
try_count += 1
>               ret = func(*args, **kwargs)
2025-04-11T04:23:18.8883149Z
colossalai/testing/utils.py:133:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
colossalai/testing/utils.py:271: in _clear_cache
get_accelerator().synchronize()
colossalai/accelerator/cuda_accelerator.py:62: in synchronize
torch.cuda.synchronize(device)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.8883925Z
device = None
2025-04-11T04:23:18.8884005Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.8884340Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.8885940Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
_________________________________ test_chatglm _________________________________
2025-04-11T04:23:18.8886362Z
args = (), kwargs = {}, try_count = 1
2025-04-11T04:23:18.8886459Z
def _run_until_success(*args, **kwargs):
try_count = 0
assert max_try is None or isinstance(
max_try, int
), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T04:23:18.8887089Z
while max_try is None or try_count < max_try:
try:
try_count += 1
>               ret = func(*args, **kwargs)
2025-04-11T04:23:18.8887445Z
colossalai/testing/utils.py:133:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
colossalai/testing/utils.py:271: in _clear_cache
get_accelerator().synchronize()
colossalai/accelerator/cuda_accelerator.py:62: in synchronize
torch.cuda.synchronize(device)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.8888258Z
device = None
2025-04-11T04:23:18.8888341Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.8888674Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.8890212Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
_________________________________ test_command _________________________________
2025-04-11T04:23:18.8890640Z
args = (), kwargs = {}, try_count = 1
2025-04-11T04:23:18.8890738Z
def _run_until_success(*args, **kwargs):
try_count = 0
assert max_try is None or isinstance(
max_try, int
), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T04:23:18.8891308Z
while max_try is None or try_count < max_try:
try:
try_count += 1
>               ret = func(*args, **kwargs)
2025-04-11T04:23:18.8891665Z
colossalai/testing/utils.py:133:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
colossalai/testing/utils.py:271: in _clear_cache
get_accelerator().synchronize()
colossalai/accelerator/cuda_accelerator.py:62: in synchronize
torch.cuda.synchronize(device)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.8892472Z
device = None
2025-04-11T04:23:18.8892598Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.8892935Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.8894522Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
_______________________________ test_deepseek[4] _______________________________
2025-04-11T04:23:18.8894894Z
args = (), kwargs = {'world_size': 4}, try_count = 1
error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T04:23:18.8895620Z
def _run_until_success(*args, **kwargs):
try_count = 0
assert max_try is None or isinstance(
max_try, int
), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T04:23:18.8896184Z
while max_try is None or try_count < max_try:
try:
try_count += 1
ret = func(*args, **kwargs)
return ret
except exception_type as e:
error_lines = str(e).split("\n")
if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
print("Exception is caught, retrying...")
# when pattern is not specified, we always skip the exception
# when pattern is specified, we only skip when pattern is matched
continue
else:
print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
>                   raise e
2025-04-11T04:23:18.8897930Z
colossalai/testing/utils.py:144:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
colossalai/testing/utils.py:133: in _run_until_success
ret = func(*args, **kwargs)
tests/test_shardformer/test_model/test_shard_deepseek.py:228: in test_deepseek
spawn(check_deepseek, world_size)
colossalai/testing/utils.py:252: in spawn
mp.spawn(wrapped_func, nprocs=nprocs)
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
while not context.join():
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.8899918Z
self = <torch.multiprocessing.spawn.ProcessContext object at 0x7fb581bc8a30>
timeout = None
2025-04-11T04:23:18.8900204Z
def join(self, timeout=None):
r"""Join one or more processes within spawn context.
2025-04-11T04:23:18.8900486Z
Attempt to join one or more processes in this spawn context.
If one of them exited with a non-zero exit status, this function
kills the remaining processes and raises an exception with the cause
of the first process exiting.
2025-04-11T04:23:18.8901101Z
Returns ``True`` if all processes have been joined successfully,
``False`` if there are more processes that need to be joined.
2025-04-11T04:23:18.8901459Z
Args:
timeout (float): Wait this long before giving up on waiting.
"""
# Ensure this function can be called even when we're done.
if len(self.sentinels) == 0:
return True
2025-04-11T04:23:18.8902114Z
# Wait for any process to fail or all of them to succeed.
ready = multiprocessing.connection.wait(
self.sentinels.keys(),
timeout=timeout,
)
2025-04-11T04:23:18.8902743Z
error_index = None
for sentinel in ready:
index = self.sentinels.pop(sentinel)
process = self.processes[index]
process.join()
if process.exitcode != 0:
error_index = index
break
2025-04-11T04:23:18.8903525Z
# Return if there was no error.
if error_index is None:
# Return whether or not all processes have been joined.
return len(self.sentinels) == 0
2025-04-11T04:23:18.8904009Z
# Assume failure. Terminate processes that are still alive.
for process in self.processes:
if process.is_alive():
process.terminate()
process.join()
2025-04-11T04:23:18.8904663Z
# There won't be an error on the queue if the process crashed.
failed_process = self.processes[error_index]
if self.error_queues[error_index].empty():
exitcode = self.processes[error_index].exitcode
if exitcode < 0:
name = signal.Signals(-exitcode).name
raise ProcessExitedException(
"process %d terminated with signal %s" % (error_index, name),
error_index=error_index,
error_pid=failed_process.pid,
exit_code=exitcode,
signal_name=name,
)
else:
raise ProcessExitedException(
"process %d terminated with exit code %d" % (error_index, exitcode),
error_index=error_index,
error_pid=failed_process.pid,
exit_code=exitcode,
)
2025-04-11T04:23:18.8906907Z
original_trace = self.error_queues[error_index].get()
msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
msg += original_trace
>       raise ProcessRaisedException(msg, error_index, failed_process.pid)
E       torch.multiprocessing.spawn.ProcessRaisedException:
E
E       -- Process 0 terminated with the following error:
E       Traceback (most recent call last):
E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
E           fn(i, *args)
E         File "/__w/ColossalAI/ColossalAI/tests/test_shardformer/test_model/test_shard_deepseek.py", line 216, in check_deepseek
E           run_deepseek_test()
E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
E           partial_func(**kwargs)
E         File "/__w/ColossalAI/ColossalAI/tests/test_shardformer/test_model/test_shard_deepseek.py", line 187, in run_deepseek_test
E           run_deepseek_commom(config)
E         File "/__w/ColossalAI/ColossalAI/tests/test_shardformer/test_model/test_shard_deepseek.py", line 77, in run_deepseek_commom
E           torch_model = AutoModel.from_config(config, trust_remote_code=True).cuda().to(dtype)
E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2548, in cuda
E           return super().cuda(*args, **kwargs)
E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in cuda
E           return self._apply(lambda t: t.cuda(device))
E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
E           module._apply(fn)
E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
E           param_applied = fn(param)
E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in <lambda>
E           return self._apply(lambda t: t.cuda(device))
E       RuntimeError: CUDA error: out of memory
E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.8912881Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
----------------------------- Captured stdout call -----------------------------
[04/11/25 04:20:26] INFO     colossalai - colossalai - INFO:
/__w/ColossalAI/ColossalAI/colossalai/initialize.py
:75 launch
INFO     colossalai - colossalai - INFO: Distributed
environment is initialized, world size: 4
rank 0 testing (0, 1, 4, 1, 1)
Maximum number of attempts is reached or pattern is not matched, no more retrying...
----------------------------- Captured stderr call -----------------------------
/opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
warnings.warn(
A new version of the following files was downloaded from https://huggingface.co/deepseek-ai/deepseek-moe-16b-base:
- configuration_deepseek.py
. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.
/opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
warnings.warn(
A new version of the following files was downloaded from https://huggingface.co/deepseek-ai/deepseek-moe-16b-base:
- configuration_deepseek.py
. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.
/opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
warnings.warn(
A new version of the following files was downloaded from https://huggingface.co/deepseek-ai/deepseek-moe-16b-base:
- configuration_deepseek.py
. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.
/opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
warnings.warn(
A new version of the following files was downloaded from https://huggingface.co/deepseek-ai/deepseek-moe-16b-base:
- configuration_deepseek.py
. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.
/opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
warnings.warn(
A new version of the following files was downloaded from https://huggingface.co/deepseek-ai/deepseek-moe-16b-base:
- modeling_deepseek.py
. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.
A new version of the following files was downloaded from https://huggingface.co/deepseek-ai/deepseek-moe-16b-base:
- modeling_deepseek.py
. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.
A new version of the following files was downloaded from https://huggingface.co/deepseek-ai/deepseek-moe-16b-base:
- modeling_deepseek.py
. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.
A new version of the following files was downloaded from https://huggingface.co/deepseek-ai/deepseek-moe-16b-base:
- modeling_deepseek.py
. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.
/opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
warnings.warn(
_____________________________ test_deepseek_v3[4] ______________________________
2025-04-11T04:23:18.8943172Z
args = (), kwargs = {'world_size': 4}, try_count = 1
error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T04:23:18.8943973Z
def _run_until_success(*args, **kwargs):
try_count = 0
assert max_try is None or isinstance(
max_try, int
), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T04:23:18.8944558Z
while max_try is None or try_count < max_try:
try:
try_count += 1
ret = func(*args, **kwargs)
return ret
except exception_type as e:
error_lines = str(e).split("\n")
if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
print("Exception is caught, retrying...")
# when pattern is not specified, we always skip the exception
# when pattern is specified, we only skip when pattern is matched
continue
else:
print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
>                   raise e
2025-04-11T04:23:18.8946384Z
colossalai/testing/utils.py:144:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
colossalai/testing/utils.py:133: in _run_until_success
ret = func(*args, **kwargs)
tests/test_shardformer/test_model/test_shard_deepseek_v3.py:100: in test_deepseek_v3
spawn(check_deepseek_v3, world_size)
colossalai/testing/utils.py:252: in spawn
mp.spawn(wrapped_func, nprocs=nprocs)
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
while not context.join():
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.8948302Z
self = <torch.multiprocessing.spawn.ProcessContext object at 0x7fb581e0dde0>
timeout = None
2025-04-11T04:23:18.8948611Z
def join(self, timeout=None):
r"""Join one or more processes within spawn context.
2025-04-11T04:23:18.8948901Z
Attempt to join one or more processes in this spawn context.
If one of them exited with a non-zero exit status, this function
kills the remaining processes and raises an exception with the cause
of the first process exiting.
2025-04-11T04:23:18.8949516Z
Returns ``True`` if all processes have been joined successfully,
``False`` if there are more processes that need to be joined.
2025-04-11T04:23:18.8949931Z
Args:
timeout (float): Wait this long before giving up on waiting.
"""
# Ensure this function can be called even when we're done.
if len(self.sentinels) == 0:
return True
2025-04-11T04:23:18.8950599Z
# Wait for any process to fail or all of them to succeed.
ready = multiprocessing.connection.wait(
self.sentinels.keys(),
timeout=timeout,
)
2025-04-11T04:23:18.8951162Z
error_index = None
for sentinel in ready:
index = self.sentinels.pop(sentinel)
process = self.processes[index]
process.join()
if process.exitcode != 0:
error_index = index
break
2025-04-11T04:23:18.8952006Z
# Return if there was no error.
if error_index is None:
# Return whether or not all processes have been joined.
return len(self.sentinels) == 0
2025-04-11T04:23:18.8952534Z
# Assume failure. Terminate processes that are still alive.
for process in self.processes:
if process.is_alive():
process.terminate()
process.join()
2025-04-11T04:23:18.8953104Z
# There won't be an error on the queue if the process crashed.
failed_process = self.processes[error_index]
if self.error_queues[error_index].empty():
exitcode = self.processes[error_index].exitcode
if exitcode < 0:
name = signal.Signals(-exitcode).name
raise ProcessExitedException(
"process %d terminated with signal %s" % (error_index, name),
error_index=error_index,
error_pid=failed_process.pid,
exit_code=exitcode,
signal_name=name,
)
else:
raise ProcessExitedException(
"process %d terminated with exit code %d" % (error_index, exitcode),
error_index=error_index,
error_pid=failed_process.pid,
exit_code=exitcode,
)
2025-04-11T04:23:18.8955292Z
original_trace = self.error_queues[error_index].get()
msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
msg += original_trace
>       raise ProcessRaisedException(msg, error_index, failed_process.pid)
E       torch.multiprocessing.spawn.ProcessRaisedException:
E
E       -- Process 0 terminated with the following error:
E       Traceback (most recent call last):
E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
E           fn(i, *args)
E         File "/__w/ColossalAI/ColossalAI/tests/test_shardformer/test_model/test_shard_deepseek_v3.py", line 93, in check_deepseek_v3
E           run_deepseek_v3_test()
E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
E           partial_func(**kwargs)
E         File "/__w/ColossalAI/ColossalAI/tests/test_shardformer/test_model/test_shard_deepseek_v3.py", line 82, in run_deepseek_v3_test
E           check_forward_backward(
E         File "/__w/ColossalAI/ColossalAI/tests/test_shardformer/test_model/test_shard_deepseek_v3.py", line 29, in check_forward_backward
E           org_model, org_optimizer, sharded_model, sharded_optimizer, criterion, booster = build_model_from_hybrid_plugin(
E         File "/__w/ColossalAI/ColossalAI/tests/test_shardformer/test_model/_utils.py", line 138, in build_model_from_hybrid_plugin
E           org_model = org_model.cuda()
E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2548, in cuda
E           return super().cuda(*args, **kwargs)
E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in cuda
E           return self._apply(lambda t: t.cuda(device))
E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
E           module._apply(fn)
E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
E           module._apply(fn)
E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
E           param_applied = fn(param)
E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in <lambda>
E           return self._apply(lambda t: t.cuda(device))
E       RuntimeError: CUDA error: out of memory
E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.8962005Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
----------------------------- Captured stdout call -----------------------------
[04/11/25 04:20:38] INFO     colossalai - colossalai - INFO:
/__w/ColossalAI/ColossalAI/colossalai/initialize.py
:75 launch
INFO     colossalai - colossalai - INFO: Distributed
environment is initialized, world size: 4
Maximum number of attempts is reached or pattern is not matched, no more retrying...
----------------------------- Captured stderr call -----------------------------
/opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
/opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
/opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
/opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
/opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
warnings.warn(
[W socket.cpp:697] [c10d] The client socket has failed to connect to [localhost]:44807 (errno: 99 - Cannot assign requested address).
/opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
warnings.warn(
A new version of the following files was downloaded from https://huggingface.co/deepseek-ai/DeepSeek-V3:
- configuration_deepseek.py
. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.
/opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
warnings.warn(
A new version of the following files was downloaded from https://huggingface.co/deepseek-ai/DeepSeek-V3:
- configuration_deepseek.py
. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.
/opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
warnings.warn(
A new version of the following files was downloaded from https://huggingface.co/deepseek-ai/DeepSeek-V3:
- configuration_deepseek.py
. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.
A new version of the following files was downloaded from https://huggingface.co/deepseek-ai/DeepSeek-V3:
- configuration_deepseek.py
. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.
/opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
warnings.warn(
A new version of the following files was downloaded from https://huggingface.co/deepseek-ai/DeepSeek-V3:
- modeling_deepseek.py
. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.
A new version of the following files was downloaded from https://huggingface.co/deepseek-ai/DeepSeek-V3:
- modeling_deepseek.py
. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.
A new version of the following files was downloaded from https://huggingface.co/deepseek-ai/DeepSeek-V3:
- modeling_deepseek.py
. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.
A new version of the following files was downloaded from https://huggingface.co/deepseek-ai/DeepSeek-V3:
- modeling_deepseek.py
. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.
_________________________________ test_falcon __________________________________
2025-04-11T04:23:18.9005862Z
args = (), kwargs = {}, try_count = 1
2025-04-11T04:23:18.9005957Z
def _run_until_success(*args, **kwargs):
try_count = 0
assert max_try is None or isinstance(
max_try, int
), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T04:23:18.9006560Z
while max_try is None or try_count < max_try:
try:
try_count += 1
>               ret = func(*args, **kwargs)
2025-04-11T04:23:18.9006941Z
colossalai/testing/utils.py:133:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
colossalai/testing/utils.py:271: in _clear_cache
get_accelerator().synchronize()
colossalai/accelerator/cuda_accelerator.py:62: in synchronize
torch.cuda.synchronize(device)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.9007796Z
device = None
2025-04-11T04:23:18.9007876Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.9008229Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.9009884Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
__________________________________ test_gpt2 ___________________________________
2025-04-11T04:23:18.9010300Z
args = (), kwargs = {}, try_count = 1
2025-04-11T04:23:18.9010403Z
def _run_until_success(*args, **kwargs):
try_count = 0
assert max_try is None or isinstance(
max_try, int
), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T04:23:18.9010989Z
while max_try is None or try_count < max_try:
try:
try_count += 1
>               ret = func(*args, **kwargs)
2025-04-11T04:23:18.9011412Z
colossalai/testing/utils.py:133:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
colossalai/testing/utils.py:271: in _clear_cache
get_accelerator().synchronize()
colossalai/accelerator/cuda_accelerator.py:62: in synchronize
torch.cuda.synchronize(device)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.9012199Z
device = None
2025-04-11T04:23:18.9012279Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.9012630Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.9014273Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
__________________________________ test_llama __________________________________
2025-04-11T04:23:18.9014651Z
args = (), kwargs = {}, try_count = 1
2025-04-11T04:23:18.9014745Z
def _run_until_success(*args, **kwargs):
try_count = 0
assert max_try is None or isinstance(
max_try, int
), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T04:23:18.9015386Z
while max_try is None or try_count < max_try:
try:
try_count += 1
>               ret = func(*args, **kwargs)
2025-04-11T04:23:18.9015756Z
colossalai/testing/utils.py:133:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
colossalai/testing/utils.py:271: in _clear_cache
get_accelerator().synchronize()
colossalai/accelerator/cuda_accelerator.py:62: in synchronize
torch.cuda.synchronize(device)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.9016599Z
device = None
2025-04-11T04:23:18.9016687Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.9017083Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.9018680Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
_________________________________ test_mistral _________________________________
2025-04-11T04:23:18.9019058Z
args = (), kwargs = {}, try_count = 1
2025-04-11T04:23:18.9019158Z
def _run_until_success(*args, **kwargs):
try_count = 0
assert max_try is None or isinstance(
max_try, int
), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T04:23:18.9019790Z
while max_try is None or try_count < max_try:
try:
try_count += 1
>               ret = func(*args, **kwargs)
2025-04-11T04:23:18.9020242Z
colossalai/testing/utils.py:133:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
colossalai/testing/utils.py:271: in _clear_cache
get_accelerator().synchronize()
colossalai/accelerator/cuda_accelerator.py:62: in synchronize
torch.cuda.synchronize(device)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.9021482Z
device = None
2025-04-11T04:23:18.9021638Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.9022159Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.9024324Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
_______________________________ test_mixtral[4] ________________________________
2025-04-11T04:23:18.9024902Z
args = (), kwargs = {'world_size': 4}, try_count = 1
error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T04:23:18.9025737Z
def _run_until_success(*args, **kwargs):
try_count = 0
assert max_try is None or isinstance(
max_try, int
), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T04:23:18.9026599Z
while max_try is None or try_count < max_try:
try:
try_count += 1
ret = func(*args, **kwargs)
return ret
except exception_type as e:
error_lines = str(e).split("\n")
if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
print("Exception is caught, retrying...")
# when pattern is not specified, we always skip the exception
# when pattern is specified, we only skip when pattern is matched
continue
else:
print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
>                   raise e
2025-04-11T04:23:18.9029155Z
colossalai/testing/utils.py:144:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
colossalai/testing/utils.py:133: in _run_until_success
ret = func(*args, **kwargs)
tests/test_shardformer/test_model/test_shard_mixtral.py:222: in test_mixtral
spawn(check_mixtral, world_size)
colossalai/testing/utils.py:252: in spawn
mp.spawn(wrapped_func, nprocs=nprocs)
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
while not context.join():
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.9031696Z
self = <torch.multiprocessing.spawn.ProcessContext object at 0x7fb5b5a214b0>
timeout = None
2025-04-11T04:23:18.9032091Z
def join(self, timeout=None):
r"""Join one or more processes within spawn context.
2025-04-11T04:23:18.9032577Z
Attempt to join one or more processes in this spawn context.
If one of them exited with a non-zero exit status, this function
kills the remaining processes and raises an exception with the cause
of the first process exiting.
2025-04-11T04:23:18.9033487Z
Returns ``True`` if all processes have been joined successfully,
``False`` if there are more processes that need to be joined.
2025-04-11T04:23:18.9033967Z
Args:
timeout (float): Wait this long before giving up on waiting.
"""
# Ensure this function can be called even when we're done.
if len(self.sentinels) == 0:
return True
2025-04-11T04:23:18.9034958Z
# Wait for any process to fail or all of them to succeed.
ready = multiprocessing.connection.wait(
self.sentinels.keys(),
timeout=timeout,
)
2025-04-11T04:23:18.9035812Z
error_index = None
for sentinel in ready:
index = self.sentinels.pop(sentinel)
process = self.processes[index]
process.join()
if process.exitcode != 0:
error_index = index
break
2025-04-11T04:23:18.9037154Z
# Return if there was no error.
if error_index is None:
# Return whether or not all processes have been joined.
return len(self.sentinels) == 0
2025-04-11T04:23:18.9037843Z
# Assume failure. Terminate processes that are still alive.
for process in self.processes:
if process.is_alive():
process.terminate()
process.join()
2025-04-11T04:23:18.9038671Z
# There won't be an error on the queue if the process crashed.
failed_process = self.processes[error_index]
if self.error_queues[error_index].empty():
exitcode = self.processes[error_index].exitcode
if exitcode < 0:
name = signal.Signals(-exitcode).name
raise ProcessExitedException(
"process %d terminated with signal %s" % (error_index, name),
error_index=error_index,
error_pid=failed_process.pid,
exit_code=exitcode,
signal_name=name,
)
else:
raise ProcessExitedException(
"process %d terminated with exit code %d" % (error_index, exitcode),
error_index=error_index,
error_pid=failed_process.pid,
exit_code=exitcode,
)
2025-04-11T04:23:18.9041979Z
original_trace = self.error_queues[error_index].get()
msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
msg += original_trace
>       raise ProcessRaisedException(msg, error_index, failed_process.pid)
E       torch.multiprocessing.spawn.ProcessRaisedException:
E
E       -- Process 0 terminated with the following error:
E       Traceback (most recent call last):
E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
E           fn(i, *args)
E         File "/__w/ColossalAI/ColossalAI/tests/test_shardformer/test_model/test_shard_mixtral.py", line 210, in check_mixtral
E           run_mixtral_test()
E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
E           partial_func(**kwargs)
E         File "/__w/ColossalAI/ColossalAI/tests/test_shardformer/test_model/test_shard_mixtral.py", line 180, in run_mixtral_test
E           run_mixtral_commom(config)
E         File "/__w/ColossalAI/ColossalAI/tests/test_shardformer/test_model/test_shard_mixtral.py", line 70, in run_mixtral_commom
E           torch_model = MixtralModel(config).to(dtype).cuda()
E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2548, in cuda
E           return super().cuda(*args, **kwargs)
E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in cuda
E           return self._apply(lambda t: t.cuda(device))
E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
E           module._apply(fn)
E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
E           param_applied = fn(param)
E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in <lambda>
E           return self._apply(lambda t: t.cuda(device))
E       RuntimeError: CUDA error: out of memory
E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.9049312Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
----------------------------- Captured stdout call -----------------------------
[04/11/25 04:20:47] INFO     colossalai - colossalai - INFO:
/__w/ColossalAI/ColossalAI/colossalai/initialize.py
:75 launch
INFO     colossalai - colossalai - INFO: Distributed
environment is initialized, world size: 4
Maximum number of attempts is reached or pattern is not matched, no more retrying...
----------------------------- Captured stderr call -----------------------------
Flash Attention 2.0 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in MixtralModel is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `torch_dtype` argument. Example: `model = AutoModel.from_pretrained("openai/whisper-tiny", attn_implementation="flash_attention_2", torch_dtype=torch.float16)`
Flash Attention 2.0 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in MixtralModel is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `torch_dtype` argument. Example: `model = AutoModel.from_pretrained("openai/whisper-tiny", attn_implementation="flash_attention_2", torch_dtype=torch.float16)`
Flash Attention 2.0 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in MixtralModel is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `torch_dtype` argument. Example: `model = AutoModel.from_pretrained("openai/whisper-tiny", attn_implementation="flash_attention_2", torch_dtype=torch.float16)`
Flash Attention 2.0 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in MixtralModel is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `torch_dtype` argument. Example: `model = AutoModel.from_pretrained("openai/whisper-tiny", attn_implementation="flash_attention_2", torch_dtype=torch.float16)`
________________________________ test_OPTModel _________________________________
2025-04-11T04:23:18.9056188Z
args = (), kwargs = {}, try_count = 1
2025-04-11T04:23:18.9056360Z
def _run_until_success(*args, **kwargs):
try_count = 0
assert max_try is None or isinstance(
max_try, int
), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T04:23:18.9057199Z
while max_try is None or try_count < max_try:
try:
try_count += 1
>               ret = func(*args, **kwargs)
2025-04-11T04:23:18.9057799Z
colossalai/testing/utils.py:133:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
colossalai/testing/utils.py:271: in _clear_cache
get_accelerator().synchronize()
colossalai/accelerator/cuda_accelerator.py:62: in synchronize
torch.cuda.synchronize(device)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.9058940Z
device = None
2025-04-11T04:23:18.9059046Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.9059625Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.9061819Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
__________________________________ test_qwen2 __________________________________
2025-04-11T04:23:18.9062295Z
args = (), kwargs = {}, try_count = 1
2025-04-11T04:23:18.9062421Z
def _run_until_success(*args, **kwargs):
try_count = 0
assert max_try is None or isinstance(
max_try, int
), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T04:23:18.9063404Z
while max_try is None or try_count < max_try:
try:
try_count += 1
>               ret = func(*args, **kwargs)
2025-04-11T04:23:18.9070613Z
colossalai/testing/utils.py:133:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
colossalai/testing/utils.py:271: in _clear_cache
get_accelerator().synchronize()
colossalai/accelerator/cuda_accelerator.py:62: in synchronize
torch.cuda.synchronize(device)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.9071544Z
device = None
2025-04-11T04:23:18.9071633Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.9072109Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.9073882Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
___________________________________ test_sam ___________________________________
2025-04-11T04:23:18.9074341Z
args = (), kwargs = {}, try_count = 1
2025-04-11T04:23:18.9074446Z
def _run_until_success(*args, **kwargs):
try_count = 0
assert max_try is None or isinstance(
max_try, int
), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T04:23:18.9075064Z
while max_try is None or try_count < max_try:
try:
try_count += 1
>               ret = func(*args, **kwargs)
2025-04-11T04:23:18.9075464Z
colossalai/testing/utils.py:133:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
colossalai/testing/utils.py:271: in _clear_cache
get_accelerator().synchronize()
colossalai/accelerator/cuda_accelerator.py:62: in synchronize
torch.cuda.synchronize(device)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.9076314Z
device = None
2025-04-11T04:23:18.9076405Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.9076859Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.9078536Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
___________________________________ test_t5 ____________________________________
2025-04-11T04:23:18.9078929Z
args = (), kwargs = {}, try_count = 1
2025-04-11T04:23:18.9079027Z
def _run_until_success(*args, **kwargs):
try_count = 0
assert max_try is None or isinstance(
max_try, int
), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T04:23:18.9079673Z
while max_try is None or try_count < max_try:
try:
try_count += 1
>               ret = func(*args, **kwargs)
2025-04-11T04:23:18.9080062Z
colossalai/testing/utils.py:133:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
colossalai/testing/utils.py:271: in _clear_cache
get_accelerator().synchronize()
colossalai/accelerator/cuda_accelerator.py:62: in synchronize
torch.cuda.synchronize(device)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.9080934Z
device = None
2025-04-11T04:23:18.9081021Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.9081384Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.9103669Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
___________________________________ test_vit ___________________________________
2025-04-11T04:23:18.9104082Z
args = (), kwargs = {}, try_count = 1
2025-04-11T04:23:18.9104184Z
def _run_until_success(*args, **kwargs):
try_count = 0
assert max_try is None or isinstance(
max_try, int
), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T04:23:18.9104776Z
while max_try is None or try_count < max_try:
try:
try_count += 1
>               ret = func(*args, **kwargs)
2025-04-11T04:23:18.9105215Z
colossalai/testing/utils.py:133:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
colossalai/testing/utils.py:271: in _clear_cache
get_accelerator().synchronize()
colossalai/accelerator/cuda_accelerator.py:62: in synchronize
torch.cuda.synchronize(device)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.9106127Z
device = None
2025-04-11T04:23:18.9106209Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.9106562Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.9108211Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
_________________________________ test_whisper _________________________________
2025-04-11T04:23:18.9108632Z
args = (), kwargs = {}, try_count = 1
2025-04-11T04:23:18.9108727Z
def _run_until_success(*args, **kwargs):
try_count = 0
assert max_try is None or isinstance(
max_try, int
), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T04:23:18.9109304Z
while max_try is None or try_count < max_try:
try:
try_count += 1
>               ret = func(*args, **kwargs)
2025-04-11T04:23:18.9109731Z
colossalai/testing/utils.py:133:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
colossalai/testing/utils.py:271: in _clear_cache
get_accelerator().synchronize()
colossalai/accelerator/cuda_accelerator.py:62: in synchronize
torch.cuda.synchronize(device)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.9110508Z
device = None
2025-04-11T04:23:18.9110590Z
def synchronize(device: _device_t = None) -> None:
r"""Wait for all kernels in all streams on a CUDA device to complete.
2025-04-11T04:23:18.9110928Z
Args:
device (torch.device or int, optional): device for which to synchronize.
It uses the current device, given by :func:`~torch.cuda.current_device`,
if :attr:`device` is ``None`` (default).
"""
_lazy_init()
with torch.cuda.device(device):
>           return torch._C._cuda_synchronize()
E           RuntimeError: CUDA error: out of memory
E           CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E           For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E           Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.9112595Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py:801: RuntimeError
________________________________ test_comm_spec ________________________________
2025-04-11T04:23:18.9113043Z
args = (), kwargs = {}, try_count = 1
error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T04:23:18.9113773Z
def _run_until_success(*args, **kwargs):
try_count = 0
assert max_try is None or isinstance(
max_try, int
), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T04:23:18.9114348Z
while max_try is None or try_count < max_try:
try:
try_count += 1
ret = func(*args, **kwargs)
return ret
except exception_type as e:
error_lines = str(e).split("\n")
if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
print("Exception is caught, retrying...")
# when pattern is not specified, we always skip the exception
# when pattern is specified, we only skip when pattern is matched
continue
else:
print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
>                   raise e
2025-04-11T04:23:18.9116129Z
colossalai/testing/utils.py:144:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
colossalai/testing/utils.py:133: in _run_until_success
ret = func(*args, **kwargs)
tests/test_tensor/test_comm_spec_apply.py:211: in test_comm_spec
spawn(check_comm, world_size)
colossalai/testing/utils.py:252: in spawn
mp.spawn(wrapped_func, nprocs=nprocs)
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
while not context.join():
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.9118006Z
self = <torch.multiprocessing.spawn.ProcessContext object at 0x7fb5b5ad0f40>
timeout = None
2025-04-11T04:23:18.9118339Z
def join(self, timeout=None):
r"""Join one or more processes within spawn context.
2025-04-11T04:23:18.9118628Z
Attempt to join one or more processes in this spawn context.
If one of them exited with a non-zero exit status, this function
kills the remaining processes and raises an exception with the cause
of the first process exiting.
2025-04-11T04:23:18.9119257Z
Returns ``True`` if all processes have been joined successfully,
``False`` if there are more processes that need to be joined.
2025-04-11T04:23:18.9119667Z
Args:
timeout (float): Wait this long before giving up on waiting.
"""
# Ensure this function can be called even when we're done.
if len(self.sentinels) == 0:
return True
2025-04-11T04:23:18.9120338Z
# Wait for any process to fail or all of them to succeed.
ready = multiprocessing.connection.wait(
self.sentinels.keys(),
timeout=timeout,
)
2025-04-11T04:23:18.9120926Z
error_index = None
for sentinel in ready:
index = self.sentinels.pop(sentinel)
process = self.processes[index]
process.join()
if process.exitcode != 0:
error_index = index
break
2025-04-11T04:23:18.9121734Z
# Return if there was no error.
if error_index is None:
# Return whether or not all processes have been joined.
return len(self.sentinels) == 0
2025-04-11T04:23:18.9122274Z
# Assume failure. Terminate processes that are still alive.
for process in self.processes:
if process.is_alive():
process.terminate()
process.join()
2025-04-11T04:23:18.9122860Z
# There won't be an error on the queue if the process crashed.
failed_process = self.processes[error_index]
if self.error_queues[error_index].empty():
exitcode = self.processes[error_index].exitcode
if exitcode < 0:
name = signal.Signals(-exitcode).name
raise ProcessExitedException(
"process %d terminated with signal %s" % (error_index, name),
error_index=error_index,
error_pid=failed_process.pid,
exit_code=exitcode,
signal_name=name,
)
else:
raise ProcessExitedException(
"process %d terminated with exit code %d" % (error_index, exitcode),
error_index=error_index,
error_pid=failed_process.pid,
exit_code=exitcode,
)
2025-04-11T04:23:18.9125169Z
original_trace = self.error_queues[error_index].get()
msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
msg += original_trace
>       raise ProcessRaisedException(msg, error_index, failed_process.pid)
E       torch.multiprocessing.spawn.ProcessRaisedException:
E
E       -- Process 0 terminated with the following error:
E       Traceback (most recent call last):
E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
E           fn(i, *args)
E         File "/__w/ColossalAI/ColossalAI/tests/test_tensor/test_comm_spec_apply.py", line 191, in check_comm
E           check_all_gather(device_mesh, rank)
E         File "/__w/ColossalAI/ColossalAI/tests/test_tensor/test_comm_spec_apply.py", line 16, in check_all_gather
E           sharded_tensor_to_comm = torch.ones(2, 2).cuda()
E       RuntimeError: CUDA error: out of memory
E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.9128168Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
----------------------------- Captured stdout call -----------------------------
[04/11/25 04:20:53] INFO     colossalai - colossalai - INFO:
/__w/ColossalAI/ColossalAI/colossalai/initialize.py
:75 launch
INFO     colossalai - colossalai - INFO: Distributed
environment is initialized, world size: 4
Maximum number of attempts is reached or pattern is not matched, no more retrying...
----------------------------- Captured stderr call -----------------------------
[W socket.cpp:697] [c10d] The client socket has failed to connect to [localhost]:56178 (errno: 99 - Cannot assign requested address).
[W socket.cpp:697] [c10d] The client socket has failed to connect to [localhost]:56178 (errno: 99 - Cannot assign requested address).
______________________________ test_padded_tensor ______________________________
2025-04-11T04:23:18.9130512Z
args = (), kwargs = {}, try_count = 1
error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T04:23:18.9131223Z
def _run_until_success(*args, **kwargs):
try_count = 0
assert max_try is None or isinstance(
max_try, int
), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T04:23:18.9131868Z
while max_try is None or try_count < max_try:
try:
try_count += 1
ret = func(*args, **kwargs)
return ret
except exception_type as e:
error_lines = str(e).split("\n")
if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
print("Exception is caught, retrying...")
# when pattern is not specified, we always skip the exception
# when pattern is specified, we only skip when pattern is matched
continue
else:
print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
>                   raise e
2025-04-11T04:23:18.9133664Z
colossalai/testing/utils.py:144:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
colossalai/testing/utils.py:133: in _run_until_success
ret = func(*args, **kwargs)
tests/test_tensor/test_padded_tensor.py:42: in test_padded_tensor
spawn(check_padded_tensor, world_size)
colossalai/testing/utils.py:252: in spawn
mp.spawn(wrapped_func, nprocs=nprocs)
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
while not context.join():
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.9135567Z
self = <torch.multiprocessing.spawn.ProcessContext object at 0x7fb581eca8f0>
timeout = None
2025-04-11T04:23:18.9135854Z
def join(self, timeout=None):
r"""Join one or more processes within spawn context.
2025-04-11T04:23:18.9136154Z
Attempt to join one or more processes in this spawn context.
If one of them exited with a non-zero exit status, this function
kills the remaining processes and raises an exception with the cause
of the first process exiting.
2025-04-11T04:23:18.9136789Z
Returns ``True`` if all processes have been joined successfully,
``False`` if there are more processes that need to be joined.
2025-04-11T04:23:18.9137209Z
Args:
timeout (float): Wait this long before giving up on waiting.
"""
# Ensure this function can be called even when we're done.
if len(self.sentinels) == 0:
return True
2025-04-11T04:23:18.9137978Z
# Wait for any process to fail or all of them to succeed.
ready = multiprocessing.connection.wait(
self.sentinels.keys(),
timeout=timeout,
)
2025-04-11T04:23:18.9138567Z
error_index = None
for sentinel in ready:
index = self.sentinels.pop(sentinel)
process = self.processes[index]
process.join()
if process.exitcode != 0:
error_index = index
break
2025-04-11T04:23:18.9139445Z
# Return if there was no error.
if error_index is None:
# Return whether or not all processes have been joined.
return len(self.sentinels) == 0
2025-04-11T04:23:18.9139934Z
# Assume failure. Terminate processes that are still alive.
for process in self.processes:
if process.is_alive():
process.terminate()
process.join()
2025-04-11T04:23:18.9140531Z
# There won't be an error on the queue if the process crashed.
failed_process = self.processes[error_index]
if self.error_queues[error_index].empty():
exitcode = self.processes[error_index].exitcode
if exitcode < 0:
name = signal.Signals(-exitcode).name
raise ProcessExitedException(
"process %d terminated with signal %s" % (error_index, name),
error_index=error_index,
error_pid=failed_process.pid,
exit_code=exitcode,
signal_name=name,
)
else:
raise ProcessExitedException(
"process %d terminated with exit code %d" % (error_index, exitcode),
error_index=error_index,
error_pid=failed_process.pid,
exit_code=exitcode,
)
2025-04-11T04:23:18.9142820Z
original_trace = self.error_queues[error_index].get()
msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
msg += original_trace
>       raise ProcessRaisedException(msg, error_index, failed_process.pid)
E       torch.multiprocessing.spawn.ProcessRaisedException:
E
E       -- Process 0 terminated with the following error:
E       Traceback (most recent call last):
E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
E           fn(i, *args)
E         File "/__w/ColossalAI/ColossalAI/tests/test_tensor/test_padded_tensor.py", line 14, in check_padded_tensor
E           original_tensor = torch.rand(32, 64).to("cuda")
E       RuntimeError: CUDA error: out of memory
E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.9145528Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
----------------------------- Captured stdout call -----------------------------
[04/11/25 04:20:59] INFO     colossalai - colossalai - INFO:
/__w/ColossalAI/ColossalAI/colossalai/initialize.py
:75 launch
INFO     colossalai - colossalai - INFO: Distributed
environment is initialized, world size: 4
Maximum number of attempts is reached or pattern is not matched, no more retrying...
----------------------------- Captured stderr call -----------------------------
[W socket.cpp:697] [c10d] The client socket has failed to connect to [localhost]:39213 (errno: 99 - Cannot assign requested address).
__________________________________ test_apply __________________________________
2025-04-11T04:23:18.9147456Z
args = (), kwargs = {}, try_count = 1
error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T04:23:18.9148221Z
def _run_until_success(*args, **kwargs):
try_count = 0
assert max_try is None or isinstance(
max_try, int
), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T04:23:18.9148847Z
while max_try is None or try_count < max_try:
try:
try_count += 1
ret = func(*args, **kwargs)
return ret
except exception_type as e:
error_lines = str(e).split("\n")
if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
print("Exception is caught, retrying...")
# when pattern is not specified, we always skip the exception
# when pattern is specified, we only skip when pattern is matched
continue
else:
print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
>                   raise e
2025-04-11T04:23:18.9150726Z
colossalai/testing/utils.py:144:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
colossalai/testing/utils.py:133: in _run_until_success
ret = func(*args, **kwargs)
tests/test_tensor/test_shape_consistency_apply.py:72: in test_apply
spawn(check_apply, world_size)
colossalai/testing/utils.py:252: in spawn
mp.spawn(wrapped_func, nprocs=nprocs)
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
while not context.join():
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.9152596Z
self = <torch.multiprocessing.spawn.ProcessContext object at 0x7fb5b5a6f6a0>
timeout = None
2025-04-11T04:23:18.9152875Z
def join(self, timeout=None):
r"""Join one or more processes within spawn context.
2025-04-11T04:23:18.9153160Z
Attempt to join one or more processes in this spawn context.
If one of them exited with a non-zero exit status, this function
kills the remaining processes and raises an exception with the cause
of the first process exiting.
2025-04-11T04:23:18.9153769Z
Returns ``True`` if all processes have been joined successfully,
``False`` if there are more processes that need to be joined.
2025-04-11T04:23:18.9154117Z
Args:
timeout (float): Wait this long before giving up on waiting.
"""
# Ensure this function can be called even when we're done.
if len(self.sentinels) == 0:
return True
2025-04-11T04:23:18.9154836Z
# Wait for any process to fail or all of them to succeed.
ready = multiprocessing.connection.wait(
self.sentinels.keys(),
timeout=timeout,
)
2025-04-11T04:23:18.9155394Z
error_index = None
for sentinel in ready:
index = self.sentinels.pop(sentinel)
process = self.processes[index]
process.join()
if process.exitcode != 0:
error_index = index
break
2025-04-11T04:23:18.9156173Z
# Return if there was no error.
if error_index is None:
# Return whether or not all processes have been joined.
return len(self.sentinels) == 0
2025-04-11T04:23:18.9156699Z
# Assume failure. Terminate processes that are still alive.
for process in self.processes:
if process.is_alive():
process.terminate()
process.join()
2025-04-11T04:23:18.9157317Z
# There won't be an error on the queue if the process crashed.
failed_process = self.processes[error_index]
if self.error_queues[error_index].empty():
exitcode = self.processes[error_index].exitcode
if exitcode < 0:
name = signal.Signals(-exitcode).name
raise ProcessExitedException(
"process %d terminated with signal %s" % (error_index, name),
error_index=error_index,
error_pid=failed_process.pid,
exit_code=exitcode,
signal_name=name,
)
else:
raise ProcessExitedException(
"process %d terminated with exit code %d" % (error_index, exitcode),
error_index=error_index,
error_pid=failed_process.pid,
exit_code=exitcode,
)
2025-04-11T04:23:18.9159508Z
original_trace = self.error_queues[error_index].get()
msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
msg += original_trace
>       raise ProcessRaisedException(msg, error_index, failed_process.pid)
E       torch.multiprocessing.spawn.ProcessRaisedException:
E
E       -- Process 0 terminated with the following error:
E       Traceback (most recent call last):
E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
E           fn(i, *args)
E         File "/__w/ColossalAI/ColossalAI/tests/test_tensor/test_shape_consistency_apply.py", line 41, in check_apply
E           tensor_to_comm = torch.cat((sharded_tensor_0, sharded_tensor_1), 1).cuda()
E       RuntimeError: CUDA error: out of memory
E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.9162102Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
----------------------------- Captured stdout call -----------------------------
[04/11/25 04:21:03] INFO     colossalai - colossalai - INFO:
/__w/ColossalAI/ColossalAI/colossalai/initialize.py
:75 launch
INFO     colossalai - colossalai - INFO: Distributed
environment is initialized, world size: 4
Maximum number of attempts is reached or pattern is not matched, no more retrying...
________________________________ test_comm_spec ________________________________
2025-04-11T04:23:18.9163644Z
args = (), kwargs = {}, try_count = 1
error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T04:23:18.9164351Z
def _run_until_success(*args, **kwargs):
try_count = 0
assert max_try is None or isinstance(
max_try, int
), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T04:23:18.9164978Z
while max_try is None or try_count < max_try:
try:
try_count += 1
ret = func(*args, **kwargs)
return ret
except exception_type as e:
error_lines = str(e).split("\n")
if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
print("Exception is caught, retrying...")
# when pattern is not specified, we always skip the exception
# when pattern is specified, we only skip when pattern is matched
continue
else:
print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
>                   raise e
2025-04-11T04:23:18.9166672Z
colossalai/testing/utils.py:144:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
colossalai/testing/utils.py:133: in _run_until_success
ret = func(*args, **kwargs)
tests/test_tensor/test_dtensor/test_comm_spec.py:157: in test_comm_spec
spawn(check_comm, world_size)
colossalai/testing/utils.py:252: in spawn
mp.spawn(wrapped_func, nprocs=nprocs)
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
while not context.join():
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.9168534Z
self = <torch.multiprocessing.spawn.ProcessContext object at 0x7fb5b5a6f700>
timeout = None
2025-04-11T04:23:18.9168867Z
def join(self, timeout=None):
r"""Join one or more processes within spawn context.
2025-04-11T04:23:18.9169150Z
Attempt to join one or more processes in this spawn context.
If one of them exited with a non-zero exit status, this function
kills the remaining processes and raises an exception with the cause
of the first process exiting.
2025-04-11T04:23:18.9169811Z
Returns ``True`` if all processes have been joined successfully,
``False`` if there are more processes that need to be joined.
2025-04-11T04:23:18.9170165Z
Args:
timeout (float): Wait this long before giving up on waiting.
"""
# Ensure this function can be called even when we're done.
if len(self.sentinels) == 0:
return True
2025-04-11T04:23:18.9170877Z
# Wait for any process to fail or all of them to succeed.
ready = multiprocessing.connection.wait(
self.sentinels.keys(),
timeout=timeout,
)
2025-04-11T04:23:18.9171445Z
error_index = None
for sentinel in ready:
index = self.sentinels.pop(sentinel)
process = self.processes[index]
process.join()
if process.exitcode != 0:
error_index = index
break
2025-04-11T04:23:18.9172221Z
# Return if there was no error.
if error_index is None:
# Return whether or not all processes have been joined.
return len(self.sentinels) == 0
2025-04-11T04:23:18.9172692Z
# Assume failure. Terminate processes that are still alive.
for process in self.processes:
if process.is_alive():
process.terminate()
process.join()
2025-04-11T04:23:18.9173306Z
# There won't be an error on the queue if the process crashed.
failed_process = self.processes[error_index]
if self.error_queues[error_index].empty():
exitcode = self.processes[error_index].exitcode
if exitcode < 0:
name = signal.Signals(-exitcode).name
raise ProcessExitedException(
"process %d terminated with signal %s" % (error_index, name),
error_index=error_index,
error_pid=failed_process.pid,
exit_code=exitcode,
signal_name=name,
)
else:
raise ProcessExitedException(
"process %d terminated with exit code %d" % (error_index, exitcode),
error_index=error_index,
error_pid=failed_process.pid,
exit_code=exitcode,
)
2025-04-11T04:23:18.9175485Z
original_trace = self.error_queues[error_index].get()
msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
msg += original_trace
>       raise ProcessRaisedException(msg, error_index, failed_process.pid)
E       torch.multiprocessing.spawn.ProcessRaisedException:
E
E       -- Process 0 terminated with the following error:
E       Traceback (most recent call last):
E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
E           fn(i, *args)
E         File "/__w/ColossalAI/ColossalAI/tests/test_tensor/test_dtensor/test_comm_spec.py", line 140, in check_comm
E           check_all_gather(process_group_dict, rank)
E         File "/__w/ColossalAI/ColossalAI/tests/test_tensor/test_dtensor/test_comm_spec.py", line 15, in check_all_gather
E           sharded_tensor_to_comm = torch.ones(2, 2).cuda()
E       RuntimeError: CUDA error: out of memory
E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.9178510Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
----------------------------- Captured stdout call -----------------------------
[04/11/25 04:21:09] INFO     colossalai - colossalai - INFO:
/__w/ColossalAI/ColossalAI/colossalai/initialize.py
:75 launch
INFO     colossalai - colossalai - INFO: Distributed
environment is initialized, world size: 4
Maximum number of attempts is reached or pattern is not matched, no more retrying...
----------------------------- Captured stderr call -----------------------------
[W socket.cpp:697] [c10d] The client socket has failed to connect to [localhost]:60535 (errno: 99 - Cannot assign requested address).
[W socket.cpp:697] [c10d] The client socket has failed to connect to [localhost]:60535 (errno: 99 - Cannot assign requested address).
_________________________________ test_dtensor _________________________________
2025-04-11T04:23:18.9180749Z
args = (), kwargs = {}, try_count = 1
error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T04:23:18.9181476Z
def _run_until_success(*args, **kwargs):
try_count = 0
assert max_try is None or isinstance(
max_try, int
), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T04:23:18.9182104Z
while max_try is None or try_count < max_try:
try:
try_count += 1
ret = func(*args, **kwargs)
return ret
except exception_type as e:
error_lines = str(e).split("\n")
if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
print("Exception is caught, retrying...")
# when pattern is not specified, we always skip the exception
# when pattern is specified, we only skip when pattern is matched
continue
else:
print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
>                   raise e
2025-04-11T04:23:18.9183912Z
colossalai/testing/utils.py:144:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
colossalai/testing/utils.py:133: in _run_until_success
ret = func(*args, **kwargs)
tests/test_tensor/test_dtensor/test_dtensor.py:83: in test_dtensor
spawn(check_dtensor, world_size)
colossalai/testing/utils.py:252: in spawn
mp.spawn(wrapped_func, nprocs=nprocs)
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
while not context.join():
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.9185718Z
self = <torch.multiprocessing.spawn.ProcessContext object at 0x7fb581db6ec0>
timeout = None
2025-04-11T04:23:18.9186061Z
def join(self, timeout=None):
r"""Join one or more processes within spawn context.
2025-04-11T04:23:18.9186345Z
Attempt to join one or more processes in this spawn context.
If one of them exited with a non-zero exit status, this function
kills the remaining processes and raises an exception with the cause
of the first process exiting.
2025-04-11T04:23:18.9186959Z
Returns ``True`` if all processes have been joined successfully,
``False`` if there are more processes that need to be joined.
2025-04-11T04:23:18.9187315Z
Args:
timeout (float): Wait this long before giving up on waiting.
"""
# Ensure this function can be called even when we're done.
if len(self.sentinels) == 0:
return True
2025-04-11T04:23:18.9188028Z
# Wait for any process to fail or all of them to succeed.
ready = multiprocessing.connection.wait(
self.sentinels.keys(),
timeout=timeout,
)
2025-04-11T04:23:18.9188670Z
error_index = None
for sentinel in ready:
index = self.sentinels.pop(sentinel)
process = self.processes[index]
process.join()
if process.exitcode != 0:
error_index = index
break
2025-04-11T04:23:18.9189459Z
# Return if there was no error.
if error_index is None:
# Return whether or not all processes have been joined.
return len(self.sentinels) == 0
2025-04-11T04:23:18.9189983Z
# Assume failure. Terminate processes that are still alive.
for process in self.processes:
if process.is_alive():
process.terminate()
process.join()
2025-04-11T04:23:18.9190545Z
# There won't be an error on the queue if the process crashed.
failed_process = self.processes[error_index]
if self.error_queues[error_index].empty():
exitcode = self.processes[error_index].exitcode
if exitcode < 0:
name = signal.Signals(-exitcode).name
raise ProcessExitedException(
"process %d terminated with signal %s" % (error_index, name),
error_index=error_index,
error_pid=failed_process.pid,
exit_code=exitcode,
signal_name=name,
)
else:
raise ProcessExitedException(
"process %d terminated with exit code %d" % (error_index, exitcode),
error_index=error_index,
error_pid=failed_process.pid,
exit_code=exitcode,
)
2025-04-11T04:23:18.9192724Z
original_trace = self.error_queues[error_index].get()
msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
msg += original_trace
>       raise ProcessRaisedException(msg, error_index, failed_process.pid)
E       torch.multiprocessing.spawn.ProcessRaisedException:
E
E       -- Process 0 terminated with the following error:
E       Traceback (most recent call last):
E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
E           fn(i, *args)
E         File "/__w/ColossalAI/ColossalAI/tests/test_tensor/test_dtensor/test_dtensor.py", line 25, in check_dtensor
E           test_model = TestModel(8, 8).to("cuda")
E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1152, in to
E           return self._apply(convert)
E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
E           module._apply(fn)
E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
E           param_applied = fn(param)
E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1150, in convert
E           return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
E       RuntimeError: CUDA error: out of memory
E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.9196915Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
----------------------------- Captured stdout call -----------------------------
[04/11/25 04:21:13] INFO     colossalai - colossalai - INFO:
/__w/ColossalAI/ColossalAI/colossalai/initialize.py
:75 launch
INFO     colossalai - colossalai - INFO: Distributed
environment is initialized, world size: 4
Maximum number of attempts is reached or pattern is not matched, no more retrying...
____________________________ test_layout_converter _____________________________
2025-04-11T04:23:18.9198369Z
args = (), kwargs = {}, try_count = 1
error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T04:23:18.9199104Z
def _run_until_success(*args, **kwargs):
try_count = 0
assert max_try is None or isinstance(
max_try, int
), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T04:23:18.9199686Z
while max_try is None or try_count < max_try:
try:
try_count += 1
ret = func(*args, **kwargs)
return ret
except exception_type as e:
error_lines = str(e).split("\n")
if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
print("Exception is caught, retrying...")
# when pattern is not specified, we always skip the exception
# when pattern is specified, we only skip when pattern is matched
continue
else:
print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
>                   raise e
2025-04-11T04:23:18.9201476Z
colossalai/testing/utils.py:144:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
colossalai/testing/utils.py:133: in _run_until_success
ret = func(*args, **kwargs)
tests/test_tensor/test_dtensor/test_layout_converter.py:180: in test_layout_converter
spawn(check_layout_converting_apply, world_size)
colossalai/testing/utils.py:252: in spawn
mp.spawn(wrapped_func, nprocs=nprocs)
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
while not context.join():
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.9203397Z
self = <torch.multiprocessing.spawn.ProcessContext object at 0x7fb5b85701f0>
timeout = None
2025-04-11T04:23:18.9203681Z
def join(self, timeout=None):
r"""Join one or more processes within spawn context.
2025-04-11T04:23:18.9203969Z
Attempt to join one or more processes in this spawn context.
If one of them exited with a non-zero exit status, this function
kills the remaining processes and raises an exception with the cause
of the first process exiting.
2025-04-11T04:23:18.9204585Z
Returns ``True`` if all processes have been joined successfully,
``False`` if there are more processes that need to be joined.
2025-04-11T04:23:18.9204985Z
Args:
timeout (float): Wait this long before giving up on waiting.
"""
# Ensure this function can be called even when we're done.
if len(self.sentinels) == 0:
return True
2025-04-11T04:23:18.9205657Z
# Wait for any process to fail or all of them to succeed.
ready = multiprocessing.connection.wait(
self.sentinels.keys(),
timeout=timeout,
)
2025-04-11T04:23:18.9206224Z
error_index = None
for sentinel in ready:
index = self.sentinels.pop(sentinel)
process = self.processes[index]
process.join()
if process.exitcode != 0:
error_index = index
break
2025-04-11T04:23:18.9207055Z
# Return if there was no error.
if error_index is None:
# Return whether or not all processes have been joined.
return len(self.sentinels) == 0
2025-04-11T04:23:18.9207574Z
# Assume failure. Terminate processes that are still alive.
for process in self.processes:
if process.is_alive():
process.terminate()
process.join()
2025-04-11T04:23:18.9208138Z
# There won't be an error on the queue if the process crashed.
failed_process = self.processes[error_index]
if self.error_queues[error_index].empty():
exitcode = self.processes[error_index].exitcode
if exitcode < 0:
name = signal.Signals(-exitcode).name
raise ProcessExitedException(
"process %d terminated with signal %s" % (error_index, name),
error_index=error_index,
error_pid=failed_process.pid,
exit_code=exitcode,
signal_name=name,
)
else:
raise ProcessExitedException(
"process %d terminated with exit code %d" % (error_index, exitcode),
error_index=error_index,
error_pid=failed_process.pid,
exit_code=exitcode,
)
2025-04-11T04:23:18.9210319Z
original_trace = self.error_queues[error_index].get()
msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
msg += original_trace
>       raise ProcessRaisedException(msg, error_index, failed_process.pid)
E       torch.multiprocessing.spawn.ProcessRaisedException:
E
E       -- Process 0 terminated with the following error:
E       Traceback (most recent call last):
E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
E           fn(i, *args)
E         File "/__w/ColossalAI/ColossalAI/tests/test_tensor/test_dtensor/test_layout_converter.py", line 162, in check_layout_converting_apply
E           original_tensor = torch.rand(global_shape).cuda()
E       RuntimeError: CUDA error: out of memory
E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.9212935Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
----------------------------- Captured stdout call -----------------------------
[04/11/25 04:21:19] INFO     colossalai - colossalai - INFO:
/__w/ColossalAI/ColossalAI/colossalai/initialize.py
:75 launch
INFO     colossalai - colossalai - INFO: Distributed
environment is initialized, world size: 4
[04/11/25 04:21:24] INFO     colossalai - colossalai - INFO:
/__w/ColossalAI/ColossalAI/colossalai/initialize.py
:75 launch
INFO     colossalai - colossalai - INFO: Distributed
environment is initialized, world size: 4
[04/11/25 04:21:28] INFO     colossalai - colossalai - INFO:
/__w/ColossalAI/ColossalAI/colossalai/initialize.py
:75 launch
INFO     colossalai - colossalai - INFO: Distributed
environment is initialized, world size: 4
Maximum number of attempts is reached or pattern is not matched, no more retrying...
----------------------------- Captured stderr call -----------------------------
[W socket.cpp:697] [c10d] The client socket has failed to connect to [localhost]:57837 (errno: 99 - Cannot assign requested address).
[W socket.cpp:697] [c10d] The client socket has failed to connect to [localhost]:57837 (errno: 99 - Cannot assign requested address).
[W socket.cpp:697] [c10d] The client socket has failed to connect to [localhost]:32601 (errno: 99 - Cannot assign requested address).
[W socket.cpp:697] [c10d] The client socket has failed to connect to [localhost]:32601 (errno: 99 - Cannot assign requested address).
[W socket.cpp:697] [c10d] The client socket has failed to connect to [localhost]:24558 (errno: 99 - Cannot assign requested address).
[W socket.cpp:697] [c10d] The client socket has failed to connect to [localhost]:24558 (errno: 99 - Cannot assign requested address).
____________________________ test_chunk_manager[2] _____________________________
2025-04-11T04:23:18.9217653Z
args = (), kwargs = {'world_size': 2}, try_count = 1
error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T04:23:18.9218385Z
def _run_until_success(*args, **kwargs):
try_count = 0
assert max_try is None or isinstance(
max_try, int
), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T04:23:18.9218964Z
while max_try is None or try_count < max_try:
try:
try_count += 1
ret = func(*args, **kwargs)
return ret
except exception_type as e:
error_lines = str(e).split("\n")
if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
print("Exception is caught, retrying...")
# when pattern is not specified, we always skip the exception
# when pattern is specified, we only skip when pattern is matched
continue
else:
print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
>                   raise e
2025-04-11T04:23:18.9220792Z
colossalai/testing/utils.py:144:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
colossalai/testing/utils.py:133: in _run_until_success
ret = func(*args, **kwargs)
tests/test_zero/test_gemini/test_chunk_mgrv2.py:60: in test_chunk_manager
spawn(run_dist, world_size)
colossalai/testing/utils.py:252: in spawn
mp.spawn(wrapped_func, nprocs=nprocs)
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
while not context.join():
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.9222660Z
self = <torch.multiprocessing.spawn.ProcessContext object at 0x7fb581d6cc70>
timeout = None
2025-04-11T04:23:18.9222938Z
def join(self, timeout=None):
r"""Join one or more processes within spawn context.
2025-04-11T04:23:18.9223223Z
Attempt to join one or more processes in this spawn context.
If one of them exited with a non-zero exit status, this function
kills the remaining processes and raises an exception with the cause
of the first process exiting.
2025-04-11T04:23:18.9223892Z
Returns ``True`` if all processes have been joined successfully,
``False`` if there are more processes that need to be joined.
2025-04-11T04:23:18.9224243Z
Args:
timeout (float): Wait this long before giving up on waiting.
"""
# Ensure this function can be called even when we're done.
if len(self.sentinels) == 0:
return True
2025-04-11T04:23:18.9224908Z
# Wait for any process to fail or all of them to succeed.
ready = multiprocessing.connection.wait(
self.sentinels.keys(),
timeout=timeout,
)
2025-04-11T04:23:18.9225518Z
error_index = None
for sentinel in ready:
index = self.sentinels.pop(sentinel)
process = self.processes[index]
process.join()
if process.exitcode != 0:
error_index = index
break
2025-04-11T04:23:18.9226353Z
# Return if there was no error.
if error_index is None:
# Return whether or not all processes have been joined.
return len(self.sentinels) == 0
2025-04-11T04:23:18.9226822Z
# Assume failure. Terminate processes that are still alive.
for process in self.processes:
if process.is_alive():
process.terminate()
process.join()
2025-04-11T04:23:18.9227433Z
# There won't be an error on the queue if the process crashed.
failed_process = self.processes[error_index]
if self.error_queues[error_index].empty():
exitcode = self.processes[error_index].exitcode
if exitcode < 0:
name = signal.Signals(-exitcode).name
raise ProcessExitedException(
"process %d terminated with signal %s" % (error_index, name),
error_index=error_index,
error_pid=failed_process.pid,
exit_code=exitcode,
signal_name=name,
)
else:
raise ProcessExitedException(
"process %d terminated with exit code %d" % (error_index, exitcode),
error_index=error_index,
error_pid=failed_process.pid,
exit_code=exitcode,
)
2025-04-11T04:23:18.9229591Z
original_trace = self.error_queues[error_index].get()
msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
msg += original_trace
>       raise ProcessRaisedException(msg, error_index, failed_process.pid)
E       torch.multiprocessing.spawn.ProcessRaisedException:
E
E       -- Process 0 terminated with the following error:
E       Traceback (most recent call last):
E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
E           fn(i, *args)
E         File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_gemini/test_chunk_mgrv2.py", line 53, in run_dist
E           exam_chunk_memory()
E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
E           partial_func(**kwargs)
E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
E           partial_func(**kwargs)
E         File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_gemini/test_chunk_mgrv2.py", line 21, in exam_chunk_memory
E           chunk_manager = ChunkManager(config)
E         File "/__w/ColossalAI/ColossalAI/colossalai/zero/gemini/chunk/manager.py", line 46, in __init__
E           self.overflow_counter = torch.tensor([0], dtype=torch.int, device=get_accelerator().get_current_device())
E       RuntimeError: CUDA error: out of memory
E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.9233793Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
----------------------------- Captured stdout call -----------------------------
[04/11/25 04:21:33] INFO     colossalai - colossalai - INFO:
/__w/ColossalAI/ColossalAI/colossalai/initialize.py
:75 launch
INFO     colossalai - colossalai - INFO: Distributed
environment is initialized, world size: 2
Maximum number of attempts is reached or pattern is not matched, no more retrying...
----------------------------- Captured stderr call -----------------------------
/__w/ColossalAI/ColossalAI/colossalai/zero/gemini/chunk/chunk.py:46: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
return tensor.storage().size() == 0
____________________________ test_chunk_function[1] ____________________________
2025-04-11T04:23:18.9236413Z
args = (), kwargs = {'world_size': 1}, try_count = 1
error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T04:23:18.9237193Z
def _run_until_success(*args, **kwargs):
try_count = 0
assert max_try is None or isinstance(
max_try, int
), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T04:23:18.9237776Z
while max_try is None or try_count < max_try:
try:
try_count += 1
ret = func(*args, **kwargs)
return ret
except exception_type as e:
error_lines = str(e).split("\n")
if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
print("Exception is caught, retrying...")
# when pattern is not specified, we always skip the exception
# when pattern is specified, we only skip when pattern is matched
continue
else:
print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
>                   raise e
2025-04-11T04:23:18.9239571Z
colossalai/testing/utils.py:144:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
colossalai/testing/utils.py:133: in _run_until_success
ret = func(*args, **kwargs)
tests/test_zero/test_gemini/test_chunkv2.py:123: in test_chunk_function
spawn(run_dist, world_size)
colossalai/testing/utils.py:252: in spawn
mp.spawn(wrapped_func, nprocs=nprocs)
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
while not context.join():
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.9241433Z
self = <torch.multiprocessing.spawn.ProcessContext object at 0x7fb5b5c77700>
timeout = None
2025-04-11T04:23:18.9241715Z
def join(self, timeout=None):
r"""Join one or more processes within spawn context.
2025-04-11T04:23:18.9242001Z
Attempt to join one or more processes in this spawn context.
If one of them exited with a non-zero exit status, this function
kills the remaining processes and raises an exception with the cause
of the first process exiting.
2025-04-11T04:23:18.9242613Z
Returns ``True`` if all processes have been joined successfully,
``False`` if there are more processes that need to be joined.
2025-04-11T04:23:18.9243015Z
Args:
timeout (float): Wait this long before giving up on waiting.
"""
# Ensure this function can be called even when we're done.
if len(self.sentinels) == 0:
return True
2025-04-11T04:23:18.9243690Z
# Wait for any process to fail or all of them to succeed.
ready = multiprocessing.connection.wait(
self.sentinels.keys(),
timeout=timeout,
)
2025-04-11T04:23:18.9244255Z
error_index = None
for sentinel in ready:
index = self.sentinels.pop(sentinel)
process = self.processes[index]
process.join()
if process.exitcode != 0:
error_index = index
break
2025-04-11T04:23:18.9245083Z
# Return if there was no error.
if error_index is None:
# Return whether or not all processes have been joined.
return len(self.sentinels) == 0
2025-04-11T04:23:18.9245597Z
# Assume failure. Terminate processes that are still alive.
for process in self.processes:
if process.is_alive():
process.terminate()
process.join()
2025-04-11T04:23:18.9246166Z
# There won't be an error on the queue if the process crashed.
failed_process = self.processes[error_index]
if self.error_queues[error_index].empty():
exitcode = self.processes[error_index].exitcode
if exitcode < 0:
name = signal.Signals(-exitcode).name
raise ProcessExitedException(
"process %d terminated with signal %s" % (error_index, name),
error_index=error_index,
error_pid=failed_process.pid,
exit_code=exitcode,
signal_name=name,
)
else:
raise ProcessExitedException(
"process %d terminated with exit code %d" % (error_index, exitcode),
error_index=error_index,
error_pid=failed_process.pid,
exit_code=exitcode,
)
2025-04-11T04:23:18.9248327Z
original_trace = self.error_queues[error_index].get()
msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
msg += original_trace
>       raise ProcessRaisedException(msg, error_index, failed_process.pid)
E       torch.multiprocessing.spawn.ProcessRaisedException:
E
E       -- Process 0 terminated with the following error:
E       Traceback (most recent call last):
E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
E           fn(i, *args)
E         File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_gemini/test_chunkv2.py", line 116, in run_dist
E           exam_chunk_basic()
E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
E           partial_func(**kwargs)
E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
E           partial_func(**kwargs)
E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
E           partial_func(**kwargs)
E         [Previous line repeated 1 more time]
E         File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_gemini/test_chunkv2.py", line 41, in exam_chunk_basic
E           my_chunk = Chunk(
E         File "/__w/ColossalAI/ColossalAI/colossalai/zero/gemini/chunk/chunk.py", line 113, in __init__
E           self.chunk_temp = torch.zeros(chunk_size, dtype=dtype, device=device)  # keep all zero
E       RuntimeError: CUDA error: out of memory
E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.9252809Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
----------------------------- Captured stdout call -----------------------------
[04/11/25 04:21:37] INFO     colossalai - colossalai - INFO:
/__w/ColossalAI/ColossalAI/colossalai/initialize.py
:75 launch
INFO     colossalai - colossalai - INFO: Distributed
environment is initialized, world size: 1
Maximum number of attempts is reached or pattern is not matched, no more retrying...
____________________________ test_chunk_function[2] ____________________________
2025-04-11T04:23:18.9254307Z
args = (), kwargs = {'world_size': 2}, try_count = 1
error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T04:23:18.9255029Z
def _run_until_success(*args, **kwargs):
try_count = 0
assert max_try is None or isinstance(
max_try, int
), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T04:23:18.9255656Z
while max_try is None or try_count < max_try:
try:
try_count += 1
ret = func(*args, **kwargs)
return ret
except exception_type as e:
error_lines = str(e).split("\n")
if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
print("Exception is caught, retrying...")
# when pattern is not specified, we always skip the exception
# when pattern is specified, we only skip when pattern is matched
continue
else:
print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
>                   raise e
2025-04-11T04:23:18.9257421Z
colossalai/testing/utils.py:144:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
colossalai/testing/utils.py:133: in _run_until_success
ret = func(*args, **kwargs)
tests/test_zero/test_gemini/test_chunkv2.py:123: in test_chunk_function
spawn(run_dist, world_size)
colossalai/testing/utils.py:252: in spawn
mp.spawn(wrapped_func, nprocs=nprocs)
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
while not context.join():
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.9259309Z
self = <torch.multiprocessing.spawn.ProcessContext object at 0x7fb5b5be2230>
timeout = None
2025-04-11T04:23:18.9259599Z
def join(self, timeout=None):
r"""Join one or more processes within spawn context.
2025-04-11T04:23:18.9259889Z
Attempt to join one or more processes in this spawn context.
If one of them exited with a non-zero exit status, this function
kills the remaining processes and raises an exception with the cause
of the first process exiting.
2025-04-11T04:23:18.9260501Z
Returns ``True`` if all processes have been joined successfully,
``False`` if there are more processes that need to be joined.
2025-04-11T04:23:18.9260860Z
Args:
timeout (float): Wait this long before giving up on waiting.
"""
# Ensure this function can be called even when we're done.
if len(self.sentinels) == 0:
return True
2025-04-11T04:23:18.9261524Z
# Wait for any process to fail or all of them to succeed.
ready = multiprocessing.connection.wait(
self.sentinels.keys(),
timeout=timeout,
)
2025-04-11T04:23:18.9262147Z
error_index = None
for sentinel in ready:
index = self.sentinels.pop(sentinel)
process = self.processes[index]
process.join()
if process.exitcode != 0:
error_index = index
break
2025-04-11T04:23:18.9262930Z
# Return if there was no error.
if error_index is None:
# Return whether or not all processes have been joined.
return len(self.sentinels) == 0
2025-04-11T04:23:18.9263405Z
# Assume failure. Terminate processes that are still alive.
for process in self.processes:
if process.is_alive():
process.terminate()
process.join()
2025-04-11T04:23:18.9264026Z
# There won't be an error on the queue if the process crashed.
failed_process = self.processes[error_index]
if self.error_queues[error_index].empty():
exitcode = self.processes[error_index].exitcode
if exitcode < 0:
name = signal.Signals(-exitcode).name
raise ProcessExitedException(
"process %d terminated with signal %s" % (error_index, name),
error_index=error_index,
error_pid=failed_process.pid,
exit_code=exitcode,
signal_name=name,
)
else:
raise ProcessExitedException(
"process %d terminated with exit code %d" % (error_index, exitcode),
error_index=error_index,
error_pid=failed_process.pid,
exit_code=exitcode,
)
2025-04-11T04:23:18.9266256Z
original_trace = self.error_queues[error_index].get()
msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
msg += original_trace
>       raise ProcessRaisedException(msg, error_index, failed_process.pid)
E       torch.multiprocessing.spawn.ProcessRaisedException:
E
E       -- Process 0 terminated with the following error:
E       Traceback (most recent call last):
E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
E           fn(i, *args)
E         File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_gemini/test_chunkv2.py", line 116, in run_dist
E           exam_chunk_basic()
E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
E           partial_func(**kwargs)
E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
E           partial_func(**kwargs)
E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
E           partial_func(**kwargs)
E         [Previous line repeated 1 more time]
E         File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_gemini/test_chunkv2.py", line 41, in exam_chunk_basic
E           my_chunk = Chunk(
E         File "/__w/ColossalAI/ColossalAI/colossalai/zero/gemini/chunk/chunk.py", line 113, in __init__
E           self.chunk_temp = torch.zeros(chunk_size, dtype=dtype, device=device)  # keep all zero
E       RuntimeError: CUDA error: out of memory
E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.9270722Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
----------------------------- Captured stdout call -----------------------------
[04/11/25 04:21:42] INFO     colossalai - colossalai - INFO:
/__w/ColossalAI/ColossalAI/colossalai/initialize.py
:75 launch
INFO     colossalai - colossalai - INFO: Distributed
environment is initialized, world size: 2
Maximum number of attempts is reached or pattern is not matched, no more retrying...
----------------------------- Captured stderr call -----------------------------
[W socket.cpp:697] [c10d] The client socket has failed to connect to [localhost]:34935 (errno: 99 - Cannot assign requested address).
____________________________ test_chunk_function[4] ____________________________
2025-04-11T04:23:18.9272707Z
args = (), kwargs = {'world_size': 4}, try_count = 1
error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T04:23:18.9273436Z
def _run_until_success(*args, **kwargs):
try_count = 0
assert max_try is None or isinstance(
max_try, int
), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T04:23:18.9274018Z
while max_try is None or try_count < max_try:
try:
try_count += 1
ret = func(*args, **kwargs)
return ret
except exception_type as e:
error_lines = str(e).split("\n")
if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
print("Exception is caught, retrying...")
# when pattern is not specified, we always skip the exception
# when pattern is specified, we only skip when pattern is matched
continue
else:
print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
>                   raise e
2025-04-11T04:23:18.9275777Z
colossalai/testing/utils.py:144:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
colossalai/testing/utils.py:133: in _run_until_success
ret = func(*args, **kwargs)
tests/test_zero/test_gemini/test_chunkv2.py:123: in test_chunk_function
spawn(run_dist, world_size)
colossalai/testing/utils.py:252: in spawn
mp.spawn(wrapped_func, nprocs=nprocs)
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
while not context.join():
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.9277751Z
self = <torch.multiprocessing.spawn.ProcessContext object at 0x7fb5b5c77700>
timeout = None
2025-04-11T04:23:18.9278032Z
def join(self, timeout=None):
r"""Join one or more processes within spawn context.
2025-04-11T04:23:18.9278362Z
Attempt to join one or more processes in this spawn context.
If one of them exited with a non-zero exit status, this function
kills the remaining processes and raises an exception with the cause
of the first process exiting.
2025-04-11T04:23:18.9278980Z
Returns ``True`` if all processes have been joined successfully,
``False`` if there are more processes that need to be joined.
2025-04-11T04:23:18.9279333Z
Args:
timeout (float): Wait this long before giving up on waiting.
"""
# Ensure this function can be called even when we're done.
if len(self.sentinels) == 0:
return True
2025-04-11T04:23:18.9279998Z
# Wait for any process to fail or all of them to succeed.
ready = multiprocessing.connection.wait(
self.sentinels.keys(),
timeout=timeout,
)
2025-04-11T04:23:18.9280557Z
error_index = None
for sentinel in ready:
index = self.sentinels.pop(sentinel)
process = self.processes[index]
process.join()
if process.exitcode != 0:
error_index = index
break
2025-04-11T04:23:18.9281392Z
# Return if there was no error.
if error_index is None:
# Return whether or not all processes have been joined.
return len(self.sentinels) == 0
2025-04-11T04:23:18.9281860Z
# Assume failure. Terminate processes that are still alive.
for process in self.processes:
if process.is_alive():
process.terminate()
process.join()
2025-04-11T04:23:18.9282426Z
# There won't be an error on the queue if the process crashed.
failed_process = self.processes[error_index]
if self.error_queues[error_index].empty():
exitcode = self.processes[error_index].exitcode
if exitcode < 0:
name = signal.Signals(-exitcode).name
raise ProcessExitedException(
"process %d terminated with signal %s" % (error_index, name),
error_index=error_index,
error_pid=failed_process.pid,
exit_code=exitcode,
signal_name=name,
)
else:
raise ProcessExitedException(
"process %d terminated with exit code %d" % (error_index, exitcode),
error_index=error_index,
error_pid=failed_process.pid,
exit_code=exitcode,
)
2025-04-11T04:23:18.9284694Z
original_trace = self.error_queues[error_index].get()
msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
msg += original_trace
>       raise ProcessRaisedException(msg, error_index, failed_process.pid)
E       torch.multiprocessing.spawn.ProcessRaisedException:
E
E       -- Process 0 terminated with the following error:
E       Traceback (most recent call last):
E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
E           fn(i, *args)
E         File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_gemini/test_chunkv2.py", line 116, in run_dist
E           exam_chunk_basic()
E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
E           partial_func(**kwargs)
E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
E           partial_func(**kwargs)
E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
E           partial_func(**kwargs)
E         [Previous line repeated 1 more time]
E         File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_gemini/test_chunkv2.py", line 41, in exam_chunk_basic
E           my_chunk = Chunk(
E         File "/__w/ColossalAI/ColossalAI/colossalai/zero/gemini/chunk/chunk.py", line 113, in __init__
E           self.chunk_temp = torch.zeros(chunk_size, dtype=dtype, device=device)  # keep all zero
E       RuntimeError: CUDA error: out of memory
E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.9289076Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
----------------------------- Captured stdout call -----------------------------
[04/11/25 04:21:48] INFO     colossalai - colossalai - INFO:
/__w/ColossalAI/ColossalAI/colossalai/initialize.py
:75 launch
INFO     colossalai - colossalai - INFO: Distributed
environment is initialized, world size: 4
Maximum number of attempts is reached or pattern is not matched, no more retrying...
----------------------------- Captured stderr call -----------------------------
[W socket.cpp:697] [c10d] The client socket has failed to connect to [localhost]:38965 (errno: 99 - Cannot assign requested address).
[W socket.cpp:697] [c10d] The client socket has failed to connect to [localhost]:38965 (errno: 99 - Cannot assign requested address).
____________________________ test_grad_accumulation ____________________________
2025-04-11T04:23:18.9291384Z
args = (), kwargs = {}, try_count = 1
error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T04:23:18.9292083Z
def _run_until_success(*args, **kwargs):
try_count = 0
assert max_try is None or isinstance(
max_try, int
), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T04:23:18.9292665Z
while max_try is None or try_count < max_try:
try:
try_count += 1
ret = func(*args, **kwargs)
return ret
except exception_type as e:
error_lines = str(e).split("\n")
if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
print("Exception is caught, retrying...")
# when pattern is not specified, we always skip the exception
# when pattern is specified, we only skip when pattern is matched
continue
else:
print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
>                   raise e
2025-04-11T04:23:18.9294418Z
colossalai/testing/utils.py:144:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
colossalai/testing/utils.py:133: in _run_until_success
ret = func(*args, **kwargs)
tests/test_zero/test_gemini/test_grad_accum.py:158: in test_grad_accumulation
spawn(run_dist, 2)
colossalai/testing/utils.py:252: in spawn
mp.spawn(wrapped_func, nprocs=nprocs)
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
while not context.join():
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.9296339Z
self = <torch.multiprocessing.spawn.ProcessContext object at 0x7fb5b5c76290>
timeout = None
2025-04-11T04:23:18.9296628Z
def join(self, timeout=None):
r"""Join one or more processes within spawn context.
2025-04-11T04:23:18.9296914Z
Attempt to join one or more processes in this spawn context.
If one of them exited with a non-zero exit status, this function
kills the remaining processes and raises an exception with the cause
of the first process exiting.
2025-04-11T04:23:18.9297570Z
Returns ``True`` if all processes have been joined successfully,
``False`` if there are more processes that need to be joined.
2025-04-11T04:23:18.9297923Z
Args:
timeout (float): Wait this long before giving up on waiting.
"""
# Ensure this function can be called even when we're done.
if len(self.sentinels) == 0:
return True
2025-04-11T04:23:18.9298589Z
# Wait for any process to fail or all of them to succeed.
ready = multiprocessing.connection.wait(
self.sentinels.keys(),
timeout=timeout,
)
2025-04-11T04:23:18.9299159Z
error_index = None
for sentinel in ready:
index = self.sentinels.pop(sentinel)
process = self.processes[index]
process.join()
if process.exitcode != 0:
error_index = index
break
2025-04-11T04:23:18.9299986Z
# Return if there was no error.
if error_index is None:
# Return whether or not all processes have been joined.
return len(self.sentinels) == 0
2025-04-11T04:23:18.9300474Z
# Assume failure. Terminate processes that are still alive.
for process in self.processes:
if process.is_alive():
process.terminate()
process.join()
2025-04-11T04:23:18.9301053Z
# There won't be an error on the queue if the process crashed.
failed_process = self.processes[error_index]
if self.error_queues[error_index].empty():
exitcode = self.processes[error_index].exitcode
if exitcode < 0:
name = signal.Signals(-exitcode).name
raise ProcessExitedException(
"process %d terminated with signal %s" % (error_index, name),
error_index=error_index,
error_pid=failed_process.pid,
exit_code=exitcode,
signal_name=name,
)
else:
raise ProcessExitedException(
"process %d terminated with exit code %d" % (error_index, exitcode),
error_index=error_index,
error_pid=failed_process.pid,
exit_code=exitcode,
)
2025-04-11T04:23:18.9303287Z
original_trace = self.error_queues[error_index].get()
msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
msg += original_trace
>       raise ProcessRaisedException(msg, error_index, failed_process.pid)
E       torch.multiprocessing.spawn.ProcessRaisedException:
E
E       -- Process 0 terminated with the following error:
E       Traceback (most recent call last):
E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
E           fn(i, *args)
E         File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_gemini/test_grad_accum.py", line 152, in run_dist
E           exam_gemini_grad_acc()
E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
E           partial_func(**kwargs)
E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
E           partial_func(**kwargs)
E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
E           partial_func(**kwargs)
E         [Previous line repeated 4 more times]
E         File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_gemini/test_grad_accum.py", line 71, in exam_gemini_grad_acc
E           torch_model = model_builder().cuda()
E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2548, in cuda
E           return super().cuda(*args, **kwargs)
E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in cuda
E           return self._apply(lambda t: t.cuda(device))
E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
E           module._apply(fn)
E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
E           module._apply(fn)
E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
E           param_applied = fn(param)
E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in <lambda>
E           return self._apply(lambda t: t.cuda(device))
E       RuntimeError: CUDA error: out of memory
E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.9309744Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
----------------------------- Captured stdout call -----------------------------
[04/11/25 04:21:55] INFO     colossalai - colossalai - INFO:
/__w/ColossalAI/ColossalAI/colossalai/initialize.py
:75 launch
INFO     colossalai - colossalai - INFO: Distributed
environment is initialized, world size: 2
Maximum number of attempts is reached or pattern is not matched, no more retrying...
----------------------------- Captured stderr call -----------------------------
/opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
/opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
/opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
warnings.warn(
/__w/ColossalAI/ColossalAI/colossalai/zero/gemini/chunk/chunk.py:46: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
return tensor.storage().size() == 0
/opt/conda/envs/pytorch/lib/python3.10/site-packages/apex/amp/_process_optimizer.py:344: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1711403380909/work/torch/csrc/tensor/python_tensor.cpp:83.)
optimizer._amp_stash.dummy_overflow_buf = torch.cuda.IntTensor([0]);
______________________________ test_grad_clip[1] _______________________________
2025-04-11T04:23:18.9323252Z
args = (), kwargs = {'world_size': 1}, try_count = 1
error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T04:23:18.9323985Z
def _run_until_success(*args, **kwargs):
try_count = 0
assert max_try is None or isinstance(
max_try, int
), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T04:23:18.9324565Z
while max_try is None or try_count < max_try:
try:
try_count += 1
ret = func(*args, **kwargs)
return ret
except exception_type as e:
error_lines = str(e).split("\n")
if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
print("Exception is caught, retrying...")
# when pattern is not specified, we always skip the exception
# when pattern is specified, we only skip when pattern is matched
continue
else:
print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
>                   raise e
2025-04-11T04:23:18.9326379Z
colossalai/testing/utils.py:144:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
colossalai/testing/utils.py:133: in _run_until_success
ret = func(*args, **kwargs)
tests/test_zero/test_gemini/test_grad_clip.py:134: in test_grad_clip
spawn(run_dist, world_size)
colossalai/testing/utils.py:252: in spawn
mp.spawn(wrapped_func, nprocs=nprocs)
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
while not context.join():
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.9328219Z
self = <torch.multiprocessing.spawn.ProcessContext object at 0x7fb5b5c77b20>
timeout = None
2025-04-11T04:23:18.9328503Z
def join(self, timeout=None):
r"""Join one or more processes within spawn context.
2025-04-11T04:23:18.9328786Z
Attempt to join one or more processes in this spawn context.
If one of them exited with a non-zero exit status, this function
kills the remaining processes and raises an exception with the cause
of the first process exiting.
2025-04-11T04:23:18.9329398Z
Returns ``True`` if all processes have been joined successfully,
``False`` if there are more processes that need to be joined.
2025-04-11T04:23:18.9329800Z
Args:
timeout (float): Wait this long before giving up on waiting.
"""
# Ensure this function can be called even when we're done.
if len(self.sentinels) == 0:
return True
2025-04-11T04:23:18.9330473Z
# Wait for any process to fail or all of them to succeed.
ready = multiprocessing.connection.wait(
self.sentinels.keys(),
timeout=timeout,
)
2025-04-11T04:23:18.9331039Z
error_index = None
for sentinel in ready:
index = self.sentinels.pop(sentinel)
process = self.processes[index]
process.join()
if process.exitcode != 0:
error_index = index
break
2025-04-11T04:23:18.9331874Z
# Return if there was no error.
if error_index is None:
# Return whether or not all processes have been joined.
return len(self.sentinels) == 0
2025-04-11T04:23:18.9332400Z
# Assume failure. Terminate processes that are still alive.
for process in self.processes:
if process.is_alive():
process.terminate()
process.join()
2025-04-11T04:23:18.9332971Z
# There won't be an error on the queue if the process crashed.
failed_process = self.processes[error_index]
if self.error_queues[error_index].empty():
exitcode = self.processes[error_index].exitcode
if exitcode < 0:
name = signal.Signals(-exitcode).name
raise ProcessExitedException(
"process %d terminated with signal %s" % (error_index, name),
error_index=error_index,
error_pid=failed_process.pid,
exit_code=exitcode,
signal_name=name,
)
else:
raise ProcessExitedException(
"process %d terminated with exit code %d" % (error_index, exitcode),
error_index=error_index,
error_pid=failed_process.pid,
exit_code=exitcode,
)
2025-04-11T04:23:18.9335158Z
original_trace = self.error_queues[error_index].get()
msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
msg += original_trace
>       raise ProcessRaisedException(msg, error_index, failed_process.pid)
E       torch.multiprocessing.spawn.ProcessRaisedException:
E
E       -- Process 0 terminated with the following error:
E       Traceback (most recent call last):
E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
E           fn(i, *args)
E         File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_gemini/test_grad_clip.py", line 127, in run_dist
E           exam_grad_clipping()
E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
E           partial_func(**kwargs)
E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
E           partial_func(**kwargs)
E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
E           partial_func(**kwargs)
E         [Previous line repeated 2 more times]
E         File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_gemini/test_grad_clip.py", line 65, in exam_grad_clipping
E           torch_model = model_builder().cuda()
E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2548, in cuda
E           return super().cuda(*args, **kwargs)
E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in cuda
E           return self._apply(lambda t: t.cuda(device))
E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
E           module._apply(fn)
E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
E           module._apply(fn)
E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
E           param_applied = fn(param)
E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in <lambda>
E           return self._apply(lambda t: t.cuda(device))
E       RuntimeError: CUDA error: out of memory
E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.9341591Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
----------------------------- Captured stdout call -----------------------------
[04/11/25 04:22:01] INFO     colossalai - colossalai - INFO:
/__w/ColossalAI/ColossalAI/colossalai/initialize.py
:75 launch
INFO     colossalai - colossalai - INFO: Distributed
environment is initialized, world size: 1
Maximum number of attempts is reached or pattern is not matched, no more retrying...
----------------------------- Captured stderr call -----------------------------
/opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
/opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
warnings.warn(
______________________________ test_grad_clip[2] _______________________________
2025-04-11T04:23:18.9348060Z
args = (), kwargs = {'world_size': 2}, try_count = 1
error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T04:23:18.9348832Z
def _run_until_success(*args, **kwargs):
try_count = 0
assert max_try is None or isinstance(
max_try, int
), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T04:23:18.9349417Z
while max_try is None or try_count < max_try:
try:
try_count += 1
ret = func(*args, **kwargs)
return ret
except exception_type as e:
error_lines = str(e).split("\n")
if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
print("Exception is caught, retrying...")
# when pattern is not specified, we always skip the exception
# when pattern is specified, we only skip when pattern is matched
continue
else:
print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
>                   raise e
2025-04-11T04:23:18.9351239Z
colossalai/testing/utils.py:144:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
colossalai/testing/utils.py:133: in _run_until_success
ret = func(*args, **kwargs)
tests/test_zero/test_gemini/test_grad_clip.py:134: in test_grad_clip
spawn(run_dist, world_size)
colossalai/testing/utils.py:252: in spawn
mp.spawn(wrapped_func, nprocs=nprocs)
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
while not context.join():
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.9353147Z
self = <torch.multiprocessing.spawn.ProcessContext object at 0x7fb5b841a9b0>
timeout = None
2025-04-11T04:23:18.9353429Z
def join(self, timeout=None):
r"""Join one or more processes within spawn context.
2025-04-11T04:23:18.9353711Z
Attempt to join one or more processes in this spawn context.
If one of them exited with a non-zero exit status, this function
kills the remaining processes and raises an exception with the cause
of the first process exiting.
2025-04-11T04:23:18.9354368Z
Returns ``True`` if all processes have been joined successfully,
``False`` if there are more processes that need to be joined.
2025-04-11T04:23:18.9354718Z
Args:
timeout (float): Wait this long before giving up on waiting.
"""
# Ensure this function can be called even when we're done.
if len(self.sentinels) == 0:
return True
2025-04-11T04:23:18.9355378Z
# Wait for any process to fail or all of them to succeed.
ready = multiprocessing.connection.wait(
self.sentinels.keys(),
timeout=timeout,
)
2025-04-11T04:23:18.9355945Z
error_index = None
for sentinel in ready:
index = self.sentinels.pop(sentinel)
process = self.processes[index]
process.join()
if process.exitcode != 0:
error_index = index
break
2025-04-11T04:23:18.9356767Z
# Return if there was no error.
if error_index is None:
# Return whether or not all processes have been joined.
return len(self.sentinels) == 0
2025-04-11T04:23:18.9357245Z
# Assume failure. Terminate processes that are still alive.
for process in self.processes:
if process.is_alive():
process.terminate()
process.join()
2025-04-11T04:23:18.9357813Z
# There won't be an error on the queue if the process crashed.
failed_process = self.processes[error_index]
if self.error_queues[error_index].empty():
exitcode = self.processes[error_index].exitcode
if exitcode < 0:
name = signal.Signals(-exitcode).name
raise ProcessExitedException(
"process %d terminated with signal %s" % (error_index, name),
error_index=error_index,
error_pid=failed_process.pid,
exit_code=exitcode,
signal_name=name,
)
else:
raise ProcessExitedException(
"process %d terminated with exit code %d" % (error_index, exitcode),
error_index=error_index,
error_pid=failed_process.pid,
exit_code=exitcode,
)
2025-04-11T04:23:18.9360050Z
original_trace = self.error_queues[error_index].get()
msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
msg += original_trace
>       raise ProcessRaisedException(msg, error_index, failed_process.pid)
E       torch.multiprocessing.spawn.ProcessRaisedException:
E
E       -- Process 0 terminated with the following error:
E       Traceback (most recent call last):
E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
E           fn(i, *args)
E         File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_gemini/test_grad_clip.py", line 127, in run_dist
E           exam_grad_clipping()
E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
E           partial_func(**kwargs)
E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
E           partial_func(**kwargs)
E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
E           partial_func(**kwargs)
E         [Previous line repeated 2 more times]
E         File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_gemini/test_grad_clip.py", line 65, in exam_grad_clipping
E           torch_model = model_builder().cuda()
E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2548, in cuda
E           return super().cuda(*args, **kwargs)
E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in cuda
E           return self._apply(lambda t: t.cuda(device))
E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
E           module._apply(fn)
E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
E           module._apply(fn)
E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
E           param_applied = fn(param)
E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in <lambda>
E           return self._apply(lambda t: t.cuda(device))
E       RuntimeError: CUDA error: out of memory
E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.9366396Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
----------------------------- Captured stdout call -----------------------------
[04/11/25 04:22:07] INFO     colossalai - colossalai - INFO:
/__w/ColossalAI/ColossalAI/colossalai/initialize.py
:75 launch
INFO     colossalai - colossalai - INFO: Distributed
environment is initialized, world size: 2
Maximum number of attempts is reached or pattern is not matched, no more retrying...
----------------------------- Captured stderr call -----------------------------
/opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
/opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
/opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/apex/amp/_process_optimizer.py:344: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1711403380909/work/torch/csrc/tensor/python_tensor.cpp:83.)
optimizer._amp_stash.dummy_overflow_buf = torch.cuda.IntTensor([0]);
______________________________ test_inference[1] _______________________________
2025-04-11T04:23:18.9378763Z
args = (), kwargs = {'world_size': 1}, try_count = 1
error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T04:23:18.9379484Z
def _run_until_success(*args, **kwargs):
try_count = 0
assert max_try is None or isinstance(
max_try, int
), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T04:23:18.9380116Z
while max_try is None or try_count < max_try:
try:
try_count += 1
ret = func(*args, **kwargs)
return ret
except exception_type as e:
error_lines = str(e).split("\n")
if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
print("Exception is caught, retrying...")
# when pattern is not specified, we always skip the exception
# when pattern is specified, we only skip when pattern is matched
continue
else:
print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
>                   raise e
2025-04-11T04:23:18.9381877Z
colossalai/testing/utils.py:144:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
colossalai/testing/utils.py:133: in _run_until_success
ret = func(*args, **kwargs)
tests/test_zero/test_gemini/test_inference.py:118: in test_inference
spawn(run_dist, world_size)
colossalai/testing/utils.py:252: in spawn
mp.spawn(wrapped_func, nprocs=nprocs)
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
while not context.join():
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.9383765Z
self = <torch.multiprocessing.spawn.ProcessContext object at 0x7fb5b5c74af0>
timeout = None
2025-04-11T04:23:18.9384046Z
def join(self, timeout=None):
r"""Join one or more processes within spawn context.
2025-04-11T04:23:18.9384331Z
Attempt to join one or more processes in this spawn context.
If one of them exited with a non-zero exit status, this function
kills the remaining processes and raises an exception with the cause
of the first process exiting.
2025-04-11T04:23:18.9384945Z
Returns ``True`` if all processes have been joined successfully,
``False`` if there are more processes that need to be joined.
2025-04-11T04:23:18.9385301Z
Args:
timeout (float): Wait this long before giving up on waiting.
"""
# Ensure this function can be called even when we're done.
if len(self.sentinels) == 0:
return True
2025-04-11T04:23:18.9386026Z
# Wait for any process to fail or all of them to succeed.
ready = multiprocessing.connection.wait(
self.sentinels.keys(),
timeout=timeout,
)
2025-04-11T04:23:18.9386595Z
error_index = None
for sentinel in ready:
index = self.sentinels.pop(sentinel)
process = self.processes[index]
process.join()
if process.exitcode != 0:
error_index = index
break
2025-04-11T04:23:18.9387375Z
# Return if there was no error.
if error_index is None:
# Return whether or not all processes have been joined.
return len(self.sentinels) == 0
2025-04-11T04:23:18.9387965Z
# Assume failure. Terminate processes that are still alive.
for process in self.processes:
if process.is_alive():
process.terminate()
process.join()
2025-04-11T04:23:18.9388612Z
# There won't be an error on the queue if the process crashed.
failed_process = self.processes[error_index]
if self.error_queues[error_index].empty():
exitcode = self.processes[error_index].exitcode
if exitcode < 0:
name = signal.Signals(-exitcode).name
raise ProcessExitedException(
"process %d terminated with signal %s" % (error_index, name),
error_index=error_index,
error_pid=failed_process.pid,
exit_code=exitcode,
signal_name=name,
)
else:
raise ProcessExitedException(
"process %d terminated with exit code %d" % (error_index, exitcode),
error_index=error_index,
error_pid=failed_process.pid,
exit_code=exitcode,
)
2025-04-11T04:23:18.9390799Z
original_trace = self.error_queues[error_index].get()
msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
msg += original_trace
>       raise ProcessRaisedException(msg, error_index, failed_process.pid)
E       torch.multiprocessing.spawn.ProcessRaisedException:
E
E       -- Process 0 terminated with the following error:
E       Traceback (most recent call last):
E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
E           fn(i, *args)
E         File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_gemini/test_inference.py", line 111, in run_dist
E           exam_inference()
E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
E           partial_func(**kwargs)
E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
E           partial_func(**kwargs)
E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
E           partial_func(**kwargs)
E         File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_gemini/test_inference.py", line 63, in exam_inference
E           torch_model = model_builder().cuda()
E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2548, in cuda
E           return super().cuda(*args, **kwargs)
E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in cuda
E           return self._apply(lambda t: t.cuda(device))
E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
E           module._apply(fn)
E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
E           module._apply(fn)
E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
E           param_applied = fn(param)
E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in <lambda>
E           return self._apply(lambda t: t.cuda(device))
E       RuntimeError: CUDA error: out of memory
E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.9397031Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
----------------------------- Captured stdout call -----------------------------
[04/11/25 04:22:14] INFO     colossalai - colossalai - INFO:
/__w/ColossalAI/ColossalAI/colossalai/initialize.py
:75 launch
INFO     colossalai - colossalai - INFO: Distributed
environment is initialized, world size: 1
Maximum number of attempts is reached or pattern is not matched, no more retrying...
----------------------------- Captured stderr call -----------------------------
/opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
/opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
warnings.warn(
______________________________ test_inference[4] _______________________________
2025-04-11T04:23:18.9403482Z
args = (), kwargs = {'world_size': 4}, try_count = 1
error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T04:23:18.9404251Z
def _run_until_success(*args, **kwargs):
try_count = 0
assert max_try is None or isinstance(
max_try, int
), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T04:23:18.9404832Z
while max_try is None or try_count < max_try:
try:
try_count += 1
ret = func(*args, **kwargs)
return ret
except exception_type as e:
error_lines = str(e).split("\n")
if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
print("Exception is caught, retrying...")
# when pattern is not specified, we always skip the exception
# when pattern is specified, we only skip when pattern is matched
continue
else:
print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
>                   raise e
2025-04-11T04:23:18.9406529Z
colossalai/testing/utils.py:144:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
colossalai/testing/utils.py:133: in _run_until_success
ret = func(*args, **kwargs)
tests/test_zero/test_gemini/test_inference.py:118: in test_inference
spawn(run_dist, world_size)
colossalai/testing/utils.py:252: in spawn
mp.spawn(wrapped_func, nprocs=nprocs)
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
while not context.join():
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.9408362Z
self = <torch.multiprocessing.spawn.ProcessContext object at 0x7fb5b8490bb0>
timeout = None
2025-04-11T04:23:18.9408693Z
def join(self, timeout=None):
r"""Join one or more processes within spawn context.
2025-04-11T04:23:18.9408981Z
Attempt to join one or more processes in this spawn context.
If one of them exited with a non-zero exit status, this function
kills the remaining processes and raises an exception with the cause
of the first process exiting.
2025-04-11T04:23:18.9409642Z
Returns ``True`` if all processes have been joined successfully,
``False`` if there are more processes that need to be joined.
2025-04-11T04:23:18.9410000Z
Args:
timeout (float): Wait this long before giving up on waiting.
"""
# Ensure this function can be called even when we're done.
if len(self.sentinels) == 0:
return True
2025-04-11T04:23:18.9410727Z
# Wait for any process to fail or all of them to succeed.
ready = multiprocessing.connection.wait(
self.sentinels.keys(),
timeout=timeout,
)
2025-04-11T04:23:18.9411286Z
error_index = None
for sentinel in ready:
index = self.sentinels.pop(sentinel)
process = self.processes[index]
process.join()
if process.exitcode != 0:
error_index = index
break
2025-04-11T04:23:18.9412064Z
# Return if there was no error.
if error_index is None:
# Return whether or not all processes have been joined.
return len(self.sentinels) == 0
2025-04-11T04:23:18.9412534Z
# Assume failure. Terminate processes that are still alive.
for process in self.processes:
if process.is_alive():
process.terminate()
process.join()
2025-04-11T04:23:18.9413143Z
# There won't be an error on the queue if the process crashed.
failed_process = self.processes[error_index]
if self.error_queues[error_index].empty():
exitcode = self.processes[error_index].exitcode
if exitcode < 0:
name = signal.Signals(-exitcode).name
raise ProcessExitedException(
"process %d terminated with signal %s" % (error_index, name),
error_index=error_index,
error_pid=failed_process.pid,
exit_code=exitcode,
signal_name=name,
)
else:
raise ProcessExitedException(
"process %d terminated with exit code %d" % (error_index, exitcode),
error_index=error_index,
error_pid=failed_process.pid,
exit_code=exitcode,
)
2025-04-11T04:23:18.9415345Z
original_trace = self.error_queues[error_index].get()
msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
msg += original_trace
>       raise ProcessRaisedException(msg, error_index, failed_process.pid)
E       torch.multiprocessing.spawn.ProcessRaisedException:
E
E       -- Process 0 terminated with the following error:
E       Traceback (most recent call last):
E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
E           fn(i, *args)
E         File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_gemini/test_inference.py", line 111, in run_dist
E           exam_inference()
E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
E           partial_func(**kwargs)
E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
E           partial_func(**kwargs)
E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
E           partial_func(**kwargs)
E         File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_gemini/test_inference.py", line 63, in exam_inference
E           torch_model = model_builder().cuda()
E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2548, in cuda
E           return super().cuda(*args, **kwargs)
E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in cuda
E           return self._apply(lambda t: t.cuda(device))
E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
E           module._apply(fn)
E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
E           module._apply(fn)
E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
E           param_applied = fn(param)
E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in <lambda>
E           return self._apply(lambda t: t.cuda(device))
E       RuntimeError: CUDA error: out of memory
E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.9421611Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
----------------------------- Captured stdout call -----------------------------
[04/11/25 04:22:24] INFO     colossalai - colossalai - INFO:
/__w/ColossalAI/ColossalAI/colossalai/initialize.py
:75 launch
INFO     colossalai - colossalai - INFO: Distributed
environment is initialized, world size: 4
Maximum number of attempts is reached or pattern is not matched, no more retrying...
----------------------------- Captured stderr call -----------------------------
/opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
/opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
/opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
/opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
/opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
warnings.warn(
[W socket.cpp:697] [c10d] The client socket has failed to connect to [localhost]:38217 (errno: 99 - Cannot assign requested address).
[W socket.cpp:697] [c10d] The client socket has failed to connect to [localhost]:38217 (errno: 99 - Cannot assign requested address).
[W socket.cpp:697] [c10d] The client socket has failed to connect to [localhost]:38217 (errno: 99 - Cannot assign requested address).
/opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
warnings.warn(
[W socket.cpp:697] [c10d] The client socket has failed to connect to [localhost]:38217 (errno: 99 - Cannot assign requested address).
[W socket.cpp:697] [c10d] The client socket has failed to connect to [localhost]:38217 (errno: 99 - Cannot assign requested address).
/opt/conda/envs/pytorch/lib/python3.10/site-packages/apex/amp/_process_optimizer.py:344: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1711403380909/work/torch/csrc/tensor/python_tensor.cpp:83.)
optimizer._amp_stash.dummy_overflow_buf = torch.cuda.IntTensor([0]);
/opt/conda/envs/pytorch/lib/python3.10/site-packages/apex/amp/_process_optimizer.py:344: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1711403380909/work/torch/csrc/tensor/python_tensor.cpp:83.)
optimizer._amp_stash.dummy_overflow_buf = torch.cuda.IntTensor([0]);
/opt/conda/envs/pytorch/lib/python3.10/site-packages/apex/amp/_process_optimizer.py:344: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1711403380909/work/torch/csrc/tensor/python_tensor.cpp:83.)
optimizer._amp_stash.dummy_overflow_buf = torch.cuda.IntTensor([0]);
________________________________ test_optim[4] _________________________________
2025-04-11T04:23:18.9447304Z
args = (), kwargs = {'world_size': 4}, try_count = 1
error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T04:23:18.9448031Z
def _run_until_success(*args, **kwargs):
try_count = 0
assert max_try is None or isinstance(
max_try, int
), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T04:23:18.9448669Z
while max_try is None or try_count < max_try:
try:
try_count += 1
ret = func(*args, **kwargs)
return ret
except exception_type as e:
error_lines = str(e).split("\n")
if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
print("Exception is caught, retrying...")
# when pattern is not specified, we always skip the exception
# when pattern is specified, we only skip when pattern is matched
continue
else:
print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
>                   raise e
2025-04-11T04:23:18.9450486Z
colossalai/testing/utils.py:144:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
colossalai/testing/utils.py:133: in _run_until_success
ret = func(*args, **kwargs)
tests/test_zero/test_gemini/test_optim.py:193: in test_optim
spawn(run_dist, world_size)
colossalai/testing/utils.py:252: in spawn
mp.spawn(wrapped_func, nprocs=nprocs)
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
while not context.join():
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.9452257Z
self = <torch.multiprocessing.spawn.ProcessContext object at 0x7fb5b5c76260>
timeout = None
2025-04-11T04:23:18.9452592Z
def join(self, timeout=None):
r"""Join one or more processes within spawn context.
2025-04-11T04:23:18.9452877Z
Attempt to join one or more processes in this spawn context.
If one of them exited with a non-zero exit status, this function
kills the remaining processes and raises an exception with the cause
of the first process exiting.
2025-04-11T04:23:18.9453489Z
Returns ``True`` if all processes have been joined successfully,
``False`` if there are more processes that need to be joined.
2025-04-11T04:23:18.9453843Z
Args:
timeout (float): Wait this long before giving up on waiting.
"""
# Ensure this function can be called even when we're done.
if len(self.sentinels) == 0:
return True
2025-04-11T04:23:18.9454558Z
# Wait for any process to fail or all of them to succeed.
ready = multiprocessing.connection.wait(
self.sentinels.keys(),
timeout=timeout,
)
2025-04-11T04:23:18.9455119Z
error_index = None
for sentinel in ready:
index = self.sentinels.pop(sentinel)
process = self.processes[index]
process.join()
if process.exitcode != 0:
error_index = index
break
2025-04-11T04:23:18.9455951Z
# Return if there was no error.
if error_index is None:
# Return whether or not all processes have been joined.
return len(self.sentinels) == 0
2025-04-11T04:23:18.9456472Z
# Assume failure. Terminate processes that are still alive.
for process in self.processes:
if process.is_alive():
process.terminate()
process.join()
2025-04-11T04:23:18.9457041Z
# There won't be an error on the queue if the process crashed.
failed_process = self.processes[error_index]
if self.error_queues[error_index].empty():
exitcode = self.processes[error_index].exitcode
if exitcode < 0:
name = signal.Signals(-exitcode).name
raise ProcessExitedException(
"process %d terminated with signal %s" % (error_index, name),
error_index=error_index,
error_pid=failed_process.pid,
exit_code=exitcode,
signal_name=name,
)
else:
raise ProcessExitedException(
"process %d terminated with exit code %d" % (error_index, exitcode),
error_index=error_index,
error_pid=failed_process.pid,
exit_code=exitcode,
)
2025-04-11T04:23:18.9459222Z
original_trace = self.error_queues[error_index].get()
msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
msg += original_trace
>       raise ProcessRaisedException(msg, error_index, failed_process.pid)
E       torch.multiprocessing.spawn.ProcessRaisedException:
E
E       -- Process 0 terminated with the following error:
E       Traceback (most recent call last):
E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
E           fn(i, *args)
E         File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_gemini/test_optim.py", line 185, in run_dist
E           exam_model_step()
E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
E           partial_func(**kwargs)
E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
E           partial_func(**kwargs)
E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
E           partial_func(**kwargs)
E         [Previous line repeated 2 more times]
E         File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_gemini/test_optim.py", line 75, in exam_model_step
E           torch_model = model_builder().cuda()
E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2548, in cuda
E           return super().cuda(*args, **kwargs)
E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in cuda
E           return self._apply(lambda t: t.cuda(device))
E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
E           module._apply(fn)
E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
E           module._apply(fn)
E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
E           param_applied = fn(param)
E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in <lambda>
E           return self._apply(lambda t: t.cuda(device))
E       RuntimeError: CUDA error: out of memory
E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.9465555Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
----------------------------- Captured stdout call -----------------------------
[04/11/25 04:22:32] INFO     colossalai - colossalai - INFO:
/__w/ColossalAI/ColossalAI/colossalai/initialize.py
:75 launch
INFO     colossalai - colossalai - INFO: Distributed
environment is initialized, world size: 4
Maximum number of attempts is reached or pattern is not matched, no more retrying...
----------------------------- Captured stderr call -----------------------------
/opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
/opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
/opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
/opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
/opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/apex/amp/_process_optimizer.py:344: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1711403380909/work/torch/csrc/tensor/python_tensor.cpp:83.)
optimizer._amp_stash.dummy_overflow_buf = torch.cuda.IntTensor([0]);
/opt/conda/envs/pytorch/lib/python3.10/site-packages/apex/amp/_process_optimizer.py:344: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1711403380909/work/torch/csrc/tensor/python_tensor.cpp:83.)
optimizer._amp_stash.dummy_overflow_buf = torch.cuda.IntTensor([0]);
/opt/conda/envs/pytorch/lib/python3.10/site-packages/apex/amp/_process_optimizer.py:344: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1711403380909/work/torch/csrc/tensor/python_tensor.cpp:83.)
optimizer._amp_stash.dummy_overflow_buf = torch.cuda.IntTensor([0]);
________________________________ test_search[1] ________________________________
2025-04-11T04:23:18.9489594Z
args = (), kwargs = {'world_size': 1}, try_count = 1
error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T04:23:18.9490322Z
def _run_until_success(*args, **kwargs):
try_count = 0
assert max_try is None or isinstance(
max_try, int
), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T04:23:18.9490912Z
while max_try is None or try_count < max_try:
try:
try_count += 1
ret = func(*args, **kwargs)
return ret
except exception_type as e:
error_lines = str(e).split("\n")
if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
print("Exception is caught, retrying...")
# when pattern is not specified, we always skip the exception
# when pattern is specified, we only skip when pattern is matched
continue
else:
print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
>                   raise e
2025-04-11T04:23:18.9492684Z
colossalai/testing/utils.py:144:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
colossalai/testing/utils.py:133: in _run_until_success
ret = func(*args, **kwargs)
tests/test_zero/test_gemini/test_search.py:68: in test_search
spawn(run_dist, world_size)
colossalai/testing/utils.py:252: in spawn
mp.spawn(wrapped_func, nprocs=nprocs)
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
while not context.join():
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.9494582Z
self = <torch.multiprocessing.spawn.ProcessContext object at 0x7fb5b5a6dcf0>
timeout = None
2025-04-11T04:23:18.9494871Z
def join(self, timeout=None):
r"""Join one or more processes within spawn context.
2025-04-11T04:23:18.9495156Z
Attempt to join one or more processes in this spawn context.
If one of them exited with a non-zero exit status, this function
kills the remaining processes and raises an exception with the cause
of the first process exiting.
2025-04-11T04:23:18.9495826Z
Returns ``True`` if all processes have been joined successfully,
``False`` if there are more processes that need to be joined.
2025-04-11T04:23:18.9496177Z
Args:
timeout (float): Wait this long before giving up on waiting.
"""
# Ensure this function can be called even when we're done.
if len(self.sentinels) == 0:
return True
2025-04-11T04:23:18.9496846Z
# Wait for any process to fail or all of them to succeed.
ready = multiprocessing.connection.wait(
self.sentinels.keys(),
timeout=timeout,
)
2025-04-11T04:23:18.9497413Z
error_index = None
for sentinel in ready:
index = self.sentinels.pop(sentinel)
process = self.processes[index]
process.join()
if process.exitcode != 0:
error_index = index
break
2025-04-11T04:23:18.9498267Z
# Return if there was no error.
if error_index is None:
# Return whether or not all processes have been joined.
return len(self.sentinels) == 0
2025-04-11T04:23:18.9498744Z
# Assume failure. Terminate processes that are still alive.
for process in self.processes:
if process.is_alive():
process.terminate()
process.join()
2025-04-11T04:23:18.9499313Z
# There won't be an error on the queue if the process crashed.
failed_process = self.processes[error_index]
if self.error_queues[error_index].empty():
exitcode = self.processes[error_index].exitcode
if exitcode < 0:
name = signal.Signals(-exitcode).name
raise ProcessExitedException(
"process %d terminated with signal %s" % (error_index, name),
error_index=error_index,
error_pid=failed_process.pid,
exit_code=exitcode,
signal_name=name,
)
else:
raise ProcessExitedException(
"process %d terminated with exit code %d" % (error_index, exitcode),
error_index=error_index,
error_pid=failed_process.pid,
exit_code=exitcode,
)
2025-04-11T04:23:18.9501599Z
original_trace = self.error_queues[error_index].get()
msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
msg += original_trace
>       raise ProcessRaisedException(msg, error_index, failed_process.pid)
E       torch.multiprocessing.spawn.ProcessRaisedException:
E
E       -- Process 0 terminated with the following error:
E       Traceback (most recent call last):
E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
E           fn(i, *args)
E         File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_gemini/test_search.py", line 61, in run_dist
E           exam_chunk_manager()
E         File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_gemini/test_search.py", line 44, in exam_chunk_manager
E           chunk_manager = init_chunk_manager(
E         File "/__w/ColossalAI/ColossalAI/colossalai/zero/gemini/chunk/utils.py", line 33, in init_chunk_manager
E           dist.barrier()
E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/distributed/c10d_logger.py", line 72, in wrapper
E           return func(*args, **kwargs)
E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py", line 3439, in barrier
E           work = default_pg.barrier(opts=opts)
E       RuntimeError: CUDA error: out of memory
E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.9505713Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
----------------------------- Captured stdout call -----------------------------
[04/11/25 04:22:36] INFO     colossalai - colossalai - INFO:
/__w/ColossalAI/ColossalAI/colossalai/initialize.py
:75 launch
INFO     colossalai - colossalai - INFO: Distributed
environment is initialized, world size: 1
Maximum number of attempts is reached or pattern is not matched, no more retrying...
________________________________ test_search[4] ________________________________
2025-04-11T04:23:18.9507253Z
args = (), kwargs = {'world_size': 4}, try_count = 1
error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T04:23:18.9507974Z
def _run_until_success(*args, **kwargs):
try_count = 0
assert max_try is None or isinstance(
max_try, int
), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T04:23:18.9508627Z
while max_try is None or try_count < max_try:
try:
try_count += 1
ret = func(*args, **kwargs)
return ret
except exception_type as e:
error_lines = str(e).split("\n")
if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
print("Exception is caught, retrying...")
# when pattern is not specified, we always skip the exception
# when pattern is specified, we only skip when pattern is matched
continue
else:
print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
>                   raise e
2025-04-11T04:23:18.9510327Z
colossalai/testing/utils.py:144:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
colossalai/testing/utils.py:133: in _run_until_success
ret = func(*args, **kwargs)
tests/test_zero/test_gemini/test_search.py:68: in test_search
spawn(run_dist, world_size)
colossalai/testing/utils.py:252: in spawn
mp.spawn(wrapped_func, nprocs=nprocs)
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
while not context.join():
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.9512170Z
self = <torch.multiprocessing.spawn.ProcessContext object at 0x7fb5b5c75d50>
timeout = None
2025-04-11T04:23:18.9512451Z
def join(self, timeout=None):
r"""Join one or more processes within spawn context.
2025-04-11T04:23:18.9512791Z
Attempt to join one or more processes in this spawn context.
If one of them exited with a non-zero exit status, this function
kills the remaining processes and raises an exception with the cause
of the first process exiting.
2025-04-11T04:23:18.9513461Z
Returns ``True`` if all processes have been joined successfully,
``False`` if there are more processes that need to be joined.
2025-04-11T04:23:18.9513807Z
Args:
timeout (float): Wait this long before giving up on waiting.
"""
# Ensure this function can be called even when we're done.
if len(self.sentinels) == 0:
return True
2025-04-11T04:23:18.9514521Z
# Wait for any process to fail or all of them to succeed.
ready = multiprocessing.connection.wait(
self.sentinels.keys(),
timeout=timeout,
)
2025-04-11T04:23:18.9515089Z
error_index = None
for sentinel in ready:
index = self.sentinels.pop(sentinel)
process = self.processes[index]
process.join()
if process.exitcode != 0:
error_index = index
break
2025-04-11T04:23:18.9515869Z
# Return if there was no error.
if error_index is None:
# Return whether or not all processes have been joined.
return len(self.sentinels) == 0
2025-04-11T04:23:18.9516343Z
# Assume failure. Terminate processes that are still alive.
for process in self.processes:
if process.is_alive():
process.terminate()
process.join()
2025-04-11T04:23:18.9516908Z
# There won't be an error on the queue if the process crashed.
failed_process = self.processes[error_index]
if self.error_queues[error_index].empty():
exitcode = self.processes[error_index].exitcode
if exitcode < 0:
name = signal.Signals(-exitcode).name
raise ProcessExitedException(
"process %d terminated with signal %s" % (error_index, name),
error_index=error_index,
error_pid=failed_process.pid,
exit_code=exitcode,
signal_name=name,
)
else:
raise ProcessExitedException(
"process %d terminated with exit code %d" % (error_index, exitcode),
error_index=error_index,
error_pid=failed_process.pid,
exit_code=exitcode,
)
2025-04-11T04:23:18.9519193Z
original_trace = self.error_queues[error_index].get()
msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
msg += original_trace
>       raise ProcessRaisedException(msg, error_index, failed_process.pid)
E       torch.multiprocessing.spawn.ProcessRaisedException:
E
E       -- Process 0 terminated with the following error:
E       Traceback (most recent call last):
E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
E           fn(i, *args)
E         File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_gemini/test_search.py", line 61, in run_dist
E           exam_chunk_manager()
E         File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_gemini/test_search.py", line 44, in exam_chunk_manager
E           chunk_manager = init_chunk_manager(
E         File "/__w/ColossalAI/ColossalAI/colossalai/zero/gemini/chunk/utils.py", line 33, in init_chunk_manager
E           dist.barrier()
E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/distributed/c10d_logger.py", line 72, in wrapper
E           return func(*args, **kwargs)
E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py", line 3439, in barrier
E           work = default_pg.barrier(opts=opts)
E       RuntimeError: CUDA error: out of memory
E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.9523254Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
----------------------------- Captured stdout call -----------------------------
[04/11/25 04:22:42] INFO     colossalai - colossalai - INFO:
/__w/ColossalAI/ColossalAI/colossalai/initialize.py
:75 launch
INFO     colossalai - colossalai - INFO: Distributed
environment is initialized, world size: 4
Maximum number of attempts is reached or pattern is not matched, no more retrying...
----------------------------- Captured stderr call -----------------------------
[W socket.cpp:697] [c10d] The client socket has failed to connect to [localhost]:43909 (errno: 99 - Cannot assign requested address).
_______________________________ test_zero_ddp[4] _______________________________
2025-04-11T04:23:18.9525216Z
args = (), kwargs = {'world_size': 4}, try_count = 1
error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T04:23:18.9525996Z
def _run_until_success(*args, **kwargs):
try_count = 0
assert max_try is None or isinstance(
max_try, int
), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T04:23:18.9526637Z
while max_try is None or try_count < max_try:
try:
try_count += 1
ret = func(*args, **kwargs)
return ret
except exception_type as e:
error_lines = str(e).split("\n")
if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
print("Exception is caught, retrying...")
# when pattern is not specified, we always skip the exception
# when pattern is specified, we only skip when pattern is matched
continue
else:
print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
>                   raise e
2025-04-11T04:23:18.9528415Z
colossalai/testing/utils.py:144:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
colossalai/testing/utils.py:133: in _run_until_success
ret = func(*args, **kwargs)
tests/test_zero/test_gemini/test_zeroddp_state_dict.py:85: in test_zero_ddp
spawn(run_dist, world_size)
colossalai/testing/utils.py:252: in spawn
mp.spawn(wrapped_func, nprocs=nprocs)
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
while not context.join():
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.9530271Z
self = <torch.multiprocessing.spawn.ProcessContext object at 0x7fb5b840b160>
timeout = None
2025-04-11T04:23:18.9530553Z
def join(self, timeout=None):
r"""Join one or more processes within spawn context.
2025-04-11T04:23:18.9530839Z
Attempt to join one or more processes in this spawn context.
If one of them exited with a non-zero exit status, this function
kills the remaining processes and raises an exception with the cause
of the first process exiting.
2025-04-11T04:23:18.9531457Z
Returns ``True`` if all processes have been joined successfully,
``False`` if there are more processes that need to be joined.
2025-04-11T04:23:18.9531857Z
Args:
timeout (float): Wait this long before giving up on waiting.
"""
# Ensure this function can be called even when we're done.
if len(self.sentinels) == 0:
return True
2025-04-11T04:23:18.9532582Z
# Wait for any process to fail or all of them to succeed.
ready = multiprocessing.connection.wait(
self.sentinels.keys(),
timeout=timeout,
)
2025-04-11T04:23:18.9533148Z
error_index = None
for sentinel in ready:
index = self.sentinels.pop(sentinel)
process = self.processes[index]
process.join()
if process.exitcode != 0:
error_index = index
break
2025-04-11T04:23:18.9533984Z
# Return if there was no error.
if error_index is None:
# Return whether or not all processes have been joined.
return len(self.sentinels) == 0
2025-04-11T04:23:18.9534462Z
# Assume failure. Terminate processes that are still alive.
for process in self.processes:
if process.is_alive():
process.terminate()
process.join()
2025-04-11T04:23:18.9535040Z
# There won't be an error on the queue if the process crashed.
failed_process = self.processes[error_index]
if self.error_queues[error_index].empty():
exitcode = self.processes[error_index].exitcode
if exitcode < 0:
name = signal.Signals(-exitcode).name
raise ProcessExitedException(
"process %d terminated with signal %s" % (error_index, name),
error_index=error_index,
error_pid=failed_process.pid,
exit_code=exitcode,
signal_name=name,
)
else:
raise ProcessExitedException(
"process %d terminated with exit code %d" % (error_index, exitcode),
error_index=error_index,
error_pid=failed_process.pid,
exit_code=exitcode,
)
2025-04-11T04:23:18.9537221Z
original_trace = self.error_queues[error_index].get()
msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
msg += original_trace
>       raise ProcessRaisedException(msg, error_index, failed_process.pid)
E       torch.multiprocessing.spawn.ProcessRaisedException:
E
E       -- Process 0 terminated with the following error:
E       Traceback (most recent call last):
E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
E           fn(i, *args)
E         File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_gemini/test_zeroddp_state_dict.py", line 78, in run_dist
E           exam_state_dict()
E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
E           partial_func(**kwargs)
E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
E           partial_func(**kwargs)
E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
E           partial_func(**kwargs)
E         [Previous line repeated 1 more time]
E         File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_gemini/test_zeroddp_state_dict.py", line 45, in exam_state_dict
E           model = GeminiDDP(model, config_dict, **placement_config, pin_memory=True, master_weights=master_weights)
E         File "/__w/ColossalAI/ColossalAI/colossalai/zero/gemini/gemini_ddp.py", line 109, in __init__
E           self.chunk_manager = ChunkManager(
E         File "/__w/ColossalAI/ColossalAI/colossalai/zero/gemini/chunk/manager.py", line 46, in __init__
E           self.overflow_counter = torch.tensor([0], dtype=torch.int, device=get_accelerator().get_current_device())
E       RuntimeError: CUDA error: out of memory
E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.9542332Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
----------------------------- Captured stdout call -----------------------------
[04/11/25 04:22:49] INFO     colossalai - colossalai - INFO:
/__w/ColossalAI/ColossalAI/colossalai/initialize.py
:75 launch
INFO     colossalai - colossalai - INFO: Distributed
environment is initialized, world size: 4
Maximum number of attempts is reached or pattern is not matched, no more retrying...
----------------------------- Captured stderr call -----------------------------
/opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
/opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
/opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
/opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
/opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
warnings.warn(
/opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
warnings.warn(
Exception ignored in: <function GeminiDDP.__del__ at 0x7ff5876a5fc0>
Traceback (most recent call last):
File "/__w/ColossalAI/ColossalAI/colossalai/zero/gemini/gemini_ddp.py", line 222, in __del__
self.remove_hooks()
File "/__w/ColossalAI/ColossalAI/colossalai/zero/gemini/gemini_ddp.py", line 213, in remove_hooks
for p in self.module.parameters():
File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1688, in __getattr__
raise AttributeError(f"'{type(self).__name__}' object has no attribute '{name}'")
AttributeError: 'GeminiDDP' object has no attribute 'module'
/__w/ColossalAI/ColossalAI/colossalai/zero/gemini/chunk/chunk.py:46: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
return tensor.storage().size() == 0
/__w/ColossalAI/ColossalAI/colossalai/zero/gemini/chunk/chunk.py:46: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
return tensor.storage().size() == 0
_________________________________ test_comm_nd _________________________________
2025-04-11T04:23:18.9566851Z
args = (), kwargs = {}, try_count = 1
error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T04:23:18.9567552Z
def _run_until_success(*args, **kwargs):
try_count = 0
assert max_try is None or isinstance(
max_try, int
), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T04:23:18.9568134Z
while max_try is None or try_count < max_try:
try:
try_count += 1
ret = func(*args, **kwargs)
return ret
except exception_type as e:
error_lines = str(e).split("\n")
if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
print("Exception is caught, retrying...")
# when pattern is not specified, we always skip the exception
# when pattern is specified, we only skip when pattern is matched
continue
else:
print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
>                   raise e
2025-04-11T04:23:18.9569896Z
colossalai/testing/utils.py:144:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
colossalai/testing/utils.py:133: in _run_until_success
ret = func(*args, **kwargs)
tests/test_zero/test_low_level/test_coll_nd.py:38: in test_comm_nd
spawn(run_dist, 4)
colossalai/testing/utils.py:252: in spawn
mp.spawn(wrapped_func, nprocs=nprocs)
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
while not context.join():
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.9571789Z
self = <torch.multiprocessing.spawn.ProcessContext object at 0x7fb5b5c76dd0>
timeout = None
2025-04-11T04:23:18.9572068Z
def join(self, timeout=None):
r"""Join one or more processes within spawn context.
2025-04-11T04:23:18.9572350Z
Attempt to join one or more processes in this spawn context.
If one of them exited with a non-zero exit status, this function
kills the remaining processes and raises an exception with the cause
of the first process exiting.
2025-04-11T04:23:18.9573019Z
Returns ``True`` if all processes have been joined successfully,
``False`` if there are more processes that need to be joined.
2025-04-11T04:23:18.9573372Z
Args:
timeout (float): Wait this long before giving up on waiting.
"""
# Ensure this function can be called even when we're done.
if len(self.sentinels) == 0:
return True
2025-04-11T04:23:18.9574041Z
# Wait for any process to fail or all of them to succeed.
ready = multiprocessing.connection.wait(
self.sentinels.keys(),
timeout=timeout,
)
2025-04-11T04:23:18.9574610Z
error_index = None
for sentinel in ready:
index = self.sentinels.pop(sentinel)
process = self.processes[index]
process.join()
if process.exitcode != 0:
error_index = index
break
2025-04-11T04:23:18.9575439Z
# Return if there was no error.
if error_index is None:
# Return whether or not all processes have been joined.
return len(self.sentinels) == 0
2025-04-11T04:23:18.9575923Z
# Assume failure. Terminate processes that are still alive.
for process in self.processes:
if process.is_alive():
process.terminate()
process.join()
2025-04-11T04:23:18.9576487Z
# There won't be an error on the queue if the process crashed.
failed_process = self.processes[error_index]
if self.error_queues[error_index].empty():
exitcode = self.processes[error_index].exitcode
if exitcode < 0:
name = signal.Signals(-exitcode).name
raise ProcessExitedException(
"process %d terminated with signal %s" % (error_index, name),
error_index=error_index,
error_pid=failed_process.pid,
exit_code=exitcode,
signal_name=name,
)
else:
raise ProcessExitedException(
"process %d terminated with exit code %d" % (error_index, exitcode),
error_index=error_index,
error_pid=failed_process.pid,
exit_code=exitcode,
)
2025-04-11T04:23:18.9578855Z
original_trace = self.error_queues[error_index].get()
msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
msg += original_trace
>       raise ProcessRaisedException(msg, error_index, failed_process.pid)
E       torch.multiprocessing.spawn.ProcessRaisedException:
E
E       -- Process 0 terminated with the following error:
E       Traceback (most recent call last):
E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
E           fn(i, *args)
E         File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_low_level/test_coll_nd.py", line 32, in run_dist
E           check_all_gather_2d()
E         File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_low_level/test_coll_nd.py", line 16, in check_all_gather_2d
E           tensor = torch.rand(128, device=get_current_device())
E       RuntimeError: CUDA error: out of memory
E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.9581747Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
----------------------------- Captured stdout call -----------------------------
[04/11/25 04:22:56] INFO     colossalai - colossalai - INFO:
/__w/ColossalAI/ColossalAI/colossalai/initialize.py
:75 launch
INFO     colossalai - colossalai - INFO: Distributed
environment is initialized, world size: 4
Maximum number of attempts is reached or pattern is not matched, no more retrying...
----------------------------- Captured stderr call -----------------------------
[W socket.cpp:697] [c10d] The client socket has failed to connect to [localhost]:44260 (errno: 99 - Cannot assign requested address).
____________________________ test_grad_accumulation ____________________________
2025-04-11T04:23:18.9583702Z
@pytest.mark.dist
def test_grad_accumulation():
>       spawn(run_dist, 2)
2025-04-11T04:23:18.9583968Z
tests/test_zero/test_low_level/test_grad_acc.py:146:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
colossalai/testing/utils.py:252: in spawn
mp.spawn(wrapped_func, nprocs=nprocs)
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
while not context.join():
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.9585396Z
self = <torch.multiprocessing.spawn.ProcessContext object at 0x7fb5b5be2fe0>
timeout = None
2025-04-11T04:23:18.9585683Z
def join(self, timeout=None):
r"""Join one or more processes within spawn context.
2025-04-11T04:23:18.9585966Z
Attempt to join one or more processes in this spawn context.
If one of them exited with a non-zero exit status, this function
kills the remaining processes and raises an exception with the cause
of the first process exiting.
2025-04-11T04:23:18.9586578Z
Returns ``True`` if all processes have been joined successfully,
``False`` if there are more processes that need to be joined.
2025-04-11T04:23:18.9586928Z
Args:
timeout (float): Wait this long before giving up on waiting.
"""
# Ensure this function can be called even when we're done.
if len(self.sentinels) == 0:
return True
2025-04-11T04:23:18.9587595Z
# Wait for any process to fail or all of them to succeed.
ready = multiprocessing.connection.wait(
self.sentinels.keys(),
timeout=timeout,
)
2025-04-11T04:23:18.9588211Z
error_index = None
for sentinel in ready:
index = self.sentinels.pop(sentinel)
process = self.processes[index]
process.join()
if process.exitcode != 0:
error_index = index
break
2025-04-11T04:23:18.9589021Z
# Return if there was no error.
if error_index is None:
# Return whether or not all processes have been joined.
return len(self.sentinels) == 0
2025-04-11T04:23:18.9589493Z
# Assume failure. Terminate processes that are still alive.
for process in self.processes:
if process.is_alive():
process.terminate()
process.join()
2025-04-11T04:23:18.9590117Z
# There won't be an error on the queue if the process crashed.
failed_process = self.processes[error_index]
if self.error_queues[error_index].empty():
exitcode = self.processes[error_index].exitcode
if exitcode < 0:
name = signal.Signals(-exitcode).name
raise ProcessExitedException(
"process %d terminated with signal %s" % (error_index, name),
error_index=error_index,
error_pid=failed_process.pid,
exit_code=exitcode,
signal_name=name,
)
else:
raise ProcessExitedException(
"process %d terminated with exit code %d" % (error_index, exitcode),
error_index=error_index,
error_pid=failed_process.pid,
exit_code=exitcode,
)
2025-04-11T04:23:18.9592367Z
original_trace = self.error_queues[error_index].get()
msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
msg += original_trace
>       raise ProcessRaisedException(msg, error_index, failed_process.pid)
E       torch.multiprocessing.spawn.ProcessRaisedException:
E
E       -- Process 0 terminated with the following error:
E       Traceback (most recent call last):
E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
E           fn(i, *args)
E         File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_low_level/test_grad_acc.py", line 139, in run_dist
E           exam_zero_1_grad_acc(sync=True)
E         File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_low_level/test_grad_acc.py", line 86, in exam_zero_1_grad_acc
E           zero_model = zero_model.to(device)
E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1152, in to
E           return self._apply(convert)
E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
E           module._apply(fn)
E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
E           param_applied = fn(param)
E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1150, in convert
E           return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
E       RuntimeError: CUDA error: out of memory
E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.9596880Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
----------------------------- Captured stdout call -----------------------------
[04/11/25 04:23:01] INFO     colossalai - colossalai - INFO:
/__w/ColossalAI/ColossalAI/colossalai/initialize.py
:75 launch
INFO     colossalai - colossalai - INFO: Distributed
environment is initialized, world size: 2
----------------------------- Captured stderr call -----------------------------
[W socket.cpp:697] [c10d] The client socket has failed to connect to [localhost]:43283 (errno: 99 - Cannot assign requested address).
________________________________ test_zero_1_2 _________________________________
2025-04-11T04:23:18.9598660Z
args = (), kwargs = {}, try_count = 1
error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T04:23:18.9599357Z
def _run_until_success(*args, **kwargs):
try_count = 0
assert max_try is None or isinstance(
max_try, int
), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T04:23:18.9599936Z
while max_try is None or try_count < max_try:
try:
try_count += 1
ret = func(*args, **kwargs)
return ret
except exception_type as e:
error_lines = str(e).split("\n")
if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
print("Exception is caught, retrying...")
# when pattern is not specified, we always skip the exception
# when pattern is specified, we only skip when pattern is matched
continue
else:
print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
>                   raise e
2025-04-11T04:23:18.9601711Z
colossalai/testing/utils.py:144:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
colossalai/testing/utils.py:133: in _run_until_success
ret = func(*args, **kwargs)
tests/test_zero/test_low_level/test_mem_leak.py:57: in test_zero_1_2
spawn(run_dist, 2)
colossalai/testing/utils.py:252: in spawn
mp.spawn(wrapped_func, nprocs=nprocs)
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
while not context.join():
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.9603605Z
self = <torch.multiprocessing.spawn.ProcessContext object at 0x7fb5b5c74e50>
timeout = None
2025-04-11T04:23:18.9603888Z
def join(self, timeout=None):
r"""Join one or more processes within spawn context.
2025-04-11T04:23:18.9604174Z
Attempt to join one or more processes in this spawn context.
If one of them exited with a non-zero exit status, this function
kills the remaining processes and raises an exception with the cause
of the first process exiting.
2025-04-11T04:23:18.9604834Z
Returns ``True`` if all processes have been joined successfully,
``False`` if there are more processes that need to be joined.
2025-04-11T04:23:18.9605182Z
Args:
timeout (float): Wait this long before giving up on waiting.
"""
# Ensure this function can be called even when we're done.
if len(self.sentinels) == 0:
return True
2025-04-11T04:23:18.9605850Z
# Wait for any process to fail or all of them to succeed.
ready = multiprocessing.connection.wait(
self.sentinels.keys(),
timeout=timeout,
)
2025-04-11T04:23:18.9606418Z
error_index = None
for sentinel in ready:
index = self.sentinels.pop(sentinel)
process = self.processes[index]
process.join()
if process.exitcode != 0:
error_index = index
break
2025-04-11T04:23:18.9607247Z
# Return if there was no error.
if error_index is None:
# Return whether or not all processes have been joined.
return len(self.sentinels) == 0
2025-04-11T04:23:18.9607724Z
# Assume failure. Terminate processes that are still alive.
for process in self.processes:
if process.is_alive():
process.terminate()
process.join()
2025-04-11T04:23:18.9608292Z
# There won't be an error on the queue if the process crashed.
failed_process = self.processes[error_index]
if self.error_queues[error_index].empty():
exitcode = self.processes[error_index].exitcode
if exitcode < 0:
name = signal.Signals(-exitcode).name
raise ProcessExitedException(
"process %d terminated with signal %s" % (error_index, name),
error_index=error_index,
error_pid=failed_process.pid,
exit_code=exitcode,
signal_name=name,
)
else:
raise ProcessExitedException(
"process %d terminated with exit code %d" % (error_index, exitcode),
error_index=error_index,
error_pid=failed_process.pid,
exit_code=exitcode,
)
2025-04-11T04:23:18.9610636Z
original_trace = self.error_queues[error_index].get()
msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
msg += original_trace
>       raise ProcessRaisedException(msg, error_index, failed_process.pid)
E       torch.multiprocessing.spawn.ProcessRaisedException:
E
E       -- Process 0 terminated with the following error:
E       Traceback (most recent call last):
E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
E           fn(i, *args)
E         File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_low_level/test_mem_leak.py", line 51, in run_dist
E           exam_mem_leak(world_size=world_size)
E         File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_low_level/test_mem_leak.py", line 36, in exam_mem_leak
E           zero_model = MlpModel().cuda()
E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in cuda
E           return self._apply(lambda t: t.cuda(device))
E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
E           module._apply(fn)
E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
E           param_applied = fn(param)
E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in <lambda>
E           return self._apply(lambda t: t.cuda(device))
E       RuntimeError: CUDA error: out of memory
E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.9615016Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
----------------------------- Captured stdout call -----------------------------
[04/11/25 04:23:05] INFO     colossalai - colossalai - INFO:
/__w/ColossalAI/ColossalAI/colossalai/initialize.py
:75 launch
INFO     colossalai - colossalai - INFO: Distributed
environment is initialized, world size: 2
Maximum number of attempts is reached or pattern is not matched, no more retrying...
________________________________ test_zero_1_2 _________________________________
2025-04-11T04:23:18.9616539Z
args = (), kwargs = {}, try_count = 1
error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T04:23:18.9617278Z
def _run_until_success(*args, **kwargs):
try_count = 0
assert max_try is None or isinstance(
max_try, int
), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T04:23:18.9617853Z
while max_try is None or try_count < max_try:
try:
try_count += 1
ret = func(*args, **kwargs)
return ret
except exception_type as e:
error_lines = str(e).split("\n")
if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
print("Exception is caught, retrying...")
# when pattern is not specified, we always skip the exception
# when pattern is specified, we only skip when pattern is matched
continue
else:
print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
>                   raise e
2025-04-11T04:23:18.9619600Z
colossalai/testing/utils.py:144:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
colossalai/testing/utils.py:133: in _run_until_success
ret = func(*args, **kwargs)
tests/test_zero/test_low_level/test_zero1_2.py:224: in test_zero_1_2
spawn(run_dist, 4)
colossalai/testing/utils.py:252: in spawn
mp.spawn(wrapped_func, nprocs=nprocs)
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
while not context.join():
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.9621383Z
self = <torch.multiprocessing.spawn.ProcessContext object at 0x7fb581d6cd00>
timeout = None
2025-04-11T04:23:18.9621712Z
def join(self, timeout=None):
r"""Join one or more processes within spawn context.
2025-04-11T04:23:18.9621996Z
Attempt to join one or more processes in this spawn context.
If one of them exited with a non-zero exit status, this function
kills the remaining processes and raises an exception with the cause
of the first process exiting.
2025-04-11T04:23:18.9622654Z
Returns ``True`` if all processes have been joined successfully,
``False`` if there are more processes that need to be joined.
2025-04-11T04:23:18.9623005Z
Args:
timeout (float): Wait this long before giving up on waiting.
"""
# Ensure this function can be called even when we're done.
if len(self.sentinels) == 0:
return True
2025-04-11T04:23:18.9623727Z
# Wait for any process to fail or all of them to succeed.
ready = multiprocessing.connection.wait(
self.sentinels.keys(),
timeout=timeout,
)
2025-04-11T04:23:18.9624291Z
error_index = None
for sentinel in ready:
index = self.sentinels.pop(sentinel)
process = self.processes[index]
process.join()
if process.exitcode != 0:
error_index = index
break
2025-04-11T04:23:18.9625079Z
# Return if there was no error.
if error_index is None:
# Return whether or not all processes have been joined.
return len(self.sentinels) == 0
2025-04-11T04:23:18.9625552Z
# Assume failure. Terminate processes that are still alive.
for process in self.processes:
if process.is_alive():
process.terminate()
process.join()
2025-04-11T04:23:18.9626184Z
# There won't be an error on the queue if the process crashed.
failed_process = self.processes[error_index]
if self.error_queues[error_index].empty():
exitcode = self.processes[error_index].exitcode
if exitcode < 0:
name = signal.Signals(-exitcode).name
raise ProcessExitedException(
"process %d terminated with signal %s" % (error_index, name),
error_index=error_index,
error_pid=failed_process.pid,
exit_code=exitcode,
signal_name=name,
)
else:
raise ProcessExitedException(
"process %d terminated with exit code %d" % (error_index, exitcode),
error_index=error_index,
error_pid=failed_process.pid,
exit_code=exitcode,
)
2025-04-11T04:23:18.9628447Z
original_trace = self.error_queues[error_index].get()
msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
msg += original_trace
>       raise ProcessRaisedException(msg, error_index, failed_process.pid)
E       torch.multiprocessing.spawn.ProcessRaisedException:
E
E       -- Process 0 terminated with the following error:
E       Traceback (most recent call last):
E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
E           fn(i, *args)
E         File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_low_level/test_zero1_2.py", line 217, in run_dist
E           exam_zero_1_torch_ddp()
E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
E           partial_func(**kwargs)
E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
E           partial_func(**kwargs)
E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
E           partial_func(**kwargs)
E         File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_low_level/test_zero1_2.py", line 151, in exam_zero_1_torch_ddp
E           torch_model = MlpModel().cuda().to(dtype)
E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in cuda
E           return self._apply(lambda t: t.cuda(device))
E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
E           module._apply(fn)
E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
E           param_applied = fn(param)
E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in <lambda>
E           return self._apply(lambda t: t.cuda(device))
E       RuntimeError: CUDA error: out of memory
E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.9633932Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
----------------------------- Captured stdout call -----------------------------
[04/11/25 04:23:11] INFO     colossalai - colossalai - INFO:
/__w/ColossalAI/ColossalAI/colossalai/initialize.py
:75 launch
INFO     colossalai - colossalai - INFO: Distributed
environment is initialized, world size: 4
Maximum number of attempts is reached or pattern is not matched, no more retrying...
----------------------------- Captured stderr call -----------------------------
[W socket.cpp:697] [c10d] The client socket has failed to connect to [localhost]:63920 (errno: 99 - Cannot assign requested address).
________________________________ test_zero_ckpt ________________________________
2025-04-11T04:23:18.9635908Z
args = (), kwargs = {}, try_count = 1
error_lines = ['', '', '-- Process 0 terminated with the following error:', 'Traceback (most recent call last):', '  File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap', '    fn(i, *args)', ...]
2025-04-11T04:23:18.9636658Z
def _run_until_success(*args, **kwargs):
try_count = 0
assert max_try is None or isinstance(
max_try, int
), f"Expected max_try to be None or int, but got {type(max_try)}"
2025-04-11T04:23:18.9637234Z
while max_try is None or try_count < max_try:
try:
try_count += 1
ret = func(*args, **kwargs)
return ret
except exception_type as e:
error_lines = str(e).split("\n")
if try_count < max_try and (pattern is None or _match_lines(error_lines, pattern)):
print("Exception is caught, retrying...")
# when pattern is not specified, we always skip the exception
# when pattern is specified, we only skip when pattern is matched
continue
else:
print("Maximum number of attempts is reached or pattern is not matched, no more retrying...")
>                   raise e
2025-04-11T04:23:18.9638978Z
colossalai/testing/utils.py:144:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
colossalai/testing/utils.py:133: in _run_until_success
ret = func(*args, **kwargs)
tests/test_zero/test_low_level/test_zero_ckpt.py:129: in test_zero_ckpt
spawn(run_dist, 4)
colossalai/testing/utils.py:252: in spawn
mp.spawn(wrapped_func, nprocs=nprocs)
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:241: in spawn
return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:197: in start_processes
while not context.join():
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
2025-04-11T04:23:18.9640821Z
self = <torch.multiprocessing.spawn.ProcessContext object at 0x7fb5b5c76dd0>
timeout = None
2025-04-11T04:23:18.9641110Z
def join(self, timeout=None):
r"""Join one or more processes within spawn context.
2025-04-11T04:23:18.9641454Z
Attempt to join one or more processes in this spawn context.
If one of them exited with a non-zero exit status, this function
kills the remaining processes and raises an exception with the cause
of the first process exiting.
2025-04-11T04:23:18.9642070Z
Returns ``True`` if all processes have been joined successfully,
``False`` if there are more processes that need to be joined.
2025-04-11T04:23:18.9642428Z
Args:
timeout (float): Wait this long before giving up on waiting.
"""
# Ensure this function can be called even when we're done.
if len(self.sentinels) == 0:
return True
2025-04-11T04:23:18.9643138Z
# Wait for any process to fail or all of them to succeed.
ready = multiprocessing.connection.wait(
self.sentinels.keys(),
timeout=timeout,
)
2025-04-11T04:23:18.9643704Z
error_index = None
for sentinel in ready:
index = self.sentinels.pop(sentinel)
process = self.processes[index]
process.join()
if process.exitcode != 0:
error_index = index
break
2025-04-11T04:23:18.9644493Z
# Return if there was no error.
if error_index is None:
# Return whether or not all processes have been joined.
return len(self.sentinels) == 0
2025-04-11T04:23:18.9644967Z
# Assume failure. Terminate processes that are still alive.
for process in self.processes:
if process.is_alive():
process.terminate()
process.join()
2025-04-11T04:23:18.9645584Z
# There won't be an error on the queue if the process crashed.
failed_process = self.processes[error_index]
if self.error_queues[error_index].empty():
exitcode = self.processes[error_index].exitcode
if exitcode < 0:
name = signal.Signals(-exitcode).name
raise ProcessExitedException(
"process %d terminated with signal %s" % (error_index, name),
error_index=error_index,
error_pid=failed_process.pid,
exit_code=exitcode,
signal_name=name,
)
else:
raise ProcessExitedException(
"process %d terminated with exit code %d" % (error_index, exitcode),
error_index=error_index,
error_pid=failed_process.pid,
exit_code=exitcode,
)
2025-04-11T04:23:18.9647839Z
original_trace = self.error_queues[error_index].get()
msg = "\n\n-- Process %d terminated with the following error:\n" % error_index
msg += original_trace
>       raise ProcessRaisedException(msg, error_index, failed_process.pid)
E       torch.multiprocessing.spawn.ProcessRaisedException:
E
E       -- Process 0 terminated with the following error:
E       Traceback (most recent call last):
E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
E           fn(i, *args)
E         File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_low_level/test_zero_ckpt.py", line 123, in run_dist
E           exam_zero_1_torch_ddp_ckpt()
E         File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
E           partial_func(**kwargs)
E         File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_low_level/test_zero_ckpt.py", line 62, in exam_zero_1_torch_ddp_ckpt
E           torch_model = MlpModel().cuda()
E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in cuda
E           return self._apply(lambda t: t.cuda(device))
E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
E           module._apply(fn)
E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
E           param_applied = fn(param)
E         File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in <lambda>
E           return self._apply(lambda t: t.cuda(device))
E       RuntimeError: CUDA error: out of memory
E       CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
E       For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
E       Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
2025-04-11T04:23:18.9652663Z
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:158: ProcessRaisedException
----------------------------- Captured stdout call -----------------------------
[04/11/25 04:23:17] INFO     colossalai - colossalai - INFO:
/__w/ColossalAI/ColossalAI/colossalai/initialize.py
:75 launch
INFO     colossalai - colossalai - INFO: Distributed
environment is initialized, world size: 4
Maximum number of attempts is reached or pattern is not matched, no more retrying...
----------------------------- Captured stderr call -----------------------------
[W socket.cpp:697] [c10d] The client socket has failed to connect to [localhost]:37496 (errno: 99 - Cannot assign requested address).
[W socket.cpp:697] [c10d] The client socket has failed to connect to [localhost]:37496 (errno: 99 - Cannot assign requested address).
=============================== warnings summary ===============================
colossalai/interface/model.py:45
/__w/ColossalAI/ColossalAI/colossalai/interface/model.py:45: DeprecationWarning: invalid escape sequence '\S'
to_return = {re.sub(f"lora_\S\.{adapter_name}\.(weight|bias)", "base_layer", k) for k in to_return}
2025-04-11T04:23:18.9655576Z
colossalai/interface/model.py:45
/__w/ColossalAI/ColossalAI/colossalai/interface/model.py:45: DeprecationWarning: invalid escape sequence '\.'
to_return = {re.sub(f"lora_\S\.{adapter_name}\.(weight|bias)", "base_layer", k) for k in to_return}
2025-04-11T04:23:18.9656167Z
colossalai/checkpoint_io/utils.py:862
/__w/ColossalAI/ColossalAI/colossalai/checkpoint_io/utils.py:862: DeprecationWarning: invalid escape sequence '\.'
reg = re.compile("(.*?).index((\..*)?).json")
2025-04-11T04:23:18.9656685Z
colossalai/nn/optimizer/cpu_adam.py:12
/__w/ColossalAI/ColossalAI/colossalai/nn/optimizer/cpu_adam.py:12: DeprecationWarning: invalid escape sequence '\:'
"""
2025-04-11T04:23:18.9657171Z
colossalai/nn/optimizer/fused_adam.py:15
/__w/ColossalAI/ColossalAI/colossalai/nn/optimizer/fused_adam.py:15: DeprecationWarning: invalid escape sequence '\:'
"""Implements Adam algorithm.
2025-04-11T04:23:18.9657670Z
colossalai/nn/optimizer/hybrid_adam.py:12
/__w/ColossalAI/ColossalAI/colossalai/nn/optimizer/hybrid_adam.py:12: DeprecationWarning: invalid escape sequence '\:'
"""Implements Adam algorithm.
2025-04-11T04:23:18.9658177Z
../../../opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34
/opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.
deprecate("Transformer2DModelOutput", "1.0.0", deprecation_message)
2025-04-11T04:23:18.9659836Z
../../../opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896
tests/test_infer/test_drafter.py::test_drafter[5]
tests/test_lazy/test_from_pretrained.py::test_lazy_from_pretrained
tests/test_lazy/test_models.py::test_models_lazy_init[cpu-subset0]
/opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
warnings.warn(
2025-04-11T04:23:18.9661387Z
../../../opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064
../../../opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064
../../../opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064
../../../opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064
../../../opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064
tests/test_infer/test_drafter.py::test_drafter[5]
tests/test_lazy/test_from_pretrained.py::test_lazy_from_pretrained
tests/test_lazy/test_models.py::test_models_lazy_init[cpu-subset0]
/opt/conda/envs/pytorch/lib/python3.10/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'vpn.luchentech.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings
warnings.warn(
2025-04-11T04:23:18.9663905Z
<frozen importlib._bootstrap>:283
tests/test_config/test_load_config.py::test_load_config
tests/test_lazy/test_models.py::test_models_lazy_init[cpu-subset0]
tests/test_lazy/test_models.py::test_models_lazy_init[cpu-subset0]
tests/test_lazy/test_models.py::test_models_lazy_init[cpu-subset0]
tests/test_lazy/test_models.py::test_models_lazy_init[cpu-subset0]
<frozen importlib._bootstrap>:283: DeprecationWarning: the load_module() method is deprecated and slated for removal in Python 3.12; use exec_module() instead
2025-04-11T04:23:18.9665143Z
colossalai/fx/profiler/dataflow.py:20
/__w/ColossalAI/ColossalAI/colossalai/fx/profiler/dataflow.py:20: DeprecationWarning: invalid escape sequence '\_'
"""
2025-04-11T04:23:18.9665638Z
colossalai/fx/profiler/dataflow.py:77
/__w/ColossalAI/ColossalAI/colossalai/fx/profiler/dataflow.py:77: DeprecationWarning: invalid escape sequence '\_'
"""Analyze the autograd node dependencies and find out the memory usage.
2025-04-11T04:23:18.9666262Z
colossalai/legacy/zero/sharded_optim/sharded_optim_v2.py:31
/__w/ColossalAI/ColossalAI/colossalai/legacy/zero/sharded_optim/sharded_optim_v2.py:31: DeprecationWarning: invalid escape sequence '\:'
"""A wrapper for optimizer. ``ShardedOptimizerV2`` and ``ShardedModelV2`` implement Zero Redundancy Optimizer (ZeRO).
2025-04-11T04:23:19.1714790Z
colossalai/inference/utils.py:80
/__w/ColossalAI/ColossalAI/colossalai/inference/utils.py:80: DeprecationWarning: invalid escape sequence '\.'
reg = re.compile("(.*?).index((\..*)?).json")
2025-04-11T04:23:19.1715994Z
colossalai/inference/executor/rpc_worker.py:188
/__w/ColossalAI/ColossalAI/colossalai/inference/executor/rpc_worker.py:188: SyntaxWarning: "is" with a literal. Did you mean "=="?
if arch is "BaichuanForCausalLM":
2025-04-11T04:23:19.1717345Z
tests/test_infer/test_async_engine/test_async_engine.py:49
/__w/ColossalAI/ColossalAI/tests/test_infer/test_async_engine/test_async_engine.py:49: PytestUnknownMarkWarning: Unknown pytest.mark.asyncio - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
@pytest.mark.asyncio
2025-04-11T04:23:19.1719517Z
tests/test_tensor/test_dtensor/test_dtensor.py:10
/__w/ColossalAI/ColossalAI/tests/test_tensor/test_dtensor/test_dtensor.py:10: PytestCollectionWarning: cannot collect test class 'TestModel' because it has a __init__ constructor (from: tests/test_tensor/test_dtensor/test_dtensor.py)
class TestModel(torch.nn.Module):
2025-04-11T04:23:19.1721484Z
tests/test_zero/test_low_level/test_mem_leak.py:23
/__w/ColossalAI/ColossalAI/tests/test_zero/test_low_level/test_mem_leak.py:23: PytestCollectionWarning: cannot collect test class 'TestLowLevelZeroOptimizer' because it has a __init__ constructor (from: tests/test_zero/test_low_level/test_mem_leak.py)
class TestLowLevelZeroOptimizer(LowLevelZeroOptimizer):
2025-04-11T04:23:19.1723525Z
tests/test_booster/test_accelerator.py: 1 warning
tests/test_checkpoint_io/test_general_checkpoint_io.py: 1 warning
tests/test_checkpoint_io/test_low_level_zero_checkpoint_io.py: 1 warning
tests/test_checkpoint_io/test_safetensors_async_io.py: 1 warning
tests/test_fp8/test_fp8_cast.py: 1 warning
tests/test_infer/test_kernels/cuda/test_flash_decoding_attention.py: 2 warnings
tests/test_shardformer/test_flash_attention.py: 1 warning
tests/test_shardformer/test_with_torch_ddp.py: 1 warning
tests/test_shardformer/test_hybrid_parallel_grad_clip_norm/test_amp_optimizer.py: 1 warning
tests/test_shardformer/test_hybrid_parallel_grad_clip_norm/test_naive_optimizer.py: 1 warning
tests/test_shardformer/test_hybrid_parallel_grad_clip_norm/test_zero_optimizer.py: 1 warning
tests/test_shardformer/test_model/test_shard_bert.py: 1 warning
tests/test_shardformer/test_model/test_shard_blip2.py: 1 warning
tests/test_shardformer/test_model/test_shard_bloom.py: 1 warning
tests/test_shardformer/test_model/test_shard_chatglm2.py: 1 warning
tests/test_shardformer/test_model/test_shard_command.py: 1 warning
tests/test_shardformer/test_model/test_shard_falcon.py: 1 warning
tests/test_shardformer/test_model/test_shard_gpt2.py: 1 warning
tests/test_shardformer/test_model/test_shard_llama.py: 1 warning
tests/test_shardformer/test_model/test_shard_mistral.py: 1 warning
tests/test_shardformer/test_model/test_shard_opt.py: 1 warning
tests/test_shardformer/test_model/test_shard_qwen2.py: 1 warning
tests/test_shardformer/test_model/test_shard_sam.py: 1 warning
tests/test_shardformer/test_model/test_shard_t5.py: 1 warning
tests/test_shardformer/test_model/test_shard_vit.py: 1 warning
tests/test_shardformer/test_model/test_shard_whisper.py: 1 warning
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
warnings.warn(
2025-04-11T04:23:19.1734904Z
tests/test_booster/test_accelerator.py: 1 warning
tests/test_checkpoint_io/test_general_checkpoint_io.py: 1 warning
tests/test_checkpoint_io/test_low_level_zero_checkpoint_io.py: 1 warning
tests/test_checkpoint_io/test_safetensors_async_io.py: 1 warning
tests/test_fp8/test_fp8_cast.py: 1 warning
tests/test_infer/test_kernels/cuda/test_flash_decoding_attention.py: 2 warnings
tests/test_shardformer/test_flash_attention.py: 1 warning
tests/test_shardformer/test_with_torch_ddp.py: 1 warning
tests/test_shardformer/test_hybrid_parallel_grad_clip_norm/test_amp_optimizer.py: 1 warning
tests/test_shardformer/test_hybrid_parallel_grad_clip_norm/test_naive_optimizer.py: 1 warning
tests/test_shardformer/test_hybrid_parallel_grad_clip_norm/test_zero_optimizer.py: 1 warning
tests/test_shardformer/test_model/test_shard_bert.py: 1 warning
tests/test_shardformer/test_model/test_shard_blip2.py: 1 warning
tests/test_shardformer/test_model/test_shard_bloom.py: 1 warning
tests/test_shardformer/test_model/test_shard_chatglm2.py: 1 warning
tests/test_shardformer/test_model/test_shard_command.py: 1 warning
tests/test_shardformer/test_model/test_shard_falcon.py: 1 warning
tests/test_shardformer/test_model/test_shard_gpt2.py: 1 warning
tests/test_shardformer/test_model/test_shard_llama.py: 1 warning
tests/test_shardformer/test_model/test_shard_mistral.py: 1 warning
tests/test_shardformer/test_model/test_shard_opt.py: 1 warning
tests/test_shardformer/test_model/test_shard_qwen2.py: 1 warning
tests/test_shardformer/test_model/test_shard_sam.py: 1 warning
tests/test_shardformer/test_model/test_shard_t5.py: 1 warning
tests/test_shardformer/test_model/test_shard_vit.py: 1 warning
tests/test_shardformer/test_model/test_shard_whisper.py: 1 warning
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/memory.py:356: FutureWarning: torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
warnings.warn(
2025-04-11T04:23:19.1746244Z
tests/test_infer/test_async_engine/test_async_engine.py::test_new_requests_event
/opt/conda/envs/pytorch/lib/python3.10/site-packages/_pytest/python.py:183: PytestUnhandledCoroutineWarning: async def functions are not natively supported and have been skipped.
You need to install a suitable plugin for your async framework, for example:
- anyio
- pytest-asyncio
- pytest-tornasync
- pytest-trio
- pytest-twisted
warnings.warn(PytestUnhandledCoroutineWarning(msg.format(nodeid)))
2025-04-11T04:23:19.1749631Z
tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype0-g_dtype0-False-CPUAdam-device1]
/__w/ColossalAI/ColossalAI/colossalai/nn/optimizer/nvme_optimizer.py:55: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
numel += p.storage().size()
2025-04-11T04:23:19.1752392Z
tests/test_optimizer/test_lr_scheduler.py::test_lr_scheduler_save_load
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:143: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
2025-04-11T04:23:19.1755638Z
tests/test_optimizer/test_lr_scheduler.py::test_lr_scheduler_save_load
/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:854: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
warnings.warn("To get the last learning rate computed by the scheduler, "
2025-04-11T04:23:19.1757476Z
-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html
============================== slowest durations ===============================
17.15s call     tests/test_lazy/test_models.py::test_models_lazy_init[cpu-subset0]
15.35s call     tests/test_infer/test_continuous_batching.py::test_continuous_batching
15.10s call     tests/test_tensor/test_dtensor/test_layout_converter.py::test_layout_converter
10.96s call     tests/test_shardformer/test_model/test_shard_deepseek_v3.py::test_deepseek_v3[4]
9.67s call     tests/test_zero/test_gemini/test_inference.py::test_inference[4]
8.90s call     tests/test_optimizer/test_dist_adafactor.py::test_dist_adafactor
8.86s call     tests/test_optimizer/test_dist_came.py::test_dist_came
8.72s call     tests/test_shardformer/test_model/test_shard_deepseek.py::test_deepseek[4]
8.72s call     tests/test_optimizer/test_dist_galore.py::test_dist_galore
8.67s call     tests/test_checkpoint_io/test_hybrid_parallel_plugin_checkpoint_io.py::test_hybrid_ckpIO[4]
8.57s call     tests/test_booster/test_plugin/test_gemini_plugin.py::test_gemini_plugin
8.45s call     tests/test_booster/test_plugin/test_3d_plugin.py::test_3d_plugin
8.35s call     tests/test_checkpoint_io/test_gemini_checkpoint_io.py::test_gemini_ckpIO
8.16s call     tests/test_optimizer/test_dist_lamb.py::test_dist_lamb
7.92s call     tests/test_zero/test_gemini/test_optim.py::test_optim[4]
7.53s call     tests/test_checkpoint_io/test_plugins_huggingface_compatibility.py::test_huggingface_compatibility[2]
7.41s call     tests/test_checkpoint_io/test_gemini_torch_compability.py::test_gemini_ckpIO[2]
7.38s call     tests/test_lora/test_lora.py::test_torch_ddp_lora
7.25s call     tests/test_zero/test_gemini/test_zeroddp_state_dict.py::test_zero_ddp[4]
7.08s call     tests/test_fp8/test_fp8_fsdp_comm_hook.py::test_fsdp
6.86s call     tests/test_booster/test_plugin/test_torch_ddp_plugin.py::test_torch_ddp_plugin
6.67s call     tests/test_zero/test_gemini/test_grad_clip.py::test_grad_clip[2]
6.46s call     tests/test_booster/test_plugin/test_torch_fsdp_plugin.py::test_torch_fsdp_plugin
6.42s call     tests/test_zero/test_gemini/test_grad_accum.py::test_grad_accumulation
6.40s call     tests/test_booster/test_plugin/test_low_level_zero_plugin.py::test_low_level_zero_plugin
6.31s call     tests/test_zero/test_gemini/test_search.py::test_search[4]
6.29s call     tests/test_zero/test_gemini/test_inference.py::test_inference[1]
6.25s call     tests/test_moe/test_moe_checkpoint.py::test_mixtral_moe_layer[4]
6.22s call     tests/test_pipeline/test_schedule/test_oneF_oneB.py::test_pp[4-12-6]
6.21s call     tests/test_pipeline/test_stage_manager.py::test_pipeline_stage_manager
6.11s call     tests/test_zero/test_gemini/test_chunkv2.py::test_chunk_function[4]
6.02s call     tests/test_zero/test_gemini/test_grad_clip.py::test_grad_clip[1]
6.01s call     tests/test_pipeline/test_schedule/test_oneF_oneB.py::test_pp[4-12-4]
5.96s call     tests/test_zero/test_low_level/test_zero1_2.py::test_zero_1_2
5.89s call     tests/test_infer/test_streamingllm.py::test_engine
5.81s call     tests/test_zero/test_low_level/test_zero_ckpt.py::test_zero_ckpt
5.79s call     tests/test_zero/test_low_level/test_coll_nd.py::test_comm_nd
5.73s call     tests/test_fp8/test_fp8_reduce_scatter.py::test_reduce_scatter
5.53s call     tests/test_checkpoint_io/test_torch_ddp_checkpoint_io.py::test_torch_ddp_checkpointIO
5.40s call     tests/test_fp8/test_fp8_allgather.py::test_all_gather
5.40s call     tests/test_fp8/test_all_to_all_single.py::test_all_to_all_single
5.37s call     tests/test_tensor/test_comm_spec_apply.py::test_comm_spec
5.35s call     tests/test_shardformer/test_model/test_shard_mixtral.py::test_mixtral[4]
5.31s call     tests/test_pipeline/test_schedule/test_interleaved.py::test_pp[2-12-12]
5.28s call     tests/test_infer/test_drafter.py::test_spec_dec
5.22s call     tests/test_pipeline/test_schedule/test_zerobubble_pp.py::test_pp
5.21s call     tests/test_pipeline/test_schedule/test_interleaved.py::test_pp[2-12-4]
5.15s call     tests/test_fp8/test_fp8_allreduce.py::test_all_reduce
5.15s call     tests/test_shardformer/test_layer/test_sequence_parallel.py::test_all_to_all_attention
5.14s call     tests/test_shardformer/test_layer/test_gpt2_qkv_fused_linear_1d.py::test_linearconv
5.14s call     tests/test_shardformer/test_layer/test_dist_crossentropy.py::test_dist_crossentropy
5.09s call     tests/test_shardformer/test_layer/test_ring_attn.py::test_double_ring
5.08s call     tests/test_device/test_init_logical_pg.py::test_logical_pg
5.06s call     tests/test_tensor/test_dtensor/test_comm_spec.py::test_comm_spec
5.04s call     tests/test_zero/test_gemini/test_chunkv2.py::test_chunk_function[2]
5.04s call     tests/test_pipeline/test_schedule/test_interleaved.py::test_pp[4-12-12]
5.03s call     tests/test_tensor/test_padded_tensor.py::test_padded_tensor
5.02s call     tests/test_zero/test_low_level/test_grad_acc.py::test_grad_accumulation
4.99s call     tests/test_pipeline/test_schedule/test_oneF_oneB.py::test_pp[2-12-4]
4.96s call     tests/test_device/test_device_mesh.py::test_device_mesh_from_process_group
4.72s call     tests/test_lazy/test_from_pretrained.py::test_lazy_from_pretrained
4.68s call     tests/test_infer/test_drafter.py::test_drafter[5]
4.66s call     tests/test_cluster/test_device_mesh_manager.py::test_device_mesh_manager
4.48s call     tests/test_checkpoint_io/test_torch_fsdp_checkpoint_io.py::test_torch_fsdp_ckpt
4.41s call     tests/test_fp8/test_fp8_all_to_all.py::test_all_to_all
4.38s call     tests/test_tensor/test_dtensor/test_dtensor.py::test_dtensor
4.31s call     tests/test_fp8/test_fp8_all_to_all_single.py::test_all_to_all_single
4.29s call     tests/test_tensor/test_shape_consistency_apply.py::test_apply
4.18s call     tests/test_zero/test_low_level/test_mem_leak.py::test_zero_1_2
4.16s call     tests/test_shardformer/test_layer/test_linear_1d.py::test_linear
4.11s call     tests/test_pipeline/test_schedule/test_interleaved.py::test_pp[4-12-4]
4.10s call     tests/test_booster/test_plugin/test_dp_plugin_base.py::test_dp_plugin_dataloader
4.10s call     tests/test_shardformer/test_layer/test_vocab_parallel_embedding_1d.py::test_vocab_embedding
4.10s call     tests/test_shardformer/test_layer/test_embedding.py::test_embedding_1d
4.09s call     tests/test_cluster/test_process_group_mesh.py::test_process_group_mesh
4.08s call     tests/test_shardformer/test_layer/test_qkv_fused_linear_1d.py::test_linearconv
4.07s call     tests/test_pipeline/test_schedule/test_oneF_oneB.py::test_pp[2-12-6]
4.07s call     tests/test_zero/test_gemini/test_chunk_mgrv2.py::test_chunk_manager[2]
4.05s call     tests/test_shardformer/test_layer/test_layernorm.py::test_layernorm
4.04s call     tests/test_shardformer/test_layer/test_ring_attn.py::test_ring_attn
4.03s call     tests/test_pipeline/test_p2p_communication.py::test_pipeline_p2p
4.03s call     tests/test_shardformer/test_layer/test_dropout.py::test_dropout
3.82s call     tests/test_infer/test_kvcache_manager.py::test_cache_manager
3.80s call     tests/test_infer/test_request_handler.py::test_running_list_and_request_handler
3.79s call     tests/test_zero/test_gemini/test_search.py::test_search[1]
3.69s call     tests/test_infer/test_config_and_struct.py::test_config_and_inference
3.48s call     tests/test_zero/test_gemini/test_chunkv2.py::test_chunk_function[1]
0.66s call     tests/test_checkpoint_io/test_general_checkpoint_io.py::test_sharded_model_checkpoint[False-True]
0.57s call     tests/test_checkpoint_io/test_general_checkpoint_io.py::test_sharded_optimizer_checkpoint[False]
0.56s call     tests/test_checkpoint_io/test_general_checkpoint_io.py::test_sharded_model_checkpoint[False-False]
0.55s setup    tests/test_infer/test_drafter.py::test_drafter[5]
0.47s call     tests/test_checkpoint_io/test_general_checkpoint_io.py::test_sharded_optimizer_multiple_param_groups[False]
0.36s call     tests/test_checkpoint_io/test_general_checkpoint_io.py::test_sharded_optimizer_checkpoint[True]
0.35s call     tests/test_checkpoint_io/test_general_checkpoint_io.py::test_sharded_optimizer_multiple_param_groups[True]
0.33s call     tests/test_checkpoint_io/test_general_checkpoint_io.py::test_sharded_model_checkpoint[True-True]
0.32s call     tests/test_checkpoint_io/test_general_checkpoint_io.py::test_sharded_model_checkpoint[True-False]
0.27s call     tests/test_checkpoint_io/test_safetensors_async_io.py::test_create_pin[4-False]
0.27s call     tests/test_checkpoint_io/test_safetensors_async_io.py::test_create_pin[4-True]
0.22s call     tests/test_shardformer/test_with_torch_ddp.py::test_gpt2
0.21s setup    tests/test_lazy/test_models.py::test_models_lazy_init[cuda-subset0]
0.20s call     tests/test_fp8/test_fp8_cast.py::test_fp8_cast
0.19s setup    tests/test_pipeline/test_schedule/test_oneF_oneB.py::test_pp[4-12-4]
0.19s setup    tests/test_infer/test_kvcache_manager.py::test_logical_blocks
0.18s setup    tests/test_pipeline/test_pipeline_utils/test_whisper_pipeline_utils.py::test_whisper_pipeline_distribution
0.17s setup    tests/test_optimizer/test_dist_lamb.py::test_dist_lamb
0.17s setup    tests/test_moe/test_kernel.py::test_moe_kernel[data_type0]
0.17s call     tests/test_checkpoint_io/test_low_level_zero_checkpoint_io.py::test_low_level_zero_checkpointIO
0.16s setup    tests/test_optimizer/test_dist_came.py::test_dist_came
0.16s setup    tests/test_optimizer/test_dist_galore.py::test_dist_galore
0.16s setup    tests/test_optimizer/test_adam_kernel.py::test_fused_adam_kernel[p_dtype0-g_dtype0-0.0-False]
0.16s setup    tests/test_pipeline/test_stage_manager.py::test_pipeline_stage_manager
0.16s setup    tests/test_optimizer/test_lr_scheduler.py::test_lr_scheduler_save_load
0.16s setup    tests/test_pipeline/test_pipeline_utils/test_t5_pipeline_utils.py::test_t5_pipeline_distribution
0.16s call     tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype0-g_dtype0-False-CPUAdam-device1]
0.16s setup    tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype1-g_dtype1-True-CPUAdam-device2]
0.15s call     tests/test_fp8/test_fp8_linear.py::test_fp8_linear[True-False]
0.15s call     tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype2-g_dtype2-False-CPUAdam-device1]
0.15s setup    tests/test_optimizer/test_adam_kernel.py::test_fused_adam_kernel[p_dtype0-g_dtype0-0.0-True]
0.15s call     tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype1-g_dtype1-False-CPUAdam-device1]
0.15s setup    tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype1-g_dtype1-False-HybridAdam-device4]
0.15s call     tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype0-g_dtype0-True-CPUAdam-device1]
0.15s call     tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype0-g_dtype0-False-HybridAdam-device3]
0.15s setup    tests/test_shardformer/test_layer/test_gpt2_qkv_fused_linear_1d.py::test_linearconv
0.15s call     tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype2-g_dtype2-True-HybridAdam-device3]
0.15s call     tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype1-g_dtype1-False-HybridAdam-device3]
0.15s call     tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype2-g_dtype2-True-CPUAdam-device1]
0.15s setup    tests/test_pipeline/test_schedule/test_oneF_oneB.py::test_pp[4-12-6]
0.14s call     tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype2-g_dtype2-False-HybridAdam-device3]
0.14s setup    tests/test_pipeline/test_schedule/test_interleaved.py::test_pp[4-12-4]
0.14s setup    tests/test_pipeline/test_schedule/test_oneF_oneB.py::test_pp[2-12-6]
0.14s setup    tests/test_pipeline/test_schedule/test_oneF_oneB.py::test_pp[2-12-4]
0.14s setup    tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype2-g_dtype2-False-HybridAdam-device3]
0.14s setup    tests/test_pipeline/test_schedule/test_interleaved.py::test_pp[2-12-12]
0.14s setup    tests/test_pipeline/test_schedule/test_interleaved.py::test_pp[4-12-12]
0.14s call     tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype1-g_dtype1-True-HybridAdam-device3]
0.14s setup    tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype2-g_dtype2-False-FusedAdam-device0]
0.14s setup    tests/test_zero/test_low_level/test_coll_nd.py::test_comm_nd
0.14s call     tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype0-g_dtype0-True-HybridAdam-device3]
0.14s setup    tests/test_zero/test_low_level/test_grad_acc.py::test_grad_accumulation
0.14s setup    tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype2-g_dtype2-False-CPUAdam-device1]
0.14s call     tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype2-g_dtype2-False-CPUAdam-device2]
0.14s call     tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype1-g_dtype1-True-CPUAdam-device1]
0.14s call     tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype2-g_dtype2-True-FusedAdam-device0]
0.14s setup    tests/test_pipeline/test_schedule/test_pipeline_schedule_utils.py::test_get_batch_size
0.14s setup    tests/test_tensor/test_shape_consistency.py::test_one_step_transform
0.14s setup    tests/test_shardformer/test_model/test_shard_falcon.py::test_falcon
0.14s setup    tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype2-g_dtype2-True-HybridAdam-device3]
0.14s setup    tests/test_shardformer/test_flash_attention.py::test_flash_attn_func
0.14s setup    tests/test_zero/test_low_level/test_zero1_2.py::test_zero_1_2
0.14s setup    tests/test_zero/test_low_level/test_zero_ckpt.py::test_zero_ckpt
0.14s setup    tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype1-g_dtype1-True-HybridAdam-device3]
0.14s setup    tests/test_infer/test_drafter.py::test_spec_dec
0.14s setup    tests/test_zero/test_low_level/test_mem_leak.py::test_zero_1_2
0.14s call     tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype2-g_dtype2-False-FusedAdam-device0]
0.14s setup    tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype2-g_dtype2-True-FusedAdam-device0]
0.14s setup    tests/test_shardformer/test_model/test_shard_deepseek_v3.py::test_deepseek_v3[4]
0.14s setup    tests/test_shardformer/test_layer/test_embedding.py::test_embedding_1d
0.14s setup    tests/test_tensor/test_padded_tensor.py::test_padded_tensor
0.14s setup    tests/test_shardformer/test_with_torch_ddp.py::test_gpt2
0.14s setup    tests/test_shardformer/test_model/test_shard_bert.py::test_bert
0.14s setup    tests/test_shardformer/test_layer/test_dropout.py::test_dropout
0.14s setup    tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype2-g_dtype2-True-CPUAdam-device1]
0.14s setup    tests/test_infer/test_kvcache_manager.py::test_cache_manager
0.14s setup    tests/test_tensor/test_dtensor/test_dtensor.py::test_dtensor
0.14s call     tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype0-g_dtype0-False-FusedAdam-device0]
0.14s setup    tests/test_tensor/test_sharding_spec.py::test_sharding_spec
0.14s setup    tests/test_infer/test_async_engine/test_async_engine.py::test_new_requests_event
0.14s setup    tests/test_tensor/test_dtensor/test_dtensor_sharding_spec.py::test_dtensor_sharding_spec
0.14s call     tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype1-g_dtype1-True-HybridAdam-device4]
0.14s setup    tests/test_shardformer/test_layer/test_layernorm.py::test_layernorm
0.14s setup    tests/test_zero/test_gemini/test_search.py::test_search[4]
0.14s setup    tests/test_shardformer/test_layer/test_ring_attn.py::test_ring_attn
0.14s setup    tests/test_shardformer/test_layer/test_ring_attn.py::test_double_ring
0.14s setup    tests/test_shardformer/test_model/test_shard_opt.py::test_OPTModel
0.14s setup    tests/test_zero/test_gemini/test_chunkv2.py::test_chunk_function[2]
0.14s setup    tests/test_shardformer/test_layer/test_vocab_parallel_embedding_1d.py::test_vocab_embedding
0.14s setup    tests/test_shardformer/test_layer/test_linear_1d.py::test_linear
0.14s setup    tests/test_zero/test_gemini/test_grad_clip.py::test_grad_clip[2]
0.14s setup    tests/test_optimizer/test_dist_adafactor.py::test_dist_adafactor
0.14s setup    tests/test_shardformer/test_layer/test_qkv_fused_linear_1d.py::test_linearconv
0.14s setup    tests/test_zero/test_gemini/test_chunkv2.py::test_chunk_function[4]
0.14s setup    tests/test_zero/test_gemini/test_grad_accum.py::test_grad_accumulation
0.14s setup    tests/test_zero/test_gemini/test_chunkv2.py::test_chunk_function[1]
0.14s setup    tests/test_zero/test_gemini/test_chunk_mgrv2.py::test_chunk_manager[2]
0.14s setup    tests/test_zero/test_gemini/test_inference.py::test_inference[1]
0.14s setup    tests/test_zero/test_gemini/test_grad_clip.py::test_grad_clip[1]
0.14s setup    tests/test_zero/test_gemini/test_search.py::test_search[1]
0.14s setup    tests/test_zero/test_gemini/test_inference.py::test_inference[4]
0.14s setup    tests/test_zero/test_gemini/test_zeroddp_state_dict.py::test_zero_ddp[4]
0.14s setup    tests/test_zero/test_gemini/test_optim.py::test_optim[4]
0.14s setup    tests/test_shardformer/test_layer/test_sequence_parallel.py::test_all_to_all_attention
0.14s call     tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype2-g_dtype2-False-HybridAdam-device4]
0.14s call     tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype1-g_dtype1-True-FusedAdam-device0]
0.14s setup    tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype1-g_dtype1-True-FusedAdam-device0]
0.14s call     tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype2-g_dtype2-True-CPUAdam-device2]
0.14s call     tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype2-g_dtype2-True-HybridAdam-device4]
0.14s call     tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype1-g_dtype1-False-HybridAdam-device4]
0.14s call     tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype1-g_dtype1-True-CPUAdam-device2]
0.14s call     tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype0-g_dtype0-True-FusedAdam-device0]
0.14s setup    tests/test_booster/test_plugin/test_torch_ddp_plugin.py::test_torch_ddp_plugin
0.13s call     tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype1-g_dtype1-False-FusedAdam-device0]
0.13s call     tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype0-g_dtype0-False-HybridAdam-device4]
0.13s setup    tests/test_optimizer/test_adam_kernel.py::test_fused_adam_kernel[p_dtype1-g_dtype1-0.0-True]
0.13s call     tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype1-g_dtype1-False-CPUAdam-device2]
0.13s call     tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype0-g_dtype0-False-CPUAdam-device2]
0.13s call     tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype0-g_dtype0-True-HybridAdam-device4]
0.13s call     tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype0-g_dtype0-True-CPUAdam-device2]
0.13s call     tests/test_infer/test_kernels/cuda/test_rotary_embdding_unpad.py::test_rotary_emb[dtype0-64-16-32-64-4]
0.13s setup    tests/test_checkpoint_io/test_general_checkpoint_io.py::test_sharded_optimizer_multiple_param_groups[True]
0.13s call     tests/test_infer/test_kernels/cuda/test_rotary_embdding_unpad.py::test_rotary_emb[dtype0-64-32-32-64-4]
0.13s setup    tests/test_lazy/test_ops.py::test_lazy_ops
0.13s setup    tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype2-g_dtype2-False-HybridAdam-device4]
0.13s setup    tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype1-g_dtype1-True-HybridAdam-device4]
0.12s setup    tests/test_checkpoint_io/test_hybrid_parallel_plugin_checkpoint_io.py::test_hybrid_ckpIO[4]
0.12s setup    tests/test_checkpoint_io/test_general_checkpoint_io.py::test_sharded_optimizer_multiple_param_groups[False]
0.12s setup    tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype2-g_dtype2-True-HybridAdam-device4]
0.12s setup    tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype1-g_dtype1-False-CPUAdam-device2]
0.12s setup    tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype0-g_dtype0-True-CPUAdam-device2]
0.12s setup    tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype1-g_dtype1-True-CPUAdam-device1]
0.12s setup    tests/test_checkpoint_io/test_low_level_zero_checkpoint_io.py::test_low_level_zero_checkpointIO
0.12s setup    tests/test_checkpoint_io/test_gemini_checkpoint_io.py::test_gemini_ckpIO
0.12s setup    tests/test_checkpoint_io/test_safetensors_async_io.py::test_create_pin[1-True]
0.12s setup    tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype0-g_dtype0-False-CPUAdam-device2]
0.12s setup    tests/test_fp8/test_fp8_cast.py::test_fp8_cast
0.12s setup    tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype2-g_dtype2-False-CPUAdam-device2]
0.12s setup    tests/test_lora/test_lora.py::test_torch_ddp_lora
0.12s setup    tests/test_checkpoint_io/test_general_checkpoint_io.py::test_sharded_optimizer_checkpoint[True]
0.12s setup    tests/test_checkpoint_io/test_general_checkpoint_io.py::test_sharded_model_checkpoint[False-False]
0.12s setup    tests/test_moe/test_kernel.py::test_moe_kernel[data_type1]
0.12s setup    tests/test_moe/test_moe_checkpoint.py::test_mixtral_moe_layer[4]
0.11s setup    tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype2-g_dtype2-True-CPUAdam-device2]
0.11s call     tests/test_infer/test_kernels/triton/test_rotary_embdding_unpad.py::test_rotary_emb[True-dtype0-64-32-64-4]
0.11s setup    tests/test_optimizer/test_adam_kernel.py::test_fused_adam_kernel[p_dtype0-g_dtype0-0.1-False]
0.11s setup    tests/test_booster/test_plugin/test_low_level_zero_plugin.py::test_low_level_zero_plugin
0.11s setup    tests/test_optimizer/test_adam_kernel.py::test_fused_adam_kernel[p_dtype0-g_dtype0-0.1-True]
0.11s setup    tests/test_checkpoint_io/test_gemini_torch_compability.py::test_gemini_ckpIO[2]
0.11s call     tests/test_checkpoint_io/test_safetensors_async_io.py::test_create_pin[1-True]
0.11s setup    tests/test_optimizer/test_adam_kernel.py::test_fused_adam_kernel[p_dtype1-g_dtype1-0.0-False]
0.11s setup    tests/test_booster/test_plugin/test_gemini_plugin.py::test_gemini_plugin
0.11s setup    tests/test_config/test_load_config.py::test_load_config
0.11s setup    tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-5-False-16-16-64-32]
0.11s setup    tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype1-g_dtype1-False-FusedAdam-device0]
0.11s setup    tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype0-g_dtype0-True-FusedAdam-device0]
0.11s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-True-1-16-16-32-7]
0.11s setup    tests/test_infer/test_streamingllm.py::test_engine
0.11s setup    tests/test_checkpoint_io/test_general_checkpoint_io.py::test_unsharded_checkpoint
0.11s setup    tests/test_booster/test_plugin/test_torch_fsdp_plugin.py::test_torch_fsdp_plugin
0.11s setup    tests/test_infer/test_request_handler.py::test_running_list_and_request_handler
0.11s setup    tests/test_fp8/test_fp8_allreduce.py::test_all_reduce
0.11s setup    tests/test_booster/test_plugin/test_dp_plugin_base.py::test_dp_plugin_dataloader
0.11s setup    tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype0-g_dtype0-True-CPUAdam-device1]
0.11s setup    tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype0-g_dtype0-True-HybridAdam-device3]
0.11s setup    tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype1-g_dtype1-False-CPUAdam-device1]
0.11s setup    tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype1-g_dtype1-False-HybridAdam-device3]
0.11s setup    tests/test_checkpoint_io/test_general_checkpoint_io.py::test_sharded_optimizer_checkpoint[False]
0.11s setup    tests/test_fp8/test_fp8_all_to_all.py::test_all_to_all
0.11s setup    tests/test_fp8/test_fp8_all_to_all_single.py::test_all_to_all_single
0.11s setup    tests/test_cluster/test_process_group_mesh.py::test_process_group_mesh
0.11s setup    tests/test_infer/test_batch_bucket.py::test_bucket
0.11s setup    tests/test_checkpoint_io/test_general_checkpoint_io.py::test_sharded_model_checkpoint[True-True]
0.11s setup    tests/test_infer/test_continuous_batching.py::test_continuous_batching
0.11s setup    tests/test_fp8/test_fp8_hook.py::test_fp8_hook
0.11s setup    tests/test_fp8/test_fp8_allgather.py::test_all_gather
0.11s setup    tests/test_device/test_init_logical_pg.py::test_logical_pg
0.11s setup    tests/test_checkpoint_io/test_torch_fsdp_checkpoint_io.py::test_torch_fsdp_ckpt
0.11s setup    tests/test_checkpoint_io/test_general_checkpoint_io.py::test_sharded_model_checkpoint[True-False]
0.11s setup    tests/test_fp8/test_all_to_all_single.py::test_all_to_all_single
0.11s setup    tests/test_cluster/test_device_mesh_manager.py::test_device_mesh_manager
0.11s setup    tests/test_shardformer/test_hybrid_parallel_grad_clip_norm/test_amp_optimizer.py::test_grad_clip_norm
0.11s setup    tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype0-g_dtype0-False-CPUAdam-device1]
0.11s setup    tests/test_fp8/test_fp8_linear.py::test_fp8_linear[False-True]
0.11s setup    tests/test_pipeline/test_p2p_communication.py::test_pipeline_p2p
0.11s setup    tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[False-16-32-64-32]
0.11s setup    tests/test_pipeline/test_pipeline_utils/test_t5_pipeline_utils.py::test_t5_pipeline_layers
0.11s setup    tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype0-g_dtype0-False-HybridAdam-device3]
0.11s setup    tests/test_shardformer/test_shard_utils.py::test_release_layer
0.11s call     tests/test_checkpoint_io/test_safetensors_async_io.py::test_create_pin[1-False]
0.11s setup    tests/test_shardformer/test_hybrid_parallel_grad_clip_norm/test_naive_optimizer.py::test_grad_clip_norm
0.10s setup    tests/test_shardformer/test_hybrid_parallel_grad_clip_norm/test_zero_optimizer.py::test_grad_clip_norm
0.10s setup    tests/test_shardformer/test_layer/test_dist_crossentropy.py::test_dist_crossentropy
0.10s call     tests/test_infer/test_kernels/cuda/test_rotary_embdding_unpad.py::test_rotary_emb[dtype1-64-16-32-64-4]
0.10s setup    tests/test_shardformer/test_model/test_shard_blip2.py::test_blip2
0.10s call     tests/test_optimizer/test_adam_kernel.py::test_fused_adam_kernel[p_dtype0-g_dtype0-0.0-False]
0.10s setup    tests/test_shardformer/test_model/test_shard_gpt2.py::test_gpt2
0.10s setup    tests/test_shardformer/test_model/test_shard_qwen2.py::test_qwen2
0.10s setup    tests/test_optimizer/test_adam_kernel.py::test_fused_adam_kernel[p_dtype3-g_dtype3-0.0-True]
0.10s setup    tests/test_shardformer/test_model/test_shard_command.py::test_command
0.10s setup    tests/test_shardformer/test_model/test_shard_llama.py::test_llama
0.10s setup    tests/test_shardformer/test_model/test_shard_bloom.py::test_bloom
0.10s setup    tests/test_shardformer/test_model/test_shard_sam.py::test_sam
0.10s setup    tests/test_shardformer/test_model/test_shard_deepseek.py::test_deepseek[4]
0.10s setup    tests/test_tensor/test_comm_spec_apply.py::test_comm_spec
0.10s setup    tests/test_shardformer/test_model/test_shard_mistral.py::test_mistral
0.10s setup    tests/test_shardformer/test_model/test_shard_chatglm2.py::test_chatglm
0.10s setup    tests/test_shardformer/test_model/test_shard_t5.py::test_t5
0.10s setup    tests/test_shardformer/test_model/test_shard_vit.py::test_vit
0.10s call     tests/test_infer/test_kernels/cuda/test_rotary_embdding_unpad.py::test_rotary_emb[dtype1-64-32-32-64-4]
0.10s setup    tests/test_shardformer/test_model/test_shard_mixtral.py::test_mixtral[4]
0.10s setup    tests/test_pipeline/test_pipeline_utils/test_whisper_pipeline_utils.py::test_whisper_pipeline_layers
0.10s setup    tests/test_shardformer/test_model/test_shard_whisper.py::test_whisper
0.10s setup    tests/test_infer/test_async_engine/test_request_tracer.py::test_request_tracer
0.10s call     tests/test_infer/test_kernels/triton/test_rotary_embdding_unpad.py::test_rotary_emb[False-dtype0-64-32-64-4]
0.10s setup    tests/test_pipeline/test_schedule/test_pipeline_schedule_utils.py::test_get_micro_batch
0.10s call     tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[False-16-8-16-4]
0.10s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-True-1-16-8-16-7]
0.10s setup    tests/test_pipeline/test_schedule/test_pipeline_schedule_utils.py::test_merge_batch
0.10s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-True-4-16-8-16-16]
0.10s call     tests/test_optimizer/test_adam_kernel.py::test_fused_adam_kernel[p_dtype1-g_dtype1-0.0-True]
0.10s setup    tests/test_pipeline/test_schedule/test_zerobubble_pp.py::test_pp
0.10s setup    tests/test_pipeline/test_schedule/test_interleaved.py::test_pp[2-12-4]
0.10s call     tests/test_optimizer/test_adam_kernel.py::test_fused_adam_kernel[p_dtype3-g_dtype3-0.0-False]
0.10s setup    tests/test_tensor/test_dtensor/test_layout_converter.py::test_layout_converter
0.10s call     tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[False-16-32-64-7]
0.10s setup    tests/test_tensor/test_dtensor/test_comm_spec.py::test_comm_spec
0.10s setup    tests/test_tensor/test_shape_consistency.py::test_shape_consistency
0.10s setup    tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype0-g_dtype0-True-HybridAdam-device4]
0.10s setup    tests/test_tensor/test_shape_consistency_apply.py::test_apply
0.10s call     tests/test_moe/test_kernel.py::test_moe_kernel[data_type0]
0.10s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-True-5-False-4-16-16-16-7]
0.10s call     tests/test_optimizer/test_adam_kernel.py::test_fused_adam_kernel[p_dtype2-g_dtype2-0.1-False]
0.10s call     tests/test_optimizer/test_adam_kernel.py::test_fused_adam_kernel[p_dtype1-g_dtype1-0.0-False]
0.10s call     tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[False-16-32-16-32]
0.10s call     tests/test_lazy/test_ops.py::test_lazy_ops
0.10s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-True-1-16-8-32-7]
0.10s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-True-4-16-8-32-7]
0.10s setup    tests/test_optimizer/test_adam_kernel.py::test_fused_adam_kernel[p_dtype4-g_dtype4-0.0-True]
0.10s call     tests/test_fp8/test_fp8_linear.py::test_fp8_linear[False-True]
0.10s setup    tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype0-g_dtype0-False-HybridAdam-device4]
0.10s call     tests/test_fp8/test_fp8_hook.py::test_fp8_hook
0.10s setup    tests/test_lazy/test_models.py::test_models_lazy_init[cpu-subset0]
0.10s call     tests/test_optimizer/test_adam_kernel.py::test_fused_adam_kernel[p_dtype0-g_dtype0-0.1-False]
0.10s setup    tests/test_optimizer/test_adam_kernel.py::test_fused_adam_kernel[p_dtype1-g_dtype1-0.1-True]
0.10s call     tests/test_optimizer/test_adam_kernel.py::test_fused_adam_kernel[p_dtype1-g_dtype1-0.1-False]
0.10s setup    tests/test_infer/test_kernels/triton/test_rotary_embdding_unpad.py::test_rotary_emb[False-dtype0-64-32-64-4]
0.10s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-True-1-16-16-16-32]
0.10s setup    tests/test_checkpoint_io/test_safetensors_async_io.py::test_create_pin[4-False]
0.10s setup    tests/test_optimizer/test_adam_kernel.py::test_fused_adam_kernel[p_dtype4-g_dtype4-0.1-True]
0.10s call     tests/test_optimizer/test_adam_kernel.py::test_fused_adam_kernel[p_dtype3-g_dtype3-0.1-False]
0.10s setup    tests/test_optimizer/test_adam_kernel.py::test_fused_adam_kernel[p_dtype1-g_dtype1-0.1-False]
0.09s call     tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[True-16-32-32-4]
0.09s call     tests/test_optimizer/test_adam_kernel.py::test_fused_adam_kernel[p_dtype3-g_dtype3-0.0-True]
0.09s call     tests/test_infer/test_kernels/triton/test_rmsnorm_triton.py::test_layer_norm
0.09s setup    tests/test_optimizer/test_adam_kernel.py::test_fused_adam_kernel[p_dtype4-g_dtype4-0.1-False]
0.09s setup    tests/test_fp8/test_fp8_fsdp_comm_hook.py::test_fsdp
0.09s call     tests/test_lazy/test_models.py::test_models_lazy_init[cuda-subset0]
0.09s setup    tests/test_checkpoint_io/test_safetensors_async_io.py::test_save_load
0.09s setup    tests/test_optimizer/test_adam_kernel.py::test_fused_adam_kernel[p_dtype3-g_dtype3-0.1-False]
0.09s setup    tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[True-16-8-64-7]
0.09s call     tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[True-16-8-16-32]
0.09s setup    tests/test_optimizer/test_adam_kernel.py::test_fused_adam_kernel[p_dtype2-g_dtype2-0.1-True]
0.09s call     tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[False-16-8-32-32]
0.09s call     tests/test_optimizer/test_adam_kernel.py::test_fused_adam_kernel[p_dtype0-g_dtype0-0.1-True]
0.09s call     tests/test_optimizer/test_adam_kernel.py::test_fused_adam_kernel[p_dtype3-g_dtype3-0.1-True]
0.09s call     tests/test_optimizer/test_adam_kernel.py::test_fused_adam_kernel[p_dtype4-g_dtype4-0.1-False]
0.09s call     tests/test_optimizer/test_adam_kernel.py::test_fused_adam_kernel[p_dtype2-g_dtype2-0.0-False]
0.09s setup    tests/test_booster/test_accelerator.py::test_accelerator
0.09s call     tests/test_infer/test_batch_bucket.py::test_bucket
0.09s call     tests/test_moe/test_kernel.py::test_moe_kernel[data_type1]
0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-True-4-16-16-16-7]
0.09s setup    tests/test_optimizer/test_adam_kernel.py::test_fused_adam_kernel[p_dtype3-g_dtype3-0.1-True]
0.09s call     tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[True-16-8-64-4]
0.09s call     tests/test_optimizer/test_adam_kernel.py::test_fused_adam_kernel[p_dtype0-g_dtype0-0.0-True]
0.09s call     tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-1-False-16-16-32-7]
0.09s call     tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[True-16-8-64-7]
0.09s call     tests/test_optimizer/test_adam_kernel.py::test_fused_adam_kernel[p_dtype4-g_dtype4-0.0-False]
0.09s setup    tests/test_optimizer/test_adam_kernel.py::test_fused_adam_kernel[p_dtype2-g_dtype2-0.0-False]
0.09s call     tests/test_fp8/test_fp8_linear.py::test_fp8_linear[False-False]
0.09s call     tests/test_optimizer/test_adam_kernel.py::test_fused_adam_kernel[p_dtype1-g_dtype1-0.1-True]
0.09s call     tests/test_shardformer/test_shard_utils.py::test_release_layer
0.09s call     tests/test_optimizer/test_adam_kernel.py::test_fused_adam_kernel[p_dtype2-g_dtype2-0.1-True]
0.09s setup    tests/test_infer/test_kernels/triton/test_xine_copy.py::test_get_xine_cache[dtype0-64-64-4]
0.09s setup    tests/test_optimizer/test_adam_kernel.py::test_fused_adam_kernel[p_dtype3-g_dtype3-0.0-False]
0.09s setup    tests/test_optimizer/test_adam_kernel.py::test_cpu_adam_kernel[p_dtype0-g_dtype0-0.0-False]
0.09s call     tests/test_infer/test_kernels/cuda/test_flash_decoding_attention.py::test_flash_decoding_attention
0.09s call     tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[True-16-8-32-7]
0.09s call     tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[False-16-32-16-4]
0.09s call     tests/test_optimizer/test_adam_kernel.py::test_fused_adam_kernel[p_dtype4-g_dtype4-0.0-True]
0.09s call     tests/test_optimizer/test_adam_kernel.py::test_fused_adam_kernel[p_dtype4-g_dtype4-0.1-True]
0.09s call     tests/test_optimizer/test_adam_kernel.py::test_fused_adam_kernel[p_dtype2-g_dtype2-0.0-True]
0.09s call     tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[True-16-32-16-32]
0.09s setup    tests/test_optimizer/test_adam_kernel.py::test_cpu_adam_kernel[p_dtype0-g_dtype0-0.1-False]
0.09s call     tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[True-16-32-16-4]
0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-False-1-16-16-16-7]
0.09s call     tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[True-16-8-32-32]
0.09s setup    tests/test_optimizer/test_adam_kernel.py::test_fused_adam_kernel[p_dtype4-g_dtype4-0.0-False]
0.09s call     tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[True-16-8-32-4]
0.09s call     tests/test_shardformer/test_model/test_shard_bert.py::test_bert
0.09s setup    tests/test_optimizer/test_adam_kernel.py::test_fused_adam_kernel[p_dtype2-g_dtype2-0.0-True]
0.09s call     tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[False-16-32-64-32]
0.09s call     tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[False-16-8-32-4]
0.09s call     tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[False-16-32-32-7]
0.09s call     tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[False-16-8-16-7]
0.09s call     tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[False-16-32-64-4]
0.09s setup    tests/test_infer/test_kernels/cuda/test_rotary_embdding_unpad.py::test_rotary_emb[dtype1-64-32-32-64-4]
0.09s call     tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[True-16-32-32-7]
0.09s call     tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[True-16-32-64-7]
0.09s call     tests/test_shardformer/test_model/test_shard_falcon.py::test_falcon
0.09s call     tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[True-16-32-64-4]
0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-False-1-16-8-16-32]
0.09s call     tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[True-16-8-16-7]
0.09s call     tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[False-16-8-64-7]
0.09s call     tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[True-16-32-64-32]
0.09s setup    tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-1-True-16-16-32-7]
0.09s call     tests/test_infer/test_kernels/cuda/test_get_cos_and_sin.py::test_get_cos_and_sin[dtype0-64-64-4]
0.09s setup    tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-1-True-16-16-64-32]
0.09s call     tests/test_infer/test_kernels/cuda/test_silu_and_mul.py::test_silu_and_mul[dtype1-11008-64-2]
0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-False-4-16-16-32-7]
0.09s setup    tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-1-False-16-16-32-32]
0.09s setup    tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-1-False-16-16-64-7]
0.09s setup    tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-1-False-16-16-16-32]
0.09s setup    tests/test_optimizer/test_adam_kernel.py::test_fused_adam_kernel[p_dtype2-g_dtype2-0.1-False]
0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-False-4-16-16-32-16]
0.09s setup    tests/test_infer/test_kernels/cuda/test_silu_and_mul.py::test_silu_and_mul[dtype0-11008-64-2]
0.09s setup    tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-1-True-16-16-16-7]
0.09s setup    tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-5-True-16-16-16-7]
0.09s call     tests/test_infer/test_kernels/triton/test_xine_copy.py::test_get_xine_cache[dtype0-64-64-4]
0.09s setup    tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-1-False-16-16-64-32]
0.09s setup    tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-1-True-16-16-32-32]
0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-False-4-16-16-16-16]
0.09s setup    tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-1-False-16-16-32-7]
0.09s setup    tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-1-False-16-16-16-7]
0.09s setup    tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-1-True-16-16-64-7]
0.09s setup    tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-1-True-16-16-16-32]
0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-False-4-16-16-16-7]
0.09s call     tests/test_infer/test_kernels/cuda/test_get_cos_and_sin.py::test_get_cos_and_sin[dtype1-64-64-4]
0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-True-1-16-8-16-7]
0.09s call     tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[False-16-32-32-4]
0.09s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-True-4-16-16-32-32]
0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-True-1-16-8-16-16]
0.09s call     tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[True-16-32-32-32]
0.09s call     tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[False-16-8-64-32]
0.09s call     tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[True-16-8-16-4]
0.09s call     tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[True-16-8-64-32]
0.09s call     tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[True-16-32-16-7]
0.09s call     tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[False-16-32-16-7]
0.09s call     tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[False-16-8-64-4]
0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-True-1-16-8-32-7]
0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-False-4-16-8-16-16]
0.09s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-True-1-16-16-16-7]
0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-False-1-16-16-32-16]
0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-False-4-16-8-32-16]
0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-True-1-16-8-16-16]
0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-False-1-16-8-32-16]
0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-False-4-16-16-32-7]
0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-True-1-16-8-16-7]
0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-False-4-16-16-16-7]
0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-False-4-16-16-16-16]
0.09s setup    tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-5-False-16-16-32-7]
0.09s call     tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[False-16-8-16-32]
0.09s setup    tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-5-False-16-16-32-32]
0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-False-4-16-16-32-16]
0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-False-1-16-16-16-7]
0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-False-4-16-8-32-7]
0.09s setup    tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-5-False-16-16-16-7]
0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-False-1-16-16-16-16]
0.09s setup    tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-1-True-16-16-32-32]
0.09s setup    tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-5-False-16-16-16-32]
0.09s setup    tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-1-True-16-16-16-7]
0.09s setup    tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-1-True-16-16-64-7]
0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-False-4-16-8-16-7]
0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-False-1-16-8-32-7]
0.09s setup    tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-5-False-16-16-64-7]
0.09s setup    tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-5-True-16-16-16-32]
0.09s setup    tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-1-True-16-16-16-32]
0.09s call     tests/test_infer/test_kernels/cuda/test_silu_and_mul.py::test_silu_and_mul[dtype0-11008-64-2]
0.09s setup    tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-5-True-16-16-32-32]
0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-False-1-16-16-32-7]
0.09s call     tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[False-16-8-32-7]
0.09s setup    tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-5-True-16-16-64-32]
0.09s setup    tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-5-True-16-16-64-7]
0.09s setup    tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-1-True-16-16-32-7]
0.09s setup    tests/test_infer/test_kernels/cuda/test_rotary_embdding_unpad.py::test_rotary_emb[dtype0-64-32-32-64-4]
0.09s call     tests/test_fp8/test_fp8_linear.py::test_fp8_linear[True-True]
0.09s setup    tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-5-True-16-16-32-7]
0.09s call     tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[False-16-32-32-32]
0.09s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-False-1-16-16-32-7]
0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-True-1-16-8-32-7]
0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-True-1-16-16-16-7]
0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-True-1-16-8-32-16]
0.09s setup    tests/test_optimizer/test_adam_kernel.py::test_cpu_adam_kernel[p_dtype0-g_dtype0-0.0-True]
0.09s setup    tests/test_optimizer/test_adam_kernel.py::test_cpu_adam_kernel[p_dtype2-g_dtype2-0.1-False]
0.09s setup    tests/test_optimizer/test_adam_kernel.py::test_cpu_adam_kernel[p_dtype1-g_dtype1-0.0-False]
0.09s setup    tests/test_optimizer/test_adam_kernel.py::test_cpu_adam_kernel[p_dtype1-g_dtype1-0.1-False]
0.09s setup    tests/test_optimizer/test_adam_kernel.py::test_cpu_adam_kernel[p_dtype0-g_dtype0-0.1-True]
0.09s setup    tests/test_infer/test_kernels/cuda/test_rms_layernorm.py::test_rms_layernorm[512-8]
0.09s setup    tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype0-g_dtype0-False-FusedAdam-device0]
0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-True-1-True-1-16-8-16-7]
0.09s setup    tests/test_optimizer/test_adam_kernel.py::test_cpu_adam_kernel[p_dtype2-g_dtype2-0.0-True]
0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-False-1-16-8-32-16]
0.09s setup    tests/test_optimizer/test_adam_kernel.py::test_cpu_adam_kernel[p_dtype1-g_dtype1-0.0-True]
0.09s setup    tests/test_optimizer/test_adam_kernel.py::test_cpu_adam_kernel[p_dtype2-g_dtype2-0.1-True]
0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-False-4-16-8-32-7]
0.09s setup    tests/test_optimizer/test_adam_kernel.py::test_cpu_adam_kernel[p_dtype1-g_dtype1-0.1-True]
0.09s setup    tests/test_optimizer/test_adam_kernel.py::test_cpu_adam_kernel[p_dtype2-g_dtype2-0.0-False]
0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-True-1-16-16-32-32]
0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-False-4-16-8-16-16]
0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-False-1-16-16-16-7]
0.09s setup    tests/test_infer/test_kernels/cuda/test_rotary_embdding_unpad.py::test_rotary_emb[dtype1-64-16-32-64-4]
0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-False-4-16-8-16-7]
0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-False-1-16-16-16-16]
0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-False-4-16-8-32-16]
0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-False-1-16-16-32-7]
0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-False-1-16-16-32-16]
0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-True-4-16-8-32-7]
0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-False-1-16-8-16-7]
0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-False-1-16-8-32-7]
0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-True-1-16-16-32-16]
0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-False-1-16-8-16-16]
0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-True-1-16-16-16-7]
0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-True-1-16-8-32-16]
0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-True-1-16-16-16-7]
0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-True-1-16-16-16-16]
0.09s call     tests/test_checkpoint_io/test_general_checkpoint_io.py::test_unsharded_checkpoint
0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-True-1-16-16-32-7]
0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-True-1-16-16-32-16]
0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-True-4-16-16-32-7]
0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-True-4-16-16-16-16]
0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-True-1-16-8-32-16]
0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-True-1-16-8-16-16]
0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-True-4-16-8-16-16]
0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-True-1-16-8-16-16]
0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-True-1-16-8-32-7]
0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-True-1-16-16-32-7]
0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-False-4-16-16-16-16]
0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-True-1-16-8-16-7]
0.09s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-True-4-16-16-32-7]
0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-True-4-16-8-16-7]
0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-True-4-16-16-32-16]
0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-True-4-16-8-32-16]
0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-False-4-16-16-32-16]
0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-True-1-16-16-16-16]
0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-False-4-16-16-32-7]
0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-False-1-16-16-32-16]
0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-False-1-16-8-32-16]
0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-True-1-16-8-16-7]
0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-False-1-16-16-16-7]
0.09s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-True-4-16-8-32-32]
0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-False-1-16-16-16-16]
0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-True-4-16-16-16-7]
0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-False-1-16-16-32-7]
0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-False-1-16-8-16-16]
0.09s setup    tests/test_infer/test_models/test_custom_model.py::test_model
0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-True-4-16-8-32-16]
0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-True-4-16-8-32-7]
0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-True-4-16-16-32-7]
0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-False-1-16-8-32-7]
0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-True-4-16-16-16-7]
0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-True-4-16-16-32-16]
0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-False-4-16-16-16-16]
0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-False-1-16-8-16-7]
0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-True-4-16-8-16-16]
0.09s setup    tests/test_infer/test_kernels/triton/test_rotary_embdding_unpad.py::test_rotary_emb[True-dtype0-64-32-64-4]
0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-True-4-16-16-16-16]
0.09s setup    tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-5-False-16-16-64-32]
0.09s setup    tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-5-False-16-16-16-7]
0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-False-4-16-16-32-7]
0.09s setup    tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-5-False-16-16-16-32]
0.09s setup    tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-5-True-16-16-64-7]
0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-True-4-16-8-16-7]
0.09s setup    tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-5-True-16-16-32-7]
0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-False-1-16-8-32-16]
0.09s setup    tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-5-False-16-16-32-7]
0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-False-1-16-16-32-7]
0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-False-4-16-8-16-16]
0.09s setup    tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-1-False-16-16-16-7]
0.09s setup    tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-5-False-16-16-64-7]
0.09s setup    tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-5-False-16-16-32-32]
0.09s setup    tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-1-False-16-16-32-32]
0.09s setup    tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-5-True-16-16-16-7]
0.09s setup    tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-1-False-16-16-16-32]
0.09s setup    tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-1-False-16-16-64-7]
0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-False-1-16-16-16-7]
0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-False-1-16-16-32-16]
0.09s setup    tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-5-True-16-16-16-32]
0.09s setup    tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-5-True-16-16-64-32]
0.09s setup    tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-1-True-16-16-64-32]
0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-False-4-16-16-32-16]
0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-False-4-16-8-32-16]
0.09s setup    tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-5-True-16-16-32-32]
0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-False-4-16-16-16-7]
0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-False-1-16-8-32-7]
0.09s setup    tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-1-False-16-16-64-32]
0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-False-4-16-8-16-7]
0.09s setup    tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-1-False-16-16-32-7]
0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-False-1-16-8-32-16]
0.09s setup    tests/test_booster/test_plugin/test_3d_plugin.py::test_3d_plugin
0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-False-1-16-8-16-16]
0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-False-1-16-16-16-16]
0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-False-4-16-8-32-7]
0.09s setup    tests/test_infer/test_kernels/triton/test_rmsnorm_triton.py::test_layer_norm
0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-False-1-16-8-16-7]
0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-False-1-16-8-32-7]
0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-True-4-16-16-32-7]
0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-True-4-16-16-32-16]
0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-True-4-16-16-16-16]
0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-True-4-16-16-16-7]
0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-False-1-16-8-16-7]
0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-True-1-16-8-32-16]
0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-False-4-16-16-16-7]
0.09s setup    tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[True-16-32-32-4]
0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-True-4-16-8-16-16]
0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-True-1-16-16-32-16]
0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-False-1-16-8-16-16]
0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-True-4-16-16-32-7]
0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-True-4-16-8-32-7]
0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-False-1-16-16-32-16]
0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-False-4-16-8-16-16]
0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-False-4-16-8-32-7]
0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-True-4-16-8-32-16]
0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-True-1-16-8-16-32]
0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-True-4-16-8-16-7]
0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-True-1-16-16-16-16]
0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-True-1-16-16-32-7]
0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-True-4-16-16-16-16]
0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-True-4-16-16-32-16]
0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-False-4-16-16-16-16]
0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-True-4-16-16-32-16]
0.09s setup    tests/test_infer/test_kernels/cuda/test_rms_layernorm.py::test_rms_layernorm[512-4]
0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-True-4-16-16-16-7]
0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-False-4-16-16-32-7]
0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-False-4-16-8-16-16]
0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-False-4-16-8-32-16]
0.09s call     tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-1-False-16-16-16-7]
0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-False-1-16-8-16-7]
0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-True-4-16-16-32-7]
0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-False-1-16-8-32-7]
0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-False-4-16-8-32-16]
0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-False-1-16-8-16-16]
0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-True-1-16-16-16-7]
0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-True-1-16-8-32-7]
0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-False-1-16-16-32-7]
0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-False-4-16-8-16-7]
0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-True-4-16-8-16-7]
0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-True-1-16-16-16-16]
0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-False-1-16-16-16-16]
0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-True-4-16-8-32-16]
0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-False-4-16-16-32-16]
0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-True-1-16-8-32-16]
0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-False-1-16-8-32-16]
0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-False-4-16-8-32-7]
0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-True-4-16-16-16-16]
0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-True-1-16-16-32-16]
0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-False-4-16-16-16-7]
0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-True-1-16-16-32-7]
0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-True-4-16-8-16-16]
0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-False-4-16-16-32-32]
0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-True-4-16-8-32-7]
0.09s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-False-1-16-16-16-32]
0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-True-1-16-8-16-7]
0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-False-4-16-16-32-7]
0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-True-1-16-8-32-7]
0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-True-1-16-16-16-16]
0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-True-True-1-16-8-16-7]
0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-True-1-16-8-16-32]
0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-False-4-16-8-32-32]
0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-False-4-16-16-16-7]
0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-False-4-16-8-32-7]
0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-False-4-16-16-16-32]
0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-False-4-16-8-32-7]
0.09s call     tests/test_booster/test_accelerator.py::test_accelerator
0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-False-4-16-8-16-7]
0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-False-1-16-16-16-32]
0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-False-4-16-8-32-32]
0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-False-4-16-16-32-32]
0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-False-4-16-8-16-32]
0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-False-1-16-8-32-32]
0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-False-4-16-16-16-32]
0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-False-4-16-16-16-7]
0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-False-1-16-8-32-7]
0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-True-4-16-16-16-32]
0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-False-1-16-16-32-7]
0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-False-1-16-16-32-32]
0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-False-4-16-16-32-7]
0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-True-1-16-8-32-32]
0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-True-4-16-16-16-7]
0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-True-4-16-16-32-32]
0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-False-1-16-8-32-7]
0.09s setup    tests/test_checkpoint_io/test_plugins_huggingface_compatibility.py::test_huggingface_compatibility[2]
0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-False-1-16-16-16-7]
0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-True-4-16-8-32-7]
0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-True-4-16-16-32-7]
0.09s setup    tests/test_infer/test_kernels/cuda/test_silu_and_mul.py::test_silu_and_mul[dtype1-11008-64-2]
0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-False-4-16-16-32-32]
0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-True-1-16-8-32-16]
0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-False-4-16-8-16-7]
0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-False-1-16-8-16-7]
0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-True-1-16-16-16-32]
0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-True-4-16-8-32-32]
0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-True-4-16-8-16-32]
0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-True-1-16-16-16-7]
0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-True-1-16-16-16-7]
0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-True-1-16-8-16-32]
0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-False-4-16-16-16-7]
0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-False-1-16-8-16-7]
0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-True-4-16-8-16-7]
0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-False-4-16-16-16-32]
0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-False-1-16-16-32-32]
0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-False-4-16-16-32-7]
0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-True-1-16-8-16-7]
0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-False-4-16-8-32-32]
0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-True-1-16-16-16-7]
0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-True-1-16-8-32-7]
0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-True-1-16-16-32-7]
0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-False-4-16-8-32-16]
0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-False-1-16-16-16-32]
0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-False-4-16-8-32-7]
0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-True-4-16-8-16-7]
0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-True-4-16-8-16-32]
0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-True-4-16-16-32-32]
0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-True-4-16-8-32-32]
0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-True-4-16-8-32-7]
0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-True-1-16-16-32-32]
0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-True-1-16-8-32-32]
0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-True-1-16-8-32-7]
0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-True-4-16-16-32-7]
0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-True-4-16-8-32-32]
0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-False-1-16-16-16-7]
0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-True-4-16-16-16-7]
0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-False-1-16-16-32-7]
0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-False-1-16-8-32-32]
0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-True-4-16-16-16-32]
0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-True-4-16-8-32-7]
0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-False-1-16-16-32-16]
0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-True-1-16-16-32-32]
0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-False-4-16-8-16-7]
0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-True-1-16-16-32-7]
0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-False-4-16-8-16-32]
0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-False-1-16-8-16-32]
0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-True-4-16-8-16-7]
0.09s call     tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-5-False-16-16-64-7]
0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-True-4-16-16-16-32]
0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-False-4-16-8-32-16]
0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-True-1-16-8-32-7]
0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-True-4-16-8-16-7]
0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-True-1-16-16-16-32]
0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-True-1-16-16-16-7]
0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-True-4-16-16-32-7]
0.09s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-False-4-16-8-32-32]
0.09s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-True-1-16-8-16-7]
0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-True-1-16-16-16-16]
0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-True-4-16-8-16-32]
0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-False-4-16-16-32-7]
0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-True-1-16-8-32-16]
0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-True-1-16-16-16-7]
0.09s call     tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-1-False-16-16-64-32]
0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-False-4-16-16-16-7]
0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-True-1-16-16-32-7]
0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-False-1-16-16-32-7]
0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-True-4-16-16-16-7]
0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-True-1-16-8-16-16]
0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-False-1-16-8-32-7]
0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-True-1-16-16-16-7]
0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-False-1-16-8-32-16]
0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-False-1-16-8-16-7]
0.09s setup    tests/test_checkpoint_io/test_general_checkpoint_io.py::test_sharded_model_checkpoint[False-True]
0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-True-1-16-16-16-16]
0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-True-1-16-8-16-7]
0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-True-1-16-8-32-16]
0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-False-1-16-8-16-16]
0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-False-4-16-8-16-16]
0.09s call     tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-1-True-16-16-16-7]
0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-False-4-16-8-16-7]
0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-False-1-16-16-16-7]
0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-True-1-16-8-16-16]
0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-True-1-16-8-32-7]
0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-True-1-16-8-32-32]
0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-True-4-16-16-16-16]
0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-True-4-16-16-32-16]
0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-True-1-True-1-16-8-16-16]
0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-True-4-16-16-16-7]
0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-False-4-16-8-32-7]
0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-False-4-16-16-32-16]
0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-True-1-16-16-32-16]
0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-True-4-16-16-32-7]
0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-True-1-16-16-32-16]
0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-False-1-16-16-16-16]
0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-True-4-16-8-16-7]
0.09s setup    tests/test_checkpoint_io/test_safetensors_async_io.py::test_create_pin[1-False]
0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-True-4-16-8-16-16]
0.09s setup    tests/test_infer/test_kernels/cuda/test_flash_decoding_attention.py::test_vllm_flash_decoding_attention
0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-True-4-16-8-32-16]
0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-True-4-16-8-32-16]
0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-False-4-16-16-16-16]
0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-True-5-True-4-16-16-16-16]
0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-True-4-16-8-32-7]
0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-True-4-16-16-16-7]
0.09s setup    tests/test_infer/test_kernels/cuda/test_rotary_embdding_unpad.py::test_rotary_emb[dtype0-64-16-32-64-4]
0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-True-5-True-1-16-8-32-16]
0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-True-1-False-1-16-8-16-16]
0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-True-5-False-1-16-8-16-16]
0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-True-1-16-16-32-7]
0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-True-4-16-8-16-7]
0.09s setup    tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[True-16-32-64-4]
0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-True-1-16-8-16-7]
0.09s call     tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-5-True-16-16-32-32]
0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-True-5-False-1-16-16-16-7]
0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-True-1-False-1-16-8-32-7]
0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-True-1-False-1-16-16-32-7]
0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-True-5-False-4-16-16-32-16]
0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-True-5-False-1-16-16-32-7]
0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-True-1-True-1-16-8-32-7]
0.09s setup    tests/test_infer/test_kernels/cuda/test_rms_layernorm.py::test_rms_layernorm[5120-8]
0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-True-5-True-4-16-16-32-16]
0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-True-1-True-4-16-8-32-16]
0.09s setup    tests/test_infer/test_kernels/cuda/test_rms_layernorm.py::test_rms_layernorm[512-16]
0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-True-1-True-1-16-16-16-16]
0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-True-5-False-4-16-16-16-16]
0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-True-5-True-1-16-16-32-16]
0.09s setup    tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[True-16-32-16-4]
0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-True-5-False-4-16-8-32-7]
0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-True-5-True-1-16-16-16-7]
0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-True-1-False-4-16-8-32-7]
0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-True-1-False-4-16-8-16-16]
0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-True-1-True-1-16-16-16-7]
0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-True-5-True-4-16-8-32-7]
0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-False-4-16-8-32-16]
0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-True-1-True-1-16-16-32-7]
0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-True-5-True-1-16-16-16-16]
0.09s setup    tests/test_infer/test_kernels/cuda/test_rms_layernorm.py::test_rms_layernorm[5120-4]
0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-True-5-False-1-16-8-32-16]
0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-True-1-False-1-16-16-16-7]
0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-True-1-True-1-16-8-32-16]
0.09s setup    tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[False-16-32-32-7]
0.09s setup    tests/test_infer/test_kernels/cuda/test_get_cos_and_sin.py::test_get_cos_and_sin[dtype0-64-64-4]
0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-True-5-False-1-16-8-32-7]
0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-True-1-False-4-16-16-16-7]
0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-True-5-False-4-16-8-32-16]
0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-True-1-False-1-16-16-16-16]
0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-True-4-16-16-32-7]
0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-True-5-True-4-16-16-32-7]
0.09s setup    tests/test_fp8/test_fp8_linear.py::test_fp8_linear[True-False]
0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-True-5-True-1-16-8-16-7]
0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-True-5-False-4-16-8-16-7]
0.09s setup    tests/test_infer/test_kernels/cuda/test_rms_layernorm.py::test_rms_layernorm[5120-16]
0.09s setup    tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[True-16-32-64-7]
0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-True-5-False-1-16-16-16-16]
0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-True-1-True-4-16-8-32-7]
0.09s setup    tests/test_checkpoint_io/test_safetensors_async_io.py::test_create_pin[4-True]
0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-True-1-False-4-16-8-32-16]
0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-True-5-True-1-16-8-32-7]
0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-True-1-16-8-16-16]
0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-True-1-True-4-16-8-16-16]
0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-True-5-True-4-16-8-16-7]
0.09s setup    tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[True-16-8-64-4]
0.09s setup    tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[True-16-8-64-32]
0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-True-4-16-8-32-7]
0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-True-1-False-1-16-8-32-16]
0.09s setup    tests/test_infer/test_kernels/cuda/test_rms_layernorm.py::test_rms_layernorm[5120-2]
0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-True-5-True-4-16-8-32-16]
0.09s setup    tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[True-16-32-32-7]
0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-True-5-True-1-16-8-16-16]
0.09s setup    tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[True-16-32-16-7]
0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-True-5-False-4-16-8-16-16]
0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-True-1-True-4-16-16-32-7]
0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-True-5-False-1-16-16-32-16]
0.09s setup    tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[True-16-32-16-32]
0.09s setup    tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[True-16-8-16-32]
0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-True-5-True-4-16-16-16-7]
0.09s call     tests/test_infer/test_kernels/cuda/test_rms_layernorm.py::test_rms_layernorm[128-2]
0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-True-5-False-4-16-16-32-7]
0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-True-1-True-4-16-16-32-16]
0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-True-1-True-1-16-16-32-16]
0.09s setup    tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[True-16-8-32-7]
0.09s setup    tests/test_infer/test_kernels/cuda/test_rms_layernorm.py::test_rms_layernorm[128-2]
0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-False-1-16-8-16-7]
0.09s setup    tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[True-16-8-16-7]
0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-True-5-False-1-16-8-16-7]
0.09s setup    tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[True-16-8-16-4]
0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-True-1-True-4-16-16-16-16]
0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-True-1-True-4-16-8-16-7]
0.09s setup    tests/test_infer/test_kernels/cuda/test_rms_layernorm.py::test_rms_layernorm[128-8]
0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-True-5-True-4-16-8-16-16]
0.09s setup    tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[True-16-32-32-32]
0.09s setup    tests/test_infer/test_kernels/cuda/test_get_cos_and_sin.py::test_get_cos_and_sin[dtype1-64-64-4]
0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-True-1-False-4-16-16-32-16]
0.09s setup    tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[False-16-32-64-7]
0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-False-1-16-8-16-7]
0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-True-1-False-4-16-16-32-7]
0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-False-4-16-16-32-7]
0.09s setup    tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[False-16-32-32-32]
0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-True-1-False-4-16-16-16-16]
0.09s setup    tests/test_infer/test_kernels/cuda/test_rms_layernorm.py::test_rms_layernorm[64-4]
0.09s call     tests/test_shardformer/test_model/test_shard_opt.py::test_OPTModel
0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-True-5-True-1-16-16-32-7]
0.09s setup    tests/test_infer/test_kernels/cuda/test_rms_layernorm.py::test_rms_layernorm[64-8]
0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-True-1-False-4-16-8-16-7]
0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-True-1-False-1-16-16-32-16]
0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-True-1-False-1-16-8-16-7]
0.09s call     tests/test_checkpoint_io/test_safetensors_async_io.py::test_save_load
0.09s setup    tests/test_infer/test_kernels/cuda/test_rms_layernorm.py::test_rms_layernorm[128-4]
0.09s setup    tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-True-1-True-4-16-16-16-7]
0.09s setup    tests/test_infer/test_kernels/cuda/test_rms_layernorm.py::test_rms_layernorm[64-2]
0.09s setup    tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[True-16-8-32-32]
0.09s setup    tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[False-16-32-64-4]
0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-False-1-16-16-32-7]
0.09s call     tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-5-True-16-16-64-7]
0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-False-1-16-16-16-32]
0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-False-4-16-8-16-7]
0.09s setup    tests/test_infer/test_kernels/cuda/test_rms_layernorm.py::test_rms_layernorm[128-16]
0.09s setup    tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[True-16-8-32-4]
0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-False-1-16-8-32-32]
0.09s setup    tests/test_infer/test_kernels/cuda/test_rms_layernorm.py::test_rms_layernorm[512-2]
0.09s setup    tests/test_infer/test_kernels/cuda/test_rms_layernorm.py::test_rms_layernorm[64-16]
0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-False-1-16-16-32-32]
0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-True-4-16-8-16-16]
0.09s call     tests/test_infer/test_kernels/cuda/test_rms_layernorm.py::test_rms_layernorm[512-4]
0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-False-4-16-8-16-32]
0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-False-1-16-16-16-7]
0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-False-1-16-8-16-32]
0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-False-4-16-8-32-7]
0.09s call     tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-5-False-16-16-16-32]
0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-False-1-16-8-32-7]
0.09s call     tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-1-False-16-16-16-7]
0.09s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-True-4-16-8-16-7]
0.09s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-False-4-16-8-32-7]
0.09s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-True-1-16-8-16-7]
0.09s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-True-4-16-8-32-32]
0.09s setup    tests/test_fp8/test_fp8_linear.py::test_fp8_linear[True-True]
0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-False-4-16-8-16-7]
0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-True-1-16-8-16-7]
0.09s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-True-1-16-16-32-7]
0.09s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-True-4-16-16-32-32]
0.09s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-False-1-16-8-16-7]
0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-True-4-16-8-32-16]
0.09s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-True-4-16-8-32-7]
0.09s call     tests/test_shardformer/test_model/test_shard_sam.py::test_sam
0.09s setup    tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[False-16-8-16-4]
0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-True-1-16-8-16-7]
0.09s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-True-1-16-16-16-32]
0.09s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-True-4-16-16-16-32]
0.09s setup    tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[False-16-8-16-32]
0.09s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-False-1-16-16-16-7]
0.09s setup    tests/test_infer/test_config_and_struct.py::test_config_and_inference
0.09s setup    tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[False-16-8-64-4]
0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-False-1-16-16-16-7]
0.09s setup    tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[False-16-8-32-7]
0.09s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-True-1-16-8-16-32]
0.09s setup    tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[False-16-8-64-32]
0.09s setup    tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[False-16-32-16-4]
0.09s setup    tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[False-16-8-16-7]
0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-False-1-16-8-16-16]
0.09s setup    tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[False-16-8-32-4]
0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-False-4-16-8-16-16]
0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-True-4-16-16-32-7]
0.09s setup    tests/test_lazy/test_from_pretrained.py::test_lazy_from_pretrained
0.09s setup    tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[True-16-32-64-32]
0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-False-1-16-8-16-7]
0.09s setup    tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[False-16-8-32-32]
0.09s setup    tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[False-16-8-64-7]
0.09s call     tests/test_shardformer/test_flash_attention.py::test_flash_attn_func
0.09s call     tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-5-True-16-16-32-32]
0.09s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-False-1-16-16-32-7]
0.09s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-True-1-16-8-16-7]
0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-False-1-16-16-16-16]
0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-True-1-16-16-16-16]
0.09s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-False-4-16-8-32-7]
0.09s setup    tests/test_fp8/test_fp8_linear.py::test_fp8_linear[False-False]
0.08s setup    tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[False-16-32-16-7]
0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-True-1-16-16-32-7]
0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-False-1-16-8-16-16]
0.08s setup    tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[False-16-32-32-4]
0.08s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-True-1-16-16-16-32]
0.08s call     tests/test_infer/test_kernels/cuda/test_rms_layernorm.py::test_rms_layernorm[512-2]
0.08s setup    tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[False-16-32-16-32]
0.08s call     tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-1-False-16-16-32-32]
0.08s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-True-4-16-8-16-32]
0.08s call     tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-1-True-16-16-32-32]
0.08s call     tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-1-True-16-16-16-32]
0.08s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-True-4-16-8-16-32]
0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-True-1-16-16-16-16]
0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-False-4-16-16-32-7]
0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-False-1-16-8-16-7]
0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-False-4-16-16-16-16]
0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-False-4-16-16-16-7]
0.08s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-True-1-16-8-32-7]
0.08s call     tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-5-True-16-16-16-7]
0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-False-4-16-8-16-7]
0.08s call     tests/test_shardformer/test_hybrid_parallel_grad_clip_norm/test_zero_optimizer.py::test_grad_clip_norm
0.08s call     tests/test_shardformer/test_model/test_shard_whisper.py::test_whisper
0.08s setup    tests/test_checkpoint_io/test_torch_ddp_checkpoint_io.py::test_torch_ddp_checkpointIO
0.08s call     tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-1-True-16-16-64-7]
0.08s call     tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-1-False-16-16-32-7]
0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-False-4-16-16-32-16]
0.08s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-False-4-16-16-32-32]
0.08s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-False-4-16-8-16-32]
0.08s call     tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-1-True-16-16-64-32]
0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-True-1-16-16-16-7]
0.08s setup    tests/test_infer/test_kernels/cuda/test_flash_decoding_attention.py::test_flash_decoding_attention
0.08s call     tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-5-False-16-16-16-7]
0.08s call     tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-1-False-16-16-16-32]
0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-False-1-16-8-32-7]
0.08s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-True-True-1-16-8-16-32]
0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-True-1-16-16-32-7]
0.08s call     tests/test_shardformer/test_model/test_shard_blip2.py::test_blip2
0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-False-1-16-8-32-16]
0.08s call     tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-5-False-16-16-16-7]
0.08s call     tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-1-True-16-16-16-7]
0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-True-1-16-16-32-16]
0.08s call     tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-1-True-16-16-32-7]
0.08s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-False-4-16-16-32-7]
0.08s setup    tests/test_fp8/test_fp8_reduce_scatter.py::test_reduce_scatter
0.08s call     tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-5-False-16-16-64-7]
0.08s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-False-4-16-8-32-32]
0.08s call     tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-1-True-16-16-16-32]
0.08s call     tests/test_shardformer/test_hybrid_parallel_grad_clip_norm/test_amp_optimizer.py::test_grad_clip_norm
0.08s call     tests/test_shardformer/test_model/test_shard_bloom.py::test_bloom
0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-True-4-16-8-32-16]
0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-False-4-16-8-16-16]
0.08s call     tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-5-True-16-16-64-32]
0.08s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-True-4-16-16-32-32]
0.08s call     tests/test_shardformer/test_hybrid_parallel_grad_clip_norm/test_naive_optimizer.py::test_grad_clip_norm
0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-True-4-16-16-16-7]
0.08s call     tests/test_infer/test_kernels/cuda/test_rms_layernorm.py::test_rms_layernorm[128-8]
0.08s call     tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-5-True-16-16-32-7]
0.08s call     tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-1-True-16-16-32-7]
0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-False-4-16-16-32-16]
0.08s call     tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-5-False-16-16-32-32]
0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-False-4-16-16-32-7]
0.08s call     tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-5-False-16-16-64-32]
0.08s call     tests/test_shardformer/test_model/test_shard_chatglm2.py::test_chatglm
0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-False-1-16-16-32-16]
0.08s call     tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-5-True-16-16-16-32]
0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-True-1-16-8-32-7]
0.08s call     tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-5-False-16-16-64-32]
0.08s call     tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-1-False-16-16-64-7]
0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-False-4-16-8-16-7]
0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-False-4-16-16-32-7]
0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-False-4-16-16-16-7]
0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-False-4-16-16-16-16]
0.08s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-True-True-4-16-8-16-7]
0.08s call     tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-5-False-16-16-16-32]
0.08s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-True-4-16-16-32-7]
0.08s call     tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-5-True-16-16-64-32]
0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-True-1-16-8-32-7]
0.08s call     tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-5-False-16-16-32-32]
0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-False-1-16-16-16-7]
0.08s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-True-4-16-8-32-32]
0.08s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-True-True-4-16-8-16-32]
0.08s call     tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-5-True-16-16-16-32]
0.08s call     tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-1-True-16-16-64-7]
0.08s call     tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-1-True-16-16-64-32]
0.08s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-False-4-16-16-16-32]
0.08s call     tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-1-True-16-16-32-32]
0.08s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-False-1-16-16-16-7]
0.08s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-False-1-16-8-16-7]
0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-False-1-16-16-16-7]
0.08s call     tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-5-True-16-16-16-7]
0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-False-1-16-16-32-7]
0.08s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-False-1-16-16-32-32]
0.08s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-True-True-1-16-16-16-7]
0.08s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-True-True-1-16-8-32-32]
0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-True-1-16-8-32-7]
0.08s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-True-False-1-16-16-16-7]
0.08s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-True-True-1-16-8-32-7]
0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-False-1-16-16-16-16]
0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-True-4-16-16-32-16]
0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-True-4-16-8-16-16]
0.08s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-False-1-16-8-16-7]
0.08s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-True-False-1-16-8-16-32]
0.08s call     tests/test_infer/test_kernels/cuda/test_rms_layernorm.py::test_rms_layernorm[64-2]
0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-False-4-16-8-32-16]
0.08s call     tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-1-False-16-16-64-32]
0.08s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-False-1-16-16-32-32]
0.08s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-True-1-16-16-16-7]
0.08s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-True-True-4-16-8-32-32]
0.08s call     tests/test_infer/test_kernels/cuda/test_rms_layernorm.py::test_rms_layernorm[64-16]
0.08s call     tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-1-False-16-16-64-7]
0.08s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-True-False-4-16-8-32-32]
0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-False-4-16-8-32-7]
0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-True-1-16-8-16-16]
0.08s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-False-1-16-16-16-32]
0.08s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-True-1-16-16-32-7]
0.08s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-True-False-4-16-16-16-32]
0.08s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-True-False-1-16-16-16-32]
0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-False-1-16-8-32-7]
0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-False-4-16-16-16-7]
0.08s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-False-4-16-16-16-32]
0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-False-4-16-8-16-7]
0.08s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-True-1-16-8-32-7]
0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-True-1-16-8-16-7]
0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-True-1-16-8-16-16]
0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-True-4-16-8-32-7]
0.08s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-True-True-4-16-16-32-32]
0.08s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-True-False-1-16-8-32-32]
0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-False-4-16-8-32-7]
0.08s call     tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-5-True-16-16-64-7]
0.08s call     tests/test_shardformer/test_model/test_shard_vit.py::test_vit
0.08s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-False-4-16-8-16-7]
0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-True-1-16-16-16-16]
0.08s call     tests/test_infer/test_kernels/cuda/test_rms_layernorm.py::test_rms_layernorm[512-16]
0.08s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-False-1-16-16-16-7]
0.08s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-False-4-16-16-16-7]
0.08s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-True-True-1-16-16-32-32]
0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-True-1-16-8-32-7]
0.08s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-True-False-4-16-8-16-32]
0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-False-4-16-16-16-16]
0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-False-1-16-16-32-7]
0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-True-4-16-16-16-16]
0.08s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-True-True-1-16-16-32-7]
0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-False-4-16-16-16-7]
0.08s call     tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-5-False-16-16-32-7]
0.08s call     tests/test_shardformer/test_model/test_shard_command.py::test_command
0.08s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-True-True-4-16-8-32-7]
0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-True-1-16-8-16-7]
0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-True-1-16-16-32-16]
0.08s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-True-False-4-16-8-32-7]
0.08s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-True-False-1-16-16-32-7]
0.08s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-True-False-4-16-8-16-7]
0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-True-1-16-8-32-16]
0.08s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-True-True-4-16-16-16-32]
0.08s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-True-False-1-16-16-32-32]
0.08s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-True-True-4-16-16-32-7]
0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-False-4-16-16-32-16]
0.08s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-True-False-1-16-8-16-7]
0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-False-1-16-8-16-7]
0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-True-1-16-8-16-16]
0.08s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-True-1-16-8-16-32]
0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-False-1-16-8-16-7]
0.08s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-True-1-16-8-16-7]
0.08s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-True-4-16-8-16-7]
0.08s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-False-4-16-16-32-32]
0.08s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-True-True-1-16-16-16-32]
0.08s call     tests/test_infer/test_kernels/cuda/test_rms_layernorm.py::test_rms_layernorm[64-4]
0.08s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-True-True-4-16-16-16-7]
0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-False-4-16-8-16-16]
0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-False-1-16-16-16-7]
0.08s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-True-False-4-16-16-32-32]
0.08s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-False-4-16-8-16-32]
0.08s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-False-4-16-16-16-32]
0.08s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-False-1-16-8-32-32]
0.08s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-True-False-4-16-16-16-7]
0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-False-1-16-16-32-16]
0.08s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-True-4-16-16-32-32]
0.08s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-False-4-16-16-16-7]
0.08s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-True-False-1-16-8-32-7]
0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-True-4-16-16-16-16]
0.08s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-True-4-16-16-16-7]
0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-True-4-16-16-32-7]
0.08s call     tests/test_infer/test_kernels/cuda/test_rms_layernorm.py::test_rms_layernorm[5120-8]
0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-True-4-16-16-16-16]
0.08s call     tests/test_infer/test_kernels/cuda/test_flash_decoding_attention.py::test_vllm_flash_decoding_attention
0.08s call     tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-1-False-16-16-32-32]
0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-True-4-16-16-16-7]
0.08s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-False-4-16-16-16-7]
0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-False-1-16-16-16-7]
0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-False-1-16-8-16-16]
0.08s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-False-1-16-8-32-7]
0.08s setup    tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-True-False-4-16-16-32-7]
0.08s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-True-4-16-16-16-7]
0.08s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-True-4-16-16-16-32]
0.08s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-True-1-16-8-32-32]
0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-True-4-16-8-16-7]
0.08s call     tests/test_infer/test_kernels/cuda/test_rms_layernorm.py::test_rms_layernorm[5120-4]
0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-False-1-16-16-32-7]
0.08s call     tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-1-False-16-16-16-32]
0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-False-1-16-16-16-16]
0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-False-4-16-16-32-16]
0.08s call     tests/test_shardformer/test_model/test_shard_t5.py::test_t5
0.08s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-False-1-16-8-16-32]
0.08s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-True-1-16-16-32-32]
0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-False-1-16-16-32-16]
0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-True-1-16-16-16-16]
0.08s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-False-4-16-8-32-7]
0.08s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-True-1-16-16-32-7]
0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-False-1-16-8-32-16]
0.08s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-False-4-16-8-32-7]
0.08s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-True-4-16-8-32-7]
0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-True-4-16-8-16-16]
0.08s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-False-4-16-8-16-7]
0.08s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-False-1-16-8-32-7]
0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-True-4-16-16-32-16]
0.08s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-True-4-16-8-16-7]
0.08s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-False-1-16-16-32-7]
0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-False-1-16-8-32-7]
0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-True-4-16-8-16-16]
0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-False-4-16-16-16-16]
0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-False-1-16-16-32-16]
0.08s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-True-4-16-8-16-32]
0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-True-4-16-16-32-7]
0.08s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-True-4-16-16-16-7]
0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-True-1-16-8-32-16]
0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-True-1-16-16-32-16]
0.08s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-False-1-16-8-32-7]
0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-True-4-16-8-16-7]
0.08s call     tests/test_shardformer/test_model/test_shard_gpt2.py::test_gpt2
0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-True-1-16-8-32-16]
0.08s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-False-4-16-16-32-32]
0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-True-1-16-8-32-16]
0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-True-4-16-16-16-16]
0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-True-4-16-16-16-7]
0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-False-4-16-16-16-16]
0.08s call     tests/test_shardformer/test_model/test_shard_mistral.py::test_mistral
0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-True-4-16-8-32-16]
0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-True-4-16-16-32-7]
0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-True-1-16-16-32-16]
0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-False-1-16-16-32-7]
0.08s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-False-1-16-16-32-32]
0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-False-4-16-16-32-16]
0.08s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-False-4-16-8-32-32]
0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-False-1-16-16-32-16]
0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-True-4-16-16-32-16]
0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-False-4-16-8-16-7]
0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-False-1-16-8-16-16]
0.08s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-False-1-16-8-16-32]
0.08s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-False-4-16-16-32-7]
0.08s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-True-4-16-8-32-7]
0.08s call     tests/test_shardformer/test_model/test_shard_qwen2.py::test_qwen2
0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-False-1-16-8-32-7]
0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-True-4-16-16-32-7]
0.08s call     tests/test_infer/test_kernels/cuda/test_rms_layernorm.py::test_rms_layernorm[512-8]
0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-True-1-16-16-32-7]
0.08s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-True-1-16-16-32-32]
0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-True-4-16-8-16-16]
0.08s call     tests/test_infer/test_kernels/cuda/test_rms_layernorm.py::test_rms_layernorm[128-4]
0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-True-1-16-16-32-7]
0.08s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-False-4-16-16-32-7]
0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-True-4-16-8-32-16]
0.08s call     tests/test_infer/test_kernels/cuda/test_rms_layernorm.py::test_rms_layernorm[64-8]
0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-True-4-16-8-16-7]
0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-False-4-16-8-16-16]
0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-False-4-16-16-16-16]
0.08s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-True-1-16-16-16-7]
0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-False-1-16-16-16-7]
0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-False-1-16-8-32-16]
0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-False-4-16-16-32-7]
0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-True-4-16-8-32-16]
0.08s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-True-1-16-8-32-32]
0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-True-4-16-8-16-7]
0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-True-1-16-16-32-7]
0.08s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-True-1-16-16-32-32]
0.08s call     tests/test_shardformer/test_model/test_shard_llama.py::test_llama
0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-True-4-16-16-16-16]
0.08s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-False-1-16-8-32-32]
0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-False-4-16-8-32-16]
0.08s call     tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-5-False-16-16-32-7]
0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-False-4-16-16-16-7]
0.08s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-False-1-16-8-32-32]
0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-False-1-16-8-32-7]
0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-True-4-16-8-32-7]
0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-False-1-16-8-16-16]
0.08s call     tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-5-True-16-16-32-7]
0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-False-1-16-8-32-16]
0.08s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-True-1-16-8-16-32]
0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-False-1-16-16-16-16]
0.08s call     tests/test_infer/test_kernels/cuda/test_rms_layernorm.py::test_rms_layernorm[5120-16]
0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-True-4-16-8-16-7]
0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-True-1-16-16-16-16]
0.08s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-True-4-16-16-16-32]
0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-True-1-16-16-32-16]
0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-False-4-16-16-32-16]
0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-True-4-16-8-32-16]
0.08s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-True-1-16-16-16-32]
0.08s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-False-4-16-8-16-32]
0.08s call     tests/test_infer/test_kernels/cuda/test_rms_layernorm.py::test_rms_layernorm[128-16]
0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-True-4-16-8-16-16]
0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-False-4-16-8-16-7]
0.08s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-True-4-16-16-32-7]
0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-True-1-16-8-16-7]
0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-False-4-16-16-16-7]
0.08s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-True-1-16-8-32-32]
0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-True-1-16-16-16-7]
0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-True-4-16-8-32-7]
0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-True-1-16-8-16-16]
0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-False-4-16-8-16-16]
0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-True-4-16-16-16-16]
0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-False-1-16-16-32-16]
0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-False-1-16-16-16-16]
0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-True-1-16-16-32-7]
0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-True-4-16-16-16-7]
0.08s call     tests/test_infer/test_kernels/cuda/test_rms_layernorm.py::test_rms_layernorm[5120-2]
0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-True-1-16-16-16-7]
0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-True-4-16-16-16-7]
0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-False-4-16-8-16-16]
0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-False-4-16-8-32-7]
0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-False-1-16-8-16-16]
0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-True-4-16-16-32-16]
0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-False-1-16-16-16-16]
0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-False-4-16-16-32-7]
0.08s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-False-1-16-16-16-32]
0.08s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-False-1-16-8-16-32]
0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-True-4-16-8-32-7]
0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-True-1-16-16-32-16]
0.08s call     tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-False-4-16-8-16-7]
0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-True-1-16-16-16-7]
0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-False-1-16-8-32-16]
0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-True-1-16-16-16-7]
0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-True-4-16-16-32-16]
0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-False-1-16-8-16-7]
0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-True-1-16-8-16-16]
0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-False-1-16-16-32-7]
0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-False-1-16-16-32-7]
0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-False-4-16-8-32-16]
0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-True-1-16-8-32-7]
0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-False-4-16-8-32-7]
0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-True-4-16-16-32-16]
0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-False-4-16-8-32-16]
0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-True-1-16-8-32-7]
0.08s call     tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-True-4-16-8-32-7]
0.08s setup    tests/test_device/test_device_mesh.py::test_device_mesh
0.08s setup    tests/test_device/test_device_mesh.py::test_device_mesh_from_process_group
2025-04-11T04:23:19.2236589Z
(1097 durations < 0.005s hidden.  Use -vv to show these durations.)
=========================== short test summary info ============================
FAILED tests/test_booster/test_accelerator.py::test_accelerator - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_booster/test_plugin/test_3d_plugin.py::test_3d_plugin - torch.multiprocessing.spawn.ProcessRaisedException:
2025-04-11T04:23:19.2238121Z
-- Process 0 terminated with the following error:
Traceback (most recent call last):
File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
fn(i, *args)
File "/__w/ColossalAI/ColossalAI/tests/test_booster/test_plugin/test_3d_plugin.py", line 271, in run_dist
check_3d_plugin(early_stop=early_stop)
File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
partial_func(**kwargs)
File "/__w/ColossalAI/ColossalAI/tests/test_booster/test_plugin/test_3d_plugin.py", line 104, in check_3d_plugin
err = run_fn(init_method, model_fn, data_gen_fn, output_transform_fn)
File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 271, in _clear_cache
get_accelerator().synchronize()
File "/__w/ColossalAI/ColossalAI/colossalai/accelerator/cuda_accelerator.py", line 62, in synchronize
torch.cuda.synchronize(device)
File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py", line 801, in synchronize
return torch._C._cuda_synchronize()
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_booster/test_plugin/test_dp_plugin_base.py::test_dp_plugin_dataloader - torch.multiprocessing.spawn.ProcessRaisedException:
2025-04-11T04:23:19.2242096Z
-- Process 0 terminated with the following error:
Traceback (most recent call last):
File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
fn(i, *args)
File "/__w/ColossalAI/ColossalAI/tests/test_booster/test_plugin/test_dp_plugin_base.py", line 89, in run_dist
check_dataloader_sharding()
File "/__w/ColossalAI/ColossalAI/tests/test_booster/test_plugin/test_dp_plugin_base.py", line 69, in check_dataloader_sharding
batch = next(iter(train_dataloader))[0].cuda()
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_booster/test_plugin/test_gemini_plugin.py::test_gemini_plugin - torch.multiprocessing.spawn.ProcessRaisedException:
2025-04-11T04:23:19.2244538Z
-- Process 0 terminated with the following error:
Traceback (most recent call last):
File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
fn(i, *args)
File "/__w/ColossalAI/ColossalAI/tests/test_booster/test_plugin/test_gemini_plugin.py", line 167, in run_dist
check_gemini_plugin(early_stop=early_stop)
File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
partial_func(**kwargs)
File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
partial_func(**kwargs)
File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
partial_func(**kwargs)
[Previous line repeated 1 more time]
File "/__w/ColossalAI/ColossalAI/tests/test_booster/test_plugin/test_gemini_plugin.py", line 149, in check_gemini_plugin
err = run_fn(init_method, model_fn, data_gen_fn, output_transform_fn, zero_size, tp_size)
File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 271, in _clear_cache
get_accelerator().synchronize()
File "/__w/ColossalAI/ColossalAI/colossalai/accelerator/cuda_accelerator.py", line 62, in synchronize
torch.cuda.synchronize(device)
File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py", line 801, in synchronize
return torch._C._cuda_synchronize()
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_booster/test_plugin/test_low_level_zero_plugin.py::test_low_level_zero_plugin - torch.multiprocessing.spawn.ProcessRaisedException:
2025-04-11T04:23:19.2249384Z
-- Process 0 terminated with the following error:
Traceback (most recent call last):
File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
fn(i, *args)
File "/__w/ColossalAI/ColossalAI/tests/test_booster/test_plugin/test_low_level_zero_plugin.py", line 135, in run_dist
check_low_level_zero_plugin(early_stop=early_stop)
File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
partial_func(**kwargs)
File "/__w/ColossalAI/ColossalAI/tests/test_booster/test_plugin/test_low_level_zero_plugin.py", line 84, in check_low_level_zero_plugin
err = run_fn(stage, model_fn, data_gen_fn, output_transform_fn)
File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 271, in _clear_cache
get_accelerator().synchronize()
File "/__w/ColossalAI/ColossalAI/colossalai/accelerator/cuda_accelerator.py", line 62, in synchronize
torch.cuda.synchronize(device)
File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py", line 801, in synchronize
return torch._C._cuda_synchronize()
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_booster/test_plugin/test_torch_ddp_plugin.py::test_torch_ddp_plugin - torch.multiprocessing.spawn.ProcessRaisedException:
2025-04-11T04:23:19.2253346Z
-- Process 0 terminated with the following error:
Traceback (most recent call last):
File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
fn(i, *args)
File "/__w/ColossalAI/ColossalAI/tests/test_booster/test_plugin/test_torch_ddp_plugin.py", line 113, in run_dist
check_torch_ddp_plugin()
File "/__w/ColossalAI/ColossalAI/tests/test_booster/test_plugin/test_torch_ddp_plugin.py", line 52, in check_torch_ddp_plugin
run_fn(model_fn, data_gen_fn, output_transform_fn)
File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 271, in _clear_cache
get_accelerator().synchronize()
File "/__w/ColossalAI/ColossalAI/colossalai/accelerator/cuda_accelerator.py", line 62, in synchronize
torch.cuda.synchronize(device)
File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py", line 801, in synchronize
return torch._C._cuda_synchronize()
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_booster/test_plugin/test_torch_fsdp_plugin.py::test_torch_fsdp_plugin - torch.multiprocessing.spawn.ProcessRaisedException:
2025-04-11T04:23:19.2256888Z
-- Process 0 terminated with the following error:
Traceback (most recent call last):
File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
fn(i, *args)
File "/__w/ColossalAI/ColossalAI/tests/test_booster/test_plugin/test_torch_fsdp_plugin.py", line 77, in run_dist
check_torch_fsdp_plugin()
File "/__w/ColossalAI/ColossalAI/tests/test_booster/test_plugin/test_torch_fsdp_plugin.py", line 70, in check_torch_fsdp_plugin
run_fn(model_fn, data_gen_fn, output_transform_fn)
File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 271, in _clear_cache
get_accelerator().synchronize()
File "/__w/ColossalAI/ColossalAI/colossalai/accelerator/cuda_accelerator.py", line 62, in synchronize
torch.cuda.synchronize(device)
File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py", line 801, in synchronize
return torch._C._cuda_synchronize()
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_checkpoint_io/test_gemini_checkpoint_io.py::test_gemini_ckpIO - torch.multiprocessing.spawn.ProcessRaisedException:
2025-04-11T04:23:19.2260422Z
-- Process 0 terminated with the following error:
Traceback (most recent call last):
File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
fn(i, *args)
File "/__w/ColossalAI/ColossalAI/tests/test_checkpoint_io/test_gemini_checkpoint_io.py", line 212, in run_dist
exam_state_dict()
File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 271, in _clear_cache
get_accelerator().synchronize()
File "/__w/ColossalAI/ColossalAI/colossalai/accelerator/cuda_accelerator.py", line 62, in synchronize
torch.cuda.synchronize(device)
File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py", line 801, in synchronize
return torch._C._cuda_synchronize()
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_checkpoint_io/test_gemini_torch_compability.py::test_gemini_ckpIO[2] - torch.multiprocessing.spawn.ProcessRaisedException:
2025-04-11T04:23:19.2263496Z
-- Process 0 terminated with the following error:
Traceback (most recent call last):
File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
fn(i, *args)
File "/__w/ColossalAI/ColossalAI/tests/test_checkpoint_io/test_gemini_torch_compability.py", line 167, in run_dist
exam_torch_load_from_gemini()
File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 271, in _clear_cache
get_accelerator().synchronize()
File "/__w/ColossalAI/ColossalAI/colossalai/accelerator/cuda_accelerator.py", line 62, in synchronize
torch.cuda.synchronize(device)
File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py", line 801, in synchronize
return torch._C._cuda_synchronize()
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_checkpoint_io/test_general_checkpoint_io.py::test_unsharded_checkpoint - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_checkpoint_io/test_general_checkpoint_io.py::test_sharded_model_checkpoint[True-True] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_checkpoint_io/test_general_checkpoint_io.py::test_sharded_model_checkpoint[True-False] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_checkpoint_io/test_general_checkpoint_io.py::test_sharded_optimizer_checkpoint[False] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_checkpoint_io/test_general_checkpoint_io.py::test_sharded_optimizer_checkpoint[True] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_checkpoint_io/test_general_checkpoint_io.py::test_sharded_optimizer_multiple_param_groups[False] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_checkpoint_io/test_general_checkpoint_io.py::test_sharded_optimizer_multiple_param_groups[True] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_checkpoint_io/test_hybrid_parallel_plugin_checkpoint_io.py::test_hybrid_ckpIO[4] - torch.multiprocessing.spawn.ProcessRaisedException:
2025-04-11T04:23:19.2273069Z
-- Process 0 terminated with the following error:
Traceback (most recent call last):
File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
fn(i, *args)
File "/__w/ColossalAI/ColossalAI/tests/test_checkpoint_io/test_hybrid_parallel_plugin_checkpoint_io.py", line 148, in run_dist
exam_state_dict()
File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
partial_func(**kwargs)
File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
partial_func(**kwargs)
File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
partial_func(**kwargs)
[Previous line repeated 3 more times]
File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 271, in _clear_cache
get_accelerator().synchronize()
File "/__w/ColossalAI/ColossalAI/colossalai/accelerator/cuda_accelerator.py", line 62, in synchronize
torch.cuda.synchronize(device)
File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py", line 801, in synchronize
return torch._C._cuda_synchronize()
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_checkpoint_io/test_low_level_zero_checkpoint_io.py::test_low_level_zero_checkpointIO - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_checkpoint_io/test_plugins_huggingface_compatibility.py::test_huggingface_compatibility[2] - torch.multiprocessing.spawn.ProcessRaisedException:
2025-04-11T04:23:19.2278291Z
-- Process 0 terminated with the following error:
Traceback (most recent call last):
File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
fn(i, *args)
File "/__w/ColossalAI/ColossalAI/tests/test_checkpoint_io/test_plugins_huggingface_compatibility.py", line 72, in run_dist
exam_from_pretrained()
File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 271, in _clear_cache
get_accelerator().synchronize()
File "/__w/ColossalAI/ColossalAI/colossalai/accelerator/cuda_accelerator.py", line 62, in synchronize
torch.cuda.synchronize(device)
File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py", line 801, in synchronize
return torch._C._cuda_synchronize()
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_checkpoint_io/test_safetensors_async_io.py::test_create_pin[1-True] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_checkpoint_io/test_safetensors_async_io.py::test_create_pin[1-False] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_checkpoint_io/test_safetensors_async_io.py::test_create_pin[4-True] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_checkpoint_io/test_safetensors_async_io.py::test_create_pin[4-False] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_checkpoint_io/test_safetensors_async_io.py::test_save_load - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_checkpoint_io/test_torch_ddp_checkpoint_io.py::test_torch_ddp_checkpointIO - torch.multiprocessing.spawn.ProcessRaisedException:
2025-04-11T04:23:19.2285790Z
-- Process 0 terminated with the following error:
Traceback (most recent call last):
File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
fn(i, *args)
File "/__w/ColossalAI/ColossalAI/tests/test_checkpoint_io/test_torch_ddp_checkpoint_io.py", line 76, in run_dist
check_torch_ddp_checkpointIO()
File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
partial_func(**kwargs)
File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
partial_func(**kwargs)
File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
partial_func(**kwargs)
[Previous line repeated 1 more time]
File "/__w/ColossalAI/ColossalAI/tests/test_checkpoint_io/test_torch_ddp_checkpoint_io.py", line 26, in check_torch_ddp_checkpointIO
model, optimizer, criterion, _, _ = booster.boost(model, optimizer, criterion, lr_scheduler=scheduler)
File "/__w/ColossalAI/ColossalAI/colossalai/booster/booster.py", line 154, in boost
model, optimizer, criterion, dataloader, lr_scheduler = self.plugin.configure(
File "/__w/ColossalAI/ColossalAI/colossalai/booster/plugin/torch_ddp_plugin.py", line 283, in configure
model = model.to(get_current_device())
File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1152, in to
return self._apply(convert)
File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
module._apply(fn)
File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
param_applied = fn(param)
File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1150, in convert
return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_checkpoint_io/test_torch_fsdp_checkpoint_io.py::test_torch_fsdp_ckpt - torch.multiprocessing.spawn.ProcessRaisedException:
2025-04-11T04:23:19.2291845Z
-- Process 0 terminated with the following error:
Traceback (most recent call last):
File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
fn(i, *args)
File "/__w/ColossalAI/ColossalAI/tests/test_checkpoint_io/test_torch_fsdp_checkpoint_io.py", line 156, in run_dist
check_torch_fsdp_ckpt()
File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
partial_func(**kwargs)
File "/__w/ColossalAI/ColossalAI/tests/test_checkpoint_io/test_torch_fsdp_checkpoint_io.py", line 53, in check_torch_fsdp_ckpt
fsdp_model, optimizer, criterion, _, _ = booster.boost(model, optimizer, criterion)
File "/__w/ColossalAI/ColossalAI/colossalai/booster/booster.py", line 154, in boost
model, optimizer, criterion, dataloader, lr_scheduler = self.plugin.configure(
File "/__w/ColossalAI/ColossalAI/colossalai/booster/plugin/torch_fsdp_plugin.py", line 533, in configure
fsdp_model = TorchFSDPModel(model, device_id=torch.cuda.current_device(), **self.fsdp_kwargs)
File "/__w/ColossalAI/ColossalAI/colossalai/booster/plugin/torch_fsdp_plugin.py", line 438, in __init__
self.module = FSDP(module, *args, **kwargs)
File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py", line 503, in __init__
_init_param_handle_from_module(
File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/distributed/fsdp/_init_utils.py", line 568, in _init_param_handle_from_module
_move_module_to_device(
File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/distributed/fsdp/_init_utils.py", line 956, in _move_module_to_device
_move_states_to_device(params_to_move, bufs_to_move, device_from_device_id)
File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/distributed/fsdp/_init_utils.py", line 986, in _move_states_to_device
param.data = param.to(device_from_device_id)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_device/test_init_logical_pg.py::test_logical_pg - torch.multiprocessing.spawn.ProcessRaisedException:
2025-04-11T04:23:19.2297967Z
-- Process 0 terminated with the following error:
Traceback (most recent call last):
File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
fn(i, *args)
File "/__w/ColossalAI/ColossalAI/tests/test_device/test_init_logical_pg.py", line 17, in check_layer
tensor_to_check = torch.tensor([2, 2, 2, 2]).cuda()
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_fp8/test_all_to_all_single.py::test_all_to_all_single - torch.multiprocessing.spawn.ProcessRaisedException:
2025-04-11T04:23:19.2299923Z
-- Process 0 terminated with the following error:
Traceback (most recent call last):
File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
fn(i, *args)
File "/__w/ColossalAI/ColossalAI/tests/test_fp8/test_all_to_all_single.py", line 67, in run_dist
check_all2all()
File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 271, in _clear_cache
get_accelerator().synchronize()
File "/__w/ColossalAI/ColossalAI/colossalai/accelerator/cuda_accelerator.py", line 62, in synchronize
torch.cuda.synchronize(device)
File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py", line 801, in synchronize
return torch._C._cuda_synchronize()
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_fp8/test_fp8_all_to_all.py::test_all_to_all - torch.multiprocessing.spawn.ProcessRaisedException:
2025-04-11T04:23:19.2302885Z
-- Process 0 terminated with the following error:
Traceback (most recent call last):
File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
fn(i, *args)
File "/__w/ColossalAI/ColossalAI/tests/test_fp8/test_fp8_all_to_all.py", line 31, in run_dist
check_4gpu()
File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 271, in _clear_cache
get_accelerator().synchronize()
File "/__w/ColossalAI/ColossalAI/colossalai/accelerator/cuda_accelerator.py", line 62, in synchronize
torch.cuda.synchronize(device)
File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py", line 801, in synchronize
return torch._C._cuda_synchronize()
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_fp8/test_fp8_all_to_all_single.py::test_all_to_all_single - torch.multiprocessing.spawn.ProcessRaisedException:
2025-04-11T04:23:19.2305810Z
-- Process 0 terminated with the following error:
Traceback (most recent call last):
File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
fn(i, *args)
File "/__w/ColossalAI/ColossalAI/tests/test_fp8/test_fp8_all_to_all_single.py", line 29, in run_dist
check_4gpu()
File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 271, in _clear_cache
get_accelerator().synchronize()
File "/__w/ColossalAI/ColossalAI/colossalai/accelerator/cuda_accelerator.py", line 62, in synchronize
torch.cuda.synchronize(device)
File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py", line 801, in synchronize
return torch._C._cuda_synchronize()
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_fp8/test_fp8_allgather.py::test_all_gather - torch.multiprocessing.spawn.ProcessRaisedException:
2025-04-11T04:23:19.2308779Z
-- Process 0 terminated with the following error:
Traceback (most recent call last):
File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
fn(i, *args)
File "/__w/ColossalAI/ColossalAI/tests/test_fp8/test_fp8_allgather.py", line 37, in run_dist
check_4gpu()
File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 271, in _clear_cache
get_accelerator().synchronize()
File "/__w/ColossalAI/ColossalAI/colossalai/accelerator/cuda_accelerator.py", line 62, in synchronize
torch.cuda.synchronize(device)
File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py", line 801, in synchronize
return torch._C._cuda_synchronize()
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_fp8/test_fp8_allreduce.py::test_all_reduce - torch.multiprocessing.spawn.ProcessRaisedException:
2025-04-11T04:23:19.2311677Z
-- Process 0 terminated with the following error:
Traceback (most recent call last):
File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
fn(i, *args)
File "/__w/ColossalAI/ColossalAI/tests/test_fp8/test_fp8_allreduce.py", line 47, in run_dist
check_4gpu()
File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
partial_func(**kwargs)
File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 271, in _clear_cache
get_accelerator().synchronize()
File "/__w/ColossalAI/ColossalAI/colossalai/accelerator/cuda_accelerator.py", line 62, in synchronize
torch.cuda.synchronize(device)
File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py", line 801, in synchronize
return torch._C._cuda_synchronize()
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_fp8/test_fp8_cast.py::test_fp8_cast - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_fp8/test_fp8_fsdp_comm_hook.py::test_fsdp - torch.multiprocessing.spawn.ProcessRaisedException:
2025-04-11T04:23:19.2315769Z
-- Process 0 terminated with the following error:
Traceback (most recent call last):
File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
fn(i, *args)
File "/__w/ColossalAI/ColossalAI/tests/test_fp8/test_fp8_fsdp_comm_hook.py", line 95, in demo_basic
run_model()
File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 271, in _clear_cache
get_accelerator().synchronize()
File "/__w/ColossalAI/ColossalAI/colossalai/accelerator/cuda_accelerator.py", line 62, in synchronize
torch.cuda.synchronize(device)
File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py", line 801, in synchronize
return torch._C._cuda_synchronize()
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_fp8/test_fp8_hook.py::test_fp8_hook - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_fp8/test_fp8_linear.py::test_fp8_linear[True-True] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_fp8/test_fp8_linear.py::test_fp8_linear[True-False] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_fp8/test_fp8_linear.py::test_fp8_linear[False-True] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_fp8/test_fp8_linear.py::test_fp8_linear[False-False] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_fp8/test_fp8_reduce_scatter.py::test_reduce_scatter - torch.multiprocessing.spawn.ProcessRaisedException:
2025-04-11T04:23:19.2322783Z
-- Process 0 terminated with the following error:
Traceback (most recent call last):
File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
fn(i, *args)
File "/__w/ColossalAI/ColossalAI/tests/test_fp8/test_fp8_reduce_scatter.py", line 36, in run_dist
check_4gpu()
File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 271, in _clear_cache
get_accelerator().synchronize()
File "/__w/ColossalAI/ColossalAI/colossalai/accelerator/cuda_accelerator.py", line 62, in synchronize
torch.cuda.synchronize(device)
File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py", line 801, in synchronize
return torch._C._cuda_synchronize()
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_batch_bucket.py::test_bucket - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_continuous_batching.py::test_continuous_batching - torch.multiprocessing.spawn.ProcessRaisedException:
2025-04-11T04:23:19.2326549Z
-- Process 0 terminated with the following error:
Traceback (most recent call last):
File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
fn(i, *args)
File "/__w/ColossalAI/ColossalAI/tests/test_infer/test_continuous_batching.py", line 61, in run_dist
check_inference_engine()
File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
partial_func(**kwargs)
File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
partial_func(**kwargs)
File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
partial_func(**kwargs)
[Previous line repeated 1 more time]
File "/__w/ColossalAI/ColossalAI/tests/test_infer/test_continuous_batching.py", line 39, in check_inference_engine
model = LlamaForCausalLM(LlamaConfig(num_hidden_layers=2)).cuda()
File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2548, in cuda
return super().cuda(*args, **kwargs)
File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in cuda
return self._apply(lambda t: t.cuda(device))
File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
module._apply(fn)
File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
module._apply(fn)
File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
param_applied = fn(param)
File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in <lambda>
return self._apply(lambda t: t.cuda(device))
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_drafter.py::test_drafter[5] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_drafter.py::test_spec_dec - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kvcache_manager.py::test_cache_manager - torch.multiprocessing.spawn.ProcessRaisedException:
2025-04-11T04:23:19.2333928Z
-- Process 0 terminated with the following error:
Traceback (most recent call last):
File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
fn(i, *args)
File "/__w/ColossalAI/ColossalAI/tests/test_infer/test_kvcache_manager.py", line 168, in run_dist
check_cache_manager()
File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
partial_func(**kwargs)
File "/__w/ColossalAI/ColossalAI/tests/test_infer/test_kvcache_manager.py", line 89, in check_cache_manager
cache_manager = KVCacheManager(inference_config, model_config)
File "/__w/ColossalAI/ColossalAI/colossalai/inference/kv_cache/kvcache_manager.py", line 105, in __init__
self._kv_caches = self._init_device_caches(alloc_shape, alloc_shape)
File "/__w/ColossalAI/ColossalAI/colossalai/inference/kv_cache/kvcache_manager.py", line 519, in _init_device_caches
k_cache.append(torch.zeros(kalloc_shape, dtype=self.kv_cache_dtype, device=self.device))
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_request_handler.py::test_running_list_and_request_handler - torch.multiprocessing.spawn.ProcessRaisedException:
2025-04-11T04:23:19.2337628Z
-- Process 0 terminated with the following error:
Traceback (most recent call last):
File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
fn(i, *args)
File "/__w/ColossalAI/ColossalAI/tests/test_infer/test_request_handler.py", line 95, in run_dist
check_request_handler()
File "/__w/ColossalAI/ColossalAI/tests/test_infer/test_request_handler.py", line 70, in check_request_handler
request_handler = RequestHandler(inference_config, model_config)
File "/__w/ColossalAI/ColossalAI/colossalai/inference/core/request_handler.py", line 160, in __init__
self._init_cache(model_config)
File "/__w/ColossalAI/ColossalAI/colossalai/inference/core/request_handler.py", line 222, in _init_cache
self.cache_manager = KVCacheManager(self.inference_config, model_config)
File "/__w/ColossalAI/ColossalAI/colossalai/inference/kv_cache/kvcache_manager.py", line 105, in __init__
self._kv_caches = self._init_device_caches(alloc_shape, alloc_shape)
File "/__w/ColossalAI/ColossalAI/colossalai/inference/kv_cache/kvcache_manager.py", line 519, in _init_device_caches
k_cache.append(torch.zeros(kalloc_shape, dtype=self.kv_cache_dtype, device=self.device))
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_streamingllm.py::test_engine - torch.multiprocessing.spawn.ProcessRaisedException:
2025-04-11T04:23:19.2341679Z
-- Process 0 terminated with the following error:
Traceback (most recent call last):
File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
fn(i, *args)
File "/__w/ColossalAI/ColossalAI/tests/test_infer/test_streamingllm.py", line 107, in run_dist
ret[rank] = func_to_run(**kwargs)
File "/__w/ColossalAI/ColossalAI/tests/test_infer/test_streamingllm.py", line 39, in check_streamingllm
).cuda()
File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2548, in cuda
return super().cuda(*args, **kwargs)
File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in cuda
return self._apply(lambda t: t.cuda(device))
File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
module._apply(fn)
File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
module._apply(fn)
File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
param_applied = fn(param)
File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in <lambda>
return self._apply(lambda t: t.cuda(device))
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/cuda/test_flash_decoding_attention.py::test_flash_decoding_attention - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/cuda/test_flash_decoding_attention.py::test_vllm_flash_decoding_attention - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/cuda/test_get_cos_and_sin.py::test_get_cos_and_sin[dtype0-64-64-4] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/cuda/test_get_cos_and_sin.py::test_get_cos_and_sin[dtype1-64-64-4] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[True-16-8-16-4] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[True-16-8-16-7] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[True-16-8-16-32] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[True-16-8-32-4] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[True-16-8-32-7] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[True-16-8-32-32] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[True-16-8-64-4] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[True-16-8-64-7] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[True-16-8-64-32] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[True-16-32-16-4] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[True-16-32-16-7] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[True-16-32-16-32] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[True-16-32-32-4] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[True-16-32-32-7] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[True-16-32-32-32] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[True-16-32-64-4] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[True-16-32-64-7] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[True-16-32-64-32] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[False-16-8-16-4] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[False-16-8-16-7] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[False-16-8-16-32] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[False-16-8-32-4] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[False-16-8-32-7] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[False-16-8-32-32] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[False-16-8-64-4] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[False-16-8-64-7] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[False-16-8-64-32] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[False-16-32-16-4] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[False-16-32-16-7] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[False-16-32-16-32] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[False-16-32-32-4] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[False-16-32-32-7] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[False-16-32-32-32] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[False-16-32-64-4] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[False-16-32-64-7] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/cuda/test_kv_cache_memcpy.py::test_kv_cache_memcopy[False-16-32-64-32] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/cuda/test_rms_layernorm.py::test_rms_layernorm[64-2] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/cuda/test_rms_layernorm.py::test_rms_layernorm[64-4] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/cuda/test_rms_layernorm.py::test_rms_layernorm[64-8] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/cuda/test_rms_layernorm.py::test_rms_layernorm[64-16] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/cuda/test_rms_layernorm.py::test_rms_layernorm[128-2] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/cuda/test_rms_layernorm.py::test_rms_layernorm[128-4] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/cuda/test_rms_layernorm.py::test_rms_layernorm[128-8] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/cuda/test_rms_layernorm.py::test_rms_layernorm[128-16] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/cuda/test_rms_layernorm.py::test_rms_layernorm[512-2] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/cuda/test_rms_layernorm.py::test_rms_layernorm[512-4] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/cuda/test_rms_layernorm.py::test_rms_layernorm[512-8] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/cuda/test_rms_layernorm.py::test_rms_layernorm[512-16] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/cuda/test_rms_layernorm.py::test_rms_layernorm[5120-2] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/cuda/test_rms_layernorm.py::test_rms_layernorm[5120-4] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/cuda/test_rms_layernorm.py::test_rms_layernorm[5120-8] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/cuda/test_rms_layernorm.py::test_rms_layernorm[5120-16] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/cuda/test_rotary_embdding_unpad.py::test_rotary_emb[dtype0-64-16-32-64-4] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/cuda/test_rotary_embdding_unpad.py::test_rotary_emb[dtype0-64-32-32-64-4] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/cuda/test_rotary_embdding_unpad.py::test_rotary_emb[dtype1-64-16-32-64-4] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/cuda/test_rotary_embdding_unpad.py::test_rotary_emb[dtype1-64-32-32-64-4] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/cuda/test_silu_and_mul.py::test_silu_and_mul[dtype0-11008-64-2] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/cuda/test_silu_and_mul.py::test_silu_and_mul[dtype1-11008-64-2] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-True-1-16-8-16-7] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-True-1-16-8-16-32] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-True-1-16-8-32-7] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-True-1-16-8-32-32] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-True-1-16-16-16-7] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-True-1-16-16-16-32] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-True-1-16-16-32-7] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-True-1-16-16-32-32] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-True-4-16-8-16-7] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-True-4-16-8-16-32] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-True-4-16-8-32-7] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-True-4-16-8-32-32] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-True-4-16-16-16-7] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-True-4-16-16-16-32] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-True-4-16-16-32-7] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-True-4-16-16-32-32] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-False-1-16-8-16-7] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-False-1-16-8-16-32] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-False-1-16-8-32-7] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-False-1-16-8-32-32] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-False-1-16-16-16-7] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-False-1-16-16-16-32] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-False-1-16-16-32-7] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-False-1-16-16-32-32] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-False-4-16-8-16-7] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-False-4-16-8-16-32] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-False-4-16-8-32-7] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-False-4-16-8-32-32] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-False-4-16-16-16-7] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-False-4-16-16-16-32] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-False-4-16-16-32-7] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[True-False-False-4-16-16-32-32] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-True-1-16-8-16-7] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-True-1-16-8-16-32] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-True-1-16-8-32-7] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-True-1-16-8-32-32] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-True-1-16-16-16-7] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-True-1-16-16-16-32] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-True-1-16-16-32-7] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-True-1-16-16-32-32] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-True-4-16-8-16-7] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-True-4-16-8-16-32] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-True-4-16-8-32-7] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-True-4-16-8-32-32] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-True-4-16-16-16-7] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-True-4-16-16-16-32] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-True-4-16-16-32-7] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-True-4-16-16-32-32] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-False-1-16-8-16-7] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-False-1-16-8-16-32] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-False-1-16-8-32-7] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-False-1-16-8-32-32] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-False-1-16-16-16-7] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-False-1-16-16-16-32] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-False-1-16-16-32-7] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-False-1-16-16-32-32] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-False-4-16-8-16-7] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-False-4-16-8-16-32] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-False-4-16-8-32-7] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-False-4-16-8-32-32] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-False-4-16-16-16-7] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-False-4-16-16-16-32] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-False-4-16-16-32-7] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-True-False-4-16-16-32-32] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-True-1-16-8-16-7] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-True-1-16-8-16-32] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-True-1-16-8-32-7] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-True-1-16-8-32-32] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-True-1-16-16-16-7] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-True-1-16-16-16-32] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-True-1-16-16-32-7] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-True-1-16-16-32-32] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-True-4-16-8-16-7] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-True-4-16-8-16-32] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-True-4-16-8-32-7] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-True-4-16-8-32-32] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-True-4-16-16-16-7] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-True-4-16-16-16-32] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-True-4-16-16-32-7] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-True-4-16-16-32-32] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-False-1-16-8-16-7] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-False-1-16-8-16-32] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-False-1-16-8-32-7] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-False-1-16-8-32-32] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-False-1-16-16-16-7] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-False-1-16-16-16-32] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-False-1-16-16-32-7] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-False-1-16-16-32-32] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-False-4-16-8-16-7] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-False-4-16-8-16-32] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-False-4-16-8-32-7] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-False-4-16-8-32-32] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-False-4-16-16-16-7] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-False-4-16-16-16-32] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-False-4-16-16-32-7] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/triton/test_context_attn_unpad.py::test_context_attention[False-False-False-4-16-16-32-32] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-True-1-16-8-16-7] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-True-1-16-8-16-16] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-True-1-16-8-32-7] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-True-1-16-8-32-16] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-True-1-16-16-16-7] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-True-1-16-16-16-16] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-True-1-16-16-32-7] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-True-1-16-16-32-16] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-True-4-16-8-16-7] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-True-4-16-8-16-16] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-True-4-16-8-32-7] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-True-4-16-8-32-16] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-True-4-16-16-16-7] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-True-4-16-16-16-16] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-True-4-16-16-32-7] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-True-4-16-16-32-16] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-False-1-16-8-16-7] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-False-1-16-8-16-16] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-False-1-16-8-32-7] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-False-1-16-8-32-16] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-False-1-16-16-16-7] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-False-1-16-16-16-16] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-False-1-16-16-32-7] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-False-1-16-16-32-16] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-False-4-16-8-16-7] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-False-4-16-8-16-16] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-False-4-16-8-32-7] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-False-4-16-8-32-16] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-False-4-16-16-16-7] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-False-4-16-16-16-16] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-False-4-16-16-32-7] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-1-False-4-16-16-32-16] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-True-1-16-8-16-7] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-True-1-16-8-16-16] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-True-1-16-8-32-7] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-True-1-16-8-32-16] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-True-1-16-16-16-7] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-True-1-16-16-16-16] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-True-1-16-16-32-7] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-True-1-16-16-32-16] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-True-4-16-8-16-7] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-True-4-16-8-16-16] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-True-4-16-8-32-7] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-True-4-16-8-32-16] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-True-4-16-16-16-7] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-True-4-16-16-16-16] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-True-4-16-16-32-7] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-True-4-16-16-32-16] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-False-1-16-8-16-7] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-False-1-16-8-16-16] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-False-1-16-8-32-7] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-False-1-16-8-32-16] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-False-1-16-16-16-7] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-False-1-16-16-16-16] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-False-1-16-16-32-7] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-False-1-16-16-32-16] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-False-4-16-8-16-7] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-False-4-16-8-16-16] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-False-4-16-8-32-7] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-False-4-16-8-32-16] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-False-4-16-16-16-7] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-False-4-16-16-16-16] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-False-4-16-16-32-7] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[True-False-5-False-4-16-16-32-16] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-True-1-16-8-16-7] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-True-1-16-8-16-16] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-True-1-16-8-32-7] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-True-1-16-8-32-16] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-True-1-16-16-16-7] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-True-1-16-16-16-16] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-True-1-16-16-32-7] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-True-1-16-16-32-16] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-True-4-16-8-16-7] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-True-4-16-8-16-16] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-True-4-16-8-32-7] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-True-4-16-8-32-16] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-True-4-16-16-16-7] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-True-4-16-16-16-16] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-True-4-16-16-32-7] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-True-4-16-16-32-16] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-False-1-16-8-16-7] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-False-1-16-8-16-16] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-False-1-16-8-32-7] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-False-1-16-8-32-16] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-False-1-16-16-16-7] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-False-1-16-16-16-16] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-False-1-16-16-32-7] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-False-1-16-16-32-16] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-False-4-16-8-16-7] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-False-4-16-8-16-16] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-False-4-16-8-32-7] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-False-4-16-8-32-16] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-False-4-16-16-16-7] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-False-4-16-16-16-16] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-False-4-16-16-32-7] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-1-False-4-16-16-32-16] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-True-1-16-8-16-7] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-True-1-16-8-16-16] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-True-1-16-8-32-7] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-True-1-16-8-32-16] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-True-1-16-16-16-7] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-True-1-16-16-16-16] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-True-1-16-16-32-7] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-True-1-16-16-32-16] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-True-4-16-8-16-7] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-True-4-16-8-16-16] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-True-4-16-8-32-7] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-True-4-16-8-32-16] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-True-4-16-16-16-7] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-True-4-16-16-16-16] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-True-4-16-16-32-7] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-True-4-16-16-32-16] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-False-1-16-8-16-7] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-False-1-16-8-16-16] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-False-1-16-8-32-7] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-False-1-16-8-32-16] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-False-1-16-16-16-7] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-False-1-16-16-16-16] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-False-1-16-16-32-7] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-False-1-16-16-32-16] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-False-4-16-8-16-7] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-False-4-16-8-16-16] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-False-4-16-8-32-7] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-False-4-16-8-32-16] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-False-4-16-16-16-7] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-False-4-16-16-16-16] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-False-4-16-16-32-7] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-True-5-False-4-16-16-32-16] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-True-1-16-8-16-7] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-True-1-16-8-16-16] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-True-1-16-8-32-7] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-True-1-16-8-32-16] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-True-1-16-16-16-7] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-True-1-16-16-16-16] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-True-1-16-16-32-7] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-True-1-16-16-32-16] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-True-4-16-8-16-7] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-True-4-16-8-16-16] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-True-4-16-8-32-7] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-True-4-16-8-32-16] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-True-4-16-16-16-7] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-True-4-16-16-16-16] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-True-4-16-16-32-7] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-True-4-16-16-32-16] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-False-1-16-8-16-7] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-False-1-16-8-16-16] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-False-1-16-8-32-7] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-False-1-16-8-32-16] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-False-1-16-16-16-7] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-False-1-16-16-16-16] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-False-1-16-16-32-7] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-False-1-16-16-32-16] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-False-4-16-8-16-7] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-False-4-16-8-16-16] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-False-4-16-8-32-7] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-False-4-16-8-32-16] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-False-4-16-16-16-7] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-False-4-16-16-16-16] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-False-4-16-16-32-7] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-1-False-4-16-16-32-16] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-True-1-16-8-16-7] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-True-1-16-8-16-16] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-True-1-16-8-32-7] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-True-1-16-8-32-16] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-True-1-16-16-16-7] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-True-1-16-16-16-16] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-True-1-16-16-32-7] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-True-1-16-16-32-16] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-True-4-16-8-16-7] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-True-4-16-8-16-16] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-True-4-16-8-32-7] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-True-4-16-8-32-16] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-True-4-16-16-16-7] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-True-4-16-16-16-16] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-True-4-16-16-32-7] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-True-4-16-16-32-16] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-False-1-16-8-16-7] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-False-1-16-8-16-16] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-False-1-16-8-32-7] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-False-1-16-8-32-16] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-False-1-16-16-16-7] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-False-1-16-16-16-16] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-False-1-16-16-32-7] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-False-1-16-16-32-16] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-False-4-16-8-16-7] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-False-4-16-8-16-16] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-False-4-16-8-32-7] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-False-4-16-8-32-16] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-False-4-16-16-16-7] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-False-4-16-16-16-16] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-False-4-16-16-32-7] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/triton/test_decoding_attn.py::test_flash_decoding[False-False-5-False-4-16-16-32-16] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-1-True-16-16-16-7] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-1-True-16-16-16-32] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-1-True-16-16-32-7] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-1-True-16-16-32-32] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-1-True-16-16-64-7] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-1-True-16-16-64-32] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-1-False-16-16-16-7] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-1-False-16-16-16-32] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-1-False-16-16-32-7] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-1-False-16-16-32-32] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-1-False-16-16-64-7] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-1-False-16-16-64-32] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-5-True-16-16-16-7] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-5-True-16-16-16-32] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-5-True-16-16-32-7] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-5-True-16-16-32-32] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-5-True-16-16-64-7] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-5-True-16-16-64-32] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-5-False-16-16-16-7] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-5-False-16-16-16-32] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-5-False-16-16-32-7] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-5-False-16-16-32-32] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-5-False-16-16-64-7] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[True-5-False-16-16-64-32] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-1-True-16-16-16-7] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-1-True-16-16-16-32] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-1-True-16-16-32-7] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-1-True-16-16-32-32] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-1-True-16-16-64-7] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-1-True-16-16-64-32] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-1-False-16-16-16-7] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-1-False-16-16-16-32] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-1-False-16-16-32-7] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-1-False-16-16-32-32] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-1-False-16-16-64-7] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-1-False-16-16-64-32] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-5-True-16-16-16-7] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-5-True-16-16-16-32] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-5-True-16-16-32-7] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-5-True-16-16-32-32] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-5-True-16-16-64-7] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-5-True-16-16-64-32] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-5-False-16-16-16-7] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-5-False-16-16-16-32] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-5-False-16-16-32-7] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-5-False-16-16-32-32] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-5-False-16-16-64-7] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/triton/test_kvcache_copy.py::test_copy_kv_to_caches[False-5-False-16-16-64-32] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/triton/test_rmsnorm_triton.py::test_layer_norm - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/triton/test_rotary_embdding_unpad.py::test_rotary_emb[True-dtype0-64-32-64-4] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/triton/test_rotary_embdding_unpad.py::test_rotary_emb[False-dtype0-64-32-64-4] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_infer/test_kernels/triton/test_xine_copy.py::test_get_xine_cache[dtype0-64-64-4] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_lazy/test_models.py::test_models_lazy_init[cuda-subset0] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_lazy/test_ops.py::test_lazy_ops - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_lora/test_lora.py::test_torch_ddp_lora - torch.multiprocessing.spawn.ProcessRaisedException:
2025-04-11T04:23:19.2727888Z
-- Process 0 terminated with the following error:
Traceback (most recent call last):
File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
fn(i, *args)
File "/__w/ColossalAI/ColossalAI/tests/test_lora/test_lora.py", line 103, in run_dist
run_lora_test()
File "/__w/ColossalAI/ColossalAI/tests/test_lora/test_lora.py", line 98, in run_lora_test
check_fwd_bwd(model_fn, data_gen_fn, output_transform_fn, loss_fn, task_type)
File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 271, in _clear_cache
get_accelerator().synchronize()
File "/__w/ColossalAI/ColossalAI/colossalai/accelerator/cuda_accelerator.py", line 62, in synchronize
torch.cuda.synchronize(device)
File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py", line 801, in synchronize
return torch._C._cuda_synchronize()
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_moe/test_kernel.py::test_moe_kernel[data_type0] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_moe/test_kernel.py::test_moe_kernel[data_type1] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_moe/test_moe_checkpoint.py::test_mixtral_moe_layer[4] - torch.multiprocessing.spawn.ProcessRaisedException:
2025-04-11T04:23:19.2732906Z
-- Process 0 terminated with the following error:
Traceback (most recent call last):
File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
fn(i, *args)
File "/__w/ColossalAI/ColossalAI/tests/test_moe/test_moe_checkpoint.py", line 165, in run_dist
check_moe_checkpoint()
File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
partial_func(**kwargs)
File "/__w/ColossalAI/ColossalAI/tests/test_moe/test_moe_checkpoint.py", line 101, in check_moe_checkpoint
dist.broadcast_object_list(broadcast_objects, src=0)
File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/distributed/c10d_logger.py", line 72, in wrapper
return func(*args, **kwargs)
File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py", line 2405, in broadcast_object_list
tensor_list, size_list = zip(*[_object_to_tensor(obj, current_device) for obj in object_list])
File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py", line 2405, in <listcomp>
tensor_list, size_list = zip(*[_object_to_tensor(obj, current_device) for obj in object_list])
File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py", line 2115, in _object_to_tensor
byte_tensor = torch.ByteTensor(byte_storage).to(device)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_optimizer/test_adam_kernel.py::test_fused_adam_kernel[p_dtype0-g_dtype0-0.0-False] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_optimizer/test_adam_kernel.py::test_fused_adam_kernel[p_dtype0-g_dtype0-0.0-True] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_optimizer/test_adam_kernel.py::test_fused_adam_kernel[p_dtype0-g_dtype0-0.1-False] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_optimizer/test_adam_kernel.py::test_fused_adam_kernel[p_dtype0-g_dtype0-0.1-True] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_optimizer/test_adam_kernel.py::test_fused_adam_kernel[p_dtype1-g_dtype1-0.0-False] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_optimizer/test_adam_kernel.py::test_fused_adam_kernel[p_dtype1-g_dtype1-0.0-True] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_optimizer/test_adam_kernel.py::test_fused_adam_kernel[p_dtype1-g_dtype1-0.1-False] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_optimizer/test_adam_kernel.py::test_fused_adam_kernel[p_dtype1-g_dtype1-0.1-True] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_optimizer/test_adam_kernel.py::test_fused_adam_kernel[p_dtype2-g_dtype2-0.0-False] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_optimizer/test_adam_kernel.py::test_fused_adam_kernel[p_dtype2-g_dtype2-0.0-True] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_optimizer/test_adam_kernel.py::test_fused_adam_kernel[p_dtype2-g_dtype2-0.1-False] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_optimizer/test_adam_kernel.py::test_fused_adam_kernel[p_dtype2-g_dtype2-0.1-True] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_optimizer/test_adam_kernel.py::test_fused_adam_kernel[p_dtype3-g_dtype3-0.0-False] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_optimizer/test_adam_kernel.py::test_fused_adam_kernel[p_dtype3-g_dtype3-0.0-True] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_optimizer/test_adam_kernel.py::test_fused_adam_kernel[p_dtype3-g_dtype3-0.1-False] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_optimizer/test_adam_kernel.py::test_fused_adam_kernel[p_dtype3-g_dtype3-0.1-True] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_optimizer/test_adam_kernel.py::test_fused_adam_kernel[p_dtype4-g_dtype4-0.0-False] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_optimizer/test_adam_kernel.py::test_fused_adam_kernel[p_dtype4-g_dtype4-0.0-True] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_optimizer/test_adam_kernel.py::test_fused_adam_kernel[p_dtype4-g_dtype4-0.1-False] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_optimizer/test_adam_kernel.py::test_fused_adam_kernel[p_dtype4-g_dtype4-0.1-True] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype0-g_dtype0-False-FusedAdam-device0] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype0-g_dtype0-False-CPUAdam-device2] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype0-g_dtype0-False-HybridAdam-device3] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype0-g_dtype0-False-HybridAdam-device4] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype0-g_dtype0-True-FusedAdam-device0] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype0-g_dtype0-True-CPUAdam-device2] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype0-g_dtype0-True-HybridAdam-device3] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype0-g_dtype0-True-HybridAdam-device4] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype1-g_dtype1-False-FusedAdam-device0] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype1-g_dtype1-False-CPUAdam-device2] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype1-g_dtype1-False-HybridAdam-device3] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype1-g_dtype1-False-HybridAdam-device4] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype1-g_dtype1-True-FusedAdam-device0] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype1-g_dtype1-True-CPUAdam-device2] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype1-g_dtype1-True-HybridAdam-device3] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype1-g_dtype1-True-HybridAdam-device4] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype2-g_dtype2-False-FusedAdam-device0] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype2-g_dtype2-False-CPUAdam-device2] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype2-g_dtype2-False-HybridAdam-device3] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype2-g_dtype2-False-HybridAdam-device4] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype2-g_dtype2-True-FusedAdam-device0] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype2-g_dtype2-True-CPUAdam-device2] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype2-g_dtype2-True-HybridAdam-device3] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_optimizer/test_adam_optim.py::test_adam_optim_on_bert[p_dtype2-g_dtype2-True-HybridAdam-device4] - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_optimizer/test_dist_adafactor.py::test_dist_adafactor - torch.multiprocessing.spawn.ProcessRaisedException:
2025-04-11T04:23:19.2778259Z
-- Process 0 terminated with the following error:
Traceback (most recent call last):
File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
fn(i, *args)
File "/__w/ColossalAI/ColossalAI/tests/test_optimizer/test_dist_adafactor.py", line 459, in run_dist
exam_dist_adafactor_base()
File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
partial_func(**kwargs)
File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
partial_func(**kwargs)
File "/__w/ColossalAI/ColossalAI/tests/test_optimizer/test_dist_adafactor.py", line 111, in exam_dist_adafactor_base
model_col = nn.Linear(H, W).to(local_rank)  # Col parallel weight
File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1152, in to
return self._apply(convert)
File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
param_applied = fn(param)
File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1150, in convert
return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_optimizer/test_dist_came.py::test_dist_came - torch.multiprocessing.spawn.ProcessRaisedException:
2025-04-11T04:23:19.2782542Z
-- Process 0 terminated with the following error:
Traceback (most recent call last):
File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
fn(i, *args)
File "/__w/ColossalAI/ColossalAI/tests/test_optimizer/test_dist_came.py", line 349, in run_dist
exam_bert_test_on_lowlevelzero_plugin()  # err in TODO layer
File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
partial_func(**kwargs)
File "/__w/ColossalAI/ColossalAI/tests/test_optimizer/test_dist_came.py", line 206, in exam_bert_test_on_lowlevelzero_plugin
) = build_model_from_low_level_zero_plugin(model_fn, loss_fn, test_config, CAME, DistributedCAME)
File "/__w/ColossalAI/ColossalAI/tests/test_shardformer/test_model/_utils.py", line 188, in build_model_from_low_level_zero_plugin
org_model = org_model.cuda()
File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2548, in cuda
return super().cuda(*args, **kwargs)
File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in cuda
return self._apply(lambda t: t.cuda(device))
File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
module._apply(fn)
File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
module._apply(fn)
File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
param_applied = fn(param)
File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in <lambda>
return self._apply(lambda t: t.cuda(device))
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_optimizer/test_dist_galore.py::test_dist_galore - torch.multiprocessing.spawn.ProcessRaisedException:
2025-04-11T04:23:19.2788048Z
-- Process 0 terminated with the following error:
Traceback (most recent call last):
File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
fn(i, *args)
File "/__w/ColossalAI/ColossalAI/tests/test_optimizer/test_dist_galore.py", line 291, in check_dist_galore
dist.barrier()
File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/distributed/c10d_logger.py", line 72, in wrapper
return func(*args, **kwargs)
File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py", line 3439, in barrier
work = default_pg.barrier(opts=opts)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_optimizer/test_dist_lamb.py::test_dist_lamb - torch.multiprocessing.spawn.ProcessRaisedException:
2025-04-11T04:23:19.2790843Z
-- Process 0 terminated with the following error:
Traceback (most recent call last):
File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
fn(i, *args)
File "/__w/ColossalAI/ColossalAI/tests/test_optimizer/test_dist_lamb.py", line 263, in check_dist_lamb
run_dist_lamb_basic()
File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
partial_func(**kwargs)
File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
partial_func(**kwargs)
File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
partial_func(**kwargs)
File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 271, in _clear_cache
get_accelerator().synchronize()
File "/__w/ColossalAI/ColossalAI/colossalai/accelerator/cuda_accelerator.py", line 62, in synchronize
torch.cuda.synchronize(device)
File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/cuda/__init__.py", line 801, in synchronize
return torch._C._cuda_synchronize()
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_pipeline/test_p2p_communication.py::test_pipeline_p2p - torch.multiprocessing.spawn.ProcessRaisedException:
2025-04-11T04:23:19.2794897Z
-- Process 0 terminated with the following error:
Traceback (most recent call last):
File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
fn(i, *args)
File "/__w/ColossalAI/ColossalAI/tests/test_pipeline/test_p2p_communication.py", line 73, in run_dist
check_p2p_communication()
File "/__w/ColossalAI/ColossalAI/tests/test_pipeline/test_p2p_communication.py", line 21, in check_p2p_communication
tensor = torch.ones(1, device=get_accelerator().get_current_device())
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_pipeline/test_stage_manager.py::test_pipeline_stage_manager - torch.multiprocessing.spawn.ProcessRaisedException:
2025-04-11T04:23:19.2797322Z
-- Process 0 terminated with the following error:
Traceback (most recent call last):
File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
fn(i, *args)
File "/__w/ColossalAI/ColossalAI/tests/test_pipeline/test_stage_manager.py", line 68, in run_dist
check_stage_manager()
File "/__w/ColossalAI/ColossalAI/tests/test_pipeline/test_stage_manager.py", line 56, in check_stage_manager
dist.barrier(group=group)
File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/distributed/c10d_logger.py", line 72, in wrapper
return func(*args, **kwargs)
File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py", line 3441, in barrier
work = group.barrier(opts=opts)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_pipeline/test_schedule/test_interleaved.py::test_pp[2-12-4] - torch.multiprocessing.spawn.ProcessRaisedException:
2025-04-11T04:23:19.2800491Z
-- Process 0 terminated with the following error:
Traceback (most recent call last):
File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
fn(i, *args)
File "/__w/ColossalAI/ColossalAI/tests/test_pipeline/test_schedule/test_interleaved.py", line 66, in run_pp
torch_model = MlpModel().cuda()
File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in cuda
return self._apply(lambda t: t.cuda(device))
File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
module._apply(fn)
File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
module._apply(fn)
File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
param_applied = fn(param)
File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in <lambda>
return self._apply(lambda t: t.cuda(device))
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_pipeline/test_schedule/test_interleaved.py::test_pp[2-12-12] - torch.multiprocessing.spawn.ProcessRaisedException:
2025-04-11T04:23:19.2804252Z
-- Process 0 terminated with the following error:
Traceback (most recent call last):
File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
fn(i, *args)
File "/__w/ColossalAI/ColossalAI/tests/test_pipeline/test_schedule/test_interleaved.py", line 66, in run_pp
torch_model = MlpModel().cuda()
File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in cuda
return self._apply(lambda t: t.cuda(device))
File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
module._apply(fn)
File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
module._apply(fn)
File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
param_applied = fn(param)
File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in <lambda>
return self._apply(lambda t: t.cuda(device))
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_pipeline/test_schedule/test_interleaved.py::test_pp[4-12-4] - torch.multiprocessing.spawn.ProcessRaisedException:
2025-04-11T04:23:19.2808106Z
-- Process 0 terminated with the following error:
Traceback (most recent call last):
File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
fn(i, *args)
File "/__w/ColossalAI/ColossalAI/tests/test_pipeline/test_schedule/test_interleaved.py", line 66, in run_pp
torch_model = MlpModel().cuda()
File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in cuda
return self._apply(lambda t: t.cuda(device))
File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
module._apply(fn)
File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
module._apply(fn)
File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
param_applied = fn(param)
File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in <lambda>
return self._apply(lambda t: t.cuda(device))
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_pipeline/test_schedule/test_interleaved.py::test_pp[4-12-12] - torch.multiprocessing.spawn.ProcessRaisedException:
2025-04-11T04:23:19.2811834Z
-- Process 0 terminated with the following error:
Traceback (most recent call last):
File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
fn(i, *args)
File "/__w/ColossalAI/ColossalAI/tests/test_pipeline/test_schedule/test_interleaved.py", line 66, in run_pp
torch_model = MlpModel().cuda()
File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in cuda
return self._apply(lambda t: t.cuda(device))
File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
module._apply(fn)
File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
module._apply(fn)
File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
param_applied = fn(param)
File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in <lambda>
return self._apply(lambda t: t.cuda(device))
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_pipeline/test_schedule/test_oneF_oneB.py::test_pp[2-12-4] - torch.multiprocessing.spawn.ProcessRaisedException:
2025-04-11T04:23:19.2815669Z
-- Process 0 terminated with the following error:
Traceback (most recent call last):
File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
fn(i, *args)
File "/__w/ColossalAI/ColossalAI/tests/test_pipeline/test_schedule/test_oneF_oneB.py", line 153, in run_dist
examine_pp(num_microbatch, batch_size)
File "/__w/ColossalAI/ColossalAI/tests/test_pipeline/test_schedule/test_oneF_oneB.py", line 58, in examine_pp
torch_model = MlpModel().cuda()
File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in cuda
return self._apply(lambda t: t.cuda(device))
File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
module._apply(fn)
File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
module._apply(fn)
File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
param_applied = fn(param)
File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in <lambda>
return self._apply(lambda t: t.cuda(device))
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_pipeline/test_schedule/test_oneF_oneB.py::test_pp[2-12-6] - torch.multiprocessing.spawn.ProcessRaisedException:
2025-04-11T04:23:19.2819803Z
-- Process 0 terminated with the following error:
Traceback (most recent call last):
File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
fn(i, *args)
File "/__w/ColossalAI/ColossalAI/tests/test_pipeline/test_schedule/test_oneF_oneB.py", line 153, in run_dist
examine_pp(num_microbatch, batch_size)
File "/__w/ColossalAI/ColossalAI/tests/test_pipeline/test_schedule/test_oneF_oneB.py", line 58, in examine_pp
torch_model = MlpModel().cuda()
File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in cuda
return self._apply(lambda t: t.cuda(device))
File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
module._apply(fn)
File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
module._apply(fn)
File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
param_applied = fn(param)
File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in <lambda>
return self._apply(lambda t: t.cuda(device))
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_pipeline/test_schedule/test_oneF_oneB.py::test_pp[4-12-4] - torch.multiprocessing.spawn.ProcessRaisedException:
2025-04-11T04:23:19.2823931Z
-- Process 0 terminated with the following error:
Traceback (most recent call last):
File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
fn(i, *args)
File "/__w/ColossalAI/ColossalAI/tests/test_pipeline/test_schedule/test_oneF_oneB.py", line 153, in run_dist
examine_pp(num_microbatch, batch_size)
File "/__w/ColossalAI/ColossalAI/tests/test_pipeline/test_schedule/test_oneF_oneB.py", line 58, in examine_pp
torch_model = MlpModel().cuda()
File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in cuda
return self._apply(lambda t: t.cuda(device))
File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
module._apply(fn)
File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
module._apply(fn)
File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
param_applied = fn(param)
File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in <lambda>
return self._apply(lambda t: t.cuda(device))
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_pipeline/test_schedule/test_oneF_oneB.py::test_pp[4-12-6] - torch.multiprocessing.spawn.ProcessRaisedException:
2025-04-11T04:23:19.2828117Z
-- Process 0 terminated with the following error:
Traceback (most recent call last):
File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
fn(i, *args)
File "/__w/ColossalAI/ColossalAI/tests/test_pipeline/test_schedule/test_oneF_oneB.py", line 153, in run_dist
examine_pp(num_microbatch, batch_size)
File "/__w/ColossalAI/ColossalAI/tests/test_pipeline/test_schedule/test_oneF_oneB.py", line 58, in examine_pp
torch_model = MlpModel().cuda()
File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in cuda
return self._apply(lambda t: t.cuda(device))
File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
module._apply(fn)
File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
module._apply(fn)
File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
param_applied = fn(param)
File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in <lambda>
return self._apply(lambda t: t.cuda(device))
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_pipeline/test_schedule/test_zerobubble_pp.py::test_pp - torch.multiprocessing.spawn.ProcessRaisedException:
2025-04-11T04:23:19.2832302Z
-- Process 0 terminated with the following error:
Traceback (most recent call last):
File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
fn(i, *args)
File "/__w/ColossalAI/ColossalAI/tests/test_pipeline/test_schedule/test_zerobubble_pp.py", line 1070, in run_dist
run_with_booster_moehybridplugin()
File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
partial_func(**kwargs)
File "/__w/ColossalAI/ColossalAI/tests/test_pipeline/test_schedule/test_zerobubble_pp.py", line 788, in run_with_booster_moehybridplugin
torch_model = MixtralModel(config).to(dtype).cuda()
File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2548, in cuda
return super().cuda(*args, **kwargs)
File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in cuda
return self._apply(lambda t: t.cuda(device))
File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
module._apply(fn)
File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
param_applied = fn(param)
File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in <lambda>
return self._apply(lambda t: t.cuda(device))
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_shardformer/test_flash_attention.py::test_flash_attn_func - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_shardformer/test_shard_utils.py::test_release_layer - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_shardformer/test_with_torch_ddp.py::test_gpt2 - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_shardformer/test_hybrid_parallel_grad_clip_norm/test_amp_optimizer.py::test_grad_clip_norm - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_shardformer/test_hybrid_parallel_grad_clip_norm/test_naive_optimizer.py::test_grad_clip_norm - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_shardformer/test_hybrid_parallel_grad_clip_norm/test_zero_optimizer.py::test_grad_clip_norm - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_shardformer/test_layer/test_dist_crossentropy.py::test_dist_crossentropy - torch.multiprocessing.spawn.ProcessRaisedException:
2025-04-11T04:23:19.2842319Z
-- Process 0 terminated with the following error:
Traceback (most recent call last):
File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
fn(i, *args)
File "/__w/ColossalAI/ColossalAI/tests/test_shardformer/test_layer/test_dist_crossentropy.py", line 20, in check_dist_crossentropy
pred = torch.randn(2, 4, 8, requires_grad=True).cuda()
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_shardformer/test_layer/test_dropout.py::test_dropout - torch.multiprocessing.spawn.ProcessRaisedException:
2025-04-11T04:23:19.2844437Z
-- Process 0 terminated with the following error:
Traceback (most recent call last):
File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
fn(i, *args)
File "/__w/ColossalAI/ColossalAI/tests/test_shardformer/test_layer/test_dropout.py", line 60, in run_dist
check_dropout_parallel_input()
File "/__w/ColossalAI/ColossalAI/tests/test_shardformer/test_layer/test_dropout.py", line 12, in check_dropout_parallel_input
dropout_1d = DropoutForParallelInput.from_native_module(dropout, process_group=None)
File "/__w/ColossalAI/ColossalAI/colossalai/shardformer/layer/dropout.py", line 42, in from_native_module
return DropoutForParallelInput(p=p, inplace=inplace, process_group=process_group)
File "/__w/ColossalAI/ColossalAI/colossalai/shardformer/layer/dropout.py", line 31, in __init__
self.randomizer = create_randomizer_with_offset(seed, process_group=process_group)
File "/__w/ColossalAI/ColossalAI/colossalai/shardformer/layer/utils.py", line 318, in create_randomizer_with_offset
is_synchronized = Randomizer.is_randomizer_index_synchronized(process_group)
File "/__w/ColossalAI/ColossalAI/colossalai/shardformer/layer/utils.py", line 258, in is_randomizer_index_synchronized
index_tensor = torch.tensor(index, dtype=torch.int32, device=get_accelerator().get_current_device())
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_shardformer/test_layer/test_embedding.py::test_embedding_1d - torch.multiprocessing.spawn.ProcessRaisedException:
2025-04-11T04:23:19.2848883Z
-- Process 0 terminated with the following error:
Traceback (most recent call last):
File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
fn(i, *args)
File "/__w/ColossalAI/ColossalAI/tests/test_shardformer/test_layer/test_embedding.py", line 47, in run_dist
check_embedding_1d()
File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
partial_func(**kwargs)
File "/__w/ColossalAI/ColossalAI/tests/test_shardformer/test_layer/test_embedding.py", line 18, in check_embedding_1d
embedding = nn.Embedding(32, 128).cuda()
File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in cuda
return self._apply(lambda t: t.cuda(device))
File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
param_applied = fn(param)
File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in <lambda>
return self._apply(lambda t: t.cuda(device))
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_shardformer/test_layer/test_gpt2_qkv_fused_linear_1d.py::test_linearconv - torch.multiprocessing.spawn.ProcessRaisedException:
2025-04-11T04:23:19.2852801Z
-- Process 0 terminated with the following error:
Traceback (most recent call last):
File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
fn(i, *args)
File "/__w/ColossalAI/ColossalAI/tests/test_shardformer/test_layer/test_gpt2_qkv_fused_linear_1d.py", line 204, in run_dist
check_gpt2_qkv_fused_linear_1d()
File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
partial_func(**kwargs)
File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
partial_func(**kwargs)
File "/__w/ColossalAI/ColossalAI/tests/test_shardformer/test_layer/test_gpt2_qkv_fused_linear_1d.py", line 194, in check_gpt2_qkv_fused_linear_1d
check_linear_conv_1d_col(lazy_init, seq_parallel_mode)
File "/__w/ColossalAI/ColossalAI/tests/test_shardformer/test_layer/test_gpt2_qkv_fused_linear_1d.py", line 47, in check_linear_conv_1d_col
linear = Conv1D(192, 48).cuda()
File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in cuda
return self._apply(lambda t: t.cuda(device))
File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
param_applied = fn(param)
File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in <lambda>
return self._apply(lambda t: t.cuda(device))
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_shardformer/test_layer/test_layernorm.py::test_layernorm - torch.multiprocessing.spawn.ProcessRaisedException:
2025-04-11T04:23:19.2857616Z
-- Process 0 terminated with the following error:
Traceback (most recent call last):
File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
fn(i, *args)
File "/__w/ColossalAI/ColossalAI/tests/test_shardformer/test_layer/test_layernorm.py", line 45, in run_dist
check_layernorm()
File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
partial_func(**kwargs)
File "/__w/ColossalAI/ColossalAI/tests/test_shardformer/test_layer/test_layernorm.py", line 17, in check_layernorm
norm = nn.LayerNorm(128, 0.00001).cuda()
File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in cuda
return self._apply(lambda t: t.cuda(device))
File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
param_applied = fn(param)
File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in <lambda>
return self._apply(lambda t: t.cuda(device))
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_shardformer/test_layer/test_linear_1d.py::test_linear - torch.multiprocessing.spawn.ProcessRaisedException:
2025-04-11T04:23:19.2861344Z
-- Process 0 terminated with the following error:
Traceback (most recent call last):
File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
fn(i, *args)
File "/__w/ColossalAI/ColossalAI/tests/test_shardformer/test_layer/test_linear_1d.py", line 279, in check_dist_linear
run_dist_linear_test()
File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
partial_func(**kwargs)
File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
partial_func(**kwargs)
File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
partial_func(**kwargs)
File "/__w/ColossalAI/ColossalAI/tests/test_shardformer/test_layer/test_linear_1d.py", line 270, in run_dist_linear_test
check_linear_1d_col(lazy_init, seq_parallel_mode, overlap)
File "/__w/ColossalAI/ColossalAI/tests/test_shardformer/test_layer/test_linear_1d.py", line 21, in check_linear_1d_col
linear = nn.Linear(32, 128).cuda()
File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in cuda
return self._apply(lambda t: t.cuda(device))
File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
param_applied = fn(param)
File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in <lambda>
return self._apply(lambda t: t.cuda(device))
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_shardformer/test_layer/test_qkv_fused_linear_1d.py::test_linearconv - torch.multiprocessing.spawn.ProcessRaisedException:
2025-04-11T04:23:19.2866491Z
-- Process 0 terminated with the following error:
Traceback (most recent call last):
File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
fn(i, *args)
File "/__w/ColossalAI/ColossalAI/tests/test_shardformer/test_layer/test_qkv_fused_linear_1d.py", line 158, in run_dist
check_linear_1d_col()
File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
partial_func(**kwargs)
File "/__w/ColossalAI/ColossalAI/tests/test_shardformer/test_layer/test_qkv_fused_linear_1d.py", line 21, in check_linear_1d_col
linear = nn.Linear(8, 80).cuda()
File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in cuda
return self._apply(lambda t: t.cuda(device))
File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
param_applied = fn(param)
File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in <lambda>
return self._apply(lambda t: t.cuda(device))
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_shardformer/test_layer/test_ring_attn.py::test_ring_attn - torch.multiprocessing.spawn.ProcessRaisedException:
2025-04-11T04:23:19.2870377Z
-- Process 0 terminated with the following error:
Traceback (most recent call last):
File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
fn(i, *args)
File "/__w/ColossalAI/ColossalAI/tests/test_shardformer/test_layer/test_ring_attn.py", line 169, in launch_single_ring
check_packed_seq()
File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
partial_func(**kwargs)
File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
partial_func(**kwargs)
File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
partial_func(**kwargs)
[Previous line repeated 2 more times]
File "/__w/ColossalAI/ColossalAI/tests/test_shardformer/test_layer/test_ring_attn.py", line 94, in check_packed_seq
padding_mask = torch.ones((bs, seqlen), dtype=torch.int, device=device)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_shardformer/test_layer/test_ring_attn.py::test_double_ring - torch.multiprocessing.spawn.ProcessRaisedException:
2025-04-11T04:23:19.2874022Z
-- Process 0 terminated with the following error:
Traceback (most recent call last):
File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
fn(i, *args)
File "/__w/ColossalAI/ColossalAI/tests/test_shardformer/test_layer/test_ring_attn.py", line 175, in launch_double_ring
check_ring_attn(inner_ring_size=2)
File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
partial_func(**kwargs)
File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
partial_func(**kwargs)
File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
partial_func(**kwargs)
[Previous line repeated 2 more times]
File "/__w/ColossalAI/ColossalAI/tests/test_shardformer/test_layer/test_ring_attn.py", line 36, in check_ring_attn
qkv = torch.randn(bs, seq_len, 3, nheads, d, device=device, dtype=dtype, requires_grad=True)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_shardformer/test_layer/test_sequence_parallel.py::test_all_to_all_attention - torch.multiprocessing.spawn.ProcessRaisedException:
2025-04-11T04:23:19.2877631Z
-- Process 0 terminated with the following error:
Traceback (most recent call last):
File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
fn(i, *args)
File "/__w/ColossalAI/ColossalAI/tests/test_shardformer/test_layer/test_sequence_parallel.py", line 169, in check_all2all_attn
run_seq_parallel_attn()
File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
partial_func(**kwargs)
File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
partial_func(**kwargs)
File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
partial_func(**kwargs)
[Previous line repeated 1 more time]
File "/__w/ColossalAI/ColossalAI/tests/test_shardformer/test_layer/test_sequence_parallel.py", line 164, in run_seq_parallel_attn
seq_parallel_attn(seq_len, hidden_dim, head_num, batch_size)
File "/__w/ColossalAI/ColossalAI/tests/test_shardformer/test_layer/test_sequence_parallel.py", line 101, in seq_parallel_attn
x = torch.randn(batch_size, seq_len, hidden_dim).cuda()
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_shardformer/test_layer/test_vocab_parallel_embedding_1d.py::test_vocab_embedding - torch.multiprocessing.spawn.ProcessRaisedException:
2025-04-11T04:23:19.2881812Z
-- Process 0 terminated with the following error:
Traceback (most recent call last):
File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
fn(i, *args)
File "/__w/ColossalAI/ColossalAI/tests/test_shardformer/test_layer/test_vocab_parallel_embedding_1d.py", line 49, in run_dist
check_vocab_embedding_1d()
File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
partial_func(**kwargs)
File "/__w/ColossalAI/ColossalAI/tests/test_shardformer/test_layer/test_vocab_parallel_embedding_1d.py", line 18, in check_vocab_embedding_1d
embedding = nn.Embedding(128, 32).to("cuda")
File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1152, in to
return self._apply(convert)
File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
param_applied = fn(param)
File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1150, in convert
return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_shardformer/test_model/test_shard_bert.py::test_bert - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_shardformer/test_model/test_shard_blip2.py::test_blip2 - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_shardformer/test_model/test_shard_bloom.py::test_bloom - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_shardformer/test_model/test_shard_chatglm2.py::test_chatglm - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_shardformer/test_model/test_shard_command.py::test_command - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_shardformer/test_model/test_shard_deepseek.py::test_deepseek[4] - torch.multiprocessing.spawn.ProcessRaisedException:
2025-04-11T04:23:19.2890091Z
-- Process 0 terminated with the following error:
Traceback (most recent call last):
File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
fn(i, *args)
File "/__w/ColossalAI/ColossalAI/tests/test_shardformer/test_model/test_shard_deepseek.py", line 216, in check_deepseek
run_deepseek_test()
File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
partial_func(**kwargs)
File "/__w/ColossalAI/ColossalAI/tests/test_shardformer/test_model/test_shard_deepseek.py", line 187, in run_deepseek_test
run_deepseek_commom(config)
File "/__w/ColossalAI/ColossalAI/tests/test_shardformer/test_model/test_shard_deepseek.py", line 77, in run_deepseek_commom
torch_model = AutoModel.from_config(config, trust_remote_code=True).cuda().to(dtype)
File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2548, in cuda
return super().cuda(*args, **kwargs)
File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in cuda
return self._apply(lambda t: t.cuda(device))
File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
module._apply(fn)
File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
param_applied = fn(param)
File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in <lambda>
return self._apply(lambda t: t.cuda(device))
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_shardformer/test_model/test_shard_deepseek_v3.py::test_deepseek_v3[4] - torch.multiprocessing.spawn.ProcessRaisedException:
2025-04-11T04:23:19.2895273Z
-- Process 0 terminated with the following error:
Traceback (most recent call last):
File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
fn(i, *args)
File "/__w/ColossalAI/ColossalAI/tests/test_shardformer/test_model/test_shard_deepseek_v3.py", line 93, in check_deepseek_v3
run_deepseek_v3_test()
File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
partial_func(**kwargs)
File "/__w/ColossalAI/ColossalAI/tests/test_shardformer/test_model/test_shard_deepseek_v3.py", line 82, in run_deepseek_v3_test
check_forward_backward(
File "/__w/ColossalAI/ColossalAI/tests/test_shardformer/test_model/test_shard_deepseek_v3.py", line 29, in check_forward_backward
org_model, org_optimizer, sharded_model, sharded_optimizer, criterion, booster = build_model_from_hybrid_plugin(
File "/__w/ColossalAI/ColossalAI/tests/test_shardformer/test_model/_utils.py", line 138, in build_model_from_hybrid_plugin
org_model = org_model.cuda()
File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2548, in cuda
return super().cuda(*args, **kwargs)
File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in cuda
return self._apply(lambda t: t.cuda(device))
File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
module._apply(fn)
File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
module._apply(fn)
File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
param_applied = fn(param)
File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in <lambda>
return self._apply(lambda t: t.cuda(device))
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_shardformer/test_model/test_shard_falcon.py::test_falcon - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_shardformer/test_model/test_shard_gpt2.py::test_gpt2 - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_shardformer/test_model/test_shard_llama.py::test_llama - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_shardformer/test_model/test_shard_mistral.py::test_mistral - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_shardformer/test_model/test_shard_mixtral.py::test_mixtral[4] - torch.multiprocessing.spawn.ProcessRaisedException:
2025-04-11T04:23:19.2904669Z
-- Process 0 terminated with the following error:
Traceback (most recent call last):
File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
fn(i, *args)
File "/__w/ColossalAI/ColossalAI/tests/test_shardformer/test_model/test_shard_mixtral.py", line 210, in check_mixtral
run_mixtral_test()
File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
partial_func(**kwargs)
File "/__w/ColossalAI/ColossalAI/tests/test_shardformer/test_model/test_shard_mixtral.py", line 180, in run_mixtral_test
run_mixtral_commom(config)
File "/__w/ColossalAI/ColossalAI/tests/test_shardformer/test_model/test_shard_mixtral.py", line 70, in run_mixtral_commom
torch_model = MixtralModel(config).to(dtype).cuda()
File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2548, in cuda
return super().cuda(*args, **kwargs)
File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in cuda
return self._apply(lambda t: t.cuda(device))
File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
module._apply(fn)
File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
param_applied = fn(param)
File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in <lambda>
return self._apply(lambda t: t.cuda(device))
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_shardformer/test_model/test_shard_opt.py::test_OPTModel - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_shardformer/test_model/test_shard_qwen2.py::test_qwen2 - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_shardformer/test_model/test_shard_sam.py::test_sam - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_shardformer/test_model/test_shard_t5.py::test_t5 - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_shardformer/test_model/test_shard_vit.py::test_vit - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_shardformer/test_model/test_shard_whisper.py::test_whisper - RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_tensor/test_comm_spec_apply.py::test_comm_spec - torch.multiprocessing.spawn.ProcessRaisedException:
2025-04-11T04:23:19.2914663Z
-- Process 0 terminated with the following error:
Traceback (most recent call last):
File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
fn(i, *args)
File "/__w/ColossalAI/ColossalAI/tests/test_tensor/test_comm_spec_apply.py", line 191, in check_comm
check_all_gather(device_mesh, rank)
File "/__w/ColossalAI/ColossalAI/tests/test_tensor/test_comm_spec_apply.py", line 16, in check_all_gather
sharded_tensor_to_comm = torch.ones(2, 2).cuda()
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_tensor/test_padded_tensor.py::test_padded_tensor - torch.multiprocessing.spawn.ProcessRaisedException:
2025-04-11T04:23:19.2917036Z
-- Process 0 terminated with the following error:
Traceback (most recent call last):
File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
fn(i, *args)
File "/__w/ColossalAI/ColossalAI/tests/test_tensor/test_padded_tensor.py", line 14, in check_padded_tensor
original_tensor = torch.rand(32, 64).to("cuda")
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_tensor/test_shape_consistency_apply.py::test_apply - torch.multiprocessing.spawn.ProcessRaisedException:
2025-04-11T04:23:19.2918998Z
-- Process 0 terminated with the following error:
Traceback (most recent call last):
File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
fn(i, *args)
File "/__w/ColossalAI/ColossalAI/tests/test_tensor/test_shape_consistency_apply.py", line 41, in check_apply
tensor_to_comm = torch.cat((sharded_tensor_0, sharded_tensor_1), 1).cuda()
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_tensor/test_dtensor/test_comm_spec.py::test_comm_spec - torch.multiprocessing.spawn.ProcessRaisedException:
2025-04-11T04:23:19.2921031Z
-- Process 0 terminated with the following error:
Traceback (most recent call last):
File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
fn(i, *args)
File "/__w/ColossalAI/ColossalAI/tests/test_tensor/test_dtensor/test_comm_spec.py", line 140, in check_comm
check_all_gather(process_group_dict, rank)
File "/__w/ColossalAI/ColossalAI/tests/test_tensor/test_dtensor/test_comm_spec.py", line 15, in check_all_gather
sharded_tensor_to_comm = torch.ones(2, 2).cuda()
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_tensor/test_dtensor/test_dtensor.py::test_dtensor - torch.multiprocessing.spawn.ProcessRaisedException:
2025-04-11T04:23:19.2923407Z
-- Process 0 terminated with the following error:
Traceback (most recent call last):
File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
fn(i, *args)
File "/__w/ColossalAI/ColossalAI/tests/test_tensor/test_dtensor/test_dtensor.py", line 25, in check_dtensor
test_model = TestModel(8, 8).to("cuda")
File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1152, in to
return self._apply(convert)
File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
module._apply(fn)
File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
param_applied = fn(param)
File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1150, in convert
return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_tensor/test_dtensor/test_layout_converter.py::test_layout_converter - torch.multiprocessing.spawn.ProcessRaisedException:
2025-04-11T04:23:19.2926905Z
-- Process 0 terminated with the following error:
Traceback (most recent call last):
File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
fn(i, *args)
File "/__w/ColossalAI/ColossalAI/tests/test_tensor/test_dtensor/test_layout_converter.py", line 162, in check_layout_converting_apply
original_tensor = torch.rand(global_shape).cuda()
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_zero/test_gemini/test_chunk_mgrv2.py::test_chunk_manager[2] - torch.multiprocessing.spawn.ProcessRaisedException:
2025-04-11T04:23:19.2928969Z
-- Process 0 terminated with the following error:
Traceback (most recent call last):
File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
fn(i, *args)
File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_gemini/test_chunk_mgrv2.py", line 53, in run_dist
exam_chunk_memory()
File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
partial_func(**kwargs)
File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
partial_func(**kwargs)
File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_gemini/test_chunk_mgrv2.py", line 21, in exam_chunk_memory
chunk_manager = ChunkManager(config)
File "/__w/ColossalAI/ColossalAI/colossalai/zero/gemini/chunk/manager.py", line 46, in __init__
self.overflow_counter = torch.tensor([0], dtype=torch.int, device=get_accelerator().get_current_device())
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_zero/test_gemini/test_chunkv2.py::test_chunk_function[1] - torch.multiprocessing.spawn.ProcessRaisedException:
2025-04-11T04:23:19.2932518Z
-- Process 0 terminated with the following error:
Traceback (most recent call last):
File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
fn(i, *args)
File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_gemini/test_chunkv2.py", line 116, in run_dist
exam_chunk_basic()
File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
partial_func(**kwargs)
File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
partial_func(**kwargs)
File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
partial_func(**kwargs)
[Previous line repeated 1 more time]
File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_gemini/test_chunkv2.py", line 41, in exam_chunk_basic
my_chunk = Chunk(
File "/__w/ColossalAI/ColossalAI/colossalai/zero/gemini/chunk/chunk.py", line 113, in __init__
self.chunk_temp = torch.zeros(chunk_size, dtype=dtype, device=device)  # keep all zero
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_zero/test_gemini/test_chunkv2.py::test_chunk_function[2] - torch.multiprocessing.spawn.ProcessRaisedException:
2025-04-11T04:23:19.2936284Z
-- Process 0 terminated with the following error:
Traceback (most recent call last):
File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
fn(i, *args)
File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_gemini/test_chunkv2.py", line 116, in run_dist
exam_chunk_basic()
File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
partial_func(**kwargs)
File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
partial_func(**kwargs)
File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
partial_func(**kwargs)
[Previous line repeated 1 more time]
File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_gemini/test_chunkv2.py", line 41, in exam_chunk_basic
my_chunk = Chunk(
File "/__w/ColossalAI/ColossalAI/colossalai/zero/gemini/chunk/chunk.py", line 113, in __init__
self.chunk_temp = torch.zeros(chunk_size, dtype=dtype, device=device)  # keep all zero
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_zero/test_gemini/test_chunkv2.py::test_chunk_function[4] - torch.multiprocessing.spawn.ProcessRaisedException:
2025-04-11T04:23:19.2940163Z
-- Process 0 terminated with the following error:
Traceback (most recent call last):
File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
fn(i, *args)
File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_gemini/test_chunkv2.py", line 116, in run_dist
exam_chunk_basic()
File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
partial_func(**kwargs)
File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
partial_func(**kwargs)
File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
partial_func(**kwargs)
[Previous line repeated 1 more time]
File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_gemini/test_chunkv2.py", line 41, in exam_chunk_basic
my_chunk = Chunk(
File "/__w/ColossalAI/ColossalAI/colossalai/zero/gemini/chunk/chunk.py", line 113, in __init__
self.chunk_temp = torch.zeros(chunk_size, dtype=dtype, device=device)  # keep all zero
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_zero/test_gemini/test_grad_accum.py::test_grad_accumulation - torch.multiprocessing.spawn.ProcessRaisedException:
2025-04-11T04:23:19.2943940Z
-- Process 0 terminated with the following error:
Traceback (most recent call last):
File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
fn(i, *args)
File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_gemini/test_grad_accum.py", line 152, in run_dist
exam_gemini_grad_acc()
File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
partial_func(**kwargs)
File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
partial_func(**kwargs)
File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
partial_func(**kwargs)
[Previous line repeated 4 more times]
File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_gemini/test_grad_accum.py", line 71, in exam_gemini_grad_acc
torch_model = model_builder().cuda()
File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2548, in cuda
return super().cuda(*args, **kwargs)
File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in cuda
return self._apply(lambda t: t.cuda(device))
File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
module._apply(fn)
File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
module._apply(fn)
File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
param_applied = fn(param)
File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in <lambda>
return self._apply(lambda t: t.cuda(device))
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_zero/test_gemini/test_grad_clip.py::test_grad_clip[1] - torch.multiprocessing.spawn.ProcessRaisedException:
2025-04-11T04:23:19.2949662Z
-- Process 0 terminated with the following error:
Traceback (most recent call last):
File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
fn(i, *args)
File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_gemini/test_grad_clip.py", line 127, in run_dist
exam_grad_clipping()
File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
partial_func(**kwargs)
File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
partial_func(**kwargs)
File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
partial_func(**kwargs)
[Previous line repeated 2 more times]
File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_gemini/test_grad_clip.py", line 65, in exam_grad_clipping
torch_model = model_builder().cuda()
File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2548, in cuda
return super().cuda(*args, **kwargs)
File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in cuda
return self._apply(lambda t: t.cuda(device))
File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
module._apply(fn)
File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
module._apply(fn)
File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
param_applied = fn(param)
File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in <lambda>
return self._apply(lambda t: t.cuda(device))
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_zero/test_gemini/test_grad_clip.py::test_grad_clip[2] - torch.multiprocessing.spawn.ProcessRaisedException:
2025-04-11T04:23:19.2955394Z
-- Process 0 terminated with the following error:
Traceback (most recent call last):
File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
fn(i, *args)
File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_gemini/test_grad_clip.py", line 127, in run_dist
exam_grad_clipping()
File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
partial_func(**kwargs)
File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
partial_func(**kwargs)
File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
partial_func(**kwargs)
[Previous line repeated 2 more times]
File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_gemini/test_grad_clip.py", line 65, in exam_grad_clipping
torch_model = model_builder().cuda()
File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2548, in cuda
return super().cuda(*args, **kwargs)
File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in cuda
return self._apply(lambda t: t.cuda(device))
File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
module._apply(fn)
File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
module._apply(fn)
File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
param_applied = fn(param)
File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in <lambda>
return self._apply(lambda t: t.cuda(device))
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_zero/test_gemini/test_inference.py::test_inference[1] - torch.multiprocessing.spawn.ProcessRaisedException:
2025-04-11T04:23:19.2962424Z
-- Process 0 terminated with the following error:
Traceback (most recent call last):
File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
fn(i, *args)
File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_gemini/test_inference.py", line 111, in run_dist
exam_inference()
File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
partial_func(**kwargs)
File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
partial_func(**kwargs)
File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
partial_func(**kwargs)
File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_gemini/test_inference.py", line 63, in exam_inference
torch_model = model_builder().cuda()
File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2548, in cuda
return super().cuda(*args, **kwargs)
File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in cuda
return self._apply(lambda t: t.cuda(device))
File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
module._apply(fn)
File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
module._apply(fn)
File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
param_applied = fn(param)
File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in <lambda>
return self._apply(lambda t: t.cuda(device))
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_zero/test_gemini/test_inference.py::test_inference[4] - torch.multiprocessing.spawn.ProcessRaisedException:
2025-04-11T04:23:19.2969575Z
-- Process 0 terminated with the following error:
Traceback (most recent call last):
File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
fn(i, *args)
File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_gemini/test_inference.py", line 111, in run_dist
exam_inference()
File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
partial_func(**kwargs)
File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
partial_func(**kwargs)
File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
partial_func(**kwargs)
File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_gemini/test_inference.py", line 63, in exam_inference
torch_model = model_builder().cuda()
File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2548, in cuda
return super().cuda(*args, **kwargs)
File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in cuda
return self._apply(lambda t: t.cuda(device))
File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
module._apply(fn)
File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
module._apply(fn)
File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
param_applied = fn(param)
File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in <lambda>
return self._apply(lambda t: t.cuda(device))
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_zero/test_gemini/test_optim.py::test_optim[4] - torch.multiprocessing.spawn.ProcessRaisedException:
2025-04-11T04:23:19.2976462Z
-- Process 0 terminated with the following error:
Traceback (most recent call last):
File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
fn(i, *args)
File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_gemini/test_optim.py", line 185, in run_dist
exam_model_step()
File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
partial_func(**kwargs)
File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
partial_func(**kwargs)
File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
partial_func(**kwargs)
[Previous line repeated 2 more times]
File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_gemini/test_optim.py", line 75, in exam_model_step
torch_model = model_builder().cuda()
File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2548, in cuda
return super().cuda(*args, **kwargs)
File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in cuda
return self._apply(lambda t: t.cuda(device))
File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
module._apply(fn)
File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
module._apply(fn)
File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
param_applied = fn(param)
File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in <lambda>
return self._apply(lambda t: t.cuda(device))
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_zero/test_gemini/test_search.py::test_search[1] - torch.multiprocessing.spawn.ProcessRaisedException:
2025-04-11T04:23:19.2991655Z
-- Process 0 terminated with the following error:
Traceback (most recent call last):
File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
fn(i, *args)
File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_gemini/test_search.py", line 61, in run_dist
exam_chunk_manager()
File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_gemini/test_search.py", line 44, in exam_chunk_manager
chunk_manager = init_chunk_manager(
File "/__w/ColossalAI/ColossalAI/colossalai/zero/gemini/chunk/utils.py", line 33, in init_chunk_manager
dist.barrier()
File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/distributed/c10d_logger.py", line 72, in wrapper
return func(*args, **kwargs)
File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py", line 3439, in barrier
work = default_pg.barrier(opts=opts)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_zero/test_gemini/test_search.py::test_search[4] - torch.multiprocessing.spawn.ProcessRaisedException:
2025-04-11T04:23:19.2995248Z
-- Process 0 terminated with the following error:
Traceback (most recent call last):
File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
fn(i, *args)
File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_gemini/test_search.py", line 61, in run_dist
exam_chunk_manager()
File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_gemini/test_search.py", line 44, in exam_chunk_manager
chunk_manager = init_chunk_manager(
File "/__w/ColossalAI/ColossalAI/colossalai/zero/gemini/chunk/utils.py", line 33, in init_chunk_manager
dist.barrier()
File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/distributed/c10d_logger.py", line 72, in wrapper
return func(*args, **kwargs)
File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py", line 3439, in barrier
work = default_pg.barrier(opts=opts)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_zero/test_gemini/test_zeroddp_state_dict.py::test_zero_ddp[4] - torch.multiprocessing.spawn.ProcessRaisedException:
2025-04-11T04:23:19.2998778Z
-- Process 0 terminated with the following error:
Traceback (most recent call last):
File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
fn(i, *args)
File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_gemini/test_zeroddp_state_dict.py", line 78, in run_dist
exam_state_dict()
File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
partial_func(**kwargs)
File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
partial_func(**kwargs)
File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
partial_func(**kwargs)
[Previous line repeated 1 more time]
File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_gemini/test_zeroddp_state_dict.py", line 45, in exam_state_dict
model = GeminiDDP(model, config_dict, **placement_config, pin_memory=True, master_weights=master_weights)
File "/__w/ColossalAI/ColossalAI/colossalai/zero/gemini/gemini_ddp.py", line 109, in __init__
self.chunk_manager = ChunkManager(
File "/__w/ColossalAI/ColossalAI/colossalai/zero/gemini/chunk/manager.py", line 46, in __init__
self.overflow_counter = torch.tensor([0], dtype=torch.int, device=get_accelerator().get_current_device())
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_zero/test_low_level/test_coll_nd.py::test_comm_nd - torch.multiprocessing.spawn.ProcessRaisedException:
2025-04-11T04:23:19.3003316Z
-- Process 0 terminated with the following error:
Traceback (most recent call last):
File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
fn(i, *args)
File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_low_level/test_coll_nd.py", line 32, in run_dist
check_all_gather_2d()
File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_low_level/test_coll_nd.py", line 16, in check_all_gather_2d
tensor = torch.rand(128, device=get_current_device())
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_zero/test_low_level/test_grad_acc.py::test_grad_accumulation - torch.multiprocessing.spawn.ProcessRaisedException:
2025-04-11T04:23:19.3005664Z
-- Process 0 terminated with the following error:
Traceback (most recent call last):
File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
fn(i, *args)
File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_low_level/test_grad_acc.py", line 139, in run_dist
exam_zero_1_grad_acc(sync=True)
File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_low_level/test_grad_acc.py", line 86, in exam_zero_1_grad_acc
zero_model = zero_model.to(device)
File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1152, in to
return self._apply(convert)
File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
module._apply(fn)
File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
param_applied = fn(param)
File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1150, in convert
return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_zero/test_low_level/test_mem_leak.py::test_zero_1_2 - torch.multiprocessing.spawn.ProcessRaisedException:
2025-04-11T04:23:19.3009523Z
-- Process 0 terminated with the following error:
Traceback (most recent call last):
File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
fn(i, *args)
File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_low_level/test_mem_leak.py", line 51, in run_dist
exam_mem_leak(world_size=world_size)
File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_low_level/test_mem_leak.py", line 36, in exam_mem_leak
zero_model = MlpModel().cuda()
File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in cuda
return self._apply(lambda t: t.cuda(device))
File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
module._apply(fn)
File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
param_applied = fn(param)
File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in <lambda>
return self._apply(lambda t: t.cuda(device))
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_zero/test_low_level/test_zero1_2.py::test_zero_1_2 - torch.multiprocessing.spawn.ProcessRaisedException:
2025-04-11T04:23:19.3013345Z
-- Process 0 terminated with the following error:
Traceback (most recent call last):
File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
fn(i, *args)
File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_low_level/test_zero1_2.py", line 217, in run_dist
exam_zero_1_torch_ddp()
File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
partial_func(**kwargs)
File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
partial_func(**kwargs)
File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
partial_func(**kwargs)
File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_low_level/test_zero1_2.py", line 151, in exam_zero_1_torch_ddp
torch_model = MlpModel().cuda().to(dtype)
File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in cuda
return self._apply(lambda t: t.cuda(device))
File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
module._apply(fn)
File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
param_applied = fn(param)
File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in <lambda>
return self._apply(lambda t: t.cuda(device))
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
FAILED tests/test_zero/test_low_level/test_zero_ckpt.py::test_zero_ckpt - torch.multiprocessing.spawn.ProcessRaisedException:
2025-04-11T04:23:19.3018196Z
-- Process 0 terminated with the following error:
Traceback (most recent call last):
File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
fn(i, *args)
File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_low_level/test_zero_ckpt.py", line 123, in run_dist
exam_zero_1_torch_ddp_ckpt()
File "/__w/ColossalAI/ColossalAI/colossalai/testing/utils.py", line 64, in _execute_function_by_param
partial_func(**kwargs)
File "/__w/ColossalAI/ColossalAI/tests/test_zero/test_low_level/test_zero_ckpt.py", line 62, in exam_zero_1_torch_ddp_ckpt
torch_model = MlpModel().cuda()
File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in cuda
return self._apply(lambda t: t.cuda(device))
File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 802, in _apply
module._apply(fn)
File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 825, in _apply
param_applied = fn(param)
File "/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py", line 911, in <lambda>
return self._apply(lambda t: t.cuda(device))
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
= 573 failed, 74 passed, 195 skipped, 23 deselected, 91 warnings in 663.23s (0:11:03) =
##[error]Process completed with exit code 1.

##[command]/usr/bin/docker exec  920416532ddad0112e30de68b3d3249d8793b2fa61e8416f9de3eb685dff9f0f sh -c "cat /etc/*release | grep ^ID"

2 25 1
Temporarily overriding HOME='/__w/_temp/84be0375-2aba-41cf-b9a0-021db580aa02' before making global git config changes

[command]/usr/bin/git config --global --add safe.directory /__w/ColossalAI/ColossalAI
core\.sshCommand
'core\.sshCommand' 'core.sshCommand'
http\.https\:\/\/github\.com\/\.extraheader


'http\.https\:\/\/github\.com\/\.extraheader' 'http.https://github.com/.extraheader'

##[command]/usr/bin/docker exec  920416532ddad0112e30de68b3d3249d8793b2fa61e8416f9de3eb685dff9f0f sh -c "cat /etc/*release | grep ^ID"
Stop and remove container: 7d6bacf453dd49bc8aea383939f91c8e_imagecloudluchentechcomhpcaitechpytorchcuda2221210_f16ff6
##[command]/usr/bin/docker rm --force 920416532ddad0112e30de68b3d3249d8793b2fa61e8416f9de3eb685dff9f0f
920416532ddad0112e30de68b3d3249d8793b2fa61e8416f9de3eb685dff9f0f
Remove container network: github_network_94998f4534db421f86753cfbbb2319dd
##[command]/usr/bin/docker network rm github_network_94998f4534db421f86753cfbbb2319dd
github_network_94998f4534db421f86753cfbbb2319dd

