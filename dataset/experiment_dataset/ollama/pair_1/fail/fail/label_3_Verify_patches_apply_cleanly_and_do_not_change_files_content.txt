##[group]Run make -f Makefile.sync clean sync
[36;1mmake -f Makefile.sync clean sync[0m
[36;1mgit diff --compact-summary --exit-code[0m
0
##[endgroup]
git clone https://github.com/ggerganov/llama.cpp.git llama/vendor
Cloning into 'llama/vendor'...
git -C llama/vendor fetch
git -C llama/vendor checkout -f 71e90e8813f90097701e62f7fce137d96ddf41e2
Note: switching to '71e90e8813f90097701e62f7fce137d96ddf41e2'.
2025-04-14T02:26:26.2661095Z



2025-04-14T02:26:26.2664561Z


2025-04-14T02:26:26.2666649Z

2025-04-14T02:26:26.2667800Z

2025-04-14T02:26:26.2668912Z

2025-04-14T02:26:26.2669800Z

2025-04-14T02:26:26.2670955Z
HEAD is now at 71e90e88 quantize: Handle user-defined quantization levels for additional tensors (#12511)
rm -f llama/patches/0001-cuda.patched llama/patches/0002-pretokenizer.patched llama/patches/0003-embeddings.patched llama/patches/0004-clip-unicode.patched llama/patches/0005-solar-pro.patched llama/patches/0006-conditional-fattn.patched llama/patches/0007-add-mllama-support.patched llama/patches/0008-add-unpad-operator.patched llama/patches/0009-fix-deepseek-deseret-regex.patched llama/patches/0010-Maintain-ordering-for-rules-for-grammar.patched llama/patches/0011-sort-devices-by-score.patched llama/patches/0012-add-phony-target-ggml-cpu-for-all-cpu-variants.patched llama/patches/0013-remove-amx.patched llama/patches/0014-fix-string-arr-kv-loading.patched llama/patches/0015-ollama-debug-tensor.patched llama/patches/0016-add-model-quantizations.patched llama/patches/0017-add-op_neg.patched llama/patches/0018-fix-compiler-error-in-clip.h.patched llama/patches/0019-Revert-Simplify-and-improve-CUDA-graphs-through-use-.patched llama/patches/0020-remove-ggml-git-build-info.patched
sed -e 's|@FETCH_HEAD@|71e90e8813f90097701e62f7fce137d96ddf41e2|' llama/build-info.cpp.in > llama/build-info.cpp
rsync -arvzc -f "merge llama/llama.cpp/.rsync-filter" llama/vendor/ llama/llama.cpp
sending incremental file list
./
common/
common/json-schema-to-grammar.cpp
examples/
examples/llava/
examples/llava/clip.cpp
examples/llava/clip.h
examples/llava/llava.cpp
include/
include/llama.h
src/
src/llama-arch.cpp
src/llama-arch.h
src/llama-batch.cpp
src/llama-context.cpp
src/llama-context.h
src/llama-cparams.h
src/llama-graph.cpp
src/llama-graph.h
src/llama-hparams.cpp
src/llama-hparams.h
src/llama-kv-cache.cpp
src/llama-model-loader.cpp
src/llama-model.cpp
src/llama-model.h
src/llama-quant.cpp
src/llama-vocab.cpp
src/unicode.cpp
2025-04-14T02:26:26.3296572Z
sent 229,349 bytes  received 590 bytes  459,878.00 bytes/sec
total size is 3,243,049  speedup is 14.10
rsync -arvzc -f "merge ml/backend/ggml/ggml/.rsync-filter" llama/vendor/ggml/ ml/backend/ggml/ggml
sending incremental file list
./
CMakeLists.txt
cmake/
include/
include/ggml.h
include/gguf.h
src/
src/CMakeLists.txt
src/ggml-backend-reg.cpp
src/ggml-backend.cpp
src/ggml.c
src/gguf.cpp
src/ggml-blas/
src/ggml-cpu/
src/ggml-cpu/ggml-cpu.c
src/ggml-cpu/ops.cpp
src/ggml-cpu/ops.h
src/ggml-cpu/amx/
src/ggml-cpu/cmake/
src/ggml-cpu/llamafile/
src/ggml-cuda/
src/ggml-cuda/common.cuh
src/ggml-cuda/cpy.cu
src/ggml-cuda/cpy.cuh
src/ggml-cuda/ggml-cuda.cu
src/ggml-cuda/pad.cu
src/ggml-cuda/pad.cuh
src/ggml-cuda/template-instances/
src/ggml-cuda/vendors/
src/ggml-hip/
src/ggml-metal/
src/ggml-metal/ggml-metal.m
src/ggml-metal/ggml-metal.metal
2025-04-14T02:26:26.3925257Z
sent 264,878 bytes  received 1,251 bytes  532,258.00 bytes/sec
total size is 4,203,907  speedup is 15.80
llama/llama.cpp/common/json-schema-to-grammar.cpp  |   2 +-
llama/llama.cpp/examples/llava/clip.cpp            |  39 --
llama/llama.cpp/examples/llava/clip.h              |   5 +-
llama/llama.cpp/examples/llava/llava.cpp           |   5 +-
llama/llama.cpp/include/llama.h                    |   6 -
llama/llama.cpp/src/llama-arch.cpp                 |  82 ----
llama/llama.cpp/src/llama-arch.h                   |  14 -
llama/llama.cpp/src/llama-batch.cpp                |   3 -
llama/llama.cpp/src/llama-context.cpp              |  33 +-
llama/llama.cpp/src/llama-context.h                |   1 -
llama/llama.cpp/src/llama-cparams.h                |   1 -
llama/llama.cpp/src/llama-graph.cpp                |  25 -
llama/llama.cpp/src/llama-graph.h                  |  12 -
llama/llama.cpp/src/llama-hparams.cpp              |  12 -
llama/llama.cpp/src/llama-hparams.h                |  12 -
llama/llama.cpp/src/llama-kv-cache.cpp             |  12 +-
llama/llama.cpp/src/llama-model-loader.cpp         |   3 -
llama/llama.cpp/src/llama-model.cpp                | 518 +--------------------
llama/llama.cpp/src/llama-model.h                  |  15 -
llama/llama.cpp/src/llama-quant.cpp                |   8 +-
llama/llama.cpp/src/llama-vocab.cpp                |  18 +-
llama/llama.cpp/src/unicode.cpp                    |  21 -
ml/backend/ggml/ggml/CMakeLists.txt                |  25 +
ml/backend/ggml/ggml/include/ggml.h                |  10 -
ml/backend/ggml/ggml/include/gguf.h                |   1 -
ml/backend/ggml/ggml/src/CMakeLists.txt            |   6 +-
ml/backend/ggml/ggml/src/ggml-backend-reg.cpp      |  27 +-
ml/backend/ggml/ggml/src/ggml-backend.cpp          |   1 +
ml/backend/ggml/ggml/src/ggml-cpu/ggml-cpu.c       |  11 -
ml/backend/ggml/ggml/src/ggml-cpu/ops.cpp          |  55 ---
ml/backend/ggml/ggml/src/ggml-cpu/ops.h            |   1 -
ml/backend/ggml/ggml/src/ggml-cuda/common.cuh      |   8 +-
ml/backend/ggml/ggml/src/ggml-cuda/cpy.cu          | 149 +++---
ml/backend/ggml/ggml/src/ggml-cuda/cpy.cuh         |   2 +
ml/backend/ggml/ggml/src/ggml-cuda/ggml-cuda.cu    | 100 +---
ml/backend/ggml/ggml/src/ggml-cuda/pad.cu          |  46 --
ml/backend/ggml/ggml/src/ggml-cuda/pad.cuh         |   1 -
ml/backend/ggml/ggml/src/ggml-metal/ggml-metal.m   |  49 --
.../ggml/ggml/src/ggml-metal/ggml-metal.metal      |  52 ---
ml/backend/ggml/ggml/src/ggml.c                    |  25 +-
ml/backend/ggml/ggml/src/gguf.cpp                  |   7 +-
41 files changed, 208 insertions(+), 1215 deletions(-)
##[error]Process completed with exit code 1.
