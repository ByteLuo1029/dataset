2025-04-13T01:15:22.7827351Z ##[group]Run make -f Makefile.sync clean sync
2025-04-13T01:15:22.7828553Z [36;1mmake -f Makefile.sync clean sync[0m
2025-04-13T01:15:22.7829605Z [36;1mgit diff --compact-summary --exit-code[0m
2025-04-13T01:15:22.7881086Z shell: /usr/bin/bash -e {0}
2025-04-13T01:15:22.7881873Z ##[endgroup]
2025-04-13T01:15:22.8000250Z git clone https://github.com/ggerganov/llama.cpp.git llama/vendor
2025-04-13T01:15:22.8010413Z Cloning into 'llama/vendor'...
2025-04-13T01:15:30.4218835Z git -C llama/vendor fetch
2025-04-13T01:15:30.8367730Z git -C llama/vendor checkout -f 7538246e7ce0606694c38055cc2fc9f60535be6c
2025-04-13T01:15:31.2449771Z Note: switching to '7538246e7ce0606694c38055cc2fc9f60535be6c'.
2025-04-13T01:15:31.2450588Z 
2025-04-13T01:15:31.2450963Z You are in 'detached HEAD' state. You can look around, make experimental
2025-04-13T01:15:31.2451867Z changes and commit them, and you can discard any commits you make in this
2025-04-13T01:15:31.2452739Z state without impacting any branches by switching back to a branch.
2025-04-13T01:15:31.2453601Z 
2025-04-13T01:15:31.2453908Z If you want to create a new branch to retain commits you create, you may
2025-04-13T01:15:31.2454699Z do so (now or later) by using -c with the switch command. Example:
2025-04-13T01:15:31.2455125Z 
2025-04-13T01:15:31.2455311Z   git switch -c <new-branch-name>
2025-04-13T01:15:31.2455696Z 
2025-04-13T01:15:31.2455890Z Or undo this operation with:
2025-04-13T01:15:31.2456174Z 
2025-04-13T01:15:31.2456369Z   git switch -
2025-04-13T01:15:31.2456589Z 
2025-04-13T01:15:31.2456954Z Turn off this advice by setting config variable advice.detachedHead to false
2025-04-13T01:15:31.2457467Z 
2025-04-13T01:15:31.2457739Z HEAD is now at 7538246e cuda : add f32 to bf16 copy op (#12806)
2025-04-13T01:15:31.2470021Z rm -f llama/patches/0001-cuda.patched llama/patches/0002-pretokenizer.patched llama/patches/0003-embeddings.patched llama/patches/0004-clip-unicode.patched llama/patches/0005-solar-pro.patched llama/patches/0006-conditional-fattn.patched llama/patches/0007-add-mllama-support.patched llama/patches/0008-add-unpad-operator.patched llama/patches/0009-fix-deepseek-deseret-regex.patched llama/patches/0010-Maintain-ordering-for-rules-for-grammar.patched llama/patches/0011-sort-devices-by-score.patched llama/patches/0012-add-phony-target-ggml-cpu-for-all-cpu-variants.patched llama/patches/0013-remove-amx.patched llama/patches/0014-fix-string-arr-kv-loading.patched llama/patches/0015-ollama-debug-tensor.patched llama/patches/0016-add-model-quantizations.patched llama/patches/0017-add-op_neg.patched llama/patches/0018-fix-compiler-error-in-clip.h.patched llama/patches/0019-Revert-Simplify-and-improve-CUDA-graphs-through-use-.patched llama/patches/0019-remove-ggml-git-build-info.patched
2025-04-13T01:15:31.2479935Z sed -e 's|@FETCH_HEAD@|7538246e7ce0606694c38055cc2fc9f60535be6c|' llama/build-info.cpp.in > llama/build-info.cpp
2025-04-13T01:15:31.2862082Z Applying: cuda
2025-04-13T01:15:31.3179653Z Applying: pretokenizer
2025-04-13T01:15:31.3483619Z Applying: embeddings
2025-04-13T01:15:31.3804127Z Applying: clip-unicode
2025-04-13T01:15:31.4249665Z Applying: solar-pro
2025-04-13T01:15:31.4589345Z Applying: conditional-fattn
2025-04-13T01:15:31.5174144Z Applying: add mllama support
2025-04-13T01:15:31.5793245Z Applying: add unpad operator
2025-04-13T01:15:31.6140405Z Applying: fix deepseek deseret regex
2025-04-13T01:15:31.6459647Z Applying: maintain ordering for rules for grammar
2025-04-13T01:15:31.6781308Z Applying: sort devices by score
2025-04-13T01:15:31.7097422Z Applying: add phony target ggml-cpu for all cpu variants
2025-04-13T01:15:31.7415248Z Applying: remove amx
2025-04-13T01:15:31.7786183Z Applying: fix string arr kv loading
2025-04-13T01:15:31.8131896Z Applying: ollama debug tensor
2025-04-13T01:15:31.8593832Z Applying: add model quantizations
2025-04-13T01:15:31.9025701Z Applying: add op_neg
2025-04-13T01:15:31.9345294Z Applying: fix compiler error in clip.h
2025-04-13T01:15:31.9715946Z Applying: Revert "Simplify and improve CUDA graphs through use of indirect copy pointers (#9017)"
2025-04-13T01:15:32.0033508Z Applying: remove ggml git build info
2025-04-13T01:15:32.0066308Z rsync -arvzc -f "merge llama/llama.cpp/.rsync-filter" llama/vendor/ llama/llama.cpp
2025-04-13T01:15:32.0143655Z sending incremental file list
2025-04-13T01:15:32.0166188Z ./
2025-04-13T01:15:32.0183666Z common/
2025-04-13T01:15:32.0184104Z examples/
2025-04-13T01:15:32.0184388Z examples/llava/
2025-04-13T01:15:32.0184675Z include/
2025-04-13T01:15:32.0184865Z src/
2025-04-13T01:15:32.0588312Z 
2025-04-13T01:15:32.0588896Z sent 2,806 bytes  received 222 bytes  6,056.00 bytes/sec
2025-04-13T01:15:32.0589622Z total size is 3,245,211  speedup is 1,071.73
2025-04-13T01:15:32.0599385Z rsync -arvzc -f "merge ml/backend/ggml/ggml/.rsync-filter" llama/vendor/ggml/ ml/backend/ggml/ggml
2025-04-13T01:15:32.0634978Z sending incremental file list
2025-04-13T01:15:32.0671906Z ./
2025-04-13T01:15:32.0718785Z cmake/
2025-04-13T01:15:32.0719110Z include/
2025-04-13T01:15:32.0719410Z src/
2025-04-13T01:15:32.0719726Z src/ggml-blas/
2025-04-13T01:15:32.0720073Z src/ggml-cpu/
2025-04-13T01:15:32.0720387Z src/ggml-cpu/amx/
2025-04-13T01:15:32.0720701Z src/ggml-cpu/cmake/
2025-04-13T01:15:32.0721047Z src/ggml-cpu/llamafile/
2025-04-13T01:15:32.0721380Z src/ggml-cuda/
2025-04-13T01:15:32.0721707Z src/ggml-cuda/template-instances/
2025-04-13T01:15:32.0722095Z src/ggml-cuda/vendors/
2025-04-13T01:15:32.0722420Z src/ggml-hip/
2025-04-13T01:15:32.0722710Z src/ggml-metal/
2025-04-13T01:15:32.1126220Z 
2025-04-13T01:15:32.4477243Z sent 13,883 bytes  received 923 bytes  29,612.00 bytes/sec
2025-04-13T01:15:32.4477751Z total size is 4,219,089  speedup is 284.96
