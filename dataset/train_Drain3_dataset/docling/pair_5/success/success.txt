2025-04-11T09:06:23.6088943Z Current runner version: '2.323.0'
2025-04-11T09:06:23.6123586Z ##[group]Operating System
2025-04-11T09:06:23.6124702Z Ubuntu
2025-04-11T09:06:23.6125432Z 24.04.2
2025-04-11T09:06:23.6126289Z LTS
2025-04-11T09:06:23.6127005Z ##[endgroup]
2025-04-11T09:06:23.6127787Z ##[group]Runner Image
2025-04-11T09:06:23.6128941Z Image: ubuntu-24.04
2025-04-11T09:06:23.6129784Z Version: 20250406.1.0
2025-04-11T09:06:23.6131500Z Included Software: https://github.com/actions/runner-images/blob/ubuntu24/20250406.1/images/ubuntu/Ubuntu2404-Readme.md
2025-04-11T09:06:23.6133927Z Image Release: https://github.com/actions/runner-images/releases/tag/ubuntu24%2F20250406.1
2025-04-11T09:06:23.6135503Z ##[endgroup]
2025-04-11T09:06:23.6136284Z ##[group]Runner Image Provisioner
2025-04-11T09:06:23.6137285Z 2.0.422.1
2025-04-11T09:06:23.6138087Z ##[endgroup]
2025-04-11T09:06:23.6139957Z ##[group]GITHUB_TOKEN Permissions
2025-04-11T09:06:23.6142513Z Contents: read
2025-04-11T09:06:23.6143368Z Metadata: read
2025-04-11T09:06:23.6144464Z Packages: read
2025-04-11T09:06:23.6145396Z ##[endgroup]
2025-04-11T09:06:23.6148367Z Secret source: Actions
2025-04-11T09:06:23.6149625Z Prepare workflow directory
2025-04-11T09:06:23.6700005Z Prepare all required actions
2025-04-11T09:06:23.6755098Z Getting action download info
2025-04-11T09:06:24.1182160Z ##[group]Download immutable action package 'actions/checkout@v4'
2025-04-11T09:06:24.1183205Z Version: 4.2.2
2025-04-11T09:06:24.1184218Z Digest: sha256:ccb2698953eaebd21c7bf6268a94f9c26518a7e38e27e0b83c1fe1ad049819b1
2025-04-11T09:06:24.1185345Z Source commit SHA: 11bd71901bbe5b1630ceea73d27597364c9af683
2025-04-11T09:06:24.1186121Z ##[endgroup]
2025-04-11T09:06:24.2172788Z ##[group]Download immutable action package 'actions/cache@v4'
2025-04-11T09:06:24.2173610Z Version: 4.2.3
2025-04-11T09:06:24.2174351Z Digest: sha256:c8a3bb963e1f1826d8fcc8d1354f0dd29d8ac1db1d4f6f20247055ae11b81ed9
2025-04-11T09:06:24.2175368Z Source commit SHA: 5a3ec84eff668545956fd18022155c47e93e2684
2025-04-11T09:06:24.2176093Z ##[endgroup]
2025-04-11T09:06:24.4070337Z Complete job name: code-checks / run-checks (3.10)
2025-04-11T09:06:24.4758309Z ##[group]Run actions/checkout@v4
2025-04-11T09:06:24.4759383Z with:
2025-04-11T09:06:24.4759817Z   repository: docling-project/docling
2025-04-11T09:06:24.4760488Z   token: ***
2025-04-11T09:06:24.4760867Z   ssh-strict: true
2025-04-11T09:06:24.4761274Z   ssh-user: git
2025-04-11T09:06:24.4761677Z   persist-credentials: true
2025-04-11T09:06:24.4762124Z   clean: true
2025-04-11T09:06:24.4762524Z   sparse-checkout-cone-mode: true
2025-04-11T09:06:24.4763015Z   fetch-depth: 1
2025-04-11T09:06:24.4763415Z   fetch-tags: false
2025-04-11T09:06:24.4763818Z   show-progress: true
2025-04-11T09:06:24.4764227Z   lfs: false
2025-04-11T09:06:24.4764598Z   submodules: false
2025-04-11T09:06:24.4765016Z   set-safe-directory: true
2025-04-11T09:06:24.4765651Z env:
2025-04-11T09:06:24.4766041Z   HF_HUB_DOWNLOAD_TIMEOUT: 60
2025-04-11T09:06:24.4766515Z   HF_HUB_ETAG_TIMEOUT: 60
2025-04-11T09:06:24.4766955Z ##[endgroup]
2025-04-11T09:06:24.6805150Z Syncing repository: docling-project/docling
2025-04-11T09:06:24.6807351Z ##[group]Getting Git version info
2025-04-11T09:06:24.6808463Z Working directory is '/home/runner/work/docling/docling'
2025-04-11T09:06:24.6810408Z [command]/usr/bin/git version
2025-04-11T09:06:24.6870920Z git version 2.49.0
2025-04-11T09:06:24.6899776Z ##[endgroup]
2025-04-11T09:06:24.6916323Z Temporarily overriding HOME='/home/runner/work/_temp/726fe592-26b3-482d-a992-e8cf00c72dac' before making global git config changes
2025-04-11T09:06:24.6918666Z Adding repository directory to the temporary git global config as a safe directory
2025-04-11T09:06:24.6931362Z [command]/usr/bin/git config --global --add safe.directory /home/runner/work/docling/docling
2025-04-11T09:06:24.6976822Z Deleting the contents of '/home/runner/work/docling/docling'
2025-04-11T09:06:24.6981874Z ##[group]Initializing the repository
2025-04-11T09:06:24.6988153Z [command]/usr/bin/git init /home/runner/work/docling/docling
2025-04-11T09:06:24.7075564Z hint: Using 'master' as the name for the initial branch. This default branch name
2025-04-11T09:06:24.7076908Z hint: is subject to change. To configure the initial branch name to use in all
2025-04-11T09:06:24.7078102Z hint: of your new repositories, which will suppress this warning, call:
2025-04-11T09:06:24.7079440Z hint:
2025-04-11T09:06:24.7080200Z hint: 	git config --global init.defaultBranch <name>
2025-04-11T09:06:24.7081114Z hint:
2025-04-11T09:06:24.7081941Z hint: Names commonly chosen instead of 'master' are 'main', 'trunk' and
2025-04-11T09:06:24.7083242Z hint: 'development'. The just-created branch can be renamed via this command:
2025-04-11T09:06:24.7083947Z hint:
2025-04-11T09:06:24.7084311Z hint: 	git branch -m <name>
2025-04-11T09:06:24.7085017Z Initialized empty Git repository in /home/runner/work/docling/docling/.git/
2025-04-11T09:06:24.7093906Z [command]/usr/bin/git remote add origin https://github.com/docling-project/docling
2025-04-11T09:06:24.7129973Z ##[endgroup]
2025-04-11T09:06:24.7130725Z ##[group]Disabling automatic garbage collection
2025-04-11T09:06:24.7134742Z [command]/usr/bin/git config --local gc.auto 0
2025-04-11T09:06:24.7164809Z ##[endgroup]
2025-04-11T09:06:24.7165488Z ##[group]Setting up auth
2025-04-11T09:06:24.7172006Z [command]/usr/bin/git config --local --name-only --get-regexp core\.sshCommand
2025-04-11T09:06:24.7202597Z [command]/usr/bin/git submodule foreach --recursive sh -c "git config --local --name-only --get-regexp 'core\.sshCommand' && git config --local --unset-all 'core.sshCommand' || :"
2025-04-11T09:06:24.7532887Z [command]/usr/bin/git config --local --name-only --get-regexp http\.https\:\/\/github\.com\/\.extraheader
2025-04-11T09:06:24.7566576Z [command]/usr/bin/git submodule foreach --recursive sh -c "git config --local --name-only --get-regexp 'http\.https\:\/\/github\.com\/\.extraheader' && git config --local --unset-all 'http.https://github.com/.extraheader' || :"
2025-04-11T09:06:24.7797763Z [command]/usr/bin/git config --local http.https://github.com/.extraheader AUTHORIZATION: basic ***
2025-04-11T09:06:24.7836961Z ##[endgroup]
2025-04-11T09:06:24.7838235Z ##[group]Fetching the repository
2025-04-11T09:06:24.7856511Z [command]/usr/bin/git -c protocol.version=2 fetch --no-tags --prune --no-recurse-submodules --depth=1 origin +4b0e3ef10457d8804ef54921b56e4ff5bb31ce62:refs/remotes/origin/fix/issue-1353
2025-04-11T09:06:26.0535819Z From https://github.com/docling-project/docling
2025-04-11T09:06:26.0537616Z  * [new ref]         4b0e3ef10457d8804ef54921b56e4ff5bb31ce62 -> origin/fix/issue-1353
2025-04-11T09:06:26.0573470Z ##[endgroup]
2025-04-11T09:06:26.0574981Z ##[group]Determining the checkout info
2025-04-11T09:06:26.0576701Z ##[endgroup]
2025-04-11T09:06:26.0577846Z [command]/usr/bin/git sparse-checkout disable
2025-04-11T09:06:26.0624945Z [command]/usr/bin/git config --local --unset-all extensions.worktreeConfig
2025-04-11T09:06:26.0656697Z ##[group]Checking out the ref
2025-04-11T09:06:26.0662604Z [command]/usr/bin/git checkout --progress --force -B fix/issue-1353 refs/remotes/origin/fix/issue-1353
2025-04-11T09:06:26.3011544Z Switched to a new branch 'fix/issue-1353'
2025-04-11T09:06:26.3013482Z branch 'fix/issue-1353' set up to track 'origin/fix/issue-1353'.
2025-04-11T09:06:26.3035688Z ##[endgroup]
2025-04-11T09:06:26.3077831Z [command]/usr/bin/git log -1 --format=%H
2025-04-11T09:06:26.3102170Z 4b0e3ef10457d8804ef54921b56e4ff5bb31ce62
2025-04-11T09:06:26.3305265Z ##[group]Run sudo apt-get update && sudo apt-get install -y tesseract-ocr tesseract-ocr-eng tesseract-ocr-fra tesseract-ocr-deu tesseract-ocr-spa tesseract-ocr-script-latn libleptonica-dev libtesseract-dev pkg-config
2025-04-11T09:06:26.3311389Z [36;1msudo apt-get update && sudo apt-get install -y tesseract-ocr tesseract-ocr-eng tesseract-ocr-fra tesseract-ocr-deu tesseract-ocr-spa tesseract-ocr-script-latn libleptonica-dev libtesseract-dev pkg-config[0m
2025-04-11T09:06:26.3365978Z shell: /usr/bin/bash -e {0}
2025-04-11T09:06:26.3367092Z env:
2025-04-11T09:06:26.3368259Z   HF_HUB_DOWNLOAD_TIMEOUT: 60
2025-04-11T09:06:26.3369605Z   HF_HUB_ETAG_TIMEOUT: 60
2025-04-11T09:06:26.3370642Z ##[endgroup]
2025-04-11T09:06:26.4154394Z Get:1 file:/etc/apt/apt-mirrors.txt Mirrorlist [142 B]
2025-04-11T09:06:26.4497521Z Hit:2 http://azure.archive.ubuntu.com/ubuntu noble InRelease
2025-04-11T09:06:26.4504728Z Hit:6 https://packages.microsoft.com/repos/azure-cli noble InRelease
2025-04-11T09:06:26.4523484Z Get:7 https://packages.microsoft.com/ubuntu/24.04/prod noble InRelease [3600 B]
2025-04-11T09:06:26.4526957Z Get:3 http://azure.archive.ubuntu.com/ubuntu noble-updates InRelease [126 kB]
2025-04-11T09:06:26.4563263Z Hit:4 http://azure.archive.ubuntu.com/ubuntu noble-backports InRelease
2025-04-11T09:06:26.4574101Z Get:5 http://azure.archive.ubuntu.com/ubuntu noble-security InRelease [126 kB]
2025-04-11T09:06:26.6359447Z Get:8 https://packages.microsoft.com/ubuntu/24.04/prod noble/main arm64 Packages [17.1 kB]
2025-04-11T09:06:26.6466152Z Get:9 https://packages.microsoft.com/ubuntu/24.04/prod noble/main amd64 Packages [27.1 kB]
2025-04-11T09:06:26.6858126Z Get:10 http://azure.archive.ubuntu.com/ubuntu noble-updates/main amd64 Packages [991 kB]
2025-04-11T09:06:26.6930403Z Get:11 http://azure.archive.ubuntu.com/ubuntu noble-updates/main Translation-en [219 kB]
2025-04-11T09:06:26.6951124Z Get:12 http://azure.archive.ubuntu.com/ubuntu noble-updates/universe amd64 Packages [1052 kB]
2025-04-11T09:06:26.7016758Z Get:13 http://azure.archive.ubuntu.com/ubuntu noble-updates/restricted Translation-en [182 kB]
2025-04-11T09:06:26.8145663Z Get:14 http://azure.archive.ubuntu.com/ubuntu noble-security/main amd64 Packages [741 kB]
2025-04-11T09:06:26.8196289Z Get:15 http://azure.archive.ubuntu.com/ubuntu noble-security/universe amd64 Packages [829 kB]
2025-04-11T09:06:26.8246241Z Get:16 http://azure.archive.ubuntu.com/ubuntu noble-security/universe Translation-en [180 kB]
2025-04-11T09:06:29.1815843Z Fetched 4495 kB in 1s (6319 kB/s)
2025-04-11T09:06:29.8205592Z Reading package lists...
2025-04-11T09:06:29.8523539Z Reading package lists...
2025-04-11T09:06:30.0086662Z Building dependency tree...
2025-04-11T09:06:30.0094607Z Reading state information...
2025-04-11T09:06:30.1547359Z pkg-config is already the newest version (1.8.1-2build1).
2025-04-11T09:06:30.1548171Z The following additional packages will be installed:
2025-04-11T09:06:30.1550793Z   bzip2-doc comerr-dev libacl1-dev libarchive-dev libattr1-dev libbz2-dev
2025-04-11T09:06:30.1553207Z   libcurl4-openssl-dev libext2fs-dev libgif7 liblept5 liblzma-dev
2025-04-11T09:06:30.1554144Z   libtesseract5 nettle-dev tesseract-ocr-osd
2025-04-11T09:06:30.1560650Z Suggested packages:
2025-04-11T09:06:30.1562119Z   doc-base libcurl4-doc libidn-dev libkrb5-dev libldap2-dev librtmp-dev
2025-04-11T09:06:30.1563195Z   libssh2-1-dev liblzma-doc
2025-04-11T09:06:30.1810040Z The following NEW packages will be installed:
2025-04-11T09:06:30.1811336Z   bzip2-doc comerr-dev libacl1-dev libarchive-dev libattr1-dev libbz2-dev
2025-04-11T09:06:30.1812600Z   libcurl4-openssl-dev libext2fs-dev libgif7 liblept5 libleptonica-dev
2025-04-11T09:06:30.1818640Z   liblzma-dev libtesseract-dev libtesseract5 nettle-dev tesseract-ocr
2025-04-11T09:06:30.1821870Z   tesseract-ocr-deu tesseract-ocr-eng tesseract-ocr-fra tesseract-ocr-osd
2025-04-11T09:06:30.1822625Z   tesseract-ocr-script-latn tesseract-ocr-spa
2025-04-11T09:06:30.2000924Z 0 upgraded, 22 newly installed, 0 to remove and 93 not upgraded.
2025-04-11T09:06:30.2001650Z Need to get 52.9 MB of archives.
2025-04-11T09:06:30.2002017Z After this operation, 141 MB of additional disk space will be used.
2025-04-11T09:06:30.2002441Z Get:1 file:/etc/apt/apt-mirrors.txt Mirrorlist [142 B]
2025-04-11T09:06:30.2441747Z Get:2 http://azure.archive.ubuntu.com/ubuntu noble-updates/main amd64 bzip2-doc all 1.0.8-5.1build0.1 [499 kB]
2025-04-11T09:06:30.2809839Z Get:3 http://azure.archive.ubuntu.com/ubuntu noble-updates/main amd64 libbz2-dev amd64 1.0.8-5.1build0.1 [33.6 kB]
2025-04-11T09:06:30.3004394Z Get:4 http://azure.archive.ubuntu.com/ubuntu noble-updates/main amd64 liblzma-dev amd64 5.6.1+really5.4.5-1ubuntu0.2 [176 kB]
2025-04-11T09:06:30.3237529Z Get:5 http://azure.archive.ubuntu.com/ubuntu noble-updates/main amd64 libattr1-dev amd64 1:2.5.2-1build1.1 [23.1 kB]
2025-04-11T09:06:30.3429987Z Get:6 http://azure.archive.ubuntu.com/ubuntu noble-updates/main amd64 libacl1-dev amd64 2.3.2-1build1.1 [78.5 kB]
2025-04-11T09:06:30.3628089Z Get:7 http://azure.archive.ubuntu.com/ubuntu noble-updates/main amd64 comerr-dev amd64 2.1-1.47.0-2.4~exp1ubuntu4.1 [43.8 kB]
2025-04-11T09:06:30.3821325Z Get:8 http://azure.archive.ubuntu.com/ubuntu noble-updates/main amd64 libext2fs-dev amd64 1.47.0-2.4~exp1ubuntu4.1 [300 kB]
2025-04-11T09:06:30.4039643Z Get:9 http://azure.archive.ubuntu.com/ubuntu noble-updates/main amd64 nettle-dev amd64 3.9.1-2.2build1.1 [1154 kB]
2025-04-11T09:06:30.4489498Z Get:10 http://azure.archive.ubuntu.com/ubuntu noble-updates/main amd64 libarchive-dev amd64 3.7.2-2ubuntu0.3 [589 kB]
2025-04-11T09:06:30.4867246Z Get:11 http://azure.archive.ubuntu.com/ubuntu noble-updates/main amd64 libcurl4-openssl-dev amd64 8.5.0-2ubuntu10.6 [446 kB]
2025-04-11T09:06:30.5223930Z Get:12 http://azure.archive.ubuntu.com/ubuntu noble/main amd64 libgif7 amd64 5.2.2-1ubuntu1 [35.2 kB]
2025-04-11T09:06:30.5420697Z Get:13 http://azure.archive.ubuntu.com/ubuntu noble/universe amd64 liblept5 amd64 1.82.0-3build4 [1099 kB]
2025-04-11T09:06:30.5846613Z Get:14 http://azure.archive.ubuntu.com/ubuntu noble/universe amd64 libleptonica-dev amd64 1.82.0-3build4 [1565 kB]
2025-04-11T09:06:30.6385535Z Get:15 http://azure.archive.ubuntu.com/ubuntu noble/universe amd64 libtesseract5 amd64 5.3.4-1build5 [1291 kB]
2025-04-11T09:06:30.6822126Z Get:16 http://azure.archive.ubuntu.com/ubuntu noble/universe amd64 libtesseract-dev amd64 5.3.4-1build5 [1633 kB]
2025-04-11T09:06:30.7363090Z Get:17 http://azure.archive.ubuntu.com/ubuntu noble/universe amd64 tesseract-ocr-eng all 1:4.1.0-2 [1818 kB]
2025-04-11T09:06:30.7994672Z Get:18 http://azure.archive.ubuntu.com/ubuntu noble/universe amd64 tesseract-ocr-osd all 1:4.1.0-2 [3841 kB]
2025-04-11T09:06:30.8792137Z Get:19 http://azure.archive.ubuntu.com/ubuntu noble/universe amd64 tesseract-ocr amd64 5.3.4-1build5 [328 kB]
2025-04-11T09:06:30.9489675Z Get:20 http://azure.archive.ubuntu.com/ubuntu noble/universe amd64 tesseract-ocr-deu all 1:4.1.0-2 [818 kB]
2025-04-11T09:06:30.9787860Z Get:21 http://azure.archive.ubuntu.com/ubuntu noble/universe amd64 tesseract-ocr-fra all 1:4.1.0-2 [584 kB]
2025-04-11T09:06:31.0267754Z Get:22 http://azure.archive.ubuntu.com/ubuntu noble/universe amd64 tesseract-ocr-script-latn all 1:4.1.0-2 [35.5 MB]
2025-04-11T09:06:31.3913803Z Get:23 http://azure.archive.ubuntu.com/ubuntu noble/universe amd64 tesseract-ocr-spa all 1:4.1.0-2 [1065 kB]
2025-04-11T09:06:31.6881824Z Fetched 52.9 MB in 1s (43.2 MB/s)
2025-04-11T09:06:31.7141959Z Selecting previously unselected package bzip2-doc.
2025-04-11T09:06:31.7430070Z (Reading database ... 
2025-04-11T09:06:31.7430578Z (Reading database ... 5%
2025-04-11T09:06:31.7430890Z (Reading database ... 10%
2025-04-11T09:06:31.7431140Z (Reading database ... 15%
2025-04-11T09:06:31.7431371Z (Reading database ... 20%
2025-04-11T09:06:31.7431609Z (Reading database ... 25%
2025-04-11T09:06:31.7431838Z (Reading database ... 30%
2025-04-11T09:06:31.7432226Z (Reading database ... 35%
2025-04-11T09:06:31.7432615Z (Reading database ... 40%
2025-04-11T09:06:31.7433001Z (Reading database ... 45%
2025-04-11T09:06:31.7434172Z (Reading database ... 50%
2025-04-11T09:06:31.7557466Z (Reading database ... 55%
2025-04-11T09:06:31.8148876Z (Reading database ... 60%
2025-04-11T09:06:31.8588611Z (Reading database ... 65%
2025-04-11T09:06:31.9034802Z (Reading database ... 70%
2025-04-11T09:06:31.9619444Z (Reading database ... 75%
2025-04-11T09:06:32.0097959Z (Reading database ... 80%
2025-04-11T09:06:32.1110601Z (Reading database ... 85%
2025-04-11T09:06:32.1891180Z (Reading database ... 90%
2025-04-11T09:06:32.2351926Z (Reading database ... 95%
2025-04-11T09:06:32.2352312Z (Reading database ... 100%
2025-04-11T09:06:32.2353202Z (Reading database ... 221801 files and directories currently installed.)
2025-04-11T09:06:32.2400807Z Preparing to unpack .../00-bzip2-doc_1.0.8-5.1build0.1_all.deb ...
2025-04-11T09:06:32.2456503Z Unpacking bzip2-doc (1.0.8-5.1build0.1) ...
2025-04-11T09:06:32.2807213Z Selecting previously unselected package libbz2-dev:amd64.
2025-04-11T09:06:32.2944454Z Preparing to unpack .../01-libbz2-dev_1.0.8-5.1build0.1_amd64.deb ...
2025-04-11T09:06:32.2956303Z Unpacking libbz2-dev:amd64 (1.0.8-5.1build0.1) ...
2025-04-11T09:06:32.3221385Z Selecting previously unselected package liblzma-dev:amd64.
2025-04-11T09:06:32.3357664Z Preparing to unpack .../02-liblzma-dev_5.6.1+really5.4.5-1ubuntu0.2_amd64.deb ...
2025-04-11T09:06:32.3380808Z Unpacking liblzma-dev:amd64 (5.6.1+really5.4.5-1ubuntu0.2) ...
2025-04-11T09:06:32.3725362Z Selecting previously unselected package libattr1-dev:amd64.
2025-04-11T09:06:32.3862283Z Preparing to unpack .../03-libattr1-dev_1%3a2.5.2-1build1.1_amd64.deb ...
2025-04-11T09:06:32.3878929Z Unpacking libattr1-dev:amd64 (1:2.5.2-1build1.1) ...
2025-04-11T09:06:32.4152791Z Selecting previously unselected package libacl1-dev:amd64.
2025-04-11T09:06:32.4289156Z Preparing to unpack .../04-libacl1-dev_2.3.2-1build1.1_amd64.deb ...
2025-04-11T09:06:32.4305301Z Unpacking libacl1-dev:amd64 (2.3.2-1build1.1) ...
2025-04-11T09:06:32.4644377Z Selecting previously unselected package comerr-dev:amd64.
2025-04-11T09:06:32.4783130Z Preparing to unpack .../05-comerr-dev_2.1-1.47.0-2.4~exp1ubuntu4.1_amd64.deb ...
2025-04-11T09:06:32.4895599Z Unpacking comerr-dev:amd64 (2.1-1.47.0-2.4~exp1ubuntu4.1) ...
2025-04-11T09:06:32.5179593Z Selecting previously unselected package libext2fs-dev.
2025-04-11T09:06:32.5315572Z Preparing to unpack .../06-libext2fs-dev_1.47.0-2.4~exp1ubuntu4.1_amd64.deb ...
2025-04-11T09:06:32.5332794Z Unpacking libext2fs-dev (1.47.0-2.4~exp1ubuntu4.1) ...
2025-04-11T09:06:32.5653794Z Selecting previously unselected package nettle-dev:amd64.
2025-04-11T09:06:32.5796011Z Preparing to unpack .../07-nettle-dev_3.9.1-2.2build1.1_amd64.deb ...
2025-04-11T09:06:32.5808304Z Unpacking nettle-dev:amd64 (3.9.1-2.2build1.1) ...
2025-04-11T09:06:32.6326957Z Selecting previously unselected package libarchive-dev:amd64.
2025-04-11T09:06:32.6466788Z Preparing to unpack .../08-libarchive-dev_3.7.2-2ubuntu0.3_amd64.deb ...
2025-04-11T09:06:32.6484249Z Unpacking libarchive-dev:amd64 (3.7.2-2ubuntu0.3) ...
2025-04-11T09:06:32.6952862Z Selecting previously unselected package libcurl4-openssl-dev:amd64.
2025-04-11T09:06:32.7097611Z Preparing to unpack .../09-libcurl4-openssl-dev_8.5.0-2ubuntu10.6_amd64.deb ...
2025-04-11T09:06:32.7120815Z Unpacking libcurl4-openssl-dev:amd64 (8.5.0-2ubuntu10.6) ...
2025-04-11T09:06:32.7528229Z Selecting previously unselected package libgif7:amd64.
2025-04-11T09:06:32.7666828Z Preparing to unpack .../10-libgif7_5.2.2-1ubuntu1_amd64.deb ...
2025-04-11T09:06:32.7696764Z Unpacking libgif7:amd64 (5.2.2-1ubuntu1) ...
2025-04-11T09:06:32.7971970Z Selecting previously unselected package liblept5:amd64.
2025-04-11T09:06:32.8109325Z Preparing to unpack .../11-liblept5_1.82.0-3build4_amd64.deb ...
2025-04-11T09:06:32.8129938Z Unpacking liblept5:amd64 (1.82.0-3build4) ...
2025-04-11T09:06:32.8534373Z Selecting previously unselected package libleptonica-dev.
2025-04-11T09:06:32.8671909Z Preparing to unpack .../12-libleptonica-dev_1.82.0-3build4_amd64.deb ...
2025-04-11T09:06:32.8687232Z Unpacking libleptonica-dev (1.82.0-3build4) ...
2025-04-11T09:06:32.9221009Z Selecting previously unselected package libtesseract5:amd64.
2025-04-11T09:06:32.9359153Z Preparing to unpack .../13-libtesseract5_5.3.4-1build5_amd64.deb ...
2025-04-11T09:06:32.9370223Z Unpacking libtesseract5:amd64 (5.3.4-1build5) ...
2025-04-11T09:06:32.9799178Z Selecting previously unselected package libtesseract-dev:amd64.
2025-04-11T09:06:32.9935763Z Preparing to unpack .../14-libtesseract-dev_5.3.4-1build5_amd64.deb ...
2025-04-11T09:06:32.9964987Z Unpacking libtesseract-dev:amd64 (5.3.4-1build5) ...
2025-04-11T09:06:33.0546557Z Selecting previously unselected package tesseract-ocr-eng.
2025-04-11T09:06:33.0684898Z Preparing to unpack .../15-tesseract-ocr-eng_1%3a4.1.0-2_all.deb ...
2025-04-11T09:06:33.0699699Z Unpacking tesseract-ocr-eng (1:4.1.0-2) ...
2025-04-11T09:06:33.1137638Z Selecting previously unselected package tesseract-ocr-osd.
2025-04-11T09:06:33.1274470Z Preparing to unpack .../16-tesseract-ocr-osd_1%3a4.1.0-2_all.deb ...
2025-04-11T09:06:33.1295444Z Unpacking tesseract-ocr-osd (1:4.1.0-2) ...
2025-04-11T09:06:33.2007136Z Selecting previously unselected package tesseract-ocr.
2025-04-11T09:06:33.2144324Z Preparing to unpack .../17-tesseract-ocr_5.3.4-1build5_amd64.deb ...
2025-04-11T09:06:33.2158955Z Unpacking tesseract-ocr (5.3.4-1build5) ...
2025-04-11T09:06:33.2652956Z Selecting previously unselected package tesseract-ocr-deu.
2025-04-11T09:06:33.2789849Z Preparing to unpack .../18-tesseract-ocr-deu_1%3a4.1.0-2_all.deb ...
2025-04-11T09:06:33.2810587Z Unpacking tesseract-ocr-deu (1:4.1.0-2) ...
2025-04-11T09:06:33.3136823Z Selecting previously unselected package tesseract-ocr-fra.
2025-04-11T09:06:33.3275739Z Preparing to unpack .../19-tesseract-ocr-fra_1%3a4.1.0-2_all.deb ...
2025-04-11T09:06:33.3289485Z Unpacking tesseract-ocr-fra (1:4.1.0-2) ...
2025-04-11T09:06:33.3589804Z Selecting previously unselected package tesseract-ocr-script-latn.
2025-04-11T09:06:33.3726311Z Preparing to unpack .../20-tesseract-ocr-script-latn_1%3a4.1.0-2_all.deb ...
2025-04-11T09:06:33.3747152Z Unpacking tesseract-ocr-script-latn (1:4.1.0-2) ...
2025-04-11T09:06:33.8314364Z Selecting previously unselected package tesseract-ocr-spa.
2025-04-11T09:06:33.8453520Z Preparing to unpack .../21-tesseract-ocr-spa_1%3a4.1.0-2_all.deb ...
2025-04-11T09:06:33.8465898Z Unpacking tesseract-ocr-spa (1:4.1.0-2) ...
2025-04-11T09:06:33.9047234Z Setting up bzip2-doc (1.0.8-5.1build0.1) ...
2025-04-11T09:06:33.9096663Z Setting up libattr1-dev:amd64 (1:2.5.2-1build1.1) ...
2025-04-11T09:06:33.9134395Z Setting up nettle-dev:amd64 (3.9.1-2.2build1.1) ...
2025-04-11T09:06:33.9172525Z Setting up tesseract-ocr-script-latn (1:4.1.0-2) ...
2025-04-11T09:06:33.9210879Z Setting up tesseract-ocr-eng (1:4.1.0-2) ...
2025-04-11T09:06:33.9259134Z Setting up libcurl4-openssl-dev:amd64 (8.5.0-2ubuntu10.6) ...
2025-04-11T09:06:33.9297167Z Setting up tesseract-ocr-fra (1:4.1.0-2) ...
2025-04-11T09:06:33.9339307Z Setting up comerr-dev:amd64 (2.1-1.47.0-2.4~exp1ubuntu4.1) ...
2025-04-11T09:06:33.9405502Z Setting up liblzma-dev:amd64 (5.6.1+really5.4.5-1ubuntu0.2) ...
2025-04-11T09:06:33.9441000Z Setting up tesseract-ocr-deu (1:4.1.0-2) ...
2025-04-11T09:06:33.9484159Z Setting up libgif7:amd64 (5.2.2-1ubuntu1) ...
2025-04-11T09:06:33.9524479Z Setting up tesseract-ocr-spa (1:4.1.0-2) ...
2025-04-11T09:06:33.9564517Z Setting up tesseract-ocr-osd (1:4.1.0-2) ...
2025-04-11T09:06:33.9610181Z Setting up libext2fs-dev (1.47.0-2.4~exp1ubuntu4.1) ...
2025-04-11T09:06:33.9643048Z Setting up libacl1-dev:amd64 (2.3.2-1build1.1) ...
2025-04-11T09:06:33.9672926Z Setting up libbz2-dev:amd64 (1.0.8-5.1build0.1) ...
2025-04-11T09:06:33.9706656Z Setting up liblept5:amd64 (1.82.0-3build4) ...
2025-04-11T09:06:33.9742882Z Setting up libleptonica-dev (1.82.0-3build4) ...
2025-04-11T09:06:33.9777088Z Setting up libtesseract5:amd64 (5.3.4-1build5) ...
2025-04-11T09:06:33.9816317Z Setting up libarchive-dev:amd64 (3.7.2-2ubuntu0.3) ...
2025-04-11T09:06:33.9853720Z Setting up libtesseract-dev:amd64 (5.3.4-1build5) ...
2025-04-11T09:06:33.9886616Z Setting up tesseract-ocr (5.3.4-1build5) ...
2025-04-11T09:06:33.9926730Z Processing triggers for libc-bin (2.39-0ubuntu8.4) ...
2025-04-11T09:06:34.0314753Z Processing triggers for man-db (2.12.0-4build2) ...
2025-04-11T09:06:42.7463432Z Processing triggers for install-info (7.1-3build2) ...
2025-04-11T09:06:43.5320814Z 
2025-04-11T09:06:43.5321445Z Running kernel seems to be up-to-date.
2025-04-11T09:06:43.5321722Z 
2025-04-11T09:06:43.5321829Z No services need to be restarted.
2025-04-11T09:06:43.5322023Z 
2025-04-11T09:06:43.5322132Z No containers need to be restarted.
2025-04-11T09:06:43.5322803Z 
2025-04-11T09:06:43.5322936Z No user sessions are running outdated binaries.
2025-04-11T09:06:43.5323178Z 
2025-04-11T09:06:43.5323403Z No VM guests are running outdated hypervisor (qemu) binaries on this host.
2025-04-11T09:06:44.5344827Z ##[group]Run echo "TESSDATA_PREFIX=$(dpkg -L tesseract-ocr-eng | grep tessdata$)" >> "$GITHUB_ENV"
2025-04-11T09:06:44.5345454Z [36;1mecho "TESSDATA_PREFIX=$(dpkg -L tesseract-ocr-eng | grep tessdata$)" >> "$GITHUB_ENV"[0m
2025-04-11T09:06:44.5395425Z shell: /usr/bin/bash -e {0}
2025-04-11T09:06:44.5395656Z env:
2025-04-11T09:06:44.5395844Z   HF_HUB_DOWNLOAD_TIMEOUT: 60
2025-04-11T09:06:44.5396067Z   HF_HUB_ETAG_TIMEOUT: 60
2025-04-11T09:06:44.5396258Z ##[endgroup]
2025-04-11T09:06:44.5682732Z ##[group]Run actions/cache@v4
2025-04-11T09:06:44.5682978Z with:
2025-04-11T09:06:44.5683151Z   path: ~/.cache/huggingface
2025-04-11T09:06:44.5683384Z   key: huggingface-cache-py3.10
2025-04-11T09:06:44.5683615Z   enableCrossOsArchive: false
2025-04-11T09:06:44.5683859Z   fail-on-cache-miss: false
2025-04-11T09:06:44.5684055Z   lookup-only: false
2025-04-11T09:06:44.5684239Z   save-always: false
2025-04-11T09:06:44.5684403Z env:
2025-04-11T09:06:44.5684567Z   HF_HUB_DOWNLOAD_TIMEOUT: 60
2025-04-11T09:06:44.5684771Z   HF_HUB_ETAG_TIMEOUT: 60
2025-04-11T09:06:44.5685013Z   TESSDATA_PREFIX: /usr/share/tesseract-ocr/5/tessdata
2025-04-11T09:06:44.5685281Z ##[endgroup]
2025-04-11T09:06:44.7797282Z Cache hit for: huggingface-cache-py3.10
2025-04-11T09:06:45.8127159Z Received 180355072 of 1113780323 (16.2%), 172.0 MBs/sec
2025-04-11T09:06:46.8144441Z Received 385875968 of 1113780323 (34.6%), 183.8 MBs/sec
2025-04-11T09:06:47.8146563Z Received 528482304 of 1113780323 (47.4%), 167.9 MBs/sec
2025-04-11T09:06:48.8165178Z Received 696254464 of 1113780323 (62.5%), 165.9 MBs/sec
2025-04-11T09:06:49.8154896Z Received 843055104 of 1113780323 (75.7%), 160.7 MBs/sec
2025-04-11T09:06:50.8154547Z Received 1031798784 of 1113780323 (92.6%), 163.9 MBs/sec
2025-04-11T09:06:51.4203931Z Received 1113780323 of 1113780323 (100.0%), 160.7 MBs/sec
2025-04-11T09:06:51.4206641Z Cache Size: ~1062 MB (1113780323 B)
2025-04-11T09:06:51.4267113Z [command]/usr/bin/tar -xf /home/runner/work/_temp/925cada0-cf09-4e46-8ac2-7e9a8b14f471/cache.tzst -P -C /home/runner/work/docling/docling --use-compress-program unzstd
2025-04-11T09:06:53.5439004Z Cache restored successfully
2025-04-11T09:06:54.0715762Z Cache restored from key: huggingface-cache-py3.10
2025-04-11T09:06:54.0943641Z Prepare all required actions
2025-04-11T09:06:54.0944088Z Getting action download info
2025-04-11T09:06:54.2130634Z ##[group]Download immutable action package 'actions/setup-python@v5'
2025-04-11T09:06:54.2131360Z Version: 5.5.0
2025-04-11T09:06:54.2131871Z Digest: sha256:1fac95d751afd58314aebd2ab01d4bf3b3109a441917bd2cac2a22898ec494f7
2025-04-11T09:06:54.2132628Z Source commit SHA: 8d9ed9ac5c53483de85588cdf95a591a75ab9f55
2025-04-11T09:06:54.2133118Z ##[endgroup]
2025-04-11T09:06:54.4464722Z ##[group]Run ./.github/actions/setup-poetry
2025-04-11T09:06:54.4465056Z with:
2025-04-11T09:06:54.4465265Z   python-version: 3.10
2025-04-11T09:06:54.4465491Z env:
2025-04-11T09:06:54.4465688Z   HF_HUB_DOWNLOAD_TIMEOUT: 60
2025-04-11T09:06:54.4465940Z   HF_HUB_ETAG_TIMEOUT: 60
2025-04-11T09:06:54.4466244Z   TESSDATA_PREFIX: /usr/share/tesseract-ocr/5/tessdata
2025-04-11T09:06:54.4466576Z ##[endgroup]
2025-04-11T09:06:54.4533680Z ##[group]Run pipx install poetry==1.8.5
2025-04-11T09:06:54.4534052Z [36;1mpipx install poetry==1.8.5[0m
2025-04-11T09:06:54.4582365Z shell: /usr/bin/bash --noprofile --norc -e -o pipefail {0}
2025-04-11T09:06:54.4582807Z env:
2025-04-11T09:06:54.4583004Z   HF_HUB_DOWNLOAD_TIMEOUT: 60
2025-04-11T09:06:54.4583251Z   HF_HUB_ETAG_TIMEOUT: 60
2025-04-11T09:06:54.4583530Z   TESSDATA_PREFIX: /usr/share/tesseract-ocr/5/tessdata
2025-04-11T09:06:54.4583827Z ##[endgroup]
2025-04-11T09:06:54.6232259Z creating virtual environment...
2025-04-11T09:06:54.7806041Z installing poetry from spec 'poetry==1.8.5'...
2025-04-11T09:07:00.4490551Z done! ✨ 🌟 ✨
2025-04-11T09:07:00.4491204Z   installed package poetry 1.8.5, installed using Python 3.12.3
2025-04-11T09:07:00.4491932Z   These apps are now globally available
2025-04-11T09:07:00.4492403Z     - poetry
2025-04-11T09:07:00.4733444Z ##[group]Run actions/setup-python@v5
2025-04-11T09:07:00.4733691Z with:
2025-04-11T09:07:00.4733866Z   python-version: 3.10
2025-04-11T09:07:00.4734060Z   cache: poetry
2025-04-11T09:07:00.4734267Z   check-latest: false
2025-04-11T09:07:00.4734569Z   token: ***
2025-04-11T09:07:00.4734754Z   update-environment: true
2025-04-11T09:07:00.4734978Z   allow-prereleases: false
2025-04-11T09:07:00.4735190Z   freethreaded: false
2025-04-11T09:07:00.4735395Z env:
2025-04-11T09:07:00.4735558Z   HF_HUB_DOWNLOAD_TIMEOUT: 60
2025-04-11T09:07:00.4735780Z   HF_HUB_ETAG_TIMEOUT: 60
2025-04-11T09:07:00.4736034Z   TESSDATA_PREFIX: /usr/share/tesseract-ocr/5/tessdata
2025-04-11T09:07:00.4736306Z ##[endgroup]
2025-04-11T09:07:00.6421294Z ##[group]Installed versions
2025-04-11T09:07:00.6485002Z Successfully set up CPython (3.10.16)
2025-04-11T09:07:00.6485821Z ##[endgroup]
2025-04-11T09:07:00.7467574Z [command]/opt/pipx_bin/poetry config --list
2025-04-11T09:07:01.2320902Z cache-dir = "/home/runner/.cache/pypoetry"
2025-04-11T09:07:01.2322773Z experimental.system-git-client = false
2025-04-11T09:07:01.2323255Z installer.max-workers = null
2025-04-11T09:07:01.2323684Z installer.modern-installation = true
2025-04-11T09:07:01.2324067Z installer.no-binary = null
2025-04-11T09:07:01.2324308Z installer.parallel = true
2025-04-11T09:07:01.2324527Z keyring.enabled = true
2025-04-11T09:07:01.2324730Z solver.lazy-wheel = true
2025-04-11T09:07:01.2324943Z virtualenvs.create = true
2025-04-11T09:07:01.2325161Z virtualenvs.in-project = null
2025-04-11T09:07:01.2325404Z virtualenvs.options.always-copy = false
2025-04-11T09:07:01.2325672Z virtualenvs.options.no-pip = false
2025-04-11T09:07:01.2325944Z virtualenvs.options.no-setuptools = false
2025-04-11T09:07:01.2326247Z virtualenvs.options.system-site-packages = false
2025-04-11T09:07:01.2326696Z virtualenvs.path = "{cache-dir}/virtualenvs"  # /home/runner/.cache/pypoetry/virtualenvs
2025-04-11T09:07:01.2327105Z virtualenvs.prefer-active-python = false
2025-04-11T09:07:01.2327417Z virtualenvs.prompt = "{project_name}-py{python_version}"
2025-04-11T09:07:01.2327708Z warnings.export = true
2025-04-11T09:07:01.4028204Z Cache hit for: setup-python-Linux-x64-python-3.10.16-poetry-v2-6537c0c4b2503108d4171f165ddac27eb1391ee7ae381f0775fd538108ab5411
2025-04-11T09:07:02.4703662Z Received 163577856 of 2922231717 (5.6%), 155.8 MBs/sec
2025-04-11T09:07:03.4705471Z Received 322961408 of 2922231717 (11.1%), 153.8 MBs/sec
2025-04-11T09:07:04.4747895Z Received 490733568 of 2922231717 (16.8%), 155.7 MBs/sec
2025-04-11T09:07:05.4745523Z Received 616562688 of 2922231717 (21.1%), 146.8 MBs/sec
2025-04-11T09:07:06.4745560Z Received 704643072 of 2922231717 (24.1%), 134.2 MBs/sec
2025-04-11T09:07:07.4755721Z Received 792723456 of 2922231717 (27.1%), 125.9 MBs/sec
2025-04-11T09:07:08.5650409Z Received 939524096 of 2922231717 (32.2%), 126.3 MBs/sec
2025-04-11T09:07:09.5673466Z Received 1119879168 of 2922231717 (38.3%), 131.9 MBs/sec
2025-04-11T09:07:10.5631446Z Received 1258291200 of 2922231717 (43.1%), 132.0 MBs/sec
2025-04-11T09:07:11.5634733Z Received 1430257664 of 2922231717 (48.9%), 135.1 MBs/sec
2025-04-11T09:07:12.5645093Z Received 1610612736 of 2922231717 (55.1%), 138.4 MBs/sec
2025-04-11T09:07:13.5661051Z Received 1828716544 of 2922231717 (62.6%), 144.2 MBs/sec
2025-04-11T09:07:14.5689995Z Received 2013265920 of 2922231717 (68.9%), 146.6 MBs/sec
2025-04-11T09:07:15.5763170Z Received 2193620992 of 2922231717 (75.1%), 148.4 MBs/sec
2025-04-11T09:07:16.5700150Z Received 2365587456 of 2922231717 (81.0%), 149.4 MBs/sec
2025-04-11T09:07:17.6218313Z Received 2550136832 of 2922231717 (87.3%), 150.6 MBs/sec
2025-04-11T09:07:18.6207679Z Received 2738880512 of 2922231717 (93.7%), 152.3 MBs/sec
2025-04-11T09:07:19.6209226Z Received 2910846976 of 2922231717 (99.6%), 152.9 MBs/sec
2025-04-11T09:07:19.7208537Z Received 2922231717 of 2922231717 (100.0%), 152.7 MBs/sec
2025-04-11T09:07:19.7210045Z Cache Size: ~2787 MB (2922231717 B)
2025-04-11T09:07:19.7329089Z [command]/usr/bin/tar -xf /home/runner/work/_temp/fe7f7c90-9a86-47b5-92fe-514c3560d312/cache.tzst -P -C /home/runner/work/docling/docling --use-compress-program unzstd
2025-04-11T09:07:34.4142195Z Cache restored successfully
2025-04-11T09:07:35.6756319Z [command]/opt/pipx_bin/poetry env use /opt/hostedtoolcache/Python/3.10.16/x64/bin/python
2025-04-11T09:07:36.8244249Z Using virtualenv: /home/runner/.cache/pypoetry/virtualenvs/docling-oEGFWdgn-py3.10
2025-04-11T09:07:36.8658597Z Cache restored from key: setup-python-Linux-x64-python-3.10.16-poetry-v2-6537c0c4b2503108d4171f165ddac27eb1391ee7ae381f0775fd538108ab5411
2025-04-11T09:07:36.8783591Z ##[group]Run poetry install --all-extras
2025-04-11T09:07:36.8783906Z [36;1mpoetry install --all-extras[0m
2025-04-11T09:07:36.8864997Z shell: /usr/bin/bash --noprofile --norc -e -o pipefail {0}
2025-04-11T09:07:36.8865350Z env:
2025-04-11T09:07:36.8865536Z   HF_HUB_DOWNLOAD_TIMEOUT: 60
2025-04-11T09:07:36.8865773Z   HF_HUB_ETAG_TIMEOUT: 60
2025-04-11T09:07:36.8866062Z   TESSDATA_PREFIX: /usr/share/tesseract-ocr/5/tessdata
2025-04-11T09:07:36.8866443Z   pythonLocation: /opt/hostedtoolcache/Python/3.10.16/x64
2025-04-11T09:07:36.8866891Z   PKG_CONFIG_PATH: /opt/hostedtoolcache/Python/3.10.16/x64/lib/pkgconfig
2025-04-11T09:07:36.8867325Z   Python_ROOT_DIR: /opt/hostedtoolcache/Python/3.10.16/x64
2025-04-11T09:07:36.8867722Z   Python2_ROOT_DIR: /opt/hostedtoolcache/Python/3.10.16/x64
2025-04-11T09:07:36.8868102Z   Python3_ROOT_DIR: /opt/hostedtoolcache/Python/3.10.16/x64
2025-04-11T09:07:36.8868477Z   LD_LIBRARY_PATH: /opt/hostedtoolcache/Python/3.10.16/x64/lib
2025-04-11T09:07:36.8868945Z ##[endgroup]
2025-04-11T09:07:37.8218274Z Installing dependencies from lock file
2025-04-11T09:07:38.5673152Z 
2025-04-11T09:07:38.5673700Z No dependencies to install or update
2025-04-11T09:07:38.5731186Z 
2025-04-11T09:07:38.5731780Z Installing the current project: docling (2.29.0)
2025-04-11T09:07:38.6651786Z ##[group]Run poetry run pre-commit run --all-files
2025-04-11T09:07:38.6652174Z [36;1mpoetry run pre-commit run --all-files[0m
2025-04-11T09:07:38.6698516Z shell: /usr/bin/bash -e {0}
2025-04-11T09:07:38.6698985Z env:
2025-04-11T09:07:38.6699189Z   HF_HUB_DOWNLOAD_TIMEOUT: 60
2025-04-11T09:07:38.6699463Z   HF_HUB_ETAG_TIMEOUT: 60
2025-04-11T09:07:38.6699727Z   TESSDATA_PREFIX: /usr/share/tesseract-ocr/5/tessdata
2025-04-11T09:07:38.6700098Z   pythonLocation: /opt/hostedtoolcache/Python/3.10.16/x64
2025-04-11T09:07:38.6700523Z   PKG_CONFIG_PATH: /opt/hostedtoolcache/Python/3.10.16/x64/lib/pkgconfig
2025-04-11T09:07:38.6700921Z   Python_ROOT_DIR: /opt/hostedtoolcache/Python/3.10.16/x64
2025-04-11T09:07:38.6701274Z   Python2_ROOT_DIR: /opt/hostedtoolcache/Python/3.10.16/x64
2025-04-11T09:07:38.6701635Z   Python3_ROOT_DIR: /opt/hostedtoolcache/Python/3.10.16/x64
2025-04-11T09:07:38.6701993Z   LD_LIBRARY_PATH: /opt/hostedtoolcache/Python/3.10.16/x64/lib
2025-04-11T09:07:38.6702344Z ##[endgroup]
2025-04-11T09:07:42.5812854Z Black....................................................................Passed
2025-04-11T09:07:43.6315869Z isort....................................................................Passed
2025-04-11T09:08:49.7579265Z MyPy.....................................................................Passed
2025-04-11T09:08:51.9588092Z nbQA Black...............................................................Passed
2025-04-11T09:08:53.8726816Z nbQA isort...............................................................Passed
2025-04-11T09:08:54.6116205Z Poetry check.............................................................Passed
2025-04-11T09:08:54.6289712Z ##[group]Run poetry install --all-extras
2025-04-11T09:08:54.6290063Z [36;1mpoetry install --all-extras[0m
2025-04-11T09:08:54.6336621Z shell: /usr/bin/bash -e {0}
2025-04-11T09:08:54.6336867Z env:
2025-04-11T09:08:54.6337062Z   HF_HUB_DOWNLOAD_TIMEOUT: 60
2025-04-11T09:08:54.6337490Z   HF_HUB_ETAG_TIMEOUT: 60
2025-04-11T09:08:54.6337772Z   TESSDATA_PREFIX: /usr/share/tesseract-ocr/5/tessdata
2025-04-11T09:08:54.6338148Z   pythonLocation: /opt/hostedtoolcache/Python/3.10.16/x64
2025-04-11T09:08:54.6338583Z   PKG_CONFIG_PATH: /opt/hostedtoolcache/Python/3.10.16/x64/lib/pkgconfig
2025-04-11T09:08:54.6339194Z   Python_ROOT_DIR: /opt/hostedtoolcache/Python/3.10.16/x64
2025-04-11T09:08:54.6339569Z   Python2_ROOT_DIR: /opt/hostedtoolcache/Python/3.10.16/x64
2025-04-11T09:08:54.6339947Z   Python3_ROOT_DIR: /opt/hostedtoolcache/Python/3.10.16/x64
2025-04-11T09:08:54.6340334Z   LD_LIBRARY_PATH: /opt/hostedtoolcache/Python/3.10.16/x64/lib
2025-04-11T09:08:54.6340648Z ##[endgroup]
2025-04-11T09:08:55.5436203Z Installing dependencies from lock file
2025-04-11T09:08:56.2738248Z 
2025-04-11T09:08:56.2739179Z No dependencies to install or update
2025-04-11T09:08:56.2795571Z 
2025-04-11T09:08:56.2795988Z Installing the current project: docling (2.29.0)
2025-04-11T09:08:56.3774726Z ##[group]Run poetry run pytest -v tests
2025-04-11T09:08:56.3775281Z [36;1mpoetry run pytest -v tests[0m
2025-04-11T09:08:56.3833562Z shell: /usr/bin/bash -e {0}
2025-04-11T09:08:56.3833939Z env:
2025-04-11T09:08:56.3834227Z   HF_HUB_DOWNLOAD_TIMEOUT: 60
2025-04-11T09:08:56.3834601Z   HF_HUB_ETAG_TIMEOUT: 60
2025-04-11T09:08:56.3835038Z   TESSDATA_PREFIX: /usr/share/tesseract-ocr/5/tessdata
2025-04-11T09:08:56.3835638Z   pythonLocation: /opt/hostedtoolcache/Python/3.10.16/x64
2025-04-11T09:08:56.3836322Z   PKG_CONFIG_PATH: /opt/hostedtoolcache/Python/3.10.16/x64/lib/pkgconfig
2025-04-11T09:08:56.3836991Z   Python_ROOT_DIR: /opt/hostedtoolcache/Python/3.10.16/x64
2025-04-11T09:08:56.3837596Z   Python2_ROOT_DIR: /opt/hostedtoolcache/Python/3.10.16/x64
2025-04-11T09:08:56.3838207Z   Python3_ROOT_DIR: /opt/hostedtoolcache/Python/3.10.16/x64
2025-04-11T09:08:56.3838997Z   LD_LIBRARY_PATH: /opt/hostedtoolcache/Python/3.10.16/x64/lib
2025-04-11T09:08:56.3839559Z ##[endgroup]
2025-04-11T09:08:57.0666384Z ============================= test session starts ==============================
2025-04-11T09:08:57.0667338Z platform linux -- Python 3.10.16, pytest-7.4.4, pluggy-1.5.0 -- /home/runner/.cache/pypoetry/virtualenvs/docling-oEGFWdgn-py3.10/bin/python
2025-04-11T09:08:57.0668149Z cachedir: .pytest_cache
2025-04-11T09:08:57.0668491Z rootdir: /home/runner/work/docling/docling
2025-04-11T09:08:57.0669254Z plugins: anyio-4.9.0, xdist-3.6.1
2025-04-11T09:09:02.8221394Z collecting ... collected 67 items
2025-04-11T09:09:02.8221686Z 
2025-04-11T09:09:02.8502686Z tests/test_backend_asciidoc.py::test_asciidocs_examples PASSED           [  1%]
2025-04-11T09:09:03.5972626Z tests/test_backend_csv.py::test_e2e_valid_csv_conversions PASSED         [  2%]
2025-04-11T09:09:03.6011581Z tests/test_backend_csv.py::test_e2e_invalid_csv_conversions PASSED       [  4%]
2025-04-11T09:09:03.7200724Z tests/test_backend_docling_json.py::test_convert_valid_docling_json PASSED [  5%]
2025-04-11T09:09:03.7213638Z tests/test_backend_docling_json.py::test_invalid_docling_json PASSED     [  7%]
2025-04-11T09:09:30.6212583Z tests/test_backend_docling_parse.py::test_text_cell_counts PASSED        [  8%]
2025-04-11T09:09:35.9692601Z tests/test_backend_docling_parse.py::test_get_text_from_rect PASSED      [ 10%]
2025-04-11T09:09:41.2771420Z tests/test_backend_docling_parse.py::test_crop_page_image PASSED         [ 11%]
2025-04-11T09:09:41.4795507Z tests/test_backend_docling_parse.py::test_num_pages PASSED               [ 13%]
2025-04-11T09:09:43.6335024Z tests/test_backend_docling_parse_v2.py::test_text_cell_counts PASSED     [ 14%]
2025-04-11T09:09:44.7094812Z tests/test_backend_docling_parse_v2.py::test_get_text_from_rect PASSED   [ 16%]
2025-04-11T09:09:45.9641413Z tests/test_backend_docling_parse_v2.py::test_crop_page_image PASSED      [ 17%]
2025-04-11T09:09:45.9716634Z tests/test_backend_docling_parse_v2.py::test_num_pages PASSED            [ 19%]
2025-04-11T09:09:46.0552469Z tests/test_backend_docling_parse_v4.py::test_text_cell_counts PASSED     [ 20%]
2025-04-11T09:09:48.8969877Z tests/test_backend_docling_parse_v4.py::test_get_text_from_rect PASSED   [ 22%]
2025-04-11T09:09:51.7242995Z tests/test_backend_docling_parse_v4.py::test_crop_page_image PASSED      [ 23%]
2025-04-11T09:09:51.7321178Z tests/test_backend_docling_parse_v4.py::test_num_pages PASSED            [ 25%]
2025-04-11T09:09:51.9822483Z tests/test_backend_html.py::test_heading_levels PASSED                   [ 26%]
2025-04-11T09:09:51.9831473Z tests/test_backend_html.py::test_ordered_lists SKIPPED (Temporarily ...) [ 28%]
2025-04-11T09:09:52.4791788Z tests/test_backend_html.py::test_e2e_html_conversions PASSED             [ 29%]
2025-04-11T09:09:52.4805571Z tests/test_backend_jats.py::test_e2e_pubmed_conversions PASSED           [ 31%]
2025-04-11T09:09:52.4816933Z tests/test_backend_jats.py::test_e2e_pubmed_conversions_stream PASSED    [ 32%]
2025-04-11T09:09:52.4828233Z tests/test_backend_jats.py::test_e2e_pubmed_conversions_no_stream PASSED [ 34%]
2025-04-11T09:09:54.9629060Z tests/test_backend_markdown.py::test_convert_valid PASSED                [ 35%]
2025-04-11T09:09:55.1426192Z tests/test_backend_msexcel.py::test_e2e_xlsx_conversions PASSED          [ 37%]
2025-04-11T09:09:55.1881218Z tests/test_backend_msexcel.py::test_pages PASSED                         [ 38%]
2025-04-11T09:09:55.2455637Z tests/test_backend_msword.py::test_heading_levels PASSED                 [ 40%]
2025-04-11T09:09:55.8060065Z tests/test_backend_msword.py::test_e2e_docx_conversions PASSED           [ 41%]
2025-04-11T09:09:55.8067460Z tests/test_backend_patent_uspto.py::test_patent_export SKIPPED (Slow...) [ 43%]
2025-04-11T09:09:58.2996667Z tests/test_backend_patent_uspto.py::test_patent_groundtruth PASSED       [ 44%]
2025-04-11T09:09:58.3082405Z tests/test_backend_patent_uspto.py::test_tables PASSED                   [ 46%]
2025-04-11T09:09:58.3091970Z tests/test_backend_patent_uspto.py::test_patent_uspto_ice PASSED         [ 47%]
2025-04-11T09:09:58.3100520Z tests/test_backend_patent_uspto.py::test_patent_uspto_grant_v2 PASSED    [ 49%]
2025-04-11T09:09:58.3109135Z tests/test_backend_patent_uspto.py::test_patent_uspto_app_v1 PASSED      [ 50%]
2025-04-11T09:09:58.3210074Z tests/test_backend_patent_uspto.py::test_patent_uspto_grant_aps PASSED   [ 52%]
2025-04-11T09:09:58.6879569Z tests/test_backend_pdfium.py::test_text_cell_counts PASSED               [ 53%]
2025-04-11T09:09:58.7183298Z tests/test_backend_pdfium.py::test_get_text_from_rect PASSED             [ 55%]
2025-04-11T09:09:58.7719325Z tests/test_backend_pdfium.py::test_crop_page_image PASSED                [ 56%]
2025-04-11T09:09:58.7766810Z tests/test_backend_pdfium.py::test_num_pages PASSED                      [ 58%]
2025-04-11T09:09:58.9520188Z tests/test_backend_pptx.py::test_e2e_pptx_conversions PASSED             [ 59%]
2025-04-11T09:09:59.0142597Z tests/test_cli.py::test_cli_help PASSED                                  [ 61%]
2025-04-11T09:09:59.0222839Z tests/test_cli.py::test_cli_version PASSED                               [ 62%]
2025-04-11T09:10:08.3691642Z tests/test_cli.py::test_cli_convert PASSED                               [ 64%]
2025-04-11T09:10:34.2237299Z tests/test_code_formula.py::test_code_and_formula_conversion PASSED      [ 65%]
2025-04-11T09:10:34.2247720Z tests/test_data_gen_flag.py::test_gen_test_data_flag PASSED              [ 67%]
2025-04-11T09:10:38.1069286Z tests/test_document_picture_classifier.py::test_picture_classifier PASSED [ 68%]
2025-04-11T09:15:10.8358299Z tests/test_e2e_conversion.py::test_e2e_pdfs_conversions PASSED           [ 70%]
2025-04-11T09:16:28.0917594Z tests/test_e2e_ocr_conversion.py::test_e2e_conversions PASSED            [ 71%]
2025-04-11T09:16:28.0992991Z tests/test_input_doc.py::test_in_doc_from_valid_path PASSED              [ 73%]
2025-04-11T09:16:28.1211257Z tests/test_input_doc.py::test_in_doc_from_invalid_path PASSED            [ 74%]
2025-04-11T09:16:28.1270571Z tests/test_input_doc.py::test_in_doc_from_valid_buf PASSED               [ 76%]
2025-04-11T09:16:28.1290370Z tests/test_input_doc.py::test_in_doc_from_invalid_buf PASSED             [ 77%]
2025-04-11T09:16:30.6896908Z tests/test_input_doc.py::test_image_in_pdf_backend PASSED                [ 79%]
2025-04-11T09:16:30.7018270Z tests/test_input_doc.py::test_in_doc_with_page_range PASSED              [ 80%]
2025-04-11T09:16:30.7091740Z tests/test_input_doc.py::test_guess_format PASSED                        [ 82%]
2025-04-11T09:16:37.0402094Z tests/test_interfaces.py::test_convert_path PASSED                       [ 83%]
2025-04-11T09:16:43.1421797Z tests/test_interfaces.py::test_convert_stream PASSED                     [ 85%]
2025-04-11T09:16:43.1462452Z tests/test_invalid_input.py::test_convert_unsupported_doc_format_wout_exception PASSED [ 86%]
2025-04-11T09:16:43.1499632Z tests/test_invalid_input.py::test_convert_unsupported_doc_format_with_exception PASSED [ 88%]
2025-04-11T09:16:43.1537251Z tests/test_invalid_input.py::test_convert_too_small_filesize_limit_wout_exception PASSED [ 89%]
2025-04-11T09:16:43.1572697Z tests/test_invalid_input.py::test_convert_too_small_filesize_limit_with_exception PASSED [ 91%]
2025-04-11T09:17:43.8096731Z tests/test_legacy_format_transform.py::test_compare_legacy_output PASSED [ 92%]
2025-04-11T09:17:43.8144300Z tests/test_options.py::test_accelerator_options PASSED                   [ 94%]
2025-04-11T09:20:42.5842975Z tests/test_options.py::test_e2e_conversions PASSED                       [ 95%]
2025-04-11T09:21:08.1130512Z tests/test_options.py::test_page_range PASSED                            [ 97%]
2025-04-11T09:21:12.2320374Z tests/test_options.py::test_ocr_coverage_threshold PASSED                [ 98%]
2025-04-11T09:21:25.0817818Z tests/test_options.py::test_parser_backends PASSED                       [100%]
2025-04-11T09:21:25.0821042Z 
2025-04-11T09:21:25.0821490Z =============================== warnings summary ===============================
2025-04-11T09:21:25.0822752Z ../../../.cache/pypoetry/virtualenvs/docling-oEGFWdgn-py3.10/lib/python3.10/site-packages/docling_core/types/doc/document.py:3847: 1 warning
2025-04-11T09:21:25.0823821Z tests/test_backend_asciidoc.py: 4 warnings
2025-04-11T09:21:25.0824290Z tests/test_backend_csv.py: 24 warnings
2025-04-11T09:21:25.0824759Z tests/test_backend_docling_json.py: 3 warnings
2025-04-11T09:21:25.0825204Z tests/test_backend_html.py: 28 warnings
2025-04-11T09:21:25.0825603Z tests/test_backend_markdown.py: 17 warnings
2025-04-11T09:21:25.0826051Z tests/test_backend_msexcel.py: 3 warnings
2025-04-11T09:21:25.0826505Z tests/test_backend_msword.py: 31 warnings
2025-04-11T09:21:25.0827007Z tests/test_backend_patent_uspto.py: 14 warnings
2025-04-11T09:21:25.0827420Z tests/test_backend_pptx.py: 6 warnings
2025-04-11T09:21:25.0827785Z tests/test_cli.py: 2 warnings
2025-04-11T09:21:25.0828147Z tests/test_code_formula.py: 1 warning
2025-04-11T09:21:25.0828606Z tests/test_document_picture_classifier.py: 1 warning
2025-04-11T09:21:25.0829319Z tests/test_e2e_conversion.py: 44 warnings
2025-04-11T09:21:25.0829745Z tests/test_e2e_ocr_conversion.py: 40 warnings
2025-04-11T09:21:25.0830199Z tests/test_interfaces.py: 8 warnings
2025-04-11T09:21:25.0830651Z tests/test_legacy_format_transform.py: 6 warnings
2025-04-11T09:21:25.0831106Z tests/test_options.py: 10 warnings
2025-04-11T09:21:25.0832320Z   /home/runner/.cache/pypoetry/virtualenvs/docling-oEGFWdgn-py3.10/lib/python3.10/site-packages/docling_core/types/doc/document.py:3847: DeprecationWarning: deprecated
2025-04-11T09:21:25.0833762Z     if not d.validate_tree(d.body) or not d.validate_tree(d.furniture):
2025-04-11T09:21:25.0834206Z 
2025-04-11T09:21:25.0834446Z tests/test_backend_asciidoc.py::test_asciidocs_examples
2025-04-11T09:21:25.0838045Z   /home/runner/.cache/pypoetry/virtualenvs/docling-oEGFWdgn-py3.10/lib/python3.10/site-packages/pydantic/_internal/_config.py:323: PydanticDeprecatedSince20: Support for class-based `config` is deprecated, use ConfigDict instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/
2025-04-11T09:21:25.0840518Z     warnings.warn(DEPRECATION_MESSAGE, DeprecationWarning)
2025-04-11T09:21:25.0840757Z 
2025-04-11T09:21:25.0840918Z tests/test_backend_csv.py::test_e2e_valid_csv_conversions
2025-04-11T09:21:25.0841873Z   /home/runner/work/docling/docling/docling/backend/csv_backend.py:76: UserWarning: Inconsistent column lengths detected in CSV data. Expected 3 columns, but found rows with varying lengths. Ensure all rows have the same number of columns.
2025-04-11T09:21:25.0842735Z     warnings.warn(
2025-04-11T09:21:25.0842860Z 
2025-04-11T09:21:25.0843006Z tests/test_backend_csv.py::test_e2e_valid_csv_conversions
2025-04-11T09:21:25.0843922Z   /home/runner/work/docling/docling/docling/backend/csv_backend.py:76: UserWarning: Inconsistent column lengths detected in CSV data. Expected 4 columns, but found rows with varying lengths. Ensure all rows have the same number of columns.
2025-04-11T09:21:25.0844772Z     warnings.warn(
2025-04-11T09:21:25.0844880Z 
2025-04-11T09:21:25.0845159Z tests/test_cli.py::test_cli_convert
2025-04-11T09:21:25.0846287Z   /home/runner/work/docling/docling/docling/pipeline/standard_pdf_pipeline.py:226: DeprecationWarning: Field `generate_table_images` is deprecated. To obtain table images, set `PdfPipelineOptions.generate_page_images = True` before conversion and then use the `TableItem.get_image` function.
2025-04-11T09:21:25.0847369Z     and self.pipeline_options.generate_table_images
2025-04-11T09:21:25.0847563Z 
2025-04-11T09:21:25.0847656Z tests/test_code_formula.py: 1 warning
2025-04-11T09:21:25.0847907Z tests/test_e2e_conversion.py: 11 warnings
2025-04-11T09:21:25.0848179Z tests/test_e2e_ocr_conversion.py: 10 warnings
2025-04-11T09:21:25.0848442Z tests/test_interfaces.py: 2 warnings
2025-04-11T09:21:25.0848910Z tests/test_legacy_format_transform.py: 2 warnings
2025-04-11T09:21:25.0849192Z tests/test_options.py: 10 warnings
2025-04-11T09:21:25.0850242Z   /home/runner/work/docling/docling/docling/pipeline/standard_pdf_pipeline.py:215: DeprecationWarning: Field `generate_table_images` is deprecated. To obtain table images, set `PdfPipelineOptions.generate_page_images = True` before conversion and then use the `TableItem.get_image` function.
2025-04-11T09:21:25.0851328Z     or self.pipeline_options.generate_table_images
2025-04-11T09:21:25.0851516Z 
2025-04-11T09:21:25.0851607Z tests/test_e2e_conversion.py: 1 warning
2025-04-11T09:21:25.0851868Z tests/test_e2e_ocr_conversion.py: 10 warnings
2025-04-11T09:21:25.0852130Z tests/test_interfaces.py: 2 warnings
2025-04-11T09:21:25.0852393Z tests/test_legacy_format_transform.py: 1 warning
2025-04-11T09:21:25.0852664Z tests/test_options.py: 10 warnings
2025-04-11T09:21:25.0853706Z   /home/runner/work/docling/docling/docling/pipeline/standard_pdf_pipeline.py:61: DeprecationWarning: Field `generate_table_images` is deprecated. To obtain table images, set `PdfPipelineOptions.generate_page_images = True` before conversion and then use the `TableItem.get_image` function.
2025-04-11T09:21:25.0854780Z     or self.pipeline_options.generate_table_images
2025-04-11T09:21:25.0854971Z 
2025-04-11T09:21:25.0855066Z tests/test_e2e_conversion.py: 11 warnings
2025-04-11T09:21:25.0855334Z tests/test_e2e_ocr_conversion.py: 10 warnings
2025-04-11T09:21:25.0855592Z tests/test_interfaces.py: 2 warnings
2025-04-11T09:21:25.0856040Z   /home/runner/work/docling/docling/tests/verify_utils.py:311: DeprecationWarning: Use document instead.
2025-04-11T09:21:25.0856532Z     doc_pred: DsDocument = doc_result.legacy_document
2025-04-11T09:21:25.0856737Z 
2025-04-11T09:21:25.0856829Z tests/test_e2e_conversion.py: 11 warnings
2025-04-11T09:21:25.0857095Z tests/test_e2e_ocr_conversion.py: 10 warnings
2025-04-11T09:21:25.0857352Z tests/test_interfaces.py: 2 warnings
2025-04-11T09:21:25.0857839Z   /home/runner/work/docling/docling/tests/verify_utils.py:397: DeprecationWarning: Use export_to_doctags() instead.
2025-04-11T09:21:25.0858393Z     doc_pred_dt = doc_result.document.export_to_document_tokens()
2025-04-11T09:21:25.0858619Z 
2025-04-11T09:21:25.0859013Z tests/test_legacy_format_transform.py::test_compare_legacy_output
2025-04-11T09:21:25.0859565Z tests/test_legacy_format_transform.py::test_compare_legacy_output
2025-04-11T09:21:25.0859950Z tests/test_legacy_format_transform.py::test_compare_legacy_output
2025-04-11T09:21:25.0860329Z tests/test_legacy_format_transform.py::test_compare_legacy_output
2025-04-11T09:21:25.0860704Z tests/test_legacy_format_transform.py::test_compare_legacy_output
2025-04-11T09:21:25.0861081Z tests/test_legacy_format_transform.py::test_compare_legacy_output
2025-04-11T09:21:25.0861745Z   /home/runner/work/docling/docling/tests/test_legacy_format_transform.py:51: DeprecationWarning: Use document instead.
2025-04-11T09:21:25.0862316Z     conv_res.legacy_document.model_dump(
2025-04-11T09:21:25.0862509Z 
2025-04-11T09:21:25.0862814Z -- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html
2025-04-11T09:21:25.0863299Z =========== 65 passed, 2 skipped, 359 warnings in 748.02s (0:12:28) ============
2025-04-11T09:21:26.3363985Z ##[group]Run for file in docs/examples/*.py; do
2025-04-11T09:21:26.3364362Z [36;1mfor file in docs/examples/*.py; do[0m
2025-04-11T09:21:26.3364634Z [36;1m  # Skip batch_convert.py[0m
2025-04-11T09:21:26.3365554Z [36;1m  if [[ "$(basename "$file")" =~ ^(batch_convert|minimal_vlm_pipeline|minimal|export_multimodal|custom_convert|develop_picture_enrichment|rapidocr_with_custom_models|offline_convert|pictures_description|pictures_description_api|vlm_pipeline_api_model).py ]]; then[0m
2025-04-11T09:21:26.3366480Z [36;1m      echo "Skipping $file"[0m
2025-04-11T09:21:26.3366708Z [36;1m      continue[0m
2025-04-11T09:21:26.3366903Z [36;1m  fi[0m
2025-04-11T09:21:26.3367069Z [36;1m[0m
2025-04-11T09:21:26.3367251Z [36;1m  echo "Running example $file"[0m
2025-04-11T09:21:26.3367530Z [36;1m  poetry run python "$file" || exit 1[0m
2025-04-11T09:21:26.3367777Z [36;1mdone[0m
2025-04-11T09:21:26.3426608Z shell: /usr/bin/bash -e {0}
2025-04-11T09:21:26.3426846Z env:
2025-04-11T09:21:26.3427068Z   HF_HUB_DOWNLOAD_TIMEOUT: 60
2025-04-11T09:21:26.3427298Z   HF_HUB_ETAG_TIMEOUT: 60
2025-04-11T09:21:26.3427560Z   TESSDATA_PREFIX: /usr/share/tesseract-ocr/5/tessdata
2025-04-11T09:21:26.3427915Z   pythonLocation: /opt/hostedtoolcache/Python/3.10.16/x64
2025-04-11T09:21:26.3428319Z   PKG_CONFIG_PATH: /opt/hostedtoolcache/Python/3.10.16/x64/lib/pkgconfig
2025-04-11T09:21:26.3428927Z   Python_ROOT_DIR: /opt/hostedtoolcache/Python/3.10.16/x64
2025-04-11T09:21:26.3429286Z   Python2_ROOT_DIR: /opt/hostedtoolcache/Python/3.10.16/x64
2025-04-11T09:21:26.3429634Z   Python3_ROOT_DIR: /opt/hostedtoolcache/Python/3.10.16/x64
2025-04-11T09:21:26.3429978Z   LD_LIBRARY_PATH: /opt/hostedtoolcache/Python/3.10.16/x64/lib
2025-04-11T09:21:26.3430270Z ##[endgroup]
2025-04-11T09:21:26.3514103Z Skipping docs/examples/batch_convert.py
2025-04-11T09:21:26.3528621Z Skipping docs/examples/custom_convert.py
2025-04-11T09:21:26.3542620Z Running example docs/examples/develop_formula_understanding.py
2025-04-11T09:21:33.0699210Z INFO:docling.document_converter:Going to convert document batch...
2025-04-11T09:21:33.0700463Z INFO:docling.document_converter:Initializing pipeline for ExampleFormulaUnderstandingPipeline with options hash 62d299dedcae39e917ecf1d733c34570
2025-04-11T09:21:33.0821818Z INFO:docling.models.factories.base_factory:Loading plugin 'docling_defaults'
2025-04-11T09:21:33.0823324Z INFO:docling.models.factories:Registered ocr engines: ['easyocr', 'ocrmac', 'rapidocr', 'tesserocr', 'tesseract']
2025-04-11T09:21:33.1248129Z INFO:docling.utils.accelerator_utils:Accelerator device: 'cpu'
2025-04-11T09:21:34.8109464Z INFO:docling.utils.accelerator_utils:Accelerator device: 'cpu'
2025-04-11T09:21:35.5002701Z INFO:docling.utils.accelerator_utils:Accelerator device: 'cpu'
2025-04-11T09:21:35.8300448Z INFO:docling.models.factories.base_factory:Loading plugin 'docling_defaults'
2025-04-11T09:21:35.8301592Z INFO:docling.models.factories:Registered picture descriptions: ['vlm', 'api']
2025-04-11T09:21:35.8302720Z INFO:docling.pipeline.base_pipeline:Processing document 2203.01017v2.pdf
2025-04-11T09:25:21.6541437Z /usr/bin/xdg-open: 882: www-browser: not found
2025-04-11T09:25:21.6546823Z /usr/bin/xdg-open: 882: www-browser: not found
2025-04-11T09:25:21.6547384Z /usr/bin/xdg-open: 882: www-browser: not found
2025-04-11T09:25:21.6556097Z /usr/bin/xdg-open: 882: links2: not found
2025-04-11T09:25:21.6559629Z /usr/bin/xdg-open: 882: links2: not found
2025-04-11T09:25:21.6567353Z /usr/bin/xdg-open: 882: www-browser: not found
2025-04-11T09:25:21.6571675Z /usr/bin/xdg-open: 882: links2: not found
2025-04-11T09:25:21.6584025Z /usr/bin/xdg-open: 882: elinks: not found
2025-04-11T09:25:21.6596535Z /usr/bin/xdg-open: 882: elinks: not found
2025-04-11T09:25:21.6598298Z /usr/bin/xdg-open: 882: links2: not found
2025-04-11T09:25:21.6598893Z /usr/bin/xdg-open: 882: elinks: not found
2025-04-11T09:25:21.6613898Z /usr/bin/xdg-open: 882: links: not found
2025-04-11T09:25:21.6625062Z /usr/bin/xdg-open: 882: /usr/bin/xdg-open: 882: elinks: not found
2025-04-11T09:25:21.6626085Z links: not found
2025-04-11T09:25:21.6634342Z /usr/bin/xdg-open: 882: links: not found
2025-04-11T09:25:21.6637193Z /usr/bin/xdg-open: 882: lynx: not found
2025-04-11T09:25:21.6641843Z /usr/bin/xdg-open: 882: links: not found
2025-04-11T09:25:21.6657389Z /usr/bin/xdg-open: 882: /usr/bin/xdg-open: 882: w3m: not found
2025-04-11T09:25:21.6658133Z xdg-open: no method available for opening '/tmp/tmpuynl38kg.PNG'
2025-04-11T09:25:21.6658629Z lynx: not found
2025-04-11T09:25:21.6671457Z /usr/bin/xdg-open: 882: lynx: not found
2025-04-11T09:25:21.6678442Z /usr/bin/xdg-open: 882: lynx: not found
2025-04-11T09:25:21.6680545Z /usr/bin/xdg-open: 882: w3m: not found
2025-04-11T09:25:21.6681120Z xdg-open: no method available for opening '/tmp/tmp0bqxx9r5.PNG'
2025-04-11T09:25:21.6681697Z /usr/bin/xdg-open: 882: w3m: not found
2025-04-11T09:25:21.6682255Z xdg-open: no method available for opening '/tmp/tmp8pcnykt2.PNG'
2025-04-11T09:25:21.6684224Z /usr/bin/xdg-open: 882: w3m: not found
2025-04-11T09:25:21.6684876Z xdg-open: no method available for opening '/tmp/tmp59nj9y4h.PNG'
2025-04-11T09:25:21.7399604Z INFO:docling.document_converter:Finished converting document 2203.01017v2.pdf in 228.75 sec.
2025-04-11T09:25:22.8551216Z Skipping docs/examples/develop_picture_enrichment.py
2025-04-11T09:25:22.8564178Z Running example docs/examples/export_figures.py
2025-04-11T09:25:27.9239674Z INFO:docling.document_converter:Going to convert document batch...
2025-04-11T09:25:27.9240537Z INFO:docling.document_converter:Initializing pipeline for StandardPdfPipeline with options hash f2c11214fea737e1772f0045681dc260
2025-04-11T09:25:27.9345153Z INFO:docling.models.factories.base_factory:Loading plugin 'docling_defaults'
2025-04-11T09:25:27.9346780Z INFO:docling.models.factories:Registered ocr engines: ['easyocr', 'ocrmac', 'rapidocr', 'tesserocr', 'tesseract']
2025-04-11T09:25:27.9648148Z INFO:docling.utils.accelerator_utils:Accelerator device: 'cpu'
2025-04-11T09:25:29.6552776Z INFO:docling.utils.accelerator_utils:Accelerator device: 'cpu'
2025-04-11T09:25:30.3116396Z INFO:docling.utils.accelerator_utils:Accelerator device: 'cpu'
2025-04-11T09:25:30.6401552Z INFO:docling.models.factories.base_factory:Loading plugin 'docling_defaults'
2025-04-11T09:25:30.6402699Z INFO:docling.models.factories:Registered picture descriptions: ['vlm', 'api']
2025-04-11T09:25:30.6403747Z INFO:docling.pipeline.base_pipeline:Processing document 2206.01062.pdf
2025-04-11T09:27:01.4320915Z INFO:docling.document_converter:Finished converting document 2206.01062.pdf in 93.52 sec.
2025-04-11T09:27:03.9376996Z INFO:__main__:Document converted and figures exported in 96.03 seconds.
2025-04-11T09:27:04.8703770Z Skipping docs/examples/export_multimodal.py
2025-04-11T09:27:04.8716924Z Running example docs/examples/export_tables.py
2025-04-11T09:27:10.0114680Z INFO:docling.document_converter:Going to convert document batch...
2025-04-11T09:27:10.0115945Z INFO:docling.document_converter:Initializing pipeline for StandardPdfPipeline with options hash 3d2abd0e021741887551c73bd132b421
2025-04-11T09:27:10.0221438Z INFO:docling.models.factories.base_factory:Loading plugin 'docling_defaults'
2025-04-11T09:27:10.0222803Z INFO:docling.models.factories:Registered ocr engines: ['easyocr', 'ocrmac', 'rapidocr', 'tesserocr', 'tesseract']
2025-04-11T09:27:10.0511471Z INFO:docling.utils.accelerator_utils:Accelerator device: 'cpu'
2025-04-11T09:27:11.7429777Z INFO:docling.utils.accelerator_utils:Accelerator device: 'cpu'
2025-04-11T09:27:12.4284028Z INFO:docling.utils.accelerator_utils:Accelerator device: 'cpu'
2025-04-11T09:27:12.7620381Z INFO:docling.models.factories.base_factory:Loading plugin 'docling_defaults'
2025-04-11T09:27:12.7621128Z INFO:docling.models.factories:Registered picture descriptions: ['vlm', 'api']
2025-04-11T09:27:12.7623133Z INFO:docling.pipeline.base_pipeline:Processing document 2206.01062.pdf
2025-04-11T09:28:41.4707083Z INFO:docling.document_converter:Finished converting document 2206.01062.pdf in 91.47 sec.
2025-04-11T09:28:41.4784833Z INFO:__main__:Saving CSV table to scratch/2206.01062-table-1.csv
2025-04-11T09:28:41.4811891Z INFO:__main__:Saving HTML table to scratch/2206.01062-table-1.html
2025-04-11T09:28:41.5951675Z INFO:__main__:Saving CSV table to scratch/2206.01062-table-2.csv
2025-04-11T09:28:41.5957934Z INFO:__main__:Saving HTML table to scratch/2206.01062-table-2.html
2025-04-11T09:28:41.6273593Z ## Table 0
2025-04-11T09:28:41.6274274Z INFO:__main__:Saving CSV table to scratch/2206.01062-table-3.csv
2025-04-11T09:28:41.6277526Z |    | class label    |   Count |   % of Total.Train |   % of Total.Test |   % of Total.Val | triple inter-annotator mAP @0.5-0.95 (%).All   | triple inter-annotator mAP @0.5-0.95 (%).Fin   | triple inter-annotator mAP @0.5-0.95 (%).Man   | triple inter-annotator mAP @0.5-0.95 (%).Sci   | triple inter-annotator mAP @0.5-0.95 (%).Law   | triple inter-annotator mAP @0.5-0.95 (%).Pat   | triple inter-annotator mAP @0.5-0.95 (%).Ten   |
2025-04-11T09:28:41.6281645Z |---:|:---------------|--------:|-------------------:|------------------:|-----------------:|:-----------------------------------------------|:-----------------------------------------------|:-----------------------------------------------|:-----------------------------------------------|:-----------------------------------------------|:-----------------------------------------------|:-----------------------------------------------|
2025-04-11T09:28:41.6284115Z |  0 | Caption        |   22524 |               2.04 |              1.77 |             2.32 | 84-89                                          | 40-61                                          | 86-92                                          | 94-99                                          | 95-99                                          | 69-78                                          | n/a                                            |
2025-04-11T09:28:41.6285684Z |  1 | Footnote       |    6318 |               0.6  |              0.31 |             0.58 | 83-91                                          | n/a                                            | 100                                            | 62-88                                          | 85-94                                          | n/a                                            | 82-97                                          |
2025-04-11T09:28:41.6286851Z |  2 | Formula        |   25027 |               2.25 |              1.9  |             2.96 | 83-85                                          | n/a                                            | n/a                                            | 84-87                                          | 86-96                                          | n/a                                            | n/a                                            |
2025-04-11T09:28:41.6287926Z |  3 | List-item      |  185660 |              17.19 |             13.34 |            15.82 | 87-88                                          | 74-83                                          | 90-92                                          | 97-97                                          | 81-85                                          | 75-88                                          | 93-95                                          |
2025-04-11T09:28:41.6289925Z |  4 | Page-footer    |   70878 |               6.51 |              5.58 |             6    | 93-94                                          | 88-90                                          | 95-96                                          | 100                                            | 92-97                                          | 100                                            | 96-98                                          |
2025-04-11T09:28:41.6291536Z |  5 | Page-header    |   58022 |               5.1  |              6.7  |             5.06 | 85-89                                          | 66-76                                          | 90-94                                          | 98-100                                         | 91-92                                          | 97-99                                          | 81-86                                          |
2025-04-11T09:28:41.6292851Z |  6 | Picture        |   45976 |               4.21 |              2.78 |             5.31 | 69-71                                          | 56-59                                          | 82-86                                          | 69-82                                          | 80-95                                          | 66-71                                          | 59-76                                          |
2025-04-11T09:28:41.6294119Z |  7 | Section-header |  142884 |              12.6  |             15.77 |            12.85 | 83-84                                          | 76-81                                          | 90-92                                          | 94-95                                          | 87-94                                          | 69-73                                          | 78-86                                          |
2025-04-11T09:28:41.6295378Z |  8 | Table          |   34733 |               3.2  |              2.27 |             3.6  | 77-81                                          | 75-80                                          | 83-86                                          | 98-99                                          | 58-80                                          | 79-84                                          | 70-85                                          |
2025-04-11T09:28:41.6296544Z |  9 | Text           |  510377 |              45.82 |             49.28 |            45    | 84-86                                          | 81-86                                          | 88-93                                          | 89-93                                          | 87-92                                          | 71-79                                          | 87-95                                          |
2025-04-11T09:28:41.6297728Z | 10 | Title          |    5071 |               0.47 |              0.3  |             0.5  | 60-72                                          | 24-63                                          | 50-63                                          | 94-100                                         | 82-96                                          | 68-79                                          | 24-56                                          |
2025-04-11T09:28:41.6299245Z | 11 | Total          | 1107470 |          941123    |          99816    |         66531    | 82-83                                          | 71-74                                          | 79-81                                          | 89-94                                          | 86-91                                          | 71-76                                          | 68-85                                          |
2025-04-11T09:28:41.6300129Z ## Table 1
2025-04-11T09:28:41.6300574Z |    |                | human.   |   MRCNN.R50 |   MRCNN.R101 |   FRCNN.R101 |   YOLO.v5x6 |
2025-04-11T09:28:41.6301519Z |---:|:---------------|:---------|------------:|-------------:|-------------:|------------:|
2025-04-11T09:28:41.6302192Z |  0 | Caption        | 84-89    |        68.4 |         71.5 |         70.1 |        77.7 |
2025-04-11T09:28:41.6302811Z |  1 | Footnote       | 83-91    |        70.9 |         71.8 |         73.7 |        77.2 |
2025-04-11T09:28:41.6303442Z |  2 | Formula        | 83-85    |        60.1 |         63.4 |         63.5 |        66.2 |
2025-04-11T09:28:41.6304050Z |  3 | List-item      | 87-88    |        81.2 |         80.8 |         81   |        86.2 |
2025-04-11T09:28:41.6304707Z |  4 | Page-footer    | 93-94    |        61.6 |         59.3 |         58.9 |        61.1 |
2025-04-11T09:28:41.6305381Z |  5 | Page-header    | 85-89    |        71.9 |         70   |         72   |        67.9 |
2025-04-11T09:28:41.6306062Z |  6 | Picture        | 69-71    |        71.7 |         72.7 |         72   |        77.1 |
2025-04-11T09:28:41.6307020Z |  7 | Section-header | 83-84    |        67.6 |         69.3 |         68.4 |        74.6 |
2025-04-11T09:28:41.6307731Z |  8 | Table          | 77-81    |        82.2 |         82.9 |         82.2 |        86.3 |
2025-04-11T09:28:41.6308314Z |  9 | Text           | 84-86    |        84.6 |         85.8 |         85.4 |        88.1 |
2025-04-11T09:28:41.6309159Z | 10 | Title          | 60-72    |        76.7 |         80.4 |         79.9 |        82.7 |
2025-04-11T09:28:41.6309736Z | 11 | All            | 82-83    |        72.4 |         73.5 |         73.4 |        76.8 |
2025-04-11T09:28:41.6310003Z ## Table 2
2025-04-11T09:28:41.6310273Z INFO:__main__:Saving HTML table to scratch/2206.01062-table-3.html
2025-04-11T09:28:41.6489680Z INFO:__main__:Saving CSV table to scratch/2206.01062-table-4.csv
2025-04-11T09:28:41.6496463Z INFO:__main__:Saving HTML table to scratch/2206.01062-table-4.html
2025-04-11T09:28:41.6718998Z INFO:__main__:Saving CSV table to scratch/2206.01062-table-5.csv
2025-04-11T09:28:41.6725561Z INFO:__main__:Saving HTML table to scratch/2206.01062-table-5.html
2025-04-11T09:28:41.6945997Z INFO:__main__:Document converted and tables exported in 91.70 seconds.
2025-04-11T09:28:41.7038989Z |    | Class-count    |   11 | 6       | 5       | 4       |
2025-04-11T09:28:41.7039706Z |---:|:---------------|-----:|:--------|:--------|:--------|
2025-04-11T09:28:41.7040322Z |  0 | Caption        |   68 | Text    | Text    | Text    |
2025-04-11T09:28:41.7040980Z |  1 | Footnote       |   71 | Text    | Text    | Text    |
2025-04-11T09:28:41.7041615Z |  2 | Formula        |   60 | Text    | Text    | Text    |
2025-04-11T09:28:41.7042164Z |  3 | List-item      |   81 | Text    | 82      | Text    |
2025-04-11T09:28:41.7042739Z |  4 | Page-footer    |   62 | 62      | -       | -       |
2025-04-11T09:28:41.7043337Z |  5 | Page-header    |   72 | 68      | -       | -       |
2025-04-11T09:28:41.7043898Z |  6 | Picture        |   72 | 72      | 72      | 72      |
2025-04-11T09:28:41.7044492Z |  7 | Section-header |   68 | 67      | 69      | 68      |
2025-04-11T09:28:41.7045051Z |  8 | Table          |   82 | 83      | 82      | 82      |
2025-04-11T09:28:41.7045622Z |  9 | Text           |   85 | 84      | 84      | 84      |
2025-04-11T09:28:41.7046273Z | 10 | Title          |   77 | Sec.-h. | Sec.-h. | Sec.-h. |
2025-04-11T09:28:41.7046797Z | 11 | Overall        |   72 | 73      | 78      | 77      |
2025-04-11T09:28:41.7047186Z ## Table 3
2025-04-11T09:28:41.7047573Z |    | Class-count.Split   |   11.Doc |   11.Page | 5.Doc   | 5.Page   |
2025-04-11T09:28:41.7048145Z |---:|:--------------------|---------:|----------:|:--------|:---------|
2025-04-11T09:28:41.7048950Z |  0 | Caption             |       68 |        83 |         |          |
2025-04-11T09:28:41.7049447Z |  1 | Footnote            |       71 |        84 |         |          |
2025-04-11T09:28:41.7049947Z |  2 | Formula             |       60 |        66 |         |          |
2025-04-11T09:28:41.7050458Z |  3 | List-item           |       81 |        88 | 82      | 88       |
2025-04-11T09:28:41.7051443Z |  4 | Page-footer         |       62 |        89 |         |          |
2025-04-11T09:28:41.7051983Z |  5 | Page-header         |       72 |        90 |         |          |
2025-04-11T09:28:41.7052530Z |  6 | Picture             |       72 |        82 | 72      | 82       |
2025-04-11T09:28:41.7053068Z |  7 | Section-header      |       68 |        83 | 69      | 83       |
2025-04-11T09:28:41.7053643Z |  8 | Table               |       82 |        89 | 82      | 90       |
2025-04-11T09:28:41.7054169Z |  9 | Text                |       85 |        91 | 84      | 90       |
2025-04-11T09:28:41.7054668Z | 10 | Title               |       77 |        81 |         |          |
2025-04-11T09:28:41.7055156Z | 11 | All                 |       72 |        84 | 78      | 87       |
2025-04-11T09:28:41.7055528Z ## Table 4
2025-04-11T09:28:41.7055949Z |    | Training on     | labels     |   Testing on.PLN | Testing on.DB   |   Testing on.DLN |
2025-04-11T09:28:41.7056852Z |---:|:----------------|:-----------|-----------------:|:----------------|-----------------:|
2025-04-11T09:28:41.7057517Z |  0 | PubLayNet (PLN) | Figure     |               96 | 43              |               23 |
2025-04-11T09:28:41.7058175Z |  1 | PubLayNet (PLN) | Sec-header |               87 | -               |               32 |
2025-04-11T09:28:41.7058967Z |  2 |                 | Table      |               95 | 24              |               49 |
2025-04-11T09:28:41.7059480Z |  3 |                 | Text       |               96 | -               |               42 |
2025-04-11T09:28:41.7059972Z |  4 |                 | total      |               93 | 34              |               30 |
2025-04-11T09:28:41.7060513Z |  5 | DocBank (DB)    | Figure     |               77 | 71              |               31 |
2025-04-11T09:28:41.7061091Z |  6 | DocBank (DB)    | Table      |               19 | 65              |               22 |
2025-04-11T09:28:41.7061678Z |  7 | DocBank (DB)    | total      |               48 | 68              |               27 |
2025-04-11T09:28:41.7062287Z |  8 | DocLayNet (DLN) | Figure     |               67 | 51              |               72 |
2025-04-11T09:28:41.7062931Z |  9 | DocLayNet (DLN) | Sec-header |               53 | -               |               68 |
2025-04-11T09:28:41.7063537Z | 10 |                 | Table      |               87 | 43              |               82 |
2025-04-11T09:28:41.7064046Z | 11 |                 | Text       |               77 | -               |               84 |
2025-04-11T09:28:41.7064555Z | 12 |                 | total      |               59 | 47              |               78 |
2025-04-11T09:28:42.7069384Z Running example docs/examples/full_page_ocr.py
2025-04-11T09:30:26.8463885Z ## DocLayNet: A Large Human-Annotated Dataset for Document-Layout Analysis
2025-04-11T09:30:26.8464447Z 
2025-04-11T09:30:26.8464766Z Birgit Pfitzmann IBM Research Rueschlikon, Switzerland bpf@zurich.ibm.com
2025-04-11T09:30:26.8465245Z 
2025-04-11T09:30:26.8465531Z Christoph Auer IBM Research Rueschlikon, Switzerland cau@zurich.ibm.com
2025-04-11T09:30:26.8465954Z 
2025-04-11T09:30:26.8466228Z Michele Dolfi IBM Research Rueschlikon, Switzerland dol@zurich.ibm.com
2025-04-11T09:30:26.8466642Z 
2025-04-11T09:30:26.8466931Z Ahmed S. Nassar IBM Research Rueschlikon, Switzerland ahn@zurich.ibm.com
2025-04-11T09:30:26.8467350Z 
2025-04-11T09:30:26.8467623Z Peter Staar IBM Research Rueschlikon, Switzerland taa@zurich.ibm.com
2025-04-11T09:30:26.8468021Z 
2025-04-11T09:30:26.8468129Z ## ABSTRACT
2025-04-11T09:30:26.8468278Z 
2025-04-11T09:30:26.8476819Z Accurate document layout analysis is a key requirement for highquality PDF document conversion. With the recent availability of public, large ground-truth datasets such as PubLayNet and DocBank, deep-learning models have proven to be very effective at layout detection and segmentation. While these datasets are of adequate size to train such models, they severely lack in layout variability since they are sourced from scientific article repositories such as PubMed and arXiv only. Consequently, the accuracy of the layout segmentation drops significantly when these models are applied on more challenging and diverse layouts. In this paper, we present DocLayNet, a new, publicly available, document-layout annotation dataset in COCO format. It contains 80863 manually annotated pages from diverse data sources to represent a wide variability in layouts. For each PDF page, the layout annotations provide labelled bounding-boxes with a choice of 11 distinct classes. DocLayNet also provides a subset of doubleand triple-annotated pages to determine the inter-annotator agreement. In multiple experiments, we provide baseline accuracy scores (in mAP) for a set of popular object detection models. We also demonstrate that these models fall approximately 10% behind the inter-annotator agreement. Furthermore, we provide evidence that DocLayNet is of sufficient size. Lastly, we compare models trained on PubLayNet, DocBank and DocLayNet, showing that layout predictions of the DocLayNettrained models are more robust and thus the preferred choice for general-purpose document-layout analysis.
2025-04-11T09:30:26.8483650Z 
2025-04-11T09:30:26.8483733Z ## CCS CONCEPTS
2025-04-11T09:30:26.8483857Z 
2025-04-11T09:30:26.8486490Z -Information systems — Document structure; + Applied computing — Document analysis; -Computing methodologies — Machine learning; Computer vision; Object detection;
2025-04-11T09:30:26.8487567Z 
2025-04-11T09:30:26.8489755Z Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for third-party components of this work must be honored. For all other uses, contact the owner/author(s).
2025-04-11T09:30:26.8491651Z 
2025-04-11T09:30:26.8491723Z KDD
2025-04-11T09:30:26.8491826Z 
2025-04-11T09:30:26.8491979Z ‘22,
2025-04-11T09:30:26.8492148Z 
2025-04-11T09:30:26.8492210Z August
2025-04-11T09:30:26.8492302Z 
2025-04-11T09:30:26.8492385Z 14-18,
2025-04-11T09:30:26.8492472Z 
2025-04-11T09:30:26.8492546Z 2022,
2025-04-11T09:30:26.8492636Z 
2025-04-11T09:30:26.8492710Z Washington,
2025-04-11T09:30:26.8492805Z 
2025-04-11T09:30:26.8492865Z DC,
2025-04-11T09:30:26.8492956Z 
2025-04-11T09:30:26.8493014Z USA
2025-04-11T09:30:26.8493098Z 
2025-04-11T09:30:26.8493179Z ©
2025-04-11T09:30:26.8493265Z 
2025-04-11T09:30:26.8493324Z 2022
2025-04-11T09:30:26.8493408Z 
2025-04-11T09:30:26.8493483Z Copyright held
2025-04-11T09:30:26.8493592Z 
2025-04-11T09:30:26.8493657Z by the
2025-04-11T09:30:26.8493744Z 
2025-04-11T09:30:26.8493816Z owner/author(s).
2025-04-11T09:30:26.8493924Z 
2025-04-11T09:30:26.8493983Z ACM
2025-04-11T09:30:26.8494075Z 
2025-04-11T09:30:26.8494133Z ISBN
2025-04-11T09:30:26.8494219Z 
2025-04-11T09:30:26.8494294Z 978-1-4503-9385-0/22/08.
2025-04-11T09:30:26.8494432Z 
2025-04-11T09:30:26.8494542Z https://doi.org/10.1145/3534678.3539043
2025-04-11T09:30:26.8494718Z 
2025-04-11T09:30:26.8494940Z Figure 1: Four examples of complex page layouts across different document categories
2025-04-11T09:30:26.8495232Z 
2025-04-11T09:30:26.8495315Z <!-- image -->
2025-04-11T09:30:26.8495418Z 
2025-04-11T09:30:26.8495490Z ## KEYWORDS
2025-04-11T09:30:26.8495583Z 
2025-04-11T09:30:26.8495834Z PDF document conversion, layout segmentation, object-detection, data set, Machine Learning
2025-04-11T09:30:26.8496159Z 
2025-04-11T09:30:26.8496235Z ## ACM Reference Format:
2025-04-11T09:30:26.8496369Z 
2025-04-11T09:30:26.8497414Z Birgit Pfitzmann, Christoph Auer, Michele Dolfi, Ahmed S. Nassar, and Peter Staar. 2022. DocLayNet: A Large Human-Annotated Dataset for DocumentLayout Analysis. In Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD '22), August 14-18, 2022, Washington, DC, USA. ACM, New York, NY, USA, 9 pages. https://doi.org/10.1145/ 3534678.3539043
2025-04-11T09:30:26.8498527Z 
2025-04-11T09:30:26.8499009Z ## 1 INTRODUCTION
2025-04-11T09:30:26.8499129Z 
2025-04-11T09:30:26.8501196Z Despite the substantial improvements achieved with machine-learning (ML) approaches and deep neural networks in recent years, document conversion remains a challenging problem, as demonstrated by the numerous public competitions held on this topic [1-4]. The challenge originates from the huge variability in PDF documents regarding layout, language and formats (scanned, programmatic or a combination of both). Engineering a single ML model that can be applied on all types of documents and provides high-quality layout segmentation remains to this day extremely challenging [5]. To highlight the variability in document layouts, we show a few example documents from the DocLayNet dataset in Figure 1.
2025-04-11T09:30:26.8503315Z 
2025-04-11T09:30:26.8508110Z A key problem in the process of document conversion is to understand the structure of a single document page, i.e. which segments of text should be grouped together in a unit. To train models for this task, there are currently two large datasets available to the community, PubLayNet [6] and DocBank [7]. They were introduced in 2019 and 2020 respectively and significantly accelerated the implementation of layout detection and segmentation models due to their sizes of 300K and 500K ground-truth pages. These sizes were achieved by leveraging an automation approach. The benefit of automated ground-truth generation is obvious: one can generate large ground-truth datasets at virtually no cost. However, the automation introduces a constraint on the variability in the dataset, because corresponding structured source data must be available. PubLayNet and DocBank were both generated from scientific document repositories (PubMed and arXiv), which provide XML or KIEX sources. Those scientific documents present a limited variability in their layouts, because they are typeset in uniform templates provided by the publishers. Obviously, documents such as technical manuals, annual company reports, legal text, government tenders, etc. have very different and partially unique layouts. As a consequence, the layout predictions obtained from models trained on PubLayNet or DocBank is very reasonable when applied on scientific documents. However, for more artistic or free-style layouts, we see sub-par prediction quality from these models, which we demonstrate in Section 5.
2025-04-11T09:30:26.8513350Z 
2025-04-11T09:30:26.8514717Z In this paper, we present the DocLayNet dataset. It provides pageby-page layout annotation ground-truth using bounding-boxes for 11 distinct class labels on 80863 unique document pages, of which a fraction carry doubleor triple-annotations. DocLayNet is similar in spirit to PubLayNet and DocBank and will likewise be made available to the public! in order to stimulate the document-layout analysis community. It distinguishes itself in the following aspects:
2025-04-11T09:30:26.8516148Z 
2025-04-11T09:30:26.8516556Z - (1) Human Annotation: In contrast to PubLayNet and DocBank, we relied on human annotation instead of automation approaches to generate the data set.
2025-04-11T09:30:26.8517318Z - (2) Large Layout Variability: We include diverse and complex layouts from a large variety of public sources.
2025-04-11T09:30:26.8518169Z - (3) Detailed Label Set: We define 11 class labels to distinguish layout features in high detail. PubLayNet provides 5 labels; DocBank provides 13, although not a superset of ours.
2025-04-11T09:30:26.8519221Z - (4) Redundant Annotations: A fraction of the pages in the DocLayNet data set carry more than one human annotation.
2025-04-11T09:30:26.8519603Z 
2025-04-11T09:30:26.8519761Z Thttps://developeribm.com/exchanges/data/all/doclaynet
2025-04-11T09:30:26.8519979Z 
2025-04-11T09:30:26.8520215Z This enables experimentation with annotation uncertainty and quality control analysis.
2025-04-11T09:30:26.8520537Z 
2025-04-11T09:30:26.8521369Z - (5) Pre-defined Train-, Test-&amp; Validation-set: Like DocBank, we provide fixed train-, test-&amp; validation-sets to ensure proportional representation of the class-labels. Further, we prevent leakage of unique layouts across sets, which has a large effect on model accuracy scores.
2025-04-11T09:30:26.8522424Z 
2025-04-11T09:30:26.8523258Z All aspects outlined above are detailed in Section 3. In Section 4, we will elaborate on how we designed and executed this large-scale human annotation campaign. We will also share key insights and lessons learned that might prove helpful for other parties planning to set up annotation campaigns.
2025-04-11T09:30:26.8524181Z 
2025-04-11T09:30:26.8525737Z In Section 5, we will present baseline accuracy numbers for a variety of object detection methods (Faster R-CNN, Mask R-CNN and YOLOv5) trained on DocLayNet. We further show how the model performance is impacted by varying the DocLayNet dataset size, reducing the label set and modifying the train/test-split. Last but not least, we compare the performance of models trained on PubLayNet, DocBank and DocLayNet and demonstrate that a model trained on DocLayNet provides overall more robust layout recovery.
2025-04-11T09:30:26.8527264Z 
2025-04-11T09:30:26.8527339Z ## 2 RELATED WORK
2025-04-11T09:30:26.8527448Z 
2025-04-11T09:30:26.8529520Z While early approaches in document-layout analysis used rulebased algorithms and heuristics [8], the problem is lately addressed with deep learning methods. The most common approach is to leverage object detection models [9-15]. In the last decade, the accuracy and speed of these models has increased dramatically. Furthermore, most state-of-the-art object detection methods can be trained and applied with very little work, thanks to a standardisation effort of the ground-truth data format [16] and common deep-learning frameworks [17]. Reference data sets such as PubLayNet [6] and DocBank provide their data in the commonly accepted COCO format [16].
2025-04-11T09:30:26.8531562Z 
2025-04-11T09:30:26.8533038Z Lately, new types of ML models for document-layout analysis have emerged in the community [18-21]. These models do not approach the problem of layout analysis purely based on an image representation of the page, as computer vision methods do. Instead, they combine the text tokens and image representation of a page in order to obtain a segmentation. While the reported accuracies appear to be promising, a broadly accepted data format which links geometric and textual features has yet to establish.
2025-04-11T09:30:26.8534535Z 
2025-04-11T09:30:26.8534613Z ## 3 THE DOCLAYNET DATASET
2025-04-11T09:30:26.8534751Z 
2025-04-11T09:30:26.8536264Z DocLayNet contains 80863 PDF pages. Among these, 7059 carry two instances of human annotations, and 1591 carry three. This amounts to 91104 total annotation instances. The annotations provide layout information in the shape of labeled, rectangular boundingboxes. We define 11 distinct labels for layout features, namely Caption, Footnote, Formula, List-item, Page-footer, Page-header, Picture, Section-header, Table, Text, and Title. Our reasoning for picking this particular label set is detailed in Section 4.
2025-04-11T09:30:26.8537857Z 
2025-04-11T09:30:26.8538379Z In addition to open intellectual property constraints for the source documents, we required that the documents in DocLayNet adhere to a few conditions. Firstly, we kept scanned documents
2025-04-11T09:30:26.8539291Z 
2025-04-11T09:30:26.8539473Z Figure 2: Distribution of DocLayNet pages across document categories.
2025-04-11T09:30:26.8539755Z 
2025-04-11T09:30:26.8539859Z <!-- image -->
2025-04-11T09:30:26.8540051Z 
2025-04-11T09:30:26.8542551Z to a minimum, since they introduce difficulties in annotation (see Section 4). As a second condition, we focussed on medium to large documents (&gt; 10 pages) with technical content, dense in complex tables, figures, plots and captions. Such documents carry a lot of information value, but are often hard to analyse with high accuracy due to their challenging layouts. Counterexamples of documents not included in the dataset are receipts, invoices, hand-written documents or photographs showing "text in the wild".
2025-04-11T09:30:26.8545277Z 
2025-04-11T09:30:26.8548608Z The pages in DocLayNet can be grouped into six distinct categories, namely Financial Reports, Manuals, Scientific Articles, Laws &amp; Regulations, Patents and Government Tenders. Each document category was sourced from various repositories. For example, Financial Reports contain both free-style format annual reports? which expose company-specific, artistic layouts as well as the more formal SEC filings. The two largest categories (Financial Reports and Manuals) contain a large amount of free-style layouts in order to obtain maximum variability. In the other four categories, we boosted the variability by mixing documents from independent providers, such as different government websites or publishers. In Figure 2, we show the document categories contained in DocLayNet with their respective sizes.
2025-04-11T09:30:26.8552244Z 
2025-04-11T09:30:26.8554608Z We did not control the document selection with regard to language. The vast majority of documents contained in DocLayNet (close to 95%) are published in English language. However, DocLayNet also contains a number of documents in other languages such as German (2.5%), French (1.0%) and Japanese (1.0%). While the document language has negligible impact on the performance of computer vision methods such as object detection and segmentation models, it might prove challenging for layout analysis methods which exploit textual features.
2025-04-11T09:30:26.8556775Z 
2025-04-11T09:30:26.8558988Z To ensure that future benchmarks in the document-layout analysis community can be easily compared, we have split up DocLayNet into pre-defined train-, testand validation-sets. In this way, we can avoid spurious variations in the evaluation scores due to random splitting in train-, testand validation-sets. We also ensured that less frequent labels are represented in train and test sets in equal proportions.
2025-04-11T09:30:26.8560475Z 
2025-04-11T09:30:26.8560650Z 2e.g. AAPL from https://www.annualreports.com/
2025-04-11T09:30:26.8561004Z 
2025-04-11T09:30:26.8562859Z Table 1 shows the overall frequency and distribution of the labels among the different sets. Importantly, we ensure that subsets are only split on full-document boundaries. This avoids that pages of the same document are spread over train, test and validation set, which can give an undesired evaluation advantage to models and lead to overestimation of their prediction accuracy. We will show the impact of this decision in Section 5.
2025-04-11T09:30:26.8564726Z 
2025-04-11T09:30:26.8567731Z In order to accommodate the different types of models currently in use by the community, we provide DocLayNet in an augmented COCO format [16]. This entails the standard COCO ground-truth file (in JSON format) with the associated page images (in PNG format, 1025x1025 pixels). Furthermore, custom fields have been added to each COCO record to specify document category, original document filename and page number. In addition, we also provide the original PDF pages, as well as sidecar files containing parsed PDF text and text-cell coordinates (in JSON). All additional files are linked to the primary page images by their matching filenames.
2025-04-11T09:30:26.8570491Z 
2025-04-11T09:30:26.8580053Z Despite being cost-intense and far less scalable than automation, human annotation has several benefits over automated groundtruth generation. The first and most obvious reason to leverage human annotations is the freedom to annotate any type of document without requiring a programmatic source. For most PDF documents, the original source document is not available. The latter is not a hard constraint with human annotation, but it is for automated methods. A second reason to use human annotations is that the latter usually provide a more natural interpretation of the page layout. The human-interpreted layout can significantly deviate from the programmatic layout used in typesetting. For example, "invisible" tables might be used solely for aligning text paragraphs on columns. Such typesetting tricks might be interpreted by automated methods incorrectly as an actual table, while the human annotation will interpret it correctly as Text or other styles. The same applies to multi-line text elements, when authors decided to space them as "invisible" list elements without bullet symbols. A third reason to gather ground-truth through human annotation is to estimate a "natural" upper bound on the segmentation accuracy. As we will show in Section 4, certain documents featuring complex layouts can have different but equally acceptable layout interpretations. This natural upper bound for segmentation accuracy can be found by annotating the same pages multiple times by different people and evaluating the inter-annotator agreement. Such a baseline consistency evaluation is very useful to define expectations for a good target accuracy in trained deep neural network models and avoid overfitting (see Table 1). On the flip side, achieving high annotation consistency proved to be a key challenge in human annotation, as we outline in Section 4.
2025-04-11T09:30:26.8589843Z 
2025-04-11T09:30:26.8589983Z ## 4 ANNOTATION CAMPAIGN
2025-04-11T09:30:26.8590196Z 
2025-04-11T09:30:26.8592378Z The annotation campaign was carried out in four phases. In phase one, we identified and prepared the data sources for annotation. In phase two, we determined the class labels and how annotations should be done on the documents in order to obtain maximum consistency. The latter was guided by a detailed requirement analysis and exhaustive experiments. In phase three, we trained the annotation staff and performed exams for quality assurance. In phase four,
2025-04-11T09:30:26.8594712Z 
2025-04-11T09:30:26.8596465Z Table 1: DocLayNet dataset overview. Along with the frequency of each class label, we present the relative occurrence (as % of row "Total") in the train, test and validation sets. The inter-annotator agreement is computed as the mAP@0.5-0.95 metric between pairwise annotations from the triple-annotated pages, from which we obtain accuracy ranges.
2025-04-11T09:30:26.8598171Z 
2025-04-11T09:30:26.8599715Z |                |         | % of Total   | % of Total   | % of Total   | triple inter-annotator mAP @0.5-0.95 (%)   | triple inter-annotator mAP @0.5-0.95 (%)   | triple inter-annotator mAP @0.5-0.95 (%)   | triple inter-annotator mAP @0.5-0.95 (%)   | triple inter-annotator mAP @0.5-0.95 (%)   | triple inter-annotator mAP @0.5-0.95 (%)   | triple inter-annotator mAP @0.5-0.95 (%)   |
2025-04-11T09:30:26.8602072Z |----------------|---------|--------------|--------------|--------------|--------------------------------------------|--------------------------------------------|--------------------------------------------|--------------------------------------------|--------------------------------------------|--------------------------------------------|--------------------------------------------|
2025-04-11T09:30:26.8603685Z | class label    | Count   | Train        | Test         | Val          | All                                        | Fin                                        | Man                                        | Sci                                        | Law                                        | Pat                                        | Ten                                        |
2025-04-11T09:30:26.8604849Z | Caption        | 22524   | 2.04         | 1.77         | 2.32         | 84-89                                      | 40-61                                      | 86-92                                      | 94-99                                      | 95-99                                      | 69-78                                      | n/a                                        |
2025-04-11T09:30:26.8605926Z | Footnote       | 6318    | 0.60         | 0.31         | 0.58         | 83-91                                      | n/a                                        | 100                                        | 62-88                                      | 85-94                                      | n/a                                        | 82-97                                      |
2025-04-11T09:30:26.8607235Z | Formula        | 25027   | 2.25         | 1.90         | 2.96         | 83-85                                      | n/a                                        | n/a                                        | 84-87                                      | 86-96                                      | n/a                                        | n/a                                        |
2025-04-11T09:30:26.8608317Z | List-item      | 185660  | 17.19        | 13.34        | 15.82        | 87-88                                      | 74-83                                      | 90-92                                      | 97-97                                      | 81-85                                      | 75-88                                      | 93-95                                      |
2025-04-11T09:30:26.8609840Z | Page-footer    | 70878   | 6.51         | 5.58         | 6.00         | 93-94                                      | 88-90                                      | 95-96                                      | 100                                        | 92-97                                      | 100                                        | 96-98                                      |
2025-04-11T09:30:26.8611005Z | Page-header    | 58022   | 5.10         | 6.70         | 5.06         | 85-89                                      | 66-76                                      | 90-94                                      | 98-100                                     | 91-92                                      | 97-99                                      | 81-86                                      |
2025-04-11T09:30:26.8612129Z | Picture        | 45976   | 4.21         | 2.78         | 5.31         | 69-71                                      | 56-59                                      | 82-86                                      | 69-82                                      | 80-95                                      | 66-71                                      | 59-76                                      |
2025-04-11T09:30:26.8613203Z | Section-header | 142884  | 12.60        | 15.77        | 12.85        | 83-84                                      | 76-81                                      | 90-92                                      | 94-95                                      | 87-94                                      | 69-73                                      | 78-86                                      |
2025-04-11T09:30:26.8614326Z | Table          | 34733   | 3.20         | 2.27         | 3.60         | 77-81                                      | 75-80                                      | 83-86                                      | 98-99                                      | 58-80                                      | 79-84                                      | 70-85                                      |
2025-04-11T09:30:26.8615464Z | Text           | 510377  | 45.82        | 49.28        | 45.00        | 84-86                                      | 81-86                                      | 88-93                                      | 89-93                                      | 87-92                                      | 71-79                                      | 87-95                                      |
2025-04-11T09:30:26.8616557Z | Title          | 5071    | 0.47         | 0.30         | 0.50         | 60-72                                      | 24-63                                      | 50-63                                      | 94-100                                     | 82-96                                      | 68-79                                      | 24-56                                      |
2025-04-11T09:30:26.8617672Z | Total          | 1107470 | 941123       | 99816        | 66531        | 82-83                                      | 71-74                                      | 79-81                                      | 89-94                                      | 86-91                                      | 71-76                                      | 68-85                                      |
2025-04-11T09:30:26.8618628Z 
2025-04-11T09:30:26.8620232Z Figure 3: Corpus Conversion Service annotation user interface. The PDF page is shown in the background, with overlaid text-cells (in darker shades). The annotation boxes can be drawn by dragging a rectangle over each segment with the respective label from the palette on the right.
2025-04-11T09:30:26.8621787Z 
2025-04-11T09:30:26.8621901Z <!-- image -->
2025-04-11T09:30:26.8622078Z 
2025-04-11T09:30:26.8623178Z we distributed the annotation workload and performed continuous quality controls. Phase one and two required a small team of experts only. For phases three and four, a group of 40 dedicated annotators were assembled and supervised.
2025-04-11T09:30:26.8624406Z 
2025-04-11T09:30:26.8627442Z Phase 1: Data selection and preparation. Our inclusion criteria for documents were described in Section 3. A large effort went into ensuring that all documents are free to use. The data sources include publication repositories such as arXiv", government offices, company websites as well as data directory services for financial reports and patents. Scanned documents were excluded wherever possible because they can be rotated or skewed. This would not allow us to perform annotation with rectangular bounding-boxes and therefore complicate the annotation process.
2025-04-11T09:30:26.8630490Z 
2025-04-11T09:30:26.8634244Z Preparation work included uploading and parsing the sourced PDF documents in the Corpus Conversion Service (CCS) [22], a cloud-native platform which provides a visual annotation interface and allows for dataset inspection and analysis. The annotation interface of CCS is shown in Figure 3. The desired balance of pages between the different document categories was achieved by selective subsampling of pages with certain desired properties. For example, we made sure to include the title page of each document and bias the remaining page selection to those with figures or tables. The latter was achieved by leveraging pre-trained object detection models from PubLayNet, which helped us estimate how many figures and tables a given page contains.
2025-04-11T09:30:26.8638108Z 
2025-04-11T09:30:26.8643897Z Phase 2: Label selection and guideline. We reviewed the collected documents and identified the most common structural features they exhibit. This was achieved by identifying recurrent layout elements and lead us to the definition of 11 distinct class labels. These 11 class labels are Caption, Footnote, Formula, List-item, Pagefooter, Page-header, Picture, Section-header, Table, Text, and Title. Critical factors that were considered for the choice of these class labels were (1) the overall occurrence of the label, (2) the specificity of the label, (3) recognisability on a single page (i.e. no need for context from previous or next page) and (4) overall coverage of the page. Specificity ensures that the choice of label is not ambiguous, while coverage ensures that all meaningful items on a page can be annotated. We refrained from class labels that are very specific to a document category, such as Abstract in the Scientific Articles category. We also avoided class labels that are tightly linked to the semantics of the text. Labels such as Author and Affiliation, as seen in DocBank, are often only distinguishable by discriminating on
2025-04-11T09:30:26.8650014Z 
2025-04-11T09:30:26.8650154Z 3https://arxiv.org/
2025-04-11T09:30:26.8650338Z 
2025-04-11T09:30:26.8650925Z the textual content of an element, which goes beyond visual layout recognition, in particular outside the Scientific Articles category.
2025-04-11T09:30:26.8651642Z 
2025-04-11T09:30:26.8655561Z At first sight, the task of visual document-layout interpretation appears intuitive enough to obtain plausible annotations in most cases. However, during early trial-runs in the core team, we observed many cases in which annotators use different annotation styles, especially for documents with challenging layouts. For example, if a figure is presented with subfigures, one annotator might draw a single figure bounding-box, while another might annotate each subfigure separately. The same applies for lists, where one might annotate all list items in one block or each list item separately. In essence, we observed that challenging layouts would be annotated in different but plausible ways. To illustrate this, we show in Figure 4 multiple examples of plausible but inconsistent annotations on the same pages.
2025-04-11T09:30:26.8659975Z 
2025-04-11T09:30:26.8662434Z Obviously, this inconsistency in annotations is not desirable for datasets which are intended to be used for model training. To minimise these inconsistencies, we created a detailed annotation guideline. While perfect consistency across 40 annotation staff members is clearly not possible to achieve, we saw a huge improvement in annotation consistency after the introduction of our annotation guideline. A few selected, non-trivial highlights of the guideline are:
2025-04-11T09:30:26.8664837Z 
2025-04-11T09:30:26.8665760Z - (1) Every list-item is an individual object instance with class label List-item. This definition is different from PubLayNet and DocBank, where all list-items are grouped together into one List object.
2025-04-11T09:30:26.8667795Z - (2) A List-item is a paragraph with hanging indentation. Singleline elements can qualify as List-item if the neighbour elements expose hanging indentation. Bullet or enumeration symbols are not a requirement.
2025-04-11T09:30:26.8669338Z - (3) For every Caption, there must be exactly one corresponding Picture or Table.
2025-04-11T09:30:26.8670068Z - (4) Connected sub-pictures are grouped together in one Picture object.
2025-04-11T09:30:26.8670661Z - (5) Formula numbers are included in a Formula object.
2025-04-11T09:30:26.8671651Z - (6) Emphasised text (e.g. in italic or bold) at the beginning of a paragraph is not considered a Section-header, unless it appears exclusively on its own line.
2025-04-11T09:30:26.8672503Z 
2025-04-11T09:30:26.8673581Z The complete annotation guideline is over 100 pages long and a detailed description is obviously out of scope for this paper. Nevertheless, it will be made publicly available alongside with DocLayNet for future reference.
2025-04-11T09:30:26.8674770Z 
2025-04-11T09:30:26.8678873Z Phase 3: Training. After a first trial with a small group of people, we realised that providing the annotation guideline and a set of random practice pages did not yield the desired quality level for layout annotation. Therefore we prepared a subset of pages with two different complexity levels, each with a practice and an exam part. 974 pages were reference-annotated by one proficient core team member. Annotation staff were then given the task to annotate the same subsets (blinded from the reference). By comparing the annotations of each staff member with the reference annotations, we could quantify how closely their annotations matched the reference. Only after passing two exam levels with high annotation quality, staff were admitted into the production phase. Practice iterations
2025-04-11T09:30:26.8682949Z 
2025-04-11T09:30:26.8683771Z Figure 4: Examples of plausible annotation alternatives for the same page. Criteria in our annotation guideline can resolve cases A to C, while the case D remains ambiguous.
2025-04-11T09:30:26.8684701Z 
2025-04-11T09:30:26.8684815Z <!-- image -->
2025-04-11T09:30:26.8684969Z 
2025-04-11T09:30:26.8685444Z were carried out over a timeframe of 12 weeks, after which 8 of the 40 initially allocated annotators did not pass the bar.
2025-04-11T09:30:26.8686104Z 
2025-04-11T09:30:26.8691144Z Phase 4: Production annotation. The previously selected 80K pages were annotated with the defined 11 class labels by 32 annotators. This production phase took around three months to complete. All annotations were created online through CCS, which visualises the programmatic PDF text-cells as an overlay on the page. The page annotation are obtained by drawing rectangular bounding-boxes, as shown in Figure 3. With regard to the annotation practices, we implemented a few constraints and capabilities on the tooling level. First, we only allow non-overlapping, vertically oriented, rectangular boxes. For the large majority of documents, this constraint was sufficient and it speeds up the annotation considerably in comparison with arbitrary segmentation shapes. Second, annotator staff were not able to see each other's annotations. This was enforced by design to avoid any bias in the annotation, which could skew the numbers of the inter-annotator agreement (see Table 1). We wanted
2025-04-11T09:30:26.8696503Z 
2025-04-11T09:30:26.8699245Z Table 2: Prediction performance (mAP@0.5-0.95) of object detection networks on DocLayNet test set. The MRCNN (Mask R-CNN) and FRCNN (Faster R-CNN) models with ResNet-50 or ResNet-101 backbone were trained based on the network architectures from the detectron2 model zoo (Mask R-CNN R50, R101-FPN 3x, Faster R-CNN R101-FPN 3x), with default configurations. The YOLO implementation utilized was YOLOv5x6 [13]. All models were initialised using pre-trained weights from the COCO 2017 dataset.
2025-04-11T09:30:26.8700715Z 
2025-04-11T09:30:26.8700834Z |                | human   | MRCNN   | MRCNN   | FRCNN   | YOLO   |
2025-04-11T09:30:26.8701151Z |----------------|---------|---------|---------|---------|--------|
2025-04-11T09:30:26.8701436Z |                |         | R50     | R101    | R101    | v5x6   |
2025-04-11T09:30:26.8701714Z | Caption        | 84-89   | 68.4    | 71.5    | 70.1    | 77.7   |
2025-04-11T09:30:26.8702021Z | Footnote       | 83-91   | 70.9    | 71.8    | 73.7    | 77.2   |
2025-04-11T09:30:26.8702325Z | Formula        | 83-85   | 60.1    | 63.4    | 63.5    | 66.2   |
2025-04-11T09:30:26.8702627Z | List-item      | 87-88   | 81.2    | 80.8    | 81.0    | 86.2   |
2025-04-11T09:30:26.8702942Z | Page-footer    | 93-94   | 61.6    | 59.3    | 58.9    | 61.1   |
2025-04-11T09:30:26.8703268Z | Page-header    | 85-89   | 71.9    | 70.0    | 72.0    | 67.9   |
2025-04-11T09:30:26.8703577Z | Picture        | 69-71   | 71.7    | 72.7    | 72.0    | 77.1   |
2025-04-11T09:30:26.8703899Z | Section-header | 83-84   | 67.6    | 69.3    | 68.4    | 74.6   |
2025-04-11T09:30:26.8704221Z | Table          | 77-81   | 82.2    | 82.9    | 82.2    | 86.3   |
2025-04-11T09:30:26.8704503Z | Text           | 84-86   | 84.6    | 85.8    | 85.4    | 88.1   |
2025-04-11T09:30:26.8704783Z | Title          | 60-72   | 76.7    | 80.4    | 79.9    | 82.7   |
2025-04-11T09:30:26.8705055Z | All            | 82-83   | 72.4    | 73.5    | 73.4    | 76.8   |
2025-04-11T09:30:26.8705237Z 
2025-04-11T09:30:26.8709277Z to avoid this at any cost in order to have clear, unbiased baseline numbers for human document-layout annotation. Third, we introduced the feature of snapping boxes around text segments to obtain a pixel-accurate annotation and again reduce time and effort. The CCS annotation tool automatically shrinks every user-drawn box to the minimum bounding-box around the enclosed text-cells for all purely text-based segments, which excludes only Table and Picture. For the latter, we instructed annotation staff to minimise inclusion of surrounding whitespace while including all graphical lines. A downside of snapping boxes to enclosed text cells is that some wrongly parsed PDF pages cannot be annotated correctly and need to be skipped. Fourth, we established a way to flag pages as rejected for cases where no valid annotation according to the label guidelines could be achieved. Example cases for this would be PDF pages that render incorrectly or contain layouts that are impossible to capture with non-overlapping rectangles. Such rejected pages are not contained in the final dataset. With all these measures in place, experienced annotation staff managed to annotate a single page in a typical timeframe of 20s to 60s, depending on its complexity.
2025-04-11T09:30:26.8713238Z 
2025-04-11T09:30:26.8713313Z ## 5 EXPERIMENTS
2025-04-11T09:30:26.8713545Z 
2025-04-11T09:30:26.8715219Z The primary goal of DocLayNet is to obtain high-quality ML models capable of accurate document-layout analysis on a wide variety of challenging layouts. As discussed in Section 2, object detection models are currently the easiest to use, due to the standardisation of ground-truth data in COCO format [16] and the availability of general frameworks such as detectron2 [17]. Furthermore, baseline numbers in PubLayNet and DocBank were obtained using standard object detection models such as Mask R-CNN and Faster R-CNN. As such, we will relate to these object detection methods in this
2025-04-11T09:30:26.8716953Z 
2025-04-11T09:30:26.8717965Z Figure 5: Prediction performance (mAP@0.5-0.95) of a Mask R-CNN network with ResNet50 backbone trained on increasing fractions of the DocLayNet dataset. The learning curve flattens around the 80% mark, indicating that increasing the size of the DocLayNet dataset with similar data will not yield significantly better predictions.
2025-04-11T09:30:26.8719083Z 
2025-04-11T09:30:26.8719155Z <!-- image -->
2025-04-11T09:30:26.8719266Z 
2025-04-11T09:30:26.8719544Z paper and leave the detailed evaluation of more recent methods mentioned in Section 2 for future work.
2025-04-11T09:30:26.8719905Z 
2025-04-11T09:30:26.8720976Z In this section, we will present several aspects related to the performance of object detection models on DocLayNet. Similarly as in PubLayNet, we will evaluate the quality of their predictions using mean average precision (mAP) with 10 overlaps that range from 0.5 to 0.95 in steps of 0.05 (mAP@0.5-0.95). These scores are computed by leveraging the evaluation code provided by the COCO API [16].
2025-04-11T09:30:26.8722115Z 
2025-04-11T09:30:26.8722202Z ## Baselines for Object Detection
2025-04-11T09:30:26.8722361Z 
2025-04-11T09:30:26.8725612Z In Table 2, we present baseline experiments (given in mAP) on Mask R-CNN [12], Faster R-CNN [11], and YOLOv5 [13]. Both training and evaluation were performed on RGB images with dimensions of 1025 x 1025 pixels. For training, we only used one annotation in case of redundantly annotated pages. As one can observe, the variation in mAP between the models is rather low, but overall between 6 and 10% lower than the mAP computed from the pairwise human annotations on triple-annotated pages. This gives a good indication that the DocLayNet dataset poses a worthwhile challenge for the research community to close the gap between human recognition and ML approaches. It is interesting to see that Mask R-CNN and Faster R-CNN produce very comparable mAP scores, indicating that pixel-based image segmentation derived from bounding-boxes does not help to obtain better predictions. On the other hand, the more recent Yolov5x model does very well and even out-performs humans on selected labels such as Text, Table and Picture. This is not entirely surprising, as Text, Table and Picture are abundant and the most visually distinctive in a document.
2025-04-11T09:30:26.8729196Z 
2025-04-11T09:30:26.8729744Z Table 3: Performance of a Mask R-CNN R50 network in mAP@0.5-0.95 scores trained on DocLayNet with different class label sets. The reduced label sets were obtained by either down-mapping or dropping labels.
2025-04-11T09:30:26.8730409Z 
2025-04-11T09:30:26.8730520Z | Class-count    |   11 | 6       | 5       | 4       |
2025-04-11T09:30:26.8730816Z |----------------|------|---------|---------|---------|
2025-04-11T09:30:26.8731096Z | Caption        |   68 | Text    | Text    | Text    |
2025-04-11T09:30:26.8731380Z | Footnote       |   71 | Text    | Text    | Text    |
2025-04-11T09:30:26.8731654Z | Formula        |   60 | Text    | Text    | Text    |
2025-04-11T09:30:26.8731930Z | List-item      |   81 | Text    | 82      | Text    |
2025-04-11T09:30:26.8732211Z | Page-footer    |   62 | 62      | -       | -       |
2025-04-11T09:30:26.8732495Z | Page-header    |   72 | 68      | -       | -       |
2025-04-11T09:30:26.8732775Z | Picture        |   72 | 72      | 72      | 72      |
2025-04-11T09:30:26.8733178Z | Section-header |   68 | 67      | 69      | 68      |
2025-04-11T09:30:26.8733457Z | Table          |   82 | 83      | 82      | 82      |
2025-04-11T09:30:26.8734047Z | Text           |   85 | 84      | 84      | 84      |
2025-04-11T09:30:26.8734457Z | Title          |   77 | Sec.-h. | Sec.-h. | Sec.-h. |
2025-04-11T09:30:26.8734892Z | Overall        |   72 | 73      | 78      | 77      |
2025-04-11T09:30:26.8735190Z 
2025-04-11T09:30:26.8735298Z ## Learning Curve
2025-04-11T09:30:26.8735476Z 
2025-04-11T09:30:26.8739937Z One of the fundamental questions related to any dataset is if it is "large enough". To answer this question for DocLayNet, we performed a data ablation study in which we evaluated a Mask R-CNN model trained on increasing fractions of the DocLayNet dataset. As can be seen in Figure 5, the mAP score rises sharply in the beginning and eventually levels out. To estimate the error-bar on the metrics, we ran the training five times on the entire data-set. This resulted in a 1% error-bar, depicted by the shaded area in Figure 5. In the inset of Figure 5, we show the exact same data-points, but with a logarithmic scale on the x-axis. As is expected, the mAP score increases linearly as a function of the data-size in the inset. The curve ultimately flattens out between the 80% and 100% mark, with the 80% mark falling within the error-bars of the 100% mark. This provides a good indication that the model would not improve significantly by yet increasing the data size. Rather, it would probably benefit more from improved data consistency (as discussed in Section 3), data augmentation methods [23], or the addition of more document categories and styles.
2025-04-11T09:30:26.8743349Z 
2025-04-11T09:30:26.8743435Z ## Impact of Class Labels
2025-04-11T09:30:26.8743577Z 
2025-04-11T09:30:26.8747333Z The choice and number of labels can have a significant effect on the overall model performance. Since PubLayNet, DocBank and DocLayNet all have different label sets, it is of particular interest to understand and quantify this influence of the label set on the model performance. We investigate this by either down-mapping labels into more common ones (e.g. Caption— Text) or excluding them from the annotations entirely. Furthermore, it must be stressed that all mappings and exclusions were performed on the data before model training. In Table 3, we present the mAP scores for a Mask R-CNN R50 network on different label sets. Where a label is down-mapped, we show its corresponding label, otherwise it was excluded. We present three different label sets, with 6, 5 and 4 different labels respectively. The set of 5 labels contains the same labels as PubLayNet. However, due to the different definition of
2025-04-11T09:30:26.8750111Z 
2025-04-11T09:30:26.8750595Z Table 4: Performance of a Mask R-CNN R50 network with document-wise and page-wise split for different label sets. Naive page-wise split will result in -10% point improvement.
2025-04-11T09:30:26.8751137Z 
2025-04-11T09:30:26.8751244Z | Class-count    | 11   | 11   | 5   | 5    |
2025-04-11T09:30:26.8751505Z |----------------|------|------|-----|------|
2025-04-11T09:30:26.8751759Z | Split          | Doc  | Page | Doc | Page |
2025-04-11T09:30:26.8752002Z | Caption        | 68   | 83   |     |      |
2025-04-11T09:30:26.8752235Z | Footnote       | 71   | 84   |     |      |
2025-04-11T09:30:26.8752466Z | Formula        | 60   | 66   |     |      |
2025-04-11T09:30:26.8752699Z | List-item      | 81   | 88   | 82  | 88   |
2025-04-11T09:30:26.8752941Z | Page-footer    | 62   | 89   |     |      |
2025-04-11T09:30:26.8753185Z | Page-header    | 72   | 90   |     |      |
2025-04-11T09:30:26.8753418Z | Picture        | 72   | 82   | 72  | 82   |
2025-04-11T09:30:26.8753657Z | Section-header | 68   | 83   | 69  | 83   |
2025-04-11T09:30:26.8753894Z | Table          | 82   | 89   | 82  | 90   |
2025-04-11T09:30:26.8754123Z | Text           | 85   | 91   | 84  | 90   |
2025-04-11T09:30:26.8754357Z | Title          | 77   | 81   |     |      |
2025-04-11T09:30:26.8754584Z | All            | 72   | 84   | 78  | 87   |
2025-04-11T09:30:26.8754886Z 
2025-04-11T09:30:26.8756233Z lists in PubLayNet (grouped list-items) versus DocLayNet (separate list-items), the label set of size 4 is the closest to PubLayNet, in the assumption that the List is down-mapped to Text in PubLayNet. The results in Table 3 show that the prediction accuracy on the remaining class labels does not change significantly when other classes are merged into them. The overall macro-average improves by around 5%, in particular when Page-footer and Page-header are excluded.
2025-04-11T09:30:26.8757617Z 
2025-04-11T09:30:26.8757731Z ## Impact of Document Split in Train and Test Set
2025-04-11T09:30:26.8757921Z 
2025-04-11T09:30:26.8760581Z Many documents in DocLayNet have a unique styling. In order to avoid overfitting on a particular style, we have split the train-, testand validation-sets of DocLayNet on document boundaries, i.e. every document contributes pages to only one set. To the best of our knowledge, this was not considered in PubLayNet or DocBank. To quantify how this affects model performance, we trained and evaluated a Mask R-CNN R50 model on a modified dataset version. Here, the train-, testand validation-sets were obtained by a randomised draw over the individual pages. As can be seen in Table 4, the difference in model performance is surprisingly large: pagewise splitting gains 10% in mAP over the document-wise splitting. Thus, random page-wise splitting of DocLayNet can easily lead to accidental overestimation of model performance and should be avoided.
2025-04-11T09:30:26.8763071Z 
2025-04-11T09:30:26.8763152Z ## Dataset Comparison
2025-04-11T09:30:26.8763275Z 
2025-04-11T09:30:26.8764747Z Throughout this paper, we claim that DocLayNet's wider variety of document layouts leads to more robust layout detection models. In Table 5, we provide evidence for that. We trained models on each of the available datasets (PubLayNet, DocBank and DocLayNet) and evaluated them on the test sets of the other datasets. Due to the different label sets and annotation styles, a direct comparison is not possible. Hence, we focussed on the common labels among the datasets. Between PubLayNet and DocLayNet, these are Picture,
2025-04-11T09:30:26.8766281Z 
2025-04-11T09:30:26.8767108Z Table 5: Prediction Performance (mAP@0.5-0.95) of a Mask R-CNN R50 network across the PubLayNet, DocBank &amp; DocLayNet data-sets. By evaluating on common label classes of each dataset, we observe that the DocLayNet-trained model has much less pronounced variations in performance across all datasets.
2025-04-11T09:30:26.8768024Z 
2025-04-11T09:30:26.8768136Z |                 |            | Testing on   | Testing on   | Testing on   |
2025-04-11T09:30:26.8768460Z |-----------------|------------|--------------|--------------|--------------|
2025-04-11T09:30:26.8768944Z | Training on     | labels     | PLN          | DB           | DLN          |
2025-04-11T09:30:26.8769316Z | PubLayNet (PLN) | Figure     | 96           | 43           | 23           |
2025-04-11T09:30:26.8769704Z | PubLayNet (PLN) | Sec-header | 87           | -            | 32           |
2025-04-11T09:30:26.8770049Z |                 | Table      | 95           | 24           | 49           |
2025-04-11T09:30:26.8770333Z |                 | Text       | 96           | -            | 42           |
2025-04-11T09:30:26.8770613Z |                 | total      | 93           | 34           | 30           |
2025-04-11T09:30:26.8770925Z | DocBank (DB)    | Figure     | 77           | 71           | 31           |
2025-04-11T09:30:26.8771262Z | DocBank (DB)    | Table      | 19           | 65           | 22           |
2025-04-11T09:30:26.8771597Z | DocBank (DB)    | total      | 48           | 68           | 27           |
2025-04-11T09:30:26.8771973Z | DocLayNet (DLN) | Figure     | 67           | 51           | 72           |
2025-04-11T09:30:26.8772344Z | DocLayNet (DLN) | Sec-header | 53           | -            | 68           |
2025-04-11T09:30:26.8772680Z |                 | Table      | 87           | 43           | 82           |
2025-04-11T09:30:26.8773113Z |                 | Text       | 77           | -            | 84           |
2025-04-11T09:30:26.8773397Z |                 | total      | 59           | 47           | 78           |
2025-04-11T09:30:26.8773593Z 
2025-04-11T09:30:26.8774395Z Section-header, Table and Text. Before training, we either mapped or excluded DocLayNet's other labels as specified in table 3, and also PubLayNet's List to Text. Note that the different clustering of lists (by list-element vs. whole list objects) naturally decreases the mAP score for Text.
2025-04-11T09:30:26.8775283Z 
2025-04-11T09:30:26.8777681Z For comparison of DocBank with DocLayNet, we trained only on Picture and Table clusters of each dataset. We had to exclude Text because successive paragraphs are often grouped together into a single object in DocBank. This paragraph grouping is incompatible with the individual paragraphs of DocLayNet. As can be seen in Table 5, DocLayNet trained models yield better performance compared to the previous datasets. It is noteworthy that the models trained on PubLayNet and DocBank perform very well on their own test set, but have a much lower performance on the foreign datasets. While this also applies to DocLayNet, the difference is far less pronounced. Thus we conclude that DocLayNet trained models are overall more robust and will produce better results for challenging, unseen layouts.
2025-04-11T09:30:26.8780155Z 
2025-04-11T09:30:26.8780246Z ## Example Predictions
2025-04-11T09:30:26.8780374Z 
2025-04-11T09:30:26.8781790Z To conclude this section, we illustrate the quality of layout predictions one can expect from DocLayNet-trained models by providing a selection of examples without any further post-processing applied. Figure 6 shows selected layout predictions on pages from the test-set of DocLayNet. Results look decent in general across document categories, however one can also observe mistakes such as overlapping clusters of different classes, or entirely missing boxes due to low confidence.
2025-04-11T09:30:26.8783277Z 
2025-04-11T09:30:26.8783352Z ## 6 CONCLUSION
2025-04-11T09:30:26.8783457Z 
2025-04-11T09:30:26.8784945Z In this paper, we presented the DocLayNet dataset. It provides the document conversion and layout analysis research community a new and challenging dataset to improve and fine-tune novel ML methods on. In contrast to many other datasets, DocLayNet was created by human annotation in order to obtain reliable layout ground-truth on a wide variety of publicationand typesettingstyles. Including a large proportion of documents outside the scientific publishing domain adds significant value in this respect.
2025-04-11T09:30:26.8786501Z 
2025-04-11T09:30:26.8788177Z From the dataset, we have derived on the one hand reference metrics for human performance on document-layout annotation (through double and triple annotations) and on the other hand evaluated the baseline performance of commonly used object detection methods. We also illustrated the impact of various dataset-related aspects on model performance through data-ablation experiments, both from a size and class-label perspective. Last but not least, we compared the accuracy of models trained on other public datasets and showed that DocLayNet trained models are more robust.
2025-04-11T09:30:26.8802769Z 
2025-04-11T09:30:26.8803305Z To date, there is still a significant gap between human and ML accuracy on the layout interpretation task, and we hope that this work will inspire the research community to close that gap.
2025-04-11T09:30:26.8803891Z 
2025-04-11T09:30:26.8803961Z ## REFERENCES
2025-04-11T09:30:26.8804072Z 
2025-04-11T09:30:26.8804837Z - 1] Max Göbel, Tamir Hassan, Ermelinda Oro, and Giorgio Orsi. Icdar 2013 table competition. In 2013 12th International Conference on Document Analysis and Recognition, pages 1449-1453, 2013.
2025-04-11T09:30:26.8806267Z - 2] Christian Clausner, Apostolos Antonacopoulos, and Stefan Pletschacher. Icdar2017 competition on recognition of documents with complex layouts -rdcl2017. In 2017 14th IAPR International Conference on Document Analysis and Recognition (ICDAR), volume 01, pages 1404-1410, 2017.
2025-04-11T09:30:26.8808097Z - 3] Hervé Déjean, Jean-Luc Meunier, Liangcai Gao, Yilun Huang, Yu Fang, Florian Kleber, and Eva-Maria Lang. ICDAR 2019 Competition on Table Detection and Recognition (cTDaR), April 2019. http://sac.founderit.com/.
2025-04-11T09:30:26.8809591Z - 4] Antonio Jimeno Yepes, Peter Zhong, and Douglas Burdick. Competition on scientific literature parsing. In Proceedings of the International Conference on Document Analysis and Recognition, ICDAR, pages 605-617. LNCS 12824, SpringerVerlag, sep 2021.
2025-04-11T09:30:26.8811081Z - 5] Logan Markewich, Hao Zhang, Yubin Xing, Navid Lambert-Shirzad, Jiang Zhexin, Roy Lee, Zhi Li, and Seok-Bum Ko. Segmentation for document layout analysis: not dead yet. International Journal on Document Analysis and Recognition (IJDAR), pages 1-11, 01 2022.
2025-04-11T09:30:26.8812635Z - 6] Xu Zhong, Jianbin Tang, and Antonio Jimeno-Yepes. Publaynet: Largest dataset ever for document layout analysis. In Proceedings of the International Conference on Document Analysis and Recognition, ICDAR, pages 1015-1022, sep 2019.
2025-04-11T09:30:26.8814245Z - 7] Minghao Li, Yiheng Xu, Lei Cui, Shaohan Huang, Furu Wei, Zhoujun Li, and Ming Zhou. Docbank: A benchmark dataset for document layout analysis. In Proceedings of the 28th International Conference on Computational Linguistics, COLING, pages 949-960. International Committee on Computational Linguistics, dec 2020.
2025-04-11T09:30:26.8815647Z - 8] Riaz Ahmad, Muhammad Tanvir Afzal, and M. Qadir. Information extraction from pdf sources based on rule-based system using integrated formats. In SemWebEval@ESWC, 2016.
2025-04-11T09:30:26.8816951Z - 9] Ross B. Girshick, Jeff Donahue, Trevor Darrell, and Jitendra Malik. Rich feature hierarchies for accurate object detection and semantic segmentation. In IEEE Conference on Computer Vision and Pattern Recognition, CVPR, pages 580-587. IEEE Computer Society, jun 2014.
2025-04-11T09:30:26.8818155Z - [10] Ross B. Girshick. Fast R-CNN. In 2015 IEEE International Conference on Computer Vision, ICCV, pages 1440-1448. IEEE Computer Society, dec 2015.
2025-04-11T09:30:26.8819312Z - [11] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster r-cnn: Towards real-time object detection with region proposal networks. IEEE Transactions on Pattern Analysis and Machine Intelligence, 39(6):1137-1149, 2017.
2025-04-11T09:30:26.8820627Z - [12] Kaiming He, Georgia Gkioxari, Piotr Dollär, and Ross B. Girshick. Mask R-CNN. In IEEE International Conference on Computer Vision, ICCV, pages 2980-2988. IEEE Computer Society, Oct 2017.
2025-04-11T09:30:26.8822043Z - [13] Glenn Jocher, Alex Stoken, Ayush Chaurasia, Jirka Borovec, NanoCode012, TaoXie, Yonghye Kwon, Kalen Michael, Liu Changyu, Jiacong Fang, Abhiram V, Laughing, tkianai, yxNONG, Piotr Skalski, Adam Hogan, Jebastin Nadar, imyhxy, Lorenzo Mammana, Alex Wang, Cristi Fati, Diego Montes, Jan Hajek, Laurentiu
2025-04-11T09:30:26.8822976Z 
2025-04-11T09:30:26.8823251Z OText OCaption Olistttem @Formula OTable OPicture @Section-Header (O Page-Header OPage-Footer OTitle
2025-04-11T09:30:26.8823612Z 
2025-04-11T09:30:26.8823683Z <!-- image -->
2025-04-11T09:30:26.8823788Z 
2025-04-11T09:30:26.8824899Z Figure 6: Example layout predictions on selected pages from the DocLayNet test-set. (A, D) exhibit favourable results on coloured backgrounds. (B, C) show accurate list-item and paragraph differentiation despite densely-spaced lines. (E) demonstrates good table and figure distinction. (F) shows predictions on a Chinese patent with multiple overlaps, label confusion and missing boxes.
2025-04-11T09:30:26.8826084Z 
2025-04-11T09:30:26.8826608Z Diaconu, Mai Thanh Minh, Marc, albinxavi, fatih, oleg, and wanghao yang. ultralytics/yolov5: v6.0 -yolov5n nano models, roboflow integration, tensorflow export, opencv dnn support, October 2021.
2025-04-11T09:30:26.8827217Z 
2025-04-11T09:30:26.8827627Z - [20] Shoubin Li, Xuyan Ma, Shuaiqun Pan, Jun Hu, Lin Shi, and Qing Wang. Vtlayout: Fusion of visual and text features for document layout analysis, 2021.
2025-04-11T09:30:26.8828858Z - 14 Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and Sergey Zagoruyko. End-to-end object detection with transformers. CoRR, abs/2005.12872, 2020.
2025-04-11T09:30:26.8829784Z - 15 Mingxing Tan, Ruoming Pang, and Quoc V. Le. Efficientdet: Scalable and efficient object detection. CoRR, abs/1911.09070, 2019.
2025-04-11T09:30:26.8830915Z - 16 Tsung-Yi Lin, Michael Maire, Serge J. Belongie, Lubomir D. Bourdev, Ross B. Girshick, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollär, and C. Lawrence Zitnick. Microsoft COCO: common objects in context, 2014.
2025-04-11T09:30:26.8831812Z - 17 Yuxin Wu, Alexander Kirillov, Francisco Massa, Wan-Yen Lo, and Ross Girshick. Detectron2, 2019.
2025-04-11T09:30:26.8833218Z - 18 Nikolaos Livathinos, Cesar Berrospi, Maksym Lysak, Viktor Kuropiatnyk, Ahmed Nassar, Andre Carvalho, Michele Dolfi, Christoph Auer, Kasper Dinkla, and Peter W. J. Staar. Robust pdf document conversion using recurrent neural networks. In Proceedings of the 35th Conference on Artificial Intelligence, AAAI, pages 1513715145, feb 2021.
2025-04-11T09:30:26.8835173Z - 19 Yiheng Xu, Minghao Li, Lei Cui, Shaohan Huang, Furu Wei, and Ming Zhou. Layoutlm: Pre-training of text and layout for document image understanding. In Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD, pages 1192-1200, New York, USA, 2020. Association for Computing Machinery.
2025-04-11T09:30:26.8836626Z - [21] Peng Zhang, Can Li, Liang Qiao, Zhanzhan Cheng, Shiliang Pu, Yi Niu, and Fei Wu. Vsr: A unified framework for document layout analysis combining vision, semantics and relations, 2021.
2025-04-11T09:30:26.8837972Z - [22] Peter W J Staar, Michele Dolfi, Christoph Auer, and Costas Bekas. Corpus conversion service: A machine learning platform to ingest documents at scale. In Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD, pages 774-782. ACM, 2018.
2025-04-11T09:30:26.8839304Z - [23] Connor Shorten and Taghi M. Khoshgoftaar. A survey on image data augmentation for deep learning. Journal of Big Data, 6(1):60, 2019.
2025-04-11T09:30:27.7981117Z Running example docs/examples/inspect_picture_content.py
2025-04-11T09:30:45.9651688Z Figure 7-26. Self-locking nuts.  contains these elements:
2025-04-11T09:30:45.9652274Z Figure 7-26. Self-locking nuts.
2025-04-11T09:30:45.9652646Z Boots aircraft nut
2025-04-11T09:30:45.9652948Z Flexloc nut
2025-04-11T09:30:45.9653203Z Fiber locknut
2025-04-11T09:30:45.9653452Z Elastic stop nut
2025-04-11T09:30:45.9653748Z Elastic anchor nut
2025-04-11T09:30:45.9653932Z 
2025-04-11T09:30:45.9653949Z 
2025-04-11T09:30:45.9654237Z Figure 7-27. Stainless steel self-locking nut.  contains these elements:
2025-04-11T09:30:45.9654807Z Figure 7-27. Stainless steel self-locking nut.
2025-04-11T09:30:45.9655235Z Tightened nut
2025-04-11T09:30:45.9655511Z Untightened nut
2025-04-11T09:30:45.9655782Z Nut case
2025-04-11T09:30:45.9656030Z Threaded nut core
2025-04-11T09:30:45.9656317Z Locking shoulder
2025-04-11T09:30:45.9656573Z Keyway
2025-04-11T09:30:45.9656713Z 
2025-04-11T09:30:45.9656720Z 
2025-04-11T09:30:46.9362035Z Skipping docs/examples/minimal.py
2025-04-11T09:30:46.9375758Z Skipping docs/examples/minimal_vlm_pipeline.py
2025-04-11T09:30:46.9389680Z Skipping docs/examples/pictures_description_api.py
2025-04-11T09:30:46.9403579Z Skipping docs/examples/rapidocr_with_custom_models.py
2025-04-11T09:30:46.9416945Z Running example docs/examples/run_md.py
2025-04-11T09:30:48.5225304Z Document README.md converted.
2025-04-11T09:30:48.5225775Z Saved markdown output to: scratch
2025-04-11T09:30:48.6618369Z Running example docs/examples/run_with_accelerator.py
2025-04-11T09:32:54.3910808Z ## DocLayNet: A Large Human-Annotated Dataset for Document-Layout Analysis
2025-04-11T09:32:54.3911426Z 
2025-04-11T09:32:54.3911914Z Birgit Pfitzmann IBM Research Rueschlikon, Switzerland bpf@zurich.ibm.com
2025-04-11T09:32:54.3913091Z 
2025-04-11T09:32:54.3913498Z Christoph Auer IBM Research Rueschlikon, Switzerland cau@zurich.ibm.com
2025-04-11T09:32:54.3914187Z 
2025-04-11T09:32:54.3914585Z Michele Dolfi IBM Research Rueschlikon, Switzerland dol@zurich.ibm.com
2025-04-11T09:32:54.3915158Z 
2025-04-11T09:32:54.3915475Z Ahmed S. Nassar IBM Research Rueschlikon, Switzerland ahn@zurich.ibm.com
2025-04-11T09:32:54.3915955Z 
2025-04-11T09:32:54.3916258Z Peter Staar IBM Research Rueschlikon, Switzerland taa@zurich.ibm.com
2025-04-11T09:32:54.3916711Z 
2025-04-11T09:32:54.3916833Z ## ABSTRACT
2025-04-11T09:32:54.3917006Z 
2025-04-11T09:32:54.3924696Z Accurate document layout analysis is a key requirement for highquality PDF document conversion. With the recent availability of public, large ground-truth datasets such as PubLayNet and DocBank, deep-learning models have proven to be very effective at layout detection and segmentation. While these datasets are of adequate size to train such models, they severely lack in layout variability since they are sourced from scientific article repositories such as PubMed and arXiv only. Consequently, the accuracy of the layout segmentation drops significantly when these models are applied on more challenging and diverse layouts. In this paper, we present DocLayNet , a new, publicly available, document-layout annotation dataset in COCO format. It contains 80863 manually annotated pages from diverse data sources to represent a wide variability in layouts. For each PDF page, the layout annotations provide labelled bounding-boxes with a choice of 11 distinct classes. DocLayNet also provides a subset of double- and triple-annotated pages to determine the inter-annotator agreement. In multiple experiments, we provide baseline accuracy scores (in mAP) for a set of popular object detection models. We also demonstrate that these models fall approximately 10% behind the inter-annotator agreement. Furthermore, we provide evidence that DocLayNet is of sufficient size. Lastly, we compare models trained on PubLayNet, DocBank and DocLayNet, showing that layout predictions of the DocLayNettrained models are more robust and thus the preferred choice for general-purpose document-layout analysis.
2025-04-11T09:32:54.3929899Z 
2025-04-11T09:32:54.3929981Z ## CCS CONCEPTS
2025-04-11T09:32:54.3930095Z 
2025-04-11T09:32:54.3930900Z · Informationsystems → Documentstructure ; · Appliedcomputing → Document analysis ; · Computing methodologies → Machine learning Computer vision ; ; Object detection ;
2025-04-11T09:32:54.3931476Z 
2025-04-11T09:32:54.3932605Z Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for third-party components of this work must be honored. For all other uses, contact the owner/author(s).
2025-04-11T09:32:54.3933787Z 
2025-04-11T09:32:54.3933902Z KDD '22, August 14-18, 2022, Washington, DC, USA
2025-04-11T09:32:54.3934088Z 
2025-04-11T09:32:54.3934246Z © 2022 Copyright held by the owner/author(s).
2025-04-11T09:32:54.3934426Z 
2025-04-11T09:32:54.3934508Z ACM ISBN 978-1-4503-9385-0/22/08.
2025-04-11T09:32:54.3934661Z 
2025-04-11T09:32:54.3934762Z https://doi.org/10.1145/3534678.3539043
2025-04-11T09:32:54.3934935Z 
2025-04-11T09:32:54.3935150Z Figure 1: Four examples of complex page layouts across different document categories
2025-04-11T09:32:54.3935452Z 
2025-04-11T09:32:54.3935522Z <!-- image -->
2025-04-11T09:32:54.3935633Z 
2025-04-11T09:32:54.3935700Z ## KEYWORDS
2025-04-11T09:32:54.3935792Z 
2025-04-11T09:32:54.3936038Z PDF document conversion, layout segmentation, object-detection, data set, Machine Learning
2025-04-11T09:32:54.3936364Z 
2025-04-11T09:32:54.3936446Z ## ACMReference Format:
2025-04-11T09:32:54.3936573Z 
2025-04-11T09:32:54.3937603Z Birgit Pfitzmann, Christoph Auer, Michele Dolfi, Ahmed S. Nassar, and Peter Staar. 2022. DocLayNet: A Large Human-Annotated Dataset for DocumentLayout Analysis. In Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD '22), August 14-18, 2022, Washington, DC, USA. ACM, New York, NY, USA, 9 pages. https://doi.org/10.1145/ 3534678.3539043
2025-04-11T09:32:54.3939018Z 
2025-04-11T09:32:54.3939094Z ## 1 INTRODUCTION
2025-04-11T09:32:54.3939211Z 
2025-04-11T09:32:54.3941388Z Despite the substantial improvements achieved with machine-learning (ML) approaches and deep neural networks in recent years, document conversion remains a challenging problem, as demonstrated by the numerous public competitions held on this topic [1-4]. The challenge originates from the huge variability in PDF documents regarding layout, language and formats (scanned, programmatic or a combination of both). Engineering a single ML model that can be applied on all types of documents and provides high-quality layout segmentation remains to this day extremely challenging [5]. To highlight the variability in document layouts, we show a few example documents from the DocLayNet dataset in Figure 1.
2025-04-11T09:32:54.3943515Z 
2025-04-11T09:32:54.3948144Z Akeyproblem in the process of document conversion is to understand the structure of a single document page, i.e. which segments of text should be grouped together in a unit. To train models for this task, there are currently two large datasets available to the community, PubLayNet [6] and DocBank [7]. They were introduced in 2019 and 2020 respectively and significantly accelerated the implementation of layout detection and segmentation models due to their sizes of 300K and 500K ground-truth pages. These sizes were achieved by leveraging an automation approach. The benefit of automated ground-truth generation is obvious: one can generate large ground-truth datasets at virtually no cost. However, the automation introduces a constraint on the variability in the dataset, because corresponding structured source data must be available. PubLayNet and DocBank were both generated from scientific document repositories (PubMed and arXiv), which provide XML or L A T E X sources. Those scientific documents present a limited variability in their layouts, because they are typeset in uniform templates provided by the publishers. Obviously, documents such as technical manuals, annual company reports, legal text, government tenders, etc. have very different and partially unique layouts. As a consequence, the layout predictions obtained from models trained on PubLayNet or DocBank is very reasonable when applied on scientific documents. However, for more artistic or free-style layouts, we see sub-par prediction quality from these models, which we demonstrate in Section 5.
2025-04-11T09:32:54.3953165Z 
2025-04-11T09:32:54.3954497Z In this paper, we present the DocLayNet dataset. It provides pageby-page layout annotation ground-truth using bounding-boxes for 11 distinct class labels on 80863 unique document pages, of which a fraction carry double- or triple-annotations. DocLayNet is similar in spirit to PubLayNet and DocBank and will likewise be made available to the public 1 in order to stimulate the document-layout analysis community. It distinguishes itself in the following aspects:
2025-04-11T09:32:54.3955929Z 
2025-04-11T09:32:54.3956319Z - (1) Human Annotation : In contrast to PubLayNet and DocBank, we relied on human annotation instead of automation approaches to generate the data set.
2025-04-11T09:32:54.3957076Z - (2) Large Layout Variability : We include diverse and complex layouts from a large variety of public sources.
2025-04-11T09:32:54.3957919Z - (3) Detailed Label Set : We define 11 class labels to distinguish layout features in high detail. PubLayNet provides 5 labels; DocBank provides 13, although not a superset of ours.
2025-04-11T09:32:54.3958969Z - (4) Redundant Annotations : A fraction of the pages in the DocLayNet data set carry more than one human annotation.
2025-04-11T09:32:54.3959357Z 
2025-04-11T09:32:54.3959644Z 1 https://developer.ibm.com/exchanges/data/all/doclaynet
2025-04-11T09:32:54.3959874Z 
2025-04-11T09:32:54.3960103Z This enables experimentation with annotation uncertainty and quality control analysis.
2025-04-11T09:32:54.3960417Z 
2025-04-11T09:32:54.3961220Z - (5) Pre-defined Train-, Test- &amp; Validation-set : Like DocBank, we provide fixed train-, test- &amp; validation-sets to ensure proportional representation of the class-labels. Further, we prevent leakage of unique layouts across sets, which has a large effect on model accuracy scores.
2025-04-11T09:32:54.3962092Z 
2025-04-11T09:32:54.3962911Z All aspects outlined above are detailed in Section 3. In Section 4, we will elaborate on how we designed and executed this large-scale human annotation campaign. We will also share key insights and lessons learned that might prove helpful for other parties planning to set up annotation campaigns.
2025-04-11T09:32:54.3963801Z 
2025-04-11T09:32:54.3965350Z In Section 5, we will present baseline accuracy numbers for a variety of object detection methods (Faster R-CNN, Mask R-CNN and YOLOv5) trained on DocLayNet. We further show how the model performance is impacted by varying the DocLayNet dataset size, reducing the label set and modifying the train/test-split. Last but not least, we compare the performance of models trained on PubLayNet, DocBank and DocLayNet and demonstrate that a model trained on DocLayNet provides overall more robust layout recovery.
2025-04-11T09:32:54.3966867Z 
2025-04-11T09:32:54.3966937Z ## 2 RELATED WORK
2025-04-11T09:32:54.3967052Z 
2025-04-11T09:32:54.3969071Z While early approaches in document-layout analysis used rulebased algorithms and heuristics [8], the problem is lately addressed with deep learning methods. The most common approach is to leverage object detection models [9-15]. In the last decade, the accuracy and speed of these models has increased dramatically. Furthermore, most state-of-the-art object detection methods can be trained and applied with very little work, thanks to a standardisation effort of the ground-truth data format [16] and common deep-learning frameworks [17]. Reference data sets such as PubLayNet [6] and DocBank provide their data in the commonly accepted COCO format [16].
2025-04-11T09:32:54.3971011Z 
2025-04-11T09:32:54.3972417Z Lately, new types of ML models for document-layout analysis have emerged in the community [18-21]. These models do not approach the problem of layout analysis purely based on an image representation of the page, as computer vision methods do. Instead, they combine the text tokens and image representation of a page in order to obtain a segmentation. While the reported accuracies appear to be promising, a broadly accepted data format which links geometric and textual features has yet to establish.
2025-04-11T09:32:54.3973874Z 
2025-04-11T09:32:54.3973958Z ## 3 THE DOCLAYNET DATASET
2025-04-11T09:32:54.3974091Z 
2025-04-11T09:32:54.3975587Z DocLayNet contains 80863 PDF pages. Among these, 7059 carry two instances of human annotations, and 1591 carry three. This amounts to 91104 total annotation instances. The annotations provide layout information in the shape of labeled, rectangular boundingboxes. We define 11 distinct labels for layout features, namely Caption , Footnote , Formula List-item , , Page-footer , Page-header , Picture , Section-header , Table , Text , and Title . Our reasoning for picking this particular label set is detailed in Section 4.
2025-04-11T09:32:54.3977139Z 
2025-04-11T09:32:54.3977649Z In addition to open intellectual property constraints for the source documents, we required that the documents in DocLayNet adhere to a few conditions. Firstly, we kept scanned documents
2025-04-11T09:32:54.3978245Z 
2025-04-11T09:32:54.3978417Z Figure 2: Distribution of DocLayNet pages across document categories.
2025-04-11T09:32:54.3978763Z 
2025-04-11T09:32:54.3978844Z <!-- image -->
2025-04-11T09:32:54.3978947Z 
2025-04-11T09:32:54.3980397Z to a minimum, since they introduce difficulties in annotation (see Section 4). As a second condition, we focussed on medium to large documents ( &gt; 10 pages) with technical content, dense in complex tables, figures, plots and captions. Such documents carry a lot of information value, but are often hard to analyse with high accuracy due to their challenging layouts. Counterexamples of documents not included in the dataset are receipts, invoices, hand-written documents or photographs showing 'text in the wild".
2025-04-11T09:32:54.3982031Z 
2025-04-11T09:32:54.3984551Z The pages in DocLayNet can be grouped into six distinct categories, namely Financial Reports , Manuals Scientific Articles , , Laws &amp; Regulations , Patents and Government Tenders . Each document category was sourced from various repositories. For example, Financial Reports contain both free-style format annual reports 2 which expose company-specific, artistic layouts as well as the more formal SEC filings. The two largest categories ( Financial Reports and Manuals ) contain a large amount of free-style layouts in order to obtain maximum variability. In the other four categories, we boosted the variability by mixing documents from independent providers, such as different government websites or publishers. In Figure 2, we show the document categories contained in DocLayNet with their respective sizes.
2025-04-11T09:32:54.3986989Z 
2025-04-11T09:32:54.3988472Z We did not control the document selection with regard to language. The vast majority of documents contained in DocLayNet (close to 95%) are published in English language. However, DocLayNet also contains a number of documents in other languages such as German (2.5%), French (1.0%) and Japanese (1.0%). While the document language has negligible impact on the performance of computer vision methods such as object detection and segmentation models, it might prove challenging for layout analysis methods which exploit textual features.
2025-04-11T09:32:54.3990132Z 
2025-04-11T09:32:54.3991297Z To ensure that future benchmarks in the document-layout analysis community can be easily compared, we have split up DocLayNet into pre-defined train-, test- and validation-sets. In this way, we can avoid spurious variations in the evaluation scores due to random splitting in train-, test- and validation-sets. We also ensured that less frequent labels are represented in train and test sets in equal proportions.
2025-04-11T09:32:54.3992525Z 
2025-04-11T09:32:54.3992654Z 2 e.g. AAPL from https://www.annualreports.com/
2025-04-11T09:32:54.3992862Z 
2025-04-11T09:32:54.3994121Z Table 1 shows the overall frequency and distribution of the labels among the different sets. Importantly, we ensure that subsets are only split on full-document boundaries. This avoids that pages of the same document are spread over train, test and validation set, which can give an undesired evaluation advantage to models and lead to overestimation of their prediction accuracy. We will show the impact of this decision in Section 5.
2025-04-11T09:32:54.3995424Z 
2025-04-11T09:32:54.3997693Z In order to accommodate the different types of models currently in use by the community, we provide DocLayNet in an augmented COCO format [16]. This entails the standard COCO ground-truth file (in JSON format) with the associated page images (in PNG format, 1025 × 1025 pixels). Furthermore, custom fields have been added to each COCO record to specify document category, original document filename and page number. In addition, we also provide the original PDF pages, as well as sidecar files containing parsed PDF text and text-cell coordinates (in JSON). All additional files are linked to the primary page images by their matching filenames.
2025-04-11T09:32:54.3999669Z 
2025-04-11T09:32:54.4005276Z Despite being cost-intense and far less scalable than automation, human annotation has several benefits over automated groundtruth generation. The first and most obvious reason to leverage human annotations is the freedom to annotate any type of document without requiring a programmatic source. For most PDF documents, the original source document is not available. The latter is not a hard constraint with human annotation, but it is for automated methods. A second reason to use human annotations is that the latter usually provide a more natural interpretation of the page layout. The human-interpreted layout can significantly deviate from the programmatic layout used in typesetting. For example, 'invisible' tables might be used solely for aligning text paragraphs on columns. Such typesetting tricks might be interpreted by automated methods incorrectly as an actual table, while the human annotation will interpret it correctly as Text or other styles. The same applies to multi-line text elements, when authors decided to space them as 'invisible' list elements without bullet symbols. A third reason to gather ground-truth through human annotation is to estimate a 'natural' upper bound on the segmentation accuracy. As we will show in Section 4, certain documents featuring complex layouts can have different but equally acceptable layout interpretations. This natural upper bound for segmentation accuracy can be found by annotating the same pages multiple times by different people and evaluating the inter-annotator agreement. Such a baseline consistency evaluation is very useful to define expectations for a good target accuracy in trained deep neural network models and avoid overfitting (see Table 1). On the flip side, achieving high annotation consistency proved to be a key challenge in human annotation, as we outline in Section 4.
2025-04-11T09:32:54.4011326Z 
2025-04-11T09:32:54.4011453Z ## 4 ANNOTATION CAMPAIGN
2025-04-11T09:32:54.4011668Z 
2025-04-11T09:32:54.4013992Z The annotation campaign was carried out in four phases. In phase one, we identified and prepared the data sources for annotation. In phase two, we determined the class labels and how annotations should be done on the documents in order to obtain maximum consistency. The latter was guided by a detailed requirement analysis and exhaustive experiments. In phase three, we trained the annotation staff and performed exams for quality assurance. In phase four,
2025-04-11T09:32:54.4016103Z 
2025-04-11T09:32:54.4017072Z Table 1: DocLayNet dataset overview. Along with the frequency of each class label, we present the relative occurrence (as % of row 'Total') in the train, test and validation sets. The inter-annotator agreement is computed as the mAP@0.5-0.95 metric between pairwise annotations from the triple-annotated pages, from which we obtain accuracy ranges.
2025-04-11T09:32:54.4018103Z 
2025-04-11T09:32:54.4019110Z |                |         | % of Total   | % of Total   | % of Total   | triple inter-annotator mAP @0.5-0.95 (%)   | triple inter-annotator mAP @0.5-0.95 (%)   | triple inter-annotator mAP @0.5-0.95 (%)   | triple inter-annotator mAP @0.5-0.95 (%)   | triple inter-annotator mAP @0.5-0.95 (%)   | triple inter-annotator mAP @0.5-0.95 (%)   | triple inter-annotator mAP @0.5-0.95 (%)   |
2025-04-11T09:32:54.4020496Z |----------------|---------|--------------|--------------|--------------|--------------------------------------------|--------------------------------------------|--------------------------------------------|--------------------------------------------|--------------------------------------------|--------------------------------------------|--------------------------------------------|
2025-04-11T09:32:54.4021421Z | class label    | Count   | Train        | Test         | Val          | All                                        | Fin                                        | Man                                        | Sci                                        | Law                                        | Pat                                        | Ten                                        |
2025-04-11T09:32:54.4022101Z | Caption        | 22524   | 2.04         | 1.77         | 2.32         | 84-89                                      | 40-61                                      | 86-92                                      | 94-99                                      | 95-99                                      | 69-78                                      | n/a                                        |
2025-04-11T09:32:54.4022907Z | Footnote       | 6318    | 0.60         | 0.31         | 0.58         | 83-91                                      | n/a                                        | 100                                        | 62-88                                      | 85-94                                      | n/a                                        | 82-97                                      |
2025-04-11T09:32:54.4023530Z | Formula        | 25027   | 2.25         | 1.90         | 2.96         | 83-85                                      | n/a                                        | n/a                                        | 84-87                                      | 86-96                                      | n/a                                        | n/a                                        |
2025-04-11T09:32:54.4024268Z | List-item      | 185660  | 17.19        | 13.34        | 15.82        | 87-88                                      | 74-83                                      | 90-92                                      | 97-97                                      | 81-85                                      | 75-88                                      | 93-95                                      |
2025-04-11T09:32:54.4024935Z | Page-footer    | 70878   | 6.51         | 5.58         | 6.00         | 93-94                                      | 88-90                                      | 95-96                                      | 100                                        | 92-97                                      | 100                                        | 96-98                                      |
2025-04-11T09:32:54.4025596Z | Page-header    | 58022   | 5.10         | 6.70         | 5.06         | 85-89                                      | 66-76                                      | 90-94                                      | 98-100                                     | 91-92                                      | 97-99                                      | 81-86                                      |
2025-04-11T09:32:54.4026247Z | Picture        | 45976   | 4.21         | 2.78         | 5.31         | 69-71                                      | 56-59                                      | 82-86                                      | 69-82                                      | 80-95                                      | 66-71                                      | 59-76                                      |
2025-04-11T09:32:54.4026907Z | Section-header | 142884  | 12.60        | 15.77        | 12.85        | 83-84                                      | 76-81                                      | 90-92                                      | 94-95                                      | 87-94                                      | 69-73                                      | 78-86                                      |
2025-04-11T09:32:54.4027566Z | Table          | 34733   | 3.20         | 2.27         | 3.60         | 77-81                                      | 75-80                                      | 83-86                                      | 98-99                                      | 58-80                                      | 79-84                                      | 70-85                                      |
2025-04-11T09:32:54.4028183Z | Text           | 510377  | 45.82        | 49.28        | 45.00        | 84-86                                      | 81-86                                      | 88-93                                      | 89-93                                      | 87-92                                      | 71-79                                      | 87-95                                      |
2025-04-11T09:32:54.4029206Z | Title          | 5071    | 0.47         | 0.30         | 0.50         | 60-72                                      | 24-63                                      | 50-63                                      | 94-100                                     | 82-96                                      | 68-79                                      | 24-56                                      |
2025-04-11T09:32:54.4030612Z | Total          | 1107470 | 941123       | 99816        | 66531        | 82-83                                      | 71-74                                      | 79-81                                      | 89-94                                      | 86-91                                      | 71-76                                      | 68-85                                      |
2025-04-11T09:32:54.4031312Z 
2025-04-11T09:32:54.4032636Z Figure 3: Corpus Conversion Service annotation user interface. The PDF page is shown in the background, with overlaid text-cells (in darker shades). The annotation boxes can be drawn by dragging a rectangle over each segment with the respective label from the palette on the right.
2025-04-11T09:32:54.4033499Z 
2025-04-11T09:32:54.4033693Z <!-- image -->
2025-04-11T09:32:54.4033813Z 
2025-04-11T09:32:54.4034455Z we distributed the annotation workload and performed continuous quality controls. Phase one and two required a small team of experts only. For phases three and four, a group of 40 dedicated annotators were assembled and supervised.
2025-04-11T09:32:54.4035166Z 
2025-04-11T09:32:54.4036809Z Phase 1: Data selection and preparation. Our inclusion criteria for documents were described in Section 3. A large effort went into ensuring that all documents are free to use. The data sources include publication repositories such as arXiv 3 , government offices, company websites as well as data directory services for financial reports and patents. Scanned documents were excluded wherever possible because they can be rotated or skewed. This would not allow us to perform annotation with rectangular bounding-boxes and therefore complicate the annotation process.
2025-04-11T09:32:54.4038510Z 
2025-04-11T09:32:54.4040945Z Preparation work included uploading and parsing the sourced PDF documents in the Corpus Conversion Service (CCS) [22], a cloud-native platform which provides a visual annotation interface and allows for dataset inspection and analysis. The annotation interface of CCS is shown in Figure 3. The desired balance of pages between the different document categories was achieved by selective subsampling of pages with certain desired properties. For example, we made sure to include the title page of each document and bias the remaining page selection to those with figures or tables. The latter was achieved by leveraging pre-trained object detection models from PubLayNet, which helped us estimate how many figures and tables a given page contains.
2025-04-11T09:32:54.4043192Z 
2025-04-11T09:32:54.4046475Z Phase 2: Label selection and guideline. We reviewed the collected documents and identified the most common structural features they exhibit. This was achieved by identifying recurrent layout elements and lead us to the definition of 11 distinct class labels. These 11 class labels are Caption , Footnote , Formula List-item , , Pagefooter , Page-header , Picture , Section-header , Table , Text , and Title . Critical factors that were considered for the choice of these class labels were (1) the overall occurrence of the label, (2) the specificity of the label, (3) recognisability on a single page (i.e. no need for context from previous or next page) and (4) overall coverage of the page. Specificity ensures that the choice of label is not ambiguous, while coverage ensures that all meaningful items on a page can be annotated. We refrained from class labels that are very specific to a document category, such as Abstract in the Scientific Articles category. We also avoided class labels that are tightly linked to the semantics of the text. Labels such as Author and Affiliation , as seen in DocBank, are often only distinguishable by discriminating on
2025-04-11T09:32:54.4050190Z 
2025-04-11T09:32:54.4050293Z 3 https://arxiv.org/
2025-04-11T09:32:54.4050428Z 
2025-04-11T09:32:54.4051062Z the textual content of an element, which goes beyond visual layout recognition, in particular outside the Scientific Articles category.
2025-04-11T09:32:54.4051510Z 
2025-04-11T09:32:54.4053993Z At first sight, the task of visual document-layout interpretation appears intuitive enough to obtain plausible annotations in most cases. However, during early trial-runs in the core team, we observed many cases in which annotators use different annotation styles, especially for documents with challenging layouts. For example, if a figure is presented with subfigures, one annotator might draw a single figure bounding-box, while another might annotate each subfigure separately. The same applies for lists, where one might annotate all list items in one block or each list item separately. In essence, we observed that challenging layouts would be annotated in different but plausible ways. To illustrate this, we show in Figure 4 multiple examples of plausible but inconsistent annotations on the same pages.
2025-04-11T09:32:54.4056465Z 
2025-04-11T09:32:54.4057828Z Obviously, this inconsistency in annotations is not desirable for datasets which are intended to be used for model training. To minimise these inconsistencies, we created a detailed annotation guideline. While perfect consistency across 40 annotation staff members is clearly not possible to achieve, we saw a huge improvement in annotation consistency after the introduction of our annotation guideline. A few selected, non-trivial highlights of the guideline are:
2025-04-11T09:32:54.4059491Z 
2025-04-11T09:32:54.4060047Z - (1) Every list-item is an individual object instance with class label List-item . This definition is different from PubLayNet and DocBank, where all list-items are grouped together into one List object.
2025-04-11T09:32:54.4061257Z - (2) A List-item is a paragraph with hanging indentation. Singleline elements can qualify as List-item if the neighbour elements expose hanging indentation. Bullet or enumeration symbols are not a requirement.
2025-04-11T09:32:54.4062112Z - (3) For every Caption , there must be exactly one corresponding Picture or Table .
2025-04-11T09:32:54.4062568Z - (4) Connected sub-pictures are grouped together in one Picture object.
2025-04-11T09:32:54.4062938Z - (5) Formula numbers are included in a Formula object.
2025-04-11T09:32:54.4063542Z - (6) Emphasised text (e.g. in italic or bold) at the beginning of a paragraph is not considered a Section-header , unless it appears exclusively on its own line.
2025-04-11T09:32:54.4064028Z 
2025-04-11T09:32:54.4064640Z The complete annotation guideline is over 100 pages long and a detailed description is obviously out of scope for this paper. Nevertheless, it will be made publicly available alongside with DocLayNet for future reference.
2025-04-11T09:32:54.4065336Z 
2025-04-11T09:32:54.4067630Z Phase 3: Training. After a first trial with a small group of people, we realised that providing the annotation guideline and a set of random practice pages did not yield the desired quality level for layout annotation. Therefore we prepared a subset of pages with two different complexity levels, each with a practice and an exam part. 974 pages were reference-annotated by one proficient core team member. Annotation staff were then given the task to annotate the same subsets (blinded from the reference). By comparing the annotations of each staff member with the reference annotations, we could quantify how closely their annotations matched the reference. Only after passing two exam levels with high annotation quality, staff were admitted into the production phase. Practice iterations
2025-04-11T09:32:54.4070079Z 
2025-04-11T09:32:54.4070149Z <!-- image -->
2025-04-11T09:32:54.4070259Z 
2025-04-11T09:32:54.4070421Z 05237a14f2524e3f53c8454b074409d05078038a6a36b770fcc8ec7e540deae0
2025-04-11T09:32:54.4070669Z 
2025-04-11T09:32:54.4071150Z Figure 4: Examples of plausible annotation alternatives for the same page. Criteria in our annotation guideline can resolve cases A to C, while the case D remains ambiguous.
2025-04-11T09:32:54.4071831Z 
2025-04-11T09:32:54.4072136Z were carried out over a timeframe of 12 weeks, after which 8 of the 40 initially allocated annotators did not pass the bar.
2025-04-11T09:32:54.4072527Z 
2025-04-11T09:32:54.4075519Z Phase 4: Production annotation. The previously selected 80K pages were annotated with the defined 11 class labels by 32 annotators. This production phase took around three months to complete. All annotations were created online through CCS, which visualises the programmatic PDF text-cells as an overlay on the page. The page annotation are obtained by drawing rectangular bounding-boxes, as shown in Figure 3. With regard to the annotation practices, we implemented a few constraints and capabilities on the tooling level. First, we only allow non-overlapping, vertically oriented, rectangular boxes. For the large majority of documents, this constraint was sufficient and it speeds up the annotation considerably in comparison with arbitrary segmentation shapes. Second, annotator staff were not able to see each other's annotations. This was enforced by design to avoid any bias in the annotation, which could skew the numbers of the inter-annotator agreement (see Table 1). We wanted
2025-04-11T09:32:54.4078466Z 
2025-04-11T09:32:54.4080108Z Table 2: Prediction performance (mAP@0.5-0.95) of object detection networks on DocLayNet test set. The MRCNN (Mask R-CNN) and FRCNN (Faster R-CNN) models with ResNet-50 or ResNet-101 backbone were trained based on the network architectures from the detectron2 model zoo (Mask R-CNN R50, R101-FPN 3x, Faster R-CNN R101-FPN 3x), with default configurations. The YOLO implementation utilized was YOLOv5x6 [13]. All models were initialised using pre-trained weights from the COCO 2017 dataset.
2025-04-11T09:32:54.4081524Z 
2025-04-11T09:32:54.4081648Z |                | human   | MRCNN   | MRCNN   | FRCNN   | YOLO   |
2025-04-11T09:32:54.4081966Z |----------------|---------|---------|---------|---------|--------|
2025-04-11T09:32:54.4082249Z |                |         | R50     | R101    | R101    | v5x6   |
2025-04-11T09:32:54.4082531Z | Caption        | 84-89   | 68.4    | 71.5    | 70.1    | 77.7   |
2025-04-11T09:32:54.4082834Z | Footnote       | 83-91   | 70.9    | 71.8    | 73.7    | 77.2   |
2025-04-11T09:32:54.4083138Z | Formula        | 83-85   | 60.1    | 63.4    | 63.5    | 66.2   |
2025-04-11T09:32:54.4083440Z | List-item      | 87-88   | 81.2    | 80.8    | 81.0    | 86.2   |
2025-04-11T09:32:54.4083754Z | Page-footer    | 93-94   | 61.6    | 59.3    | 58.9    | 61.1   |
2025-04-11T09:32:54.4084073Z | Page-header    | 85-89   | 71.9    | 70.0    | 72.0    | 67.9   |
2025-04-11T09:32:54.4084380Z | Picture        | 69-71   | 71.7    | 72.7    | 72.0    | 77.1   |
2025-04-11T09:32:54.4084698Z | Section-header | 83-84   | 67.6    | 69.3    | 68.4    | 74.6   |
2025-04-11T09:32:54.4085014Z | Table          | 77-81   | 82.2    | 82.9    | 82.2    | 86.3   |
2025-04-11T09:32:54.4085298Z | Text           | 84-86   | 84.6    | 85.8    | 85.4    | 88.1   |
2025-04-11T09:32:54.4085582Z | Title          | 60-72   | 76.7    | 80.4    | 79.9    | 82.7   |
2025-04-11T09:32:54.4085859Z | All            | 82-83   | 72.4    | 73.5    | 73.4    | 76.8   |
2025-04-11T09:32:54.4086038Z 
2025-04-11T09:32:54.4089807Z to avoid this at any cost in order to have clear, unbiased baseline numbers for human document-layout annotation. Third, we introduced the feature of snapping boxes around text segments to obtain a pixel-accurate annotation and again reduce time and effort. The CCS annotation tool automatically shrinks every user-drawn box to the minimum bounding-box around the enclosed text-cells for all purely text-based segments, which excludes only Table and Picture . For the latter, we instructed annotation staff to minimise inclusion of surrounding whitespace while including all graphical lines. A downside of snapping boxes to enclosed text cells is that some wrongly parsed PDF pages cannot be annotated correctly and need to be skipped. Fourth, we established a way to flag pages as rejected for cases where no valid annotation according to the label guidelines could be achieved. Example cases for this would be PDF pages that render incorrectly or contain layouts that are impossible to capture with non-overlapping rectangles. Such rejected pages are not contained in the final dataset. With all these measures in place, experienced annotation staff managed to annotate a single page in a typical timeframe of 20s to 60s, depending on its complexity.
2025-04-11T09:32:54.4094491Z 
2025-04-11T09:32:54.4094565Z ## 5 EXPERIMENTS
2025-04-11T09:32:54.4094680Z 
2025-04-11T09:32:54.4096472Z The primary goal of DocLayNet is to obtain high-quality ML models capable of accurate document-layout analysis on a wide variety of challenging layouts. As discussed in Section 2, object detection models are currently the easiest to use, due to the standardisation of ground-truth data in COCO format [16] and the availability of general frameworks such as detectron2 [17]. Furthermore, baseline numbers in PubLayNet and DocBank were obtained using standard object detection models such as Mask R-CNN and Faster R-CNN. As such, we will relate to these object detection methods in this
2025-04-11T09:32:54.4098199Z 
2025-04-11T09:32:54.4099277Z Figure 5: Prediction performance (mAP@0.5-0.95) of a Mask R-CNNnetworkwithResNet50backbonetrainedonincreasing fractions of the DocLayNet dataset. The learning curve flattens around the 80% mark, indicating that increasing the size of the DocLayNet dataset with similar data will not yield significantly better predictions.
2025-04-11T09:32:54.4100295Z 
2025-04-11T09:32:54.4100371Z <!-- image -->
2025-04-11T09:32:54.4100473Z 
2025-04-11T09:32:54.4100743Z paper and leave the detailed evaluation of more recent methods mentioned in Section 2 for future work.
2025-04-11T09:32:54.4101095Z 
2025-04-11T09:32:54.4102173Z In this section, we will present several aspects related to the performance of object detection models on DocLayNet. Similarly as in PubLayNet, we will evaluate the quality of their predictions using mean average precision (mAP) with 10 overlaps that range from 0.5 to 0.95 in steps of 0.05 (mAP@0.5-0.95). These scores are computed by leveraging the evaluation code provided by the COCO API [16].
2025-04-11T09:32:54.4103308Z 
2025-04-11T09:32:54.4103402Z ## Baselines for Object Detection
2025-04-11T09:32:54.4103554Z 
2025-04-11T09:32:54.4107595Z In Table 2, we present baseline experiments (given in mAP) on Mask R-CNN [12], Faster R-CNN [11], and YOLOv5 [13]. Both training and evaluation were performed on RGB images with dimensions of 1025 × 1025 pixels. For training, we only used one annotation in case of redundantly annotated pages. As one can observe, the variation in mAP between the models is rather low, but overall between 6 and 10% lower than the mAP computed from the pairwise human annotations on triple-annotated pages. This gives a good indication that the DocLayNet dataset poses a worthwhile challenge for the research community to close the gap between human recognition and ML approaches. It is interesting to see that Mask R-CNN and Faster R-CNN produce very comparable mAP scores, indicating that pixel-based image segmentation derived from bounding-boxes does not help to obtain better predictions. On the other hand, the more recent Yolov5x model does very well and even out-performs humans on selected labels such as Text , Table and Picture . This is not entirely surprising, as Text , Table and Picture are abundant and the most visually distinctive in a document.
2025-04-11T09:32:54.4111114Z 
2025-04-11T09:32:54.4111652Z Table 3: Performance of a Mask R-CNN R50 network in mAP@0.5-0.95 scores trained on DocLayNet with different class label sets. The reduced label sets were obtained by either down-mapping or dropping labels.
2025-04-11T09:32:54.4112265Z 
2025-04-11T09:32:54.4112373Z | Class-count    |   11 | 6       | 5       | 4       |
2025-04-11T09:32:54.4112660Z |----------------|------|---------|---------|---------|
2025-04-11T09:32:54.4112943Z | Caption        |   68 | Text    | Text    | Text    |
2025-04-11T09:32:54.4113352Z | Footnote       |   71 | Text    | Text    | Text    |
2025-04-11T09:32:54.4113625Z | Formula        |   60 | Text    | Text    | Text    |
2025-04-11T09:32:54.4113903Z | List-item      |   81 | Text    | 82      | Text    |
2025-04-11T09:32:54.4114184Z | Page-footer    |   62 | 62      | -       | -       |
2025-04-11T09:32:54.4114465Z | Page-header    |   72 | 68      | -       | -       |
2025-04-11T09:32:54.4114739Z | Picture        |   72 | 72      | 72      | 72      |
2025-04-11T09:32:54.4115019Z | Section-header |   68 | 67      | 69      | 68      |
2025-04-11T09:32:54.4115288Z | Table          |   82 | 83      | 82      | 82      |
2025-04-11T09:32:54.4115537Z | Text           |   85 | 84      | 84      | 84      |
2025-04-11T09:32:54.4115800Z | Title          |   77 | Sec.-h. | Sec.-h. | Sec.-h. |
2025-04-11T09:32:54.4116074Z | Overall        |   72 | 73      | 78      | 77      |
2025-04-11T09:32:54.4116363Z 
2025-04-11T09:32:54.4116442Z ## Learning Curve
2025-04-11T09:32:54.4116555Z 
2025-04-11T09:32:54.4119826Z One of the fundamental questions related to any dataset is if it is 'large enough'. To answer this question for DocLayNet, we performed a data ablation study in which we evaluated a Mask R-CNN model trained on increasing fractions of the DocLayNet dataset. As can be seen in Figure 5, the mAP score rises sharply in the beginning and eventually levels out. To estimate the error-bar on the metrics, we ran the training five times on the entire data-set. This resulted in a 1% error-bar, depicted by the shaded area in Figure 5. In the inset of Figure 5, we show the exact same data-points, but with a logarithmic scale on the x-axis. As is expected, the mAP score increases linearly as a function of the data-size in the inset. The curve ultimately flattens out between the 80% and 100% mark, with the 80% mark falling within the error-bars of the 100% mark. This provides a good indication that the model would not improve significantly by yet increasing the data size. Rather, it would probably benefit more from improved data consistency (as discussed in Section 3), data augmentation methods [23], or the addition of more document categories and styles.
2025-04-11T09:32:54.4123175Z 
2025-04-11T09:32:54.4123255Z ## Impact of Class Labels
2025-04-11T09:32:54.4123390Z 
2025-04-11T09:32:54.4126497Z The choice and number of labels can have a significant effect on the overall model performance. Since PubLayNet, DocBank and DocLayNet all have different label sets, it is of particular interest to understand and quantify this influence of the label set on the model performance. We investigate this by either down-mapping labels into more common ones (e.g. Caption → Text ) or excluding them from the annotations entirely. Furthermore, it must be stressed that all mappings and exclusions were performed on the data before model training. In Table 3, we present the mAP scores for a Mask R-CNN R50 network on different label sets. Where a label is down-mapped, we show its corresponding label, otherwise it was excluded. We present three different label sets, with 6, 5 and 4 different labels respectively. The set of 5 labels contains the same labels as PubLayNet. However, due to the different definition of
2025-04-11T09:32:54.4129195Z 
2025-04-11T09:32:54.4129692Z Table 4: Performance of a Mask R-CNN R50 network with document-wise and page-wise split for different label sets. Naive page-wise split will result in /tildelow 10% point improvement.
2025-04-11T09:32:54.4130255Z 
2025-04-11T09:32:54.4130351Z | Class-count    | 11   | 11   | 5   | 5    |
2025-04-11T09:32:54.4130607Z |----------------|------|------|-----|------|
2025-04-11T09:32:54.4130853Z | Split          | Doc  | Page | Doc | Page |
2025-04-11T09:32:54.4131103Z | Caption        | 68   | 83   |     |      |
2025-04-11T09:32:54.4131343Z | Footnote       | 71   | 84   |     |      |
2025-04-11T09:32:54.4131583Z | Formula        | 60   | 66   |     |      |
2025-04-11T09:32:54.4131830Z | List-item      | 81   | 88   | 82  | 88   |
2025-04-11T09:32:54.4132229Z | Page-footer    | 62   | 89   |     |      |
2025-04-11T09:32:54.4132473Z | Page-header    | 72   | 90   |     |      |
2025-04-11T09:32:54.4132712Z | Picture        | 72   | 82   | 72  | 82   |
2025-04-11T09:32:54.4132959Z | Section-header | 68   | 83   | 69  | 83   |
2025-04-11T09:32:54.4133203Z | Table          | 82   | 89   | 82  | 90   |
2025-04-11T09:32:54.4133439Z | Text           | 85   | 91   | 84  | 90   |
2025-04-11T09:32:54.4133672Z | Title          | 77   | 81   |     |      |
2025-04-11T09:32:54.4133903Z | All            | 72   | 84   | 78  | 87   |
2025-04-11T09:32:54.4134059Z 
2025-04-11T09:32:54.4135485Z lists in PubLayNet (grouped list-items) versus DocLayNet (separate list-items), the label set of size 4 is the closest to PubLayNet, in the assumption that the List is down-mapped to Text in PubLayNet. The results in Table 3 show that the prediction accuracy on the remaining class labels does not change significantly when other classes are merged into them. The overall macro-average improves by around 5%, in particular when Page-footer and Page-header are excluded.
2025-04-11T09:32:54.4136867Z 
2025-04-11T09:32:54.4136981Z ## Impact of Document Split in Train and Test Set
2025-04-11T09:32:54.4137170Z 
2025-04-11T09:32:54.4140182Z Many documents in DocLayNet have a unique styling. In order to avoid overfitting on a particular style, we have split the train-, test- and validation-sets of DocLayNet on document boundaries, i.e. every document contributes pages to only one set. To the best of our knowledge, this was not considered in PubLayNet or DocBank. To quantify how this affects model performance, we trained and evaluated a Mask R-CNN R50 model on a modified dataset version. Here, the train-, test- and validation-sets were obtained by a randomised draw over the individual pages. As can be seen in Table 4, the difference in model performance is surprisingly large: pagewise splitting gains ˜ 0% in mAP over the document-wise splitting. 1 Thus, random page-wise splitting of DocLayNet can easily lead to accidental overestimation of model performance and should be avoided.
2025-04-11T09:32:54.4142635Z 
2025-04-11T09:32:54.4142711Z ## Dataset Comparison
2025-04-11T09:32:54.4142830Z 
2025-04-11T09:32:54.4144295Z Throughout this paper, we claim that DocLayNet's wider variety of document layouts leads to more robust layout detection models. In Table 5, we provide evidence for that. We trained models on each of the available datasets (PubLayNet, DocBank and DocLayNet) and evaluated them on the test sets of the other datasets. Due to the different label sets and annotation styles, a direct comparison is not possible. Hence, we focussed on the common labels among the datasets. Between PubLayNet and DocLayNet, these are Picture ,
2025-04-11T09:32:54.4145819Z 
2025-04-11T09:32:54.4146649Z Table 5: Prediction Performance (mAP@0.5-0.95) of a Mask R-CNN R50 network across the PubLayNet, DocBank &amp; DocLayNet data-sets. By evaluating on common label classes of each dataset, we observe that the DocLayNet-trained model has much less pronounced variations in performance across all datasets.
2025-04-11T09:32:54.4147554Z 
2025-04-11T09:32:54.4147667Z |                 |            | Testing on   | Testing on   | Testing on   |
2025-04-11T09:32:54.4147992Z |-----------------|------------|--------------|--------------|--------------|
2025-04-11T09:32:54.4148343Z | Training on     | labels     | PLN          | DB           | DLN          |
2025-04-11T09:32:54.4148794Z | PubLayNet (PLN) | Figure     | 96           | 43           | 23           |
2025-04-11T09:32:54.4149172Z | PubLayNet (PLN) | Sec-header | 87           | -            | 32           |
2025-04-11T09:32:54.4149511Z |                 | Table      | 95           | 24           | 49           |
2025-04-11T09:32:54.4149793Z |                 | Text       | 96           | -            | 42           |
2025-04-11T09:32:54.4150083Z |                 | total      | 93           | 34           | 30           |
2025-04-11T09:32:54.4150525Z | DocBank (DB)    | Figure     | 77           | 71           | 31           |
2025-04-11T09:32:54.4150864Z | DocBank (DB)    | Table      | 19           | 65           | 22           |
2025-04-11T09:32:54.4151195Z | DocBank (DB)    | total      | 48           | 68           | 27           |
2025-04-11T09:32:54.4151544Z | DocLayNet (DLN) | Figure     | 67           | 51           | 72           |
2025-04-11T09:32:54.4151910Z | DocLayNet (DLN) | Sec-header | 53           | -            | 68           |
2025-04-11T09:32:54.4152235Z |                 | Table      | 87           | 43           | 82           |
2025-04-11T09:32:54.4152526Z |                 | Text       | 77           | -            | 84           |
2025-04-11T09:32:54.4152811Z |                 | total      | 59           | 47           | 78           |
2025-04-11T09:32:54.4153004Z 
2025-04-11T09:32:54.4153918Z Section-header , Table and Text . Before training, we either mapped or excluded DocLayNet's other labels as specified in table 3, and also PubLayNet's List to Text . Note that the different clustering of lists (by list-element vs. whole list objects) naturally decreases the mAP score for Text .
2025-04-11T09:32:54.4154806Z 
2025-04-11T09:32:54.4157120Z For comparison of DocBank with DocLayNet, we trained only on Picture and Table clusters of each dataset. We had to exclude Text because successive paragraphs are often grouped together into a single object in DocBank. This paragraph grouping is incompatible with the individual paragraphs of DocLayNet. As can be seen in Table 5, DocLayNet trained models yield better performance compared to the previous datasets. It is noteworthy that the models trained on PubLayNet and DocBank perform very well on their own test set, but have a much lower performance on the foreign datasets. While this also applies to DocLayNet, the difference is far less pronounced. Thus we conclude that DocLayNet trained models are overall more robust and will produce better results for challenging, unseen layouts.
2025-04-11T09:32:54.4159568Z 
2025-04-11T09:32:54.4159653Z ## Example Predictions
2025-04-11T09:32:54.4159781Z 
2025-04-11T09:32:54.4161189Z To conclude this section, we illustrate the quality of layout predictions one can expect from DocLayNet-trained models by providing a selection of examples without any further post-processing applied. Figure 6 shows selected layout predictions on pages from the test-set of DocLayNet. Results look decent in general across document categories, however one can also observe mistakes such as overlapping clusters of different classes, or entirely missing boxes due to low confidence.
2025-04-11T09:32:54.4162658Z 
2025-04-11T09:32:54.4162731Z ## 6 CONCLUSION
2025-04-11T09:32:54.4162834Z 
2025-04-11T09:32:54.4164321Z In this paper, we presented the DocLayNet dataset. It provides the document conversion and layout analysis research community a new and challenging dataset to improve and fine-tune novel ML methods on. In contrast to many other datasets, DocLayNet was created by human annotation in order to obtain reliable layout ground-truth on a wide variety of publication- and typesettingstyles. Including a large proportion of documents outside the scientific publishing domain adds significant value in this respect.
2025-04-11T09:32:54.4165865Z 
2025-04-11T09:32:54.4167536Z From the dataset, we have derived on the one hand reference metrics for human performance on document-layout annotation (through double and triple annotations) and on the other hand evaluated the baseline performance of commonly used object detection methods. We also illustrated the impact of various dataset-related aspects on model performance through data-ablation experiments, both from a size and class-label perspective. Last but not least, we compared the accuracy of models trained on other public datasets and showed that DocLayNet trained models are more robust.
2025-04-11T09:32:54.4169365Z 
2025-04-11T09:32:54.4169862Z To date, there is still a significant gap between human and ML accuracy on the layout interpretation task, and we hope that this work will inspire the research community to close that gap.
2025-04-11T09:32:54.4170559Z 
2025-04-11T09:32:54.4170624Z ## REFERENCES
2025-04-11T09:32:54.4170723Z 
2025-04-11T09:32:54.4171424Z - [1] Max Göbel, Tamir Hassan, Ermelinda Oro, and Giorgio Orsi. Icdar 2013 table competition. In 2013 12th International Conference on Document Analysis and Recognition , pages 1449-1453, 2013.
2025-04-11T09:32:54.4172822Z - [2] Christian Clausner, Apostolos Antonacopoulos, and Stefan Pletschacher. Icdar2017 competition on recognition of documents with complex layouts rdcl2017. In 2017 14th IAPR International Conference on Document Analysis and Recognition (ICDAR) , volume 01, pages 1404-1410, 2017.
2025-04-11T09:32:54.4174449Z - [3] Hervé Déjean, Jean-Luc Meunier, Liangcai Gao, Yilun Huang, Yu Fang, Florian Kleber, and Eva-Maria Lang. ICDAR 2019 Competition on Table Detection and Recognition (cTDaR), April 2019. http://sac.founderit.com/.
2025-04-11T09:32:54.4176110Z - [4] Antonio Jimeno Yepes, Peter Zhong, and Douglas Burdick. Competition on scientific literature parsing. In Proceedings of the International Conference on Document Analysis and Recognition , ICDAR, pages 605-617. LNCS 12824, SpringerVerlag, sep 2021.
2025-04-11T09:32:54.4177630Z - [5] Logan Markewich, Hao Zhang, Yubin Xing, Navid Lambert-Shirzad, Jiang Zhexin, Roy Lee, Zhi Li, and Seok-Bum Ko. Segmentation for document layout analysis: not dead yet. International Journal on Document Analysis and Recognition (IJDAR) , pages 1-11, 01 2022.
2025-04-11T09:32:54.4179161Z - [6] Xu Zhong, Jianbin Tang, and Antonio Jimeno-Yepes. Publaynet: Largest dataset ever for document layout analysis. In Proceedings of the International Conference on Document Analysis and Recognition , ICDAR, pages 1015-1022, sep 2019.
2025-04-11T09:32:54.4180792Z - [7] Minghao Li, Yiheng Xu, Lei Cui, Shaohan Huang, Furu Wei, Zhoujun Li, and Ming Zhou. Docbank: A benchmark dataset for document layout analysis. In Proceedings of the 28th International Conference on Computational Linguistics , COLING, pages 949-960. International Committee on Computational Linguistics, dec 2020.
2025-04-11T09:32:54.4182216Z - [8] Riaz Ahmad, Muhammad Tanvir Afzal, and M. Qadir. Information extraction from pdf sources based on rule-based system using integrated formats. In SemWebEval@ESWC , 2016.
2025-04-11T09:32:54.4183515Z - [9] Ross B. Girshick, Jeff Donahue, Trevor Darrell, and Jitendra Malik. Rich feature hierarchies for accurate object detection and semantic segmentation. In IEEE Conference on Computer Vision and Pattern Recognition , CVPR, pages 580-587. IEEE Computer Society, jun 2014.
2025-04-11T09:32:54.4184733Z - [10] Ross B. Girshick. Fast R-CNN. In 2015 IEEE International Conference on Computer Vision , ICCV, pages 1440-1448. IEEE Computer Society, dec 2015.
2025-04-11T09:32:54.4185803Z - [11] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster r-cnn: Towards real-time object detection with region proposal networks. IEEE Transactions on Pattern Analysis and Machine Intelligence , 39(6):1137-1149, 2017.
2025-04-11T09:32:54.4187138Z - [12] Kaiming He, Georgia Gkioxari, Piotr Dollár, and Ross B. Girshick. Mask R-CNN. In IEEE International Conference on Computer Vision , ICCV, pages 2980-2988. IEEE Computer Society, Oct 2017.
2025-04-11T09:32:54.4188557Z - [13] Glenn Jocher, Alex Stoken, Ayush Chaurasia, Jirka Borovec, NanoCode012, TaoXie, Yonghye Kwon, Kalen Michael, Liu Changyu, Jiacong Fang, Abhiram V, Laughing, tkianai, yxNONG, Piotr Skalski, Adam Hogan, Jebastin Nadar, imyhxy, Lorenzo Mammana, Alex Wang, Cristi Fati, Diego Montes, Jan Hajek, Laurentiu
2025-04-11T09:32:54.4189580Z 
2025-04-11T09:32:54.4189824Z Text Caption List-Item Formula Table Section-Header Picture Page-Header Page-Footer Title
2025-04-11T09:32:54.4190149Z 
2025-04-11T09:32:54.4190221Z <!-- image -->
2025-04-11T09:32:54.4190325Z 
2025-04-11T09:32:54.4191448Z Figure 6: Example layout predictions on selected pages from the DocLayNet test-set. (A, D) exhibit favourable results on coloured backgrounds. (B, C) show accurate list-item and paragraph differentiation despite densely-spaced lines. (E) demonstrates good table and figure distinction. (F) shows predictions on a Chinese patent with multiple overlaps, label confusion and missing boxes.
2025-04-11T09:32:54.4192757Z 
2025-04-11T09:32:54.4193298Z Diaconu, Mai Thanh Minh, Marc, albinxavi, fatih, oleg, and wanghao yang. ultralytics/yolov5: v6.0 - yolov5n nano models, roboflow integration, tensorflow export, opencv dnn support, October 2021.
2025-04-11T09:32:54.4193907Z 
2025-04-11T09:32:54.4194301Z - [20] Shoubin Li, Xuyan Ma, Shuaiqun Pan, Jun Hu, Lin Shi, and Qing Wang. Vtlayout: Fusion of visual and text features for document layout analysis, 2021.
2025-04-11T09:32:54.4195270Z - [14] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and Sergey Zagoruyko. End-to-end object detection with transformers. CoRR , abs/2005.12872, 2020.
2025-04-11T09:32:54.4196331Z - [15] Mingxing Tan, Ruoming Pang, and Quoc V. Le. Efficientdet: Scalable and efficient object detection. CoRR , abs/1911.09070, 2019.
2025-04-11T09:32:54.4197467Z - [16] Tsung-Yi Lin, Michael Maire, Serge J. Belongie, Lubomir D. Bourdev, Ross B. Girshick, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, and C. Lawrence Zitnick. Microsoft COCO: common objects in context, 2014.
2025-04-11T09:32:54.4198374Z - [17] Yuxin Wu, Alexander Kirillov, Francisco Massa, Wan-Yen Lo, and Ross Girshick. Detectron2, 2019.
2025-04-11T09:32:54.4199786Z - [18] Nikolaos Livathinos, Cesar Berrospi, Maksym Lysak, Viktor Kuropiatnyk, Ahmed Nassar, Andre Carvalho, Michele Dolfi, Christoph Auer, Kasper Dinkla, and Peter W. J. Staar. Robust pdf document conversion using recurrent neural networks. In Proceedings of the 35th Conference on Artificial Intelligence , AAAI, pages 1513715145, feb 2021.
2025-04-11T09:32:54.4201749Z - [19] Yiheng Xu, Minghao Li, Lei Cui, Shaohan Huang, Furu Wei, and Ming Zhou. Layoutlm: Pre-training of text and layout for document image understanding. In Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining , KDD, pages 1192-1200, New York, USA, 2020. Association for Computing Machinery.
2025-04-11T09:32:54.4203212Z - [21] Peng Zhang, Can Li, Liang Qiao, Zhanzhan Cheng, Shiliang Pu, Yi Niu, and Fei Wu. Vsr: A unified framework for document layout analysis combining vision, semantics and relations, 2021.
2025-04-11T09:32:54.4204565Z - [22] Peter W J Staar, Michele Dolfi, Christoph Auer, and Costas Bekas. Corpus conversion service: A machine learning platform to ingest documents at scale. In Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining , KDD, pages 774-782. ACM, 2018.
2025-04-11T09:32:54.4205841Z - [23] Connor Shorten and Taghi M. Khoshgoftaar. A survey on image data augmentation for deep learning. Journal of Big Data , 6(1):60, 2019.
2025-04-11T09:32:54.4206361Z Conversion secs: [117.48811739000007]
2025-04-11T09:32:55.3816445Z Running example docs/examples/run_with_formats.py
2025-04-11T09:35:30.8843082Z Document README.md converted.
2025-04-11T09:35:30.8843709Z Saved markdown output to: scratch
2025-04-11T09:35:30.8844044Z Document wiki_duck.html converted.
2025-04-11T09:35:30.8844364Z Saved markdown output to: scratch
2025-04-11T09:35:30.8844673Z Document word_sample.docx converted.
2025-04-11T09:35:30.8844994Z Saved markdown output to: scratch
2025-04-11T09:35:30.8845290Z Document lorem_ipsum.docx converted.
2025-04-11T09:35:30.8845594Z Saved markdown output to: scratch
2025-04-11T09:35:30.8845919Z Document powerpoint_sample.pptx converted.
2025-04-11T09:35:30.8846253Z Saved markdown output to: scratch
2025-04-11T09:35:30.8846579Z Document 2305.03393v1-pg9-img.png converted.
2025-04-11T09:35:30.8846910Z Saved markdown output to: scratch
2025-04-11T09:35:30.8847202Z Document 2206.01062.pdf converted.
2025-04-11T09:35:30.8847491Z Saved markdown output to: scratch
2025-04-11T09:35:30.8847752Z Document test_01.asciidoc converted.
2025-04-11T09:35:30.8847991Z Saved markdown output to: scratch
2025-04-11T09:35:31.8319245Z Running example docs/examples/tesseract_lang_detection.py
2025-04-11T09:35:48.4565809Z Tesseract detected the script 'Fraktur' and language 'Fraktur'. However this language is not installed in your system and will be ignored.
2025-04-11T09:36:33.7765351Z Tesseract cannot detect the script of the page
2025-04-11T09:36:34.0433746Z Tesseract cannot detect the script of the page
2025-04-11T09:36:34.3515587Z Tesseract cannot detect the script of the page
2025-04-11T09:36:35.8957941Z ## DocLayNet: A Large Human-Annotated Dataset for Document-Layout Analysis
2025-04-11T09:36:35.8958944Z 
2025-04-11T09:36:35.8959439Z Birgit Pfitzmann IBM Research Rueschlikon, Switzerland bpf@zurich.ibm.com
2025-04-11T09:36:35.8960168Z 
2025-04-11T09:36:35.8960633Z Christoph Auer IBM Research Rueschlikon, Switzerland cau@zurich.ibm.com
2025-04-11T09:36:35.8961279Z 
2025-04-11T09:36:35.8962194Z Michele Dolfi IBM Research Rueschlikon, Switzerland dol@zurich.ibm.com
2025-04-11T09:36:35.8962599Z 
2025-04-11T09:36:35.8962872Z Ahmed S. Nassar IBM Research Rueschlikon, Switzerland ahn@zurich.ibm.com
2025-04-11T09:36:35.8963248Z 
2025-04-11T09:36:35.8963491Z Peter Staar IBM Research Rueschlikon, Switzerland taa@zurich.ibm.com
2025-04-11T09:36:35.8963877Z 
2025-04-11T09:36:35.8963982Z ## ABSTRACT
2025-04-11T09:36:35.8964133Z 
2025-04-11T09:36:35.8970498Z Accurate document layout analysis is a key requirement for highquality PDF document conversion. With the recent availability of public, large ground-truth datasets such as PubLayNet and DocBank, deep-learning models have proven to be very effective at layout detection and segmentation. While these datasets are of adequate size to train such models, they severely lack in layout variability since they are sourced from scientific article repositories such as PubMed and arXiv only. Consequently, the accuracy of the layout segmentation drops significantly when these models are applied on more challenging and diverse layouts. In this paper, we present DocLayNet , a new, publicly available, document-layout annotation dataset in COCO format. It contains 80863 manually annotated pages from diverse data sources to represent a wide variability in layouts. For each PDF page, the layout annotations provide labelled bounding-boxes with a choice of 11 distinct classes. DocLayNet also provides a subset of double- and triple-annotated pages to determine the inter-annotator agreement. In multiple experiments, we provide baseline accuracy scores (in mAP) for a set of popular object detection models. We also demonstrate that these models fall approximately 10% behind the inter-annotator agreement. Furthermore, we provide evidence that DocLayNet is of sufficient size. Lastly, we compare models trained on PubLayNet, DocBank and DocLayNet, showing that layout predictions of the DocLayNettrained models are more robust and thus the preferred choice for general-purpose document-layout analysis.
2025-04-11T09:36:35.8976492Z 
2025-04-11T09:36:35.8976576Z ## CCS CONCEPTS
2025-04-11T09:36:35.8976690Z 
2025-04-11T09:36:35.8977504Z · Informationsystems → Documentstructure ; · Appliedcomputing → Document analysis ; · Computing methodologies → Machine learning Computer vision ; ; Object detection ;
2025-04-11T09:36:35.8978083Z 
2025-04-11T09:36:35.8979299Z Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for third-party components of this work must be honored. For all other uses, contact the owner/author(s).
2025-04-11T09:36:35.8980468Z 
2025-04-11T09:36:35.8980578Z KDD '22, August 14-18, 2022, Washington, DC, USA
2025-04-11T09:36:35.8980760Z 
2025-04-11T09:36:35.8980929Z © 2022 Copyright held by the owner/author(s).
2025-04-11T09:36:35.8981108Z 
2025-04-11T09:36:35.8981202Z ACM ISBN 978-1-4503-9385-0/22/08.
2025-04-11T09:36:35.8981354Z 
2025-04-11T09:36:35.8981652Z https://doi.org/10.1145/3534678.3539043
2025-04-11T09:36:35.8981831Z 
2025-04-11T09:36:35.8982042Z Figure 1: Four examples of complex page layouts across different document categories
2025-04-11T09:36:35.8982336Z 
2025-04-11T09:36:35.8982407Z <!-- image -->
2025-04-11T09:36:35.8982523Z 
2025-04-11T09:36:35.8982590Z ## KEYWORDS
2025-04-11T09:36:35.8982681Z 
2025-04-11T09:36:35.8982934Z PDF document conversion, layout segmentation, object-detection, data set, Machine Learning
2025-04-11T09:36:35.8983259Z 
2025-04-11T09:36:35.8983340Z ## ACMReference Format:
2025-04-11T09:36:35.8983466Z 
2025-04-11T09:36:35.8984623Z Birgit Pfitzmann, Christoph Auer, Michele Dolfi, Ahmed S. Nassar, and Peter Staar. 2022. DocLayNet: A Large Human-Annotated Dataset for DocumentLayout Analysis. In Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD '22), August 14-18, 2022, Washington, DC, USA. ACM, New York, NY, USA, 9 pages. https://doi.org/10.1145/ 3534678.3539043
2025-04-11T09:36:35.8985723Z 
2025-04-11T09:36:35.8985798Z ## 1 INTRODUCTION
2025-04-11T09:36:35.8985908Z 
2025-04-11T09:36:35.8987962Z Despite the substantial improvements achieved with machine-learning (ML) approaches and deep neural networks in recent years, document conversion remains a challenging problem, as demonstrated by the numerous public competitions held on this topic [1-4]. The challenge originates from the huge variability in PDF documents regarding layout, language and formats (scanned, programmatic or a combination of both). Engineering a single ML model that can be applied on all types of documents and provides high-quality layout segmentation remains to this day extremely challenging [5]. To highlight the variability in document layouts, we show a few example documents from the DocLayNet dataset in Figure 1.
2025-04-11T09:36:35.8990185Z 
2025-04-11T09:36:35.8995324Z Akeyproblem in the process of document conversion is to understand the structure of a single document page, i.e. which segments of text should be grouped together in a unit. To train models for this task, there are currently two large datasets available to the community, PubLayNet [6] and DocBank [7]. They were introduced in 2019 and 2020 respectively and significantly accelerated the implementation of layout detection and segmentation models due to their sizes of 300K and 500K ground-truth pages. These sizes were achieved by leveraging an automation approach. The benefit of automated ground-truth generation is obvious: one can generate large ground-truth datasets at virtually no cost. However, the automation introduces a constraint on the variability in the dataset, because corresponding structured source data must be available. PubLayNet and DocBank were both generated from scientific document repositories (PubMed and arXiv), which provide XML or L A T E X sources. Those scientific documents present a limited variability in their layouts, because they are typeset in uniform templates provided by the publishers. Obviously, documents such as technical manuals, annual company reports, legal text, government tenders, etc. have very different and partially unique layouts. As a consequence, the layout predictions obtained from models trained on PubLayNet or DocBank is very reasonable when applied on scientific documents. However, for more artistic or free-style layouts, we see sub-par prediction quality from these models, which we demonstrate in Section 5.
2025-04-11T09:36:35.9000427Z 
2025-04-11T09:36:35.9001768Z In this paper, we present the DocLayNet dataset. It provides pageby-page layout annotation ground-truth using bounding-boxes for 11 distinct class labels on 80863 unique document pages, of which a fraction carry double- or triple-annotations. DocLayNet is similar in spirit to PubLayNet and DocBank and will likewise be made available to the public 1 in order to stimulate the document-layout analysis community. It distinguishes itself in the following aspects:
2025-04-11T09:36:35.9003167Z 
2025-04-11T09:36:35.9003565Z - (1) Human Annotation : In contrast to PubLayNet and DocBank, we relied on human annotation instead of automation approaches to generate the data set.
2025-04-11T09:36:35.9004518Z - (2) Large Layout Variability : We include diverse and complex layouts from a large variety of public sources.
2025-04-11T09:36:35.9005374Z - (3) Detailed Label Set : We define 11 class labels to distinguish layout features in high detail. PubLayNet provides 5 labels; DocBank provides 13, although not a superset of ours.
2025-04-11T09:36:35.9006225Z - (4) Redundant Annotations : A fraction of the pages in the DocLayNet data set carry more than one human annotation.
2025-04-11T09:36:35.9006601Z 
2025-04-11T09:36:35.9006755Z 1 https://developer.ibm.com/exchanges/data/all/doclaynet
2025-04-11T09:36:35.9006990Z 
2025-04-11T09:36:35.9007222Z This enables experimentation with annotation uncertainty and quality control analysis.
2025-04-11T09:36:35.9007544Z 
2025-04-11T09:36:35.9008465Z - (5) Pre-defined Train-, Test- &amp; Validation-set : Like DocBank, we provide fixed train-, test- &amp; validation-sets to ensure proportional representation of the class-labels. Further, we prevent leakage of unique layouts across sets, which has a large effect on model accuracy scores.
2025-04-11T09:36:35.9009612Z 
2025-04-11T09:36:35.9010441Z All aspects outlined above are detailed in Section 3. In Section 4, we will elaborate on how we designed and executed this large-scale human annotation campaign. We will also share key insights and lessons learned that might prove helpful for other parties planning to set up annotation campaigns.
2025-04-11T09:36:35.9011586Z 
2025-04-11T09:36:35.9013133Z In Section 5, we will present baseline accuracy numbers for a variety of object detection methods (Faster R-CNN, Mask R-CNN and YOLOv5) trained on DocLayNet. We further show how the model performance is impacted by varying the DocLayNet dataset size, reducing the label set and modifying the train/test-split. Last but not least, we compare the performance of models trained on PubLayNet, DocBank and DocLayNet and demonstrate that a model trained on DocLayNet provides overall more robust layout recovery.
2025-04-11T09:36:35.9014641Z 
2025-04-11T09:36:35.9014713Z ## 2 RELATED WORK
2025-04-11T09:36:35.9014827Z 
2025-04-11T09:36:35.9016700Z While early approaches in document-layout analysis used rulebased algorithms and heuristics [8], the problem is lately addressed with deep learning methods. The most common approach is to leverage object detection models [9-15]. In the last decade, the accuracy and speed of these models has increased dramatically. Furthermore, most state-of-the-art object detection methods can be trained and applied with very little work, thanks to a standardisation effort of the ground-truth data format [16] and common deep-learning frameworks [17]. Reference data sets such as PubLayNet [6] and DocBank provide their data in the commonly accepted COCO format [16].
2025-04-11T09:36:35.9018640Z 
2025-04-11T09:36:35.9020231Z Lately, new types of ML models for document-layout analysis have emerged in the community [18-21]. These models do not approach the problem of layout analysis purely based on an image representation of the page, as computer vision methods do. Instead, they combine the text tokens and image representation of a page in order to obtain a segmentation. While the reported accuracies appear to be promising, a broadly accepted data format which links geometric and textual features has yet to establish.
2025-04-11T09:36:35.9021705Z 
2025-04-11T09:36:35.9021790Z ## 3 THE DOCLAYNET DATASET
2025-04-11T09:36:35.9021924Z 
2025-04-11T09:36:35.9023459Z DocLayNet contains 80863 PDF pages. Among these, 7059 carry two instances of human annotations, and 1591 carry three. This amounts to 91104 total annotation instances. The annotations provide layout information in the shape of labeled, rectangular boundingboxes. We define 11 distinct labels for layout features, namely Caption , Footnote , Formula List-item , , Page-footer , Page-header , Picture , Section-header , Table , Text , and Title . Our reasoning for picking this particular label set is detailed in Section 4.
2025-04-11T09:36:35.9025182Z 
2025-04-11T09:36:35.9025698Z In addition to open intellectual property constraints for the source documents, we required that the documents in DocLayNet adhere to a few conditions. Firstly, we kept scanned documents
2025-04-11T09:36:35.9026286Z 
2025-04-11T09:36:35.9026464Z Figure 2: Distribution of DocLayNet pages across document categories.
2025-04-11T09:36:35.9026718Z 
2025-04-11T09:36:35.9026792Z <!-- image -->
2025-04-11T09:36:35.9026896Z 
2025-04-11T09:36:35.9028479Z to a minimum, since they introduce difficulties in annotation (see Section 4). As a second condition, we focussed on medium to large documents ( &gt; 10 pages) with technical content, dense in complex tables, figures, plots and captions. Such documents carry a lot of information value, but are often hard to analyse with high accuracy due to their challenging layouts. Counterexamples of documents not included in the dataset are receipts, invoices, hand-written documents or photographs showing 'text in the wild".
2025-04-11T09:36:35.9030187Z 
2025-04-11T09:36:35.9032588Z The pages in DocLayNet can be grouped into six distinct categories, namely Financial Reports , Manuals Scientific Articles , , Laws &amp; Regulations , Patents and Government Tenders . Each document category was sourced from various repositories. For example, Financial Reports contain both free-style format annual reports 2 which expose company-specific, artistic layouts as well as the more formal SEC filings. The two largest categories ( Financial Reports and Manuals ) contain a large amount of free-style layouts in order to obtain maximum variability. In the other four categories, we boosted the variability by mixing documents from independent providers, such as different government websites or publishers. In Figure 2, we show the document categories contained in DocLayNet with their respective sizes.
2025-04-11T09:36:35.9035035Z 
2025-04-11T09:36:35.9036529Z We did not control the document selection with regard to language. The vast majority of documents contained in DocLayNet (close to 95%) are published in English language. However, DocLayNet also contains a number of documents in other languages such as German (2.5%), French (1.0%) and Japanese (1.0%). While the document language has negligible impact on the performance of computer vision methods such as object detection and segmentation models, it might prove challenging for layout analysis methods which exploit textual features.
2025-04-11T09:36:35.9038088Z 
2025-04-11T09:36:35.9039564Z To ensure that future benchmarks in the document-layout analysis community can be easily compared, we have split up DocLayNet into pre-defined train-, test- and validation-sets. In this way, we can avoid spurious variations in the evaluation scores due to random splitting in train-, test- and validation-sets. We also ensured that less frequent labels are represented in train and test sets in equal proportions.
2025-04-11T09:36:35.9040817Z 
2025-04-11T09:36:35.9040941Z 2 e.g. AAPL from https://www.annualreports.com/
2025-04-11T09:36:35.9041148Z 
2025-04-11T09:36:35.9042384Z Table 1 shows the overall frequency and distribution of the labels among the different sets. Importantly, we ensure that subsets are only split on full-document boundaries. This avoids that pages of the same document are spread over train, test and validation set, which can give an undesired evaluation advantage to models and lead to overestimation of their prediction accuracy. We will show the impact of this decision in Section 5.
2025-04-11T09:36:35.9043681Z 
2025-04-11T09:36:35.9046093Z In order to accommodate the different types of models currently in use by the community, we provide DocLayNet in an augmented COCO format [16]. This entails the standard COCO ground-truth file (in JSON format) with the associated page images (in PNG format, 1025 × 1025 pixels). Furthermore, custom fields have been added to each COCO record to specify document category, original document filename and page number. In addition, we also provide the original PDF pages, as well as sidecar files containing parsed PDF text and text-cell coordinates (in JSON). All additional files are linked to the primary page images by their matching filenames.
2025-04-11T09:36:35.9048103Z 
2025-04-11T09:36:35.9053806Z Despite being cost-intense and far less scalable than automation, human annotation has several benefits over automated groundtruth generation. The first and most obvious reason to leverage human annotations is the freedom to annotate any type of document without requiring a programmatic source. For most PDF documents, the original source document is not available. The latter is not a hard constraint with human annotation, but it is for automated methods. A second reason to use human annotations is that the latter usually provide a more natural interpretation of the page layout. The human-interpreted layout can significantly deviate from the programmatic layout used in typesetting. For example, 'invisible' tables might be used solely for aligning text paragraphs on columns. Such typesetting tricks might be interpreted by automated methods incorrectly as an actual table, while the human annotation will interpret it correctly as Text or other styles. The same applies to multi-line text elements, when authors decided to space them as 'invisible' list elements without bullet symbols. A third reason to gather ground-truth through human annotation is to estimate a 'natural' upper bound on the segmentation accuracy. As we will show in Section 4, certain documents featuring complex layouts can have different but equally acceptable layout interpretations. This natural upper bound for segmentation accuracy can be found by annotating the same pages multiple times by different people and evaluating the inter-annotator agreement. Such a baseline consistency evaluation is very useful to define expectations for a good target accuracy in trained deep neural network models and avoid overfitting (see Table 1). On the flip side, achieving high annotation consistency proved to be a key challenge in human annotation, as we outline in Section 4.
2025-04-11T09:36:35.9059551Z 
2025-04-11T09:36:35.9059641Z ## 4 ANNOTATION CAMPAIGN
2025-04-11T09:36:35.9059772Z 
2025-04-11T09:36:35.9061056Z The annotation campaign was carried out in four phases. In phase one, we identified and prepared the data sources for annotation. In phase two, we determined the class labels and how annotations should be done on the documents in order to obtain maximum consistency. The latter was guided by a detailed requirement analysis and exhaustive experiments. In phase three, we trained the annotation staff and performed exams for quality assurance. In phase four,
2025-04-11T09:36:35.9062395Z 
2025-04-11T09:36:35.9063382Z Table 1: DocLayNet dataset overview. Along with the frequency of each class label, we present the relative occurrence (as % of row 'Total') in the train, test and validation sets. The inter-annotator agreement is computed as the mAP@0.5-0.95 metric between pairwise annotations from the triple-annotated pages, from which we obtain accuracy ranges.
2025-04-11T09:36:35.9064413Z 
2025-04-11T09:36:35.9065199Z |                |         | % of Total   | % of Total   | % of Total   | triple inter-annotator mAP @0.5-0.95 (%)   | triple inter-annotator mAP @0.5-0.95 (%)   | triple inter-annotator mAP @0.5-0.95 (%)   | triple inter-annotator mAP @0.5-0.95 (%)   | triple inter-annotator mAP @0.5-0.95 (%)   | triple inter-annotator mAP @0.5-0.95 (%)   | triple inter-annotator mAP @0.5-0.95 (%)   |
2025-04-11T09:36:35.9066569Z |----------------|---------|--------------|--------------|--------------|--------------------------------------------|--------------------------------------------|--------------------------------------------|--------------------------------------------|--------------------------------------------|--------------------------------------------|--------------------------------------------|
2025-04-11T09:36:35.9067493Z | class label    | Count   | Train        | Test         | Val          | All                                        | Fin                                        | Man                                        | Sci                                        | Law                                        | Pat                                        | Ten                                        |
2025-04-11T09:36:35.9068303Z | Caption        | 22524   | 2.04         | 1.77         | 2.32         | 84-89                                      | 40-61                                      | 86-92                                      | 94-99                                      | 95-99                                      | 69-78                                      | n/a                                        |
2025-04-11T09:36:35.9069143Z | Footnote       | 6318    | 0.60         | 0.31         | 0.58         | 83-91                                      | n/a                                        | 100                                        | 62-88                                      | 85-94                                      | n/a                                        | 82-97                                      |
2025-04-11T09:36:35.9069779Z | Formula        | 25027   | 2.25         | 1.90         | 2.96         | 83-85                                      | n/a                                        | n/a                                        | 84-87                                      | 86-96                                      | n/a                                        | n/a                                        |
2025-04-11T09:36:35.9070424Z | List-item      | 185660  | 17.19        | 13.34        | 15.82        | 87-88                                      | 74-83                                      | 90-92                                      | 97-97                                      | 81-85                                      | 75-88                                      | 93-95                                      |
2025-04-11T09:36:35.9071102Z | Page-footer    | 70878   | 6.51         | 5.58         | 6.00         | 93-94                                      | 88-90                                      | 95-96                                      | 100                                        | 92-97                                      | 100                                        | 96-98                                      |
2025-04-11T09:36:35.9071774Z | Page-header    | 58022   | 5.10         | 6.70         | 5.06         | 85-89                                      | 66-76                                      | 90-94                                      | 98-100                                     | 91-92                                      | 97-99                                      | 81-86                                      |
2025-04-11T09:36:35.9072438Z | Picture        | 45976   | 4.21         | 2.78         | 5.31         | 69-71                                      | 56-59                                      | 82-86                                      | 69-82                                      | 80-95                                      | 66-71                                      | 59-76                                      |
2025-04-11T09:36:35.9073109Z | Section-header | 142884  | 12.60        | 15.77        | 12.85        | 83-84                                      | 76-81                                      | 90-92                                      | 94-95                                      | 87-94                                      | 69-73                                      | 78-86                                      |
2025-04-11T09:36:35.9073770Z | Table          | 34733   | 3.20         | 2.27         | 3.60         | 77-81                                      | 75-80                                      | 83-86                                      | 98-99                                      | 58-80                                      | 79-84                                      | 70-85                                      |
2025-04-11T09:36:35.9074505Z | Text           | 510377  | 45.82        | 49.28        | 45.00        | 84-86                                      | 81-86                                      | 88-93                                      | 89-93                                      | 87-92                                      | 71-79                                      | 87-95                                      |
2025-04-11T09:36:35.9075122Z | Title          | 5071    | 0.47         | 0.30         | 0.50         | 60-72                                      | 24-63                                      | 50-63                                      | 94-100                                     | 82-96                                      | 68-79                                      | 24-56                                      |
2025-04-11T09:36:35.9075849Z | Total          | 1107470 | 941123       | 99816        | 66531        | 82-83                                      | 71-74                                      | 79-81                                      | 89-94                                      | 86-91                                      | 71-76                                      | 68-85                                      |
2025-04-11T09:36:35.9076250Z 
2025-04-11T09:36:35.9077034Z Figure 3: Corpus Conversion Service annotation user interface. The PDF page is shown in the background, with overlaid text-cells (in darker shades). The annotation boxes can be drawn by dragging a rectangle over each segment with the respective label from the palette on the right.
2025-04-11T09:36:35.9077890Z 
2025-04-11T09:36:35.9077962Z <!-- image -->
2025-04-11T09:36:35.9078073Z 
2025-04-11T09:36:35.9078844Z we distributed the annotation workload and performed continuous quality controls. Phase one and two required a small team of experts only. For phases three and four, a group of 40 dedicated annotators were assembled and supervised.
2025-04-11T09:36:35.9079568Z 
2025-04-11T09:36:35.9081217Z Phase 1: Data selection and preparation. Our inclusion criteria for documents were described in Section 3. A large effort went into ensuring that all documents are free to use. The data sources include publication repositories such as arXiv 3 , government offices, company websites as well as data directory services for financial reports and patents. Scanned documents were excluded wherever possible because they can be rotated or skewed. This would not allow us to perform annotation with rectangular bounding-boxes and therefore complicate the annotation process.
2025-04-11T09:36:35.9082931Z 
2025-04-11T09:36:35.9085109Z Preparation work included uploading and parsing the sourced PDF documents in the Corpus Conversion Service (CCS) [22], a cloud-native platform which provides a visual annotation interface and allows for dataset inspection and analysis. The annotation interface of CCS is shown in Figure 3. The desired balance of pages between the different document categories was achieved by selective subsampling of pages with certain desired properties. For example, we made sure to include the title page of each document and bias the remaining page selection to those with figures or tables. The latter was achieved by leveraging pre-trained object detection models from PubLayNet, which helped us estimate how many figures and tables a given page contains.
2025-04-11T09:36:35.9087352Z 
2025-04-11T09:36:35.9090695Z Phase 2: Label selection and guideline. We reviewed the collected documents and identified the most common structural features they exhibit. This was achieved by identifying recurrent layout elements and lead us to the definition of 11 distinct class labels. These 11 class labels are Caption , Footnote , Formula List-item , , Pagefooter , Page-header , Picture , Section-header , Table , Text , and Title . Critical factors that were considered for the choice of these class labels were (1) the overall occurrence of the label, (2) the specificity of the label, (3) recognisability on a single page (i.e. no need for context from previous or next page) and (4) overall coverage of the page. Specificity ensures that the choice of label is not ambiguous, while coverage ensures that all meaningful items on a page can be annotated. We refrained from class labels that are very specific to a document category, such as Abstract in the Scientific Articles category. We also avoided class labels that are tightly linked to the semantics of the text. Labels such as Author and Affiliation , as seen in DocBank, are often only distinguishable by discriminating on
2025-04-11T09:36:35.9094280Z 
2025-04-11T09:36:35.9094364Z 3 https://arxiv.org/
2025-04-11T09:36:35.9094501Z 
2025-04-11T09:36:35.9094866Z the textual content of an element, which goes beyond visual layout recognition, in particular outside the Scientific Articles category.
2025-04-11T09:36:35.9095314Z 
2025-04-11T09:36:35.9097878Z At first sight, the task of visual document-layout interpretation appears intuitive enough to obtain plausible annotations in most cases. However, during early trial-runs in the core team, we observed many cases in which annotators use different annotation styles, especially for documents with challenging layouts. For example, if a figure is presented with subfigures, one annotator might draw a single figure bounding-box, while another might annotate each subfigure separately. The same applies for lists, where one might annotate all list items in one block or each list item separately. In essence, we observed that challenging layouts would be annotated in different but plausible ways. To illustrate this, we show in Figure 4 multiple examples of plausible but inconsistent annotations on the same pages.
2025-04-11T09:36:35.9100412Z 
2025-04-11T09:36:35.9101780Z Obviously, this inconsistency in annotations is not desirable for datasets which are intended to be used for model training. To minimise these inconsistencies, we created a detailed annotation guideline. While perfect consistency across 40 annotation staff members is clearly not possible to achieve, we saw a huge improvement in annotation consistency after the introduction of our annotation guideline. A few selected, non-trivial highlights of the guideline are:
2025-04-11T09:36:35.9103204Z 
2025-04-11T09:36:35.9103757Z - (1) Every list-item is an individual object instance with class label List-item . This definition is different from PubLayNet and DocBank, where all list-items are grouped together into one List object.
2025-04-11T09:36:35.9104967Z - (2) A List-item is a paragraph with hanging indentation. Singleline elements can qualify as List-item if the neighbour elements expose hanging indentation. Bullet or enumeration symbols are not a requirement.
2025-04-11T09:36:35.9105818Z - (3) For every Caption , there must be exactly one corresponding Picture or Table .
2025-04-11T09:36:35.9106272Z - (4) Connected sub-pictures are grouped together in one Picture object.
2025-04-11T09:36:35.9106645Z - (5) Formula numbers are included in a Formula object.
2025-04-11T09:36:35.9107255Z - (6) Emphasised text (e.g. in italic or bold) at the beginning of a paragraph is not considered a Section-header , unless it appears exclusively on its own line.
2025-04-11T09:36:35.9107749Z 
2025-04-11T09:36:35.9108361Z The complete annotation guideline is over 100 pages long and a detailed description is obviously out of scope for this paper. Nevertheless, it will be made publicly available alongside with DocLayNet for future reference.
2025-04-11T09:36:35.9109136Z 
2025-04-11T09:36:35.9111418Z Phase 3: Training. After a first trial with a small group of people, we realised that providing the annotation guideline and a set of random practice pages did not yield the desired quality level for layout annotation. Therefore we prepared a subset of pages with two different complexity levels, each with a practice and an exam part. 974 pages were reference-annotated by one proficient core team member. Annotation staff were then given the task to annotate the same subsets (blinded from the reference). By comparing the annotations of each staff member with the reference annotations, we could quantify how closely their annotations matched the reference. Only after passing two exam levels with high annotation quality, staff were admitted into the production phase. Practice iterations
2025-04-11T09:36:35.9113880Z 
2025-04-11T09:36:35.9113961Z <!-- image -->
2025-04-11T09:36:35.9114066Z 
2025-04-11T09:36:35.9114226Z 05237a14f2524e3f53c8454b074409d05078038a6a36b770fcc8ec7e540deae0
2025-04-11T09:36:35.9114476Z 
2025-04-11T09:36:35.9114950Z Figure 4: Examples of plausible annotation alternatives for the same page. Criteria in our annotation guideline can resolve cases A to C, while the case D remains ambiguous.
2025-04-11T09:36:35.9115509Z 
2025-04-11T09:36:35.9115812Z were carried out over a timeframe of 12 weeks, after which 8 of the 40 initially allocated annotators did not pass the bar.
2025-04-11T09:36:35.9116199Z 
2025-04-11T09:36:35.9119310Z Phase 4: Production annotation. The previously selected 80K pages were annotated with the defined 11 class labels by 32 annotators. This production phase took around three months to complete. All annotations were created online through CCS, which visualises the programmatic PDF text-cells as an overlay on the page. The page annotation are obtained by drawing rectangular bounding-boxes, as shown in Figure 3. With regard to the annotation practices, we implemented a few constraints and capabilities on the tooling level. First, we only allow non-overlapping, vertically oriented, rectangular boxes. For the large majority of documents, this constraint was sufficient and it speeds up the annotation considerably in comparison with arbitrary segmentation shapes. Second, annotator staff were not able to see each other's annotations. This was enforced by design to avoid any bias in the annotation, which could skew the numbers of the inter-annotator agreement (see Table 1). We wanted
2025-04-11T09:36:35.9122250Z 
2025-04-11T09:36:35.9123629Z Table 2: Prediction performance (mAP@0.5-0.95) of object detection networks on DocLayNet test set. The MRCNN (Mask R-CNN) and FRCNN (Faster R-CNN) models with ResNet-50 or ResNet-101 backbone were trained based on the network architectures from the detectron2 model zoo (Mask R-CNN R50, R101-FPN 3x, Faster R-CNN R101-FPN 3x), with default configurations. The YOLO implementation utilized was YOLOv5x6 [13]. All models were initialised using pre-trained weights from the COCO 2017 dataset.
2025-04-11T09:36:35.9125043Z 
2025-04-11T09:36:35.9125160Z |                | human   | MRCNN   | MRCNN   | FRCNN   | YOLO   |
2025-04-11T09:36:35.9125473Z |----------------|---------|---------|---------|---------|--------|
2025-04-11T09:36:35.9125760Z |                |         | R50     | R101    | R101    | v5x6   |
2025-04-11T09:36:35.9126039Z | Caption        | 84-89   | 68.4    | 71.5    | 70.1    | 77.7   |
2025-04-11T09:36:35.9126345Z | Footnote       | 83-91   | 70.9    | 71.8    | 73.7    | 77.2   |
2025-04-11T09:36:35.9126647Z | Formula        | 83-85   | 60.1    | 63.4    | 63.5    | 66.2   |
2025-04-11T09:36:35.9126955Z | List-item      | 87-88   | 81.2    | 80.8    | 81.0    | 86.2   |
2025-04-11T09:36:35.9127270Z | Page-footer    | 93-94   | 61.6    | 59.3    | 58.9    | 61.1   |
2025-04-11T09:36:35.9127590Z | Page-header    | 85-89   | 71.9    | 70.0    | 72.0    | 67.9   |
2025-04-11T09:36:35.9127900Z | Picture        | 69-71   | 71.7    | 72.7    | 72.0    | 77.1   |
2025-04-11T09:36:35.9128220Z | Section-header | 83-84   | 67.6    | 69.3    | 68.4    | 74.6   |
2025-04-11T09:36:35.9128529Z | Table          | 77-81   | 82.2    | 82.9    | 82.2    | 86.3   |
2025-04-11T09:36:35.9128926Z | Text           | 84-86   | 84.6    | 85.8    | 85.4    | 88.1   |
2025-04-11T09:36:35.9129207Z | Title          | 60-72   | 76.7    | 80.4    | 79.9    | 82.7   |
2025-04-11T09:36:35.9129489Z | All            | 82-83   | 72.4    | 73.5    | 73.4    | 76.8   |
2025-04-11T09:36:35.9129668Z 
2025-04-11T09:36:35.9133431Z to avoid this at any cost in order to have clear, unbiased baseline numbers for human document-layout annotation. Third, we introduced the feature of snapping boxes around text segments to obtain a pixel-accurate annotation and again reduce time and effort. The CCS annotation tool automatically shrinks every user-drawn box to the minimum bounding-box around the enclosed text-cells for all purely text-based segments, which excludes only Table and Picture . For the latter, we instructed annotation staff to minimise inclusion of surrounding whitespace while including all graphical lines. A downside of snapping boxes to enclosed text cells is that some wrongly parsed PDF pages cannot be annotated correctly and need to be skipped. Fourth, we established a way to flag pages as rejected for cases where no valid annotation according to the label guidelines could be achieved. Example cases for this would be PDF pages that render incorrectly or contain layouts that are impossible to capture with non-overlapping rectangles. Such rejected pages are not contained in the final dataset. With all these measures in place, experienced annotation staff managed to annotate a single page in a typical timeframe of 20s to 60s, depending on its complexity.
2025-04-11T09:36:35.9137390Z 
2025-04-11T09:36:35.9137466Z ## 5 EXPERIMENTS
2025-04-11T09:36:35.9137575Z 
2025-04-11T09:36:35.9139442Z The primary goal of DocLayNet is to obtain high-quality ML models capable of accurate document-layout analysis on a wide variety of challenging layouts. As discussed in Section 2, object detection models are currently the easiest to use, due to the standardisation of ground-truth data in COCO format [16] and the availability of general frameworks such as detectron2 [17]. Furthermore, baseline numbers in PubLayNet and DocBank were obtained using standard object detection models such as Mask R-CNN and Faster R-CNN. As such, we will relate to these object detection methods in this
2025-04-11T09:36:35.9141153Z 
2025-04-11T09:36:35.9142103Z Figure 5: Prediction performance (mAP@0.5-0.95) of a Mask R-CNNnetworkwithResNet50backbonetrainedonincreasing fractions of the DocLayNet dataset. The learning curve flattens around the 80% mark, indicating that increasing the size of the DocLayNet dataset with similar data will not yield significantly better predictions.
2025-04-11T09:36:35.9143131Z 
2025-04-11T09:36:35.9143205Z <!-- image -->
2025-04-11T09:36:35.9143309Z 
2025-04-11T09:36:35.9143576Z paper and leave the detailed evaluation of more recent methods mentioned in Section 2 for future work.
2025-04-11T09:36:35.9143925Z 
2025-04-11T09:36:35.9144986Z In this section, we will present several aspects related to the performance of object detection models on DocLayNet. Similarly as in PubLayNet, we will evaluate the quality of their predictions using mean average precision (mAP) with 10 overlaps that range from 0.5 to 0.95 in steps of 0.05 (mAP@0.5-0.95). These scores are computed by leveraging the evaluation code provided by the COCO API [16].
2025-04-11T09:36:35.9146109Z 
2025-04-11T09:36:35.9146204Z ## Baselines for Object Detection
2025-04-11T09:36:35.9146365Z 
2025-04-11T09:36:35.9150365Z In Table 2, we present baseline experiments (given in mAP) on Mask R-CNN [12], Faster R-CNN [11], and YOLOv5 [13]. Both training and evaluation were performed on RGB images with dimensions of 1025 × 1025 pixels. For training, we only used one annotation in case of redundantly annotated pages. As one can observe, the variation in mAP between the models is rather low, but overall between 6 and 10% lower than the mAP computed from the pairwise human annotations on triple-annotated pages. This gives a good indication that the DocLayNet dataset poses a worthwhile challenge for the research community to close the gap between human recognition and ML approaches. It is interesting to see that Mask R-CNN and Faster R-CNN produce very comparable mAP scores, indicating that pixel-based image segmentation derived from bounding-boxes does not help to obtain better predictions. On the other hand, the more recent Yolov5x model does very well and even out-performs humans on selected labels such as Text , Table and Picture . This is not entirely surprising, as Text , Table and Picture are abundant and the most visually distinctive in a document.
2025-04-11T09:36:35.9153928Z 
2025-04-11T09:36:35.9154463Z Table 3: Performance of a Mask R-CNN R50 network in mAP@0.5-0.95 scores trained on DocLayNet with different class label sets. The reduced label sets were obtained by either down-mapping or dropping labels.
2025-04-11T09:36:35.9155077Z 
2025-04-11T09:36:35.9155185Z | Class-count    |   11 | 6       | 5       | 4       |
2025-04-11T09:36:35.9155474Z |----------------|------|---------|---------|---------|
2025-04-11T09:36:35.9155755Z | Caption        |   68 | Text    | Text    | Text    |
2025-04-11T09:36:35.9156043Z | Footnote       |   71 | Text    | Text    | Text    |
2025-04-11T09:36:35.9156323Z | Formula        |   60 | Text    | Text    | Text    |
2025-04-11T09:36:35.9156603Z | List-item      |   81 | Text    | 82      | Text    |
2025-04-11T09:36:35.9156994Z | Page-footer    |   62 | 62      | -       | -       |
2025-04-11T09:36:35.9157291Z | Page-header    |   72 | 68      | -       | -       |
2025-04-11T09:36:35.9157570Z | Picture        |   72 | 72      | 72      | 72      |
2025-04-11T09:36:35.9157850Z | Section-header |   68 | 67      | 69      | 68      |
2025-04-11T09:36:35.9158124Z | Table          |   82 | 83      | 82      | 82      |
2025-04-11T09:36:35.9158382Z | Text           |   85 | 84      | 84      | 84      |
2025-04-11T09:36:35.9158643Z | Title          |   77 | Sec.-h. | Sec.-h. | Sec.-h. |
2025-04-11T09:36:35.9159033Z | Overall        |   72 | 73      | 78      | 77      |
2025-04-11T09:36:35.9159213Z 
2025-04-11T09:36:35.9159283Z ## Learning Curve
2025-04-11T09:36:35.9159398Z 
2025-04-11T09:36:35.9162555Z One of the fundamental questions related to any dataset is if it is 'large enough'. To answer this question for DocLayNet, we performed a data ablation study in which we evaluated a Mask R-CNN model trained on increasing fractions of the DocLayNet dataset. As can be seen in Figure 5, the mAP score rises sharply in the beginning and eventually levels out. To estimate the error-bar on the metrics, we ran the training five times on the entire data-set. This resulted in a 1% error-bar, depicted by the shaded area in Figure 5. In the inset of Figure 5, we show the exact same data-points, but with a logarithmic scale on the x-axis. As is expected, the mAP score increases linearly as a function of the data-size in the inset. The curve ultimately flattens out between the 80% and 100% mark, with the 80% mark falling within the error-bars of the 100% mark. This provides a good indication that the model would not improve significantly by yet increasing the data size. Rather, it would probably benefit more from improved data consistency (as discussed in Section 3), data augmentation methods [23], or the addition of more document categories and styles.
2025-04-11T09:36:35.9165892Z 
2025-04-11T09:36:35.9165981Z ## Impact of Class Labels
2025-04-11T09:36:35.9166117Z 
2025-04-11T09:36:35.9169290Z The choice and number of labels can have a significant effect on the overall model performance. Since PubLayNet, DocBank and DocLayNet all have different label sets, it is of particular interest to understand and quantify this influence of the label set on the model performance. We investigate this by either down-mapping labels into more common ones (e.g. Caption → Text ) or excluding them from the annotations entirely. Furthermore, it must be stressed that all mappings and exclusions were performed on the data before model training. In Table 3, we present the mAP scores for a Mask R-CNN R50 network on different label sets. Where a label is down-mapped, we show its corresponding label, otherwise it was excluded. We present three different label sets, with 6, 5 and 4 different labels respectively. The set of 5 labels contains the same labels as PubLayNet. However, due to the different definition of
2025-04-11T09:36:35.9171912Z 
2025-04-11T09:36:35.9172429Z Table 4: Performance of a Mask R-CNN R50 network with document-wise and page-wise split for different label sets. Naive page-wise split will result in /tildelow 10% point improvement.
2025-04-11T09:36:35.9173164Z 
2025-04-11T09:36:35.9173259Z | Class-count    | 11   | 11   | 5   | 5    |
2025-04-11T09:36:35.9173518Z |----------------|------|------|-----|------|
2025-04-11T09:36:35.9173775Z | Split          | Doc  | Page | Doc | Page |
2025-04-11T09:36:35.9174036Z | Caption        | 68   | 83   |     |      |
2025-04-11T09:36:35.9174274Z | Footnote       | 71   | 84   |     |      |
2025-04-11T09:36:35.9174512Z | Formula        | 60   | 66   |     |      |
2025-04-11T09:36:35.9174752Z | List-item      | 81   | 88   | 82  | 88   |
2025-04-11T09:36:35.9174998Z | Page-footer    | 62   | 89   |     |      |
2025-04-11T09:36:35.9175243Z | Page-header    | 72   | 90   |     |      |
2025-04-11T09:36:35.9175487Z | Picture        | 72   | 82   | 72  | 82   |
2025-04-11T09:36:35.9175847Z | Section-header | 68   | 83   | 69  | 83   |
2025-04-11T09:36:35.9176107Z | Table          | 82   | 89   | 82  | 90   |
2025-04-11T09:36:35.9176341Z | Text           | 85   | 91   | 84  | 90   |
2025-04-11T09:36:35.9176577Z | Title          | 77   | 81   |     |      |
2025-04-11T09:36:35.9176812Z | All            | 72   | 84   | 78  | 87   |
2025-04-11T09:36:35.9176964Z 
2025-04-11T09:36:35.9178285Z lists in PubLayNet (grouped list-items) versus DocLayNet (separate list-items), the label set of size 4 is the closest to PubLayNet, in the assumption that the List is down-mapped to Text in PubLayNet. The results in Table 3 show that the prediction accuracy on the remaining class labels does not change significantly when other classes are merged into them. The overall macro-average improves by around 5%, in particular when Page-footer and Page-header are excluded.
2025-04-11T09:36:35.9179761Z 
2025-04-11T09:36:35.9179882Z ## Impact of Document Split in Train and Test Set
2025-04-11T09:36:35.9180074Z 
2025-04-11T09:36:35.9182979Z Many documents in DocLayNet have a unique styling. In order to avoid overfitting on a particular style, we have split the train-, test- and validation-sets of DocLayNet on document boundaries, i.e. every document contributes pages to only one set. To the best of our knowledge, this was not considered in PubLayNet or DocBank. To quantify how this affects model performance, we trained and evaluated a Mask R-CNN R50 model on a modified dataset version. Here, the train-, test- and validation-sets were obtained by a randomised draw over the individual pages. As can be seen in Table 4, the difference in model performance is surprisingly large: pagewise splitting gains ˜ 0% in mAP over the document-wise splitting. 1 Thus, random page-wise splitting of DocLayNet can easily lead to accidental overestimation of model performance and should be avoided.
2025-04-11T09:36:35.9185430Z 
2025-04-11T09:36:35.9185508Z ## Dataset Comparison
2025-04-11T09:36:35.9185633Z 
2025-04-11T09:36:35.9187096Z Throughout this paper, we claim that DocLayNet's wider variety of document layouts leads to more robust layout detection models. In Table 5, we provide evidence for that. We trained models on each of the available datasets (PubLayNet, DocBank and DocLayNet) and evaluated them on the test sets of the other datasets. Due to the different label sets and annotation styles, a direct comparison is not possible. Hence, we focussed on the common labels among the datasets. Between PubLayNet and DocLayNet, these are Picture ,
2025-04-11T09:36:35.9188631Z 
2025-04-11T09:36:35.9189540Z Table 5: Prediction Performance (mAP@0.5-0.95) of a Mask R-CNN R50 network across the PubLayNet, DocBank &amp; DocLayNet data-sets. By evaluating on common label classes of each dataset, we observe that the DocLayNet-trained model has much less pronounced variations in performance across all datasets.
2025-04-11T09:36:35.9190445Z 
2025-04-11T09:36:35.9190557Z |                 |            | Testing on   | Testing on   | Testing on   |
2025-04-11T09:36:35.9190892Z |-----------------|------------|--------------|--------------|--------------|
2025-04-11T09:36:35.9191371Z | Training on     | labels     | PLN          | DB           | DLN          |
2025-04-11T09:36:35.9191742Z | PubLayNet (PLN) | Figure     | 96           | 43           | 23           |
2025-04-11T09:36:35.9192118Z | PubLayNet (PLN) | Sec-header | 87           | -            | 32           |
2025-04-11T09:36:35.9192454Z |                 | Table      | 95           | 24           | 49           |
2025-04-11T09:36:35.9192743Z |                 | Text       | 96           | -            | 42           |
2025-04-11T09:36:35.9193026Z |                 | total      | 93           | 34           | 30           |
2025-04-11T09:36:35.9193344Z | DocBank (DB)    | Figure     | 77           | 71           | 31           |
2025-04-11T09:36:35.9193687Z | DocBank (DB)    | Table      | 19           | 65           | 22           |
2025-04-11T09:36:35.9194134Z | DocBank (DB)    | total      | 48           | 68           | 27           |
2025-04-11T09:36:35.9194495Z | DocLayNet (DLN) | Figure     | 67           | 51           | 72           |
2025-04-11T09:36:35.9194868Z | DocLayNet (DLN) | Sec-header | 53           | -            | 68           |
2025-04-11T09:36:35.9195204Z |                 | Table      | 87           | 43           | 82           |
2025-04-11T09:36:35.9195491Z |                 | Text       | 77           | -            | 84           |
2025-04-11T09:36:35.9195773Z |                 | total      | 59           | 47           | 78           |
2025-04-11T09:36:35.9195967Z 
2025-04-11T09:36:35.9196769Z Section-header , Table and Text . Before training, we either mapped or excluded DocLayNet's other labels as specified in table 3, and also PubLayNet's List to Text . Note that the different clustering of lists (by list-element vs. whole list objects) naturally decreases the mAP score for Text .
2025-04-11T09:36:35.9197651Z 
2025-04-11T09:36:35.9200052Z For comparison of DocBank with DocLayNet, we trained only on Picture and Table clusters of each dataset. We had to exclude Text because successive paragraphs are often grouped together into a single object in DocBank. This paragraph grouping is incompatible with the individual paragraphs of DocLayNet. As can be seen in Table 5, DocLayNet trained models yield better performance compared to the previous datasets. It is noteworthy that the models trained on PubLayNet and DocBank perform very well on their own test set, but have a much lower performance on the foreign datasets. While this also applies to DocLayNet, the difference is far less pronounced. Thus we conclude that DocLayNet trained models are overall more robust and will produce better results for challenging, unseen layouts.
2025-04-11T09:36:35.9202387Z 
2025-04-11T09:36:35.9202470Z ## Example Predictions
2025-04-11T09:36:35.9202592Z 
2025-04-11T09:36:35.9204010Z To conclude this section, we illustrate the quality of layout predictions one can expect from DocLayNet-trained models by providing a selection of examples without any further post-processing applied. Figure 6 shows selected layout predictions on pages from the test-set of DocLayNet. Results look decent in general across document categories, however one can also observe mistakes such as overlapping clusters of different classes, or entirely missing boxes due to low confidence.
2025-04-11T09:36:35.9205490Z 
2025-04-11T09:36:35.9205562Z ## 6 CONCLUSION
2025-04-11T09:36:35.9205667Z 
2025-04-11T09:36:35.9207141Z In this paper, we presented the DocLayNet dataset. It provides the document conversion and layout analysis research community a new and challenging dataset to improve and fine-tune novel ML methods on. In contrast to many other datasets, DocLayNet was created by human annotation in order to obtain reliable layout ground-truth on a wide variety of publication- and typesettingstyles. Including a large proportion of documents outside the scientific publishing domain adds significant value in this respect.
2025-04-11T09:36:35.9208779Z 
2025-04-11T09:36:35.9210458Z From the dataset, we have derived on the one hand reference metrics for human performance on document-layout annotation (through double and triple annotations) and on the other hand evaluated the baseline performance of commonly used object detection methods. We also illustrated the impact of various dataset-related aspects on model performance through data-ablation experiments, both from a size and class-label perspective. Last but not least, we compared the accuracy of models trained on other public datasets and showed that DocLayNet trained models are more robust.
2025-04-11T09:36:35.9212361Z 
2025-04-11T09:36:35.9212853Z To date, there is still a significant gap between human and ML accuracy on the layout interpretation task, and we hope that this work will inspire the research community to close that gap.
2025-04-11T09:36:35.9213425Z 
2025-04-11T09:36:35.9213491Z ## REFERENCES
2025-04-11T09:36:35.9213596Z 
2025-04-11T09:36:35.9214381Z - [1] Max Göbel, Tamir Hassan, Ermelinda Oro, and Giorgio Orsi. Icdar 2013 table competition. In 2013 12th International Conference on Document Analysis and Recognition , pages 1449-1453, 2013.
2025-04-11T09:36:35.9219093Z - [2] Christian Clausner, Apostolos Antonacopoulos, and Stefan Pletschacher. Icdar2017 competition on recognition of documents with complex layouts rdcl2017. In 2017 14th IAPR International Conference on Document Analysis and Recognition (ICDAR) , volume 01, pages 1404-1410, 2017.
2025-04-11T09:36:35.9222177Z - [3] Hervé Déjean, Jean-Luc Meunier, Liangcai Gao, Yilun Huang, Yu Fang, Florian Kleber, and Eva-Maria Lang. ICDAR 2019 Competition on Table Detection and Recognition (cTDaR), April 2019. http://sac.founderit.com/.
2025-04-11T09:36:35.9224700Z - [4] Antonio Jimeno Yepes, Peter Zhong, and Douglas Burdick. Competition on scientific literature parsing. In Proceedings of the International Conference on Document Analysis and Recognition , ICDAR, pages 605-617. LNCS 12824, SpringerVerlag, sep 2021.
2025-04-11T09:36:35.9227387Z - [5] Logan Markewich, Hao Zhang, Yubin Xing, Navid Lambert-Shirzad, Jiang Zhexin, Roy Lee, Zhi Li, and Seok-Bum Ko. Segmentation for document layout analysis: not dead yet. International Journal on Document Analysis and Recognition (IJDAR) , pages 1-11, 01 2022.
2025-04-11T09:36:35.9230315Z - [6] Xu Zhong, Jianbin Tang, and Antonio Jimeno-Yepes. Publaynet: Largest dataset ever for document layout analysis. In Proceedings of the International Conference on Document Analysis and Recognition , ICDAR, pages 1015-1022, sep 2019.
2025-04-11T09:36:35.9233031Z - [7] Minghao Li, Yiheng Xu, Lei Cui, Shaohan Huang, Furu Wei, Zhoujun Li, and Ming Zhou. Docbank: A benchmark dataset for document layout analysis. In Proceedings of the 28th International Conference on Computational Linguistics , COLING, pages 949-960. International Committee on Computational Linguistics, dec 2020.
2025-04-11T09:36:35.9234520Z - [8] Riaz Ahmad, Muhammad Tanvir Afzal, and M. Qadir. Information extraction from pdf sources based on rule-based system using integrated formats. In SemWebEval@ESWC , 2016.
2025-04-11T09:36:35.9235826Z - [9] Ross B. Girshick, Jeff Donahue, Trevor Darrell, and Jitendra Malik. Rich feature hierarchies for accurate object detection and semantic segmentation. In IEEE Conference on Computer Vision and Pattern Recognition , CVPR, pages 580-587. IEEE Computer Society, jun 2014.
2025-04-11T09:36:35.9237035Z - [10] Ross B. Girshick. Fast R-CNN. In 2015 IEEE International Conference on Computer Vision , ICCV, pages 1440-1448. IEEE Computer Society, dec 2015.
2025-04-11T09:36:35.9238094Z - [11] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster r-cnn: Towards real-time object detection with region proposal networks. IEEE Transactions on Pattern Analysis and Machine Intelligence , 39(6):1137-1149, 2017.
2025-04-11T09:36:35.9239725Z - [12] Kaiming He, Georgia Gkioxari, Piotr Dollár, and Ross B. Girshick. Mask R-CNN. In IEEE International Conference on Computer Vision , ICCV, pages 2980-2988. IEEE Computer Society, Oct 2017.
2025-04-11T09:36:35.9241189Z - [13] Glenn Jocher, Alex Stoken, Ayush Chaurasia, Jirka Borovec, NanoCode012, TaoXie, Yonghye Kwon, Kalen Michael, Liu Changyu, Jiacong Fang, Abhiram V, Laughing, tkianai, yxNONG, Piotr Skalski, Adam Hogan, Jebastin Nadar, imyhxy, Lorenzo Mammana, Alex Wang, Cristi Fati, Diego Montes, Jan Hajek, Laurentiu
2025-04-11T09:36:35.9242325Z 
2025-04-11T09:36:35.9242676Z Text Caption List-Item Formula Table Section-Header Picture Page-Header Page-Footer Title © © @
2025-04-11T09:36:35.9243023Z 
2025-04-11T09:36:35.9243101Z <!-- image -->
2025-04-11T09:36:35.9243208Z 
2025-04-11T09:36:35.9244321Z Figure 6: Example layout predictions on selected pages from the DocLayNet test-set. (A, D) exhibit favourable results on coloured backgrounds. (B, C) show accurate list-item and paragraph differentiation despite densely-spaced lines. (E) demonstrates good table and figure distinction. (F) shows predictions on a Chinese patent with multiple overlaps, label confusion and missing boxes.
2025-04-11T09:36:35.9245499Z 
2025-04-11T09:36:35.9246182Z Diaconu, Mai Thanh Minh, Marc, albinxavi, fatih, oleg, and wanghao yang. ultralytics/yolov5: v6.0 - yolov5n nano models, roboflow integration, tensorflow export, opencv dnn support, October 2021.
2025-04-11T09:36:35.9246802Z 
2025-04-11T09:36:35.9247175Z - [20] Shoubin Li, Xuyan Ma, Shuaiqun Pan, Jun Hu, Lin Shi, and Qing Wang. Vtlayout: Fusion of visual and text features for document layout analysis, 2021.
2025-04-11T09:36:35.9248141Z - [14] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and Sergey Zagoruyko. End-to-end object detection with transformers. CoRR , abs/2005.12872, 2020.
2025-04-11T09:36:35.9249253Z - [15] Mingxing Tan, Ruoming Pang, and Quoc V. Le. Efficientdet: Scalable and efficient object detection. CoRR , abs/1911.09070, 2019.
2025-04-11T09:36:35.9250392Z - [16] Tsung-Yi Lin, Michael Maire, Serge J. Belongie, Lubomir D. Bourdev, Ross B. Girshick, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, and C. Lawrence Zitnick. Microsoft COCO: common objects in context, 2014.
2025-04-11T09:36:35.9251295Z - [17] Yuxin Wu, Alexander Kirillov, Francisco Massa, Wan-Yen Lo, and Ross Girshick. Detectron2, 2019.
2025-04-11T09:36:35.9252590Z - [18] Nikolaos Livathinos, Cesar Berrospi, Maksym Lysak, Viktor Kuropiatnyk, Ahmed Nassar, Andre Carvalho, Michele Dolfi, Christoph Auer, Kasper Dinkla, and Peter W. J. Staar. Robust pdf document conversion using recurrent neural networks. In Proceedings of the 35th Conference on Artificial Intelligence , AAAI, pages 1513715145, feb 2021.
2025-04-11T09:36:35.9254537Z - [19] Yiheng Xu, Minghao Li, Lei Cui, Shaohan Huang, Furu Wei, and Ming Zhou. Layoutlm: Pre-training of text and layout for document image understanding. In Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining , KDD, pages 1192-1200, New York, USA, 2020. Association for Computing Machinery.
2025-04-11T09:36:35.9256005Z - [21] Peng Zhang, Can Li, Liang Qiao, Zhanzhan Cheng, Shiliang Pu, Yi Niu, and Fei Wu. Vsr: A unified framework for document layout analysis combining vision, semantics and relations, 2021.
2025-04-11T09:36:35.9257345Z - [22] Peter W J Staar, Michele Dolfi, Christoph Auer, and Costas Bekas. Corpus conversion service: A machine learning platform to ingest documents at scale. In Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining , KDD, pages 774-782. ACM, 2018.
2025-04-11T09:36:35.9258576Z - [23] Connor Shorten and Taghi M. Khoshgoftaar. A survey on image data augmentation for deep learning. Journal of Big Data , 6(1):60, 2019.
2025-04-11T09:36:36.8100822Z Running example docs/examples/translate.py
2025-04-11T09:36:42.9632000Z Skipping docs/examples/vlm_pipeline_api_model.py
2025-04-11T09:36:42.9678598Z ##[group]Run poetry build
2025-04-11T09:36:42.9679116Z [36;1mpoetry build[0m
2025-04-11T09:36:42.9725559Z shell: /usr/bin/bash -e {0}
2025-04-11T09:36:42.9725790Z env:
2025-04-11T09:36:42.9725966Z   HF_HUB_DOWNLOAD_TIMEOUT: 60
2025-04-11T09:36:42.9726191Z   HF_HUB_ETAG_TIMEOUT: 60
2025-04-11T09:36:42.9726651Z   TESSDATA_PREFIX: /usr/share/tesseract-ocr/5/tessdata
2025-04-11T09:36:42.9727011Z   pythonLocation: /opt/hostedtoolcache/Python/3.10.16/x64
2025-04-11T09:36:42.9727410Z   PKG_CONFIG_PATH: /opt/hostedtoolcache/Python/3.10.16/x64/lib/pkgconfig
2025-04-11T09:36:42.9727797Z   Python_ROOT_DIR: /opt/hostedtoolcache/Python/3.10.16/x64
2025-04-11T09:36:42.9728134Z   Python2_ROOT_DIR: /opt/hostedtoolcache/Python/3.10.16/x64
2025-04-11T09:36:42.9728482Z   Python3_ROOT_DIR: /opt/hostedtoolcache/Python/3.10.16/x64
2025-04-11T09:36:42.9729058Z   LD_LIBRARY_PATH: /opt/hostedtoolcache/Python/3.10.16/x64/lib
2025-04-11T09:36:42.9729369Z ##[endgroup]
2025-04-11T09:36:43.4110897Z Building docling (2.29.0)
2025-04-11T09:36:43.4200468Z   - Building sdist
2025-04-11T09:36:43.6058543Z   - Built docling-2.29.0.tar.gz
2025-04-11T09:36:43.6072887Z   - Building wheel
2025-04-11T09:36:43.6855289Z   - Built docling-2.29.0-py3-none-any.whl
2025-04-11T09:36:43.7419241Z Post job cleanup.
2025-04-11T09:36:43.7453027Z Post job cleanup.
2025-04-11T09:36:43.9066605Z Cache hit occurred on the primary key setup-python-Linux-x64-python-3.10.16-poetry-v2-6537c0c4b2503108d4171f165ddac27eb1391ee7ae381f0775fd538108ab5411, not saving cache.
2025-04-11T09:36:43.9153238Z Post job cleanup.
2025-04-11T09:36:44.0571733Z Cache hit occurred on the primary key huggingface-cache-py3.10, not saving cache.
2025-04-11T09:36:44.0672436Z Post job cleanup.
2025-04-11T09:36:44.1706233Z [command]/usr/bin/git version
2025-04-11T09:36:44.1749545Z git version 2.49.0
2025-04-11T09:36:44.1796501Z Temporarily overriding HOME='/home/runner/work/_temp/249ebf27-5bc0-41b6-b85b-18dc53fb6d1b' before making global git config changes
2025-04-11T09:36:44.1797776Z Adding repository directory to the temporary git global config as a safe directory
2025-04-11T09:36:44.1812460Z [command]/usr/bin/git config --global --add safe.directory /home/runner/work/docling/docling
2025-04-11T09:36:44.1853843Z [command]/usr/bin/git config --local --name-only --get-regexp core\.sshCommand
2025-04-11T09:36:44.1892332Z [command]/usr/bin/git submodule foreach --recursive sh -c "git config --local --name-only --get-regexp 'core\.sshCommand' && git config --local --unset-all 'core.sshCommand' || :"
2025-04-11T09:36:44.2194688Z [command]/usr/bin/git config --local --name-only --get-regexp http\.https\:\/\/github\.com\/\.extraheader
2025-04-11T09:36:44.2221642Z http.https://github.com/.extraheader
2025-04-11T09:36:44.2235708Z [command]/usr/bin/git config --local --unset-all http.https://github.com/.extraheader
2025-04-11T09:36:44.2273868Z [command]/usr/bin/git submodule foreach --recursive sh -c "git config --local --name-only --get-regexp 'http\.https\:\/\/github\.com\/\.extraheader' && git config --local --unset-all 'http.https://github.com/.extraheader' || :"
2025-04-11T09:36:44.2657005Z Cleaning up orphan processes
