2025-04-11T23:45:28.2250993Z ##[group]Run make -f Makefile.sync clean sync
2025-04-11T23:45:28.2252532Z [36;1mmake -f Makefile.sync clean sync[0m
2025-04-11T23:45:28.2253986Z [36;1mgit diff --compact-summary --exit-code[0m
2025-04-11T23:45:28.2306172Z shell: /usr/bin/bash -e {0}
2025-04-11T23:45:28.2307354Z ##[endgroup]
2025-04-11T23:45:28.2431847Z git clone https://github.com/ggerganov/llama.cpp.git llama/vendor
2025-04-11T23:45:28.2442085Z Cloning into 'llama/vendor'...
2025-04-11T23:45:35.8172882Z git -C llama/vendor fetch
2025-04-11T23:45:35.9969739Z git -C llama/vendor checkout -f d7cfe1ffe0f435d0048a6058d529daf76e072d9c
2025-04-11T23:45:36.3896690Z Note: switching to 'd7cfe1ffe0f435d0048a6058d529daf76e072d9c'.
2025-04-11T23:45:36.3897723Z 
2025-04-11T23:45:36.3898160Z You are in 'detached HEAD' state. You can look around, make experimental
2025-04-11T23:45:36.3899320Z changes and commit them, and you can discard any commits you make in this
2025-04-11T23:45:36.3900184Z state without impacting any branches by switching back to a branch.
2025-04-11T23:45:36.3900765Z 
2025-04-11T23:45:36.3901160Z If you want to create a new branch to retain commits you create, you may
2025-04-11T23:45:36.3901941Z do so (now or later) by using -c with the switch command. Example:
2025-04-11T23:45:36.3902367Z 
2025-04-11T23:45:36.3902562Z   git switch -c <new-branch-name>
2025-04-11T23:45:36.3902890Z 
2025-04-11T23:45:36.3903076Z Or undo this operation with:
2025-04-11T23:45:36.3903376Z 
2025-04-11T23:45:36.3903572Z   git switch -
2025-04-11T23:45:36.3903785Z 
2025-04-11T23:45:36.3904160Z Turn off this advice by setting config variable advice.detachedHead to false
2025-04-11T23:45:36.3904673Z 
2025-04-11T23:45:36.3905194Z HEAD is now at d7cfe1ff docs: add docs/function-calling.md to lighten server/README.md's plight (#12069)
2025-04-11T23:45:36.3918839Z rm -f llama/patches/0001-cuda.patched llama/patches/0002-pretokenizer.patched llama/patches/0003-embeddings.patched llama/patches/0004-clip-unicode.patched llama/patches/0005-solar-pro.patched llama/patches/0006-conditional-fattn.patched llama/patches/0007-add-mllama-support.patched llama/patches/0008-add-unpad-operator.patched llama/patches/0009-fix-deepseek-deseret-regex.patched llama/patches/0010-Maintain-ordering-for-rules-for-grammar.patched llama/patches/0011-llama-Ensure-KV-cache-is-fully-defragmented.patched llama/patches/0012-use-dynamic-backend-loading-for-clip.patched llama/patches/0013-sort-devices-by-score.patched llama/patches/0014-add-phony-target-ggml-cpu-for-all-cpu-variants.patched llama/patches/0015-use-std-filesystem-path-instead-of-wstring.patched llama/patches/0016-remove-amx.patched llama/patches/0017-fix-clip-compiler-error.patched llama/patches/0018-add-phi4-support.patched llama/patches/0019-fix-string-arr-kv-loading.patched llama/patches/0020-ollama-debug-tensor.patched llama/patches/0021-add-model-quantizations.patched llama/patches/0022-add-rdna4-support.patched llama/patches/0022-metal-add-op_neg.patched
2025-04-11T23:45:36.3932717Z sed -e 's|@FETCH_HEAD@|d7cfe1ffe0f435d0048a6058d529daf76e072d9c|' llama/build-info.cpp.in > llama/build-info.cpp
2025-04-11T23:45:36.4553381Z Applying: cuda
2025-04-11T23:45:36.5093099Z Applying: pretokenizer
2025-04-11T23:45:36.5689725Z Applying: embeddings
2025-04-11T23:45:36.6228804Z Applying: clip-unicode
2025-04-11T23:45:36.6896626Z Applying: solar-pro
2025-04-11T23:45:36.7434250Z Applying: conditional-fattn
2025-04-11T23:45:36.8200390Z Applying: add mllama support
2025-04-11T23:45:36.9084331Z Applying: add unpad operator
2025-04-11T23:45:36.9634473Z Applying: fix deepseek deseret regex
2025-04-11T23:45:37.0152968Z Applying: Maintain ordering for rules for grammar
2025-04-11T23:45:37.0375007Z Applying: llama: Ensure KV cache is fully defragmented.
2025-04-11T23:45:37.0548337Z Applying: use dynamic backend loading for clip
2025-04-11T23:45:37.0702791Z Applying: sort devices by score
2025-04-11T23:45:37.0862803Z Applying: add phony target ggml-cpu for all cpu variants
2025-04-11T23:45:37.1024176Z Applying: use std::filesystem::path instead of wstring
2025-04-11T23:45:37.1176971Z Applying: remove amx
2025-04-11T23:45:37.1359357Z Applying: fix-clip-compiler-error
2025-04-11T23:45:37.1586950Z Applying: add phi4 support
2025-04-11T23:45:37.1798156Z Applying: fix string arr kv loading
2025-04-11T23:45:37.2052235Z Applying: ollama debug tensor
2025-04-11T23:45:37.2303565Z Applying: add model quantizations
2025-04-11T23:45:37.2531031Z Applying: add rdna4 support
2025-04-11T23:45:37.2815315Z Applying: metal: add op_neg
2025-04-11T23:45:37.2847292Z rsync -arvzc -f "merge llama/llama.cpp/.rsync-filter" llama/vendor/ llama/llama.cpp
2025-04-11T23:45:37.2901597Z sending incremental file list
2025-04-11T23:45:37.2923546Z ./
2025-04-11T23:45:37.2940518Z common/
2025-04-11T23:45:37.2940890Z examples/
2025-04-11T23:45:37.2941194Z examples/llava/
2025-04-11T23:45:37.2941509Z include/
2025-04-11T23:45:37.2941733Z src/
2025-04-11T23:45:37.3344697Z 
2025-04-11T23:45:37.3344940Z sent 2,546 bytes  received 201 bytes  5,494.00 bytes/sec
2025-04-11T23:45:37.3345486Z total size is 3,145,635  speedup is 1,145.12
2025-04-11T23:45:37.3354492Z rsync -arvzc -f "merge ml/backend/ggml/ggml/.rsync-filter" llama/vendor/ggml/ ml/backend/ggml/ggml
2025-04-11T23:45:37.3385578Z sending incremental file list
2025-04-11T23:45:37.3418599Z ./
2025-04-11T23:45:37.3461934Z include/
2025-04-11T23:45:37.3462226Z src/
2025-04-11T23:45:37.3462486Z src/ggml-blas/
2025-04-11T23:45:37.3462787Z src/ggml-cpu/
2025-04-11T23:45:37.3463078Z src/ggml-cpu/amx/
2025-04-11T23:45:37.3463321Z src/ggml-cpu/llamafile/
2025-04-11T23:45:37.3463561Z src/ggml-cuda/
2025-04-11T23:45:37.3463897Z src/ggml-cuda/template-instances/
2025-04-11T23:45:37.3464318Z src/ggml-cuda/vendors/
2025-04-11T23:45:37.3464604Z src/ggml-hip/
2025-04-11T23:45:37.3464911Z src/ggml-metal/
2025-04-11T23:45:37.3867562Z 
2025-04-11T23:45:37.3868028Z sent 13,450 bytes  received 864 bytes  28,628.00 bytes/sec
2025-04-11T23:45:37.3868790Z total size is 3,939,264  speedup is 275.20
