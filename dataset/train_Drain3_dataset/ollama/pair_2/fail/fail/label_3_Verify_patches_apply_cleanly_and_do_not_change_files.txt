2025-04-12T19:07:49.7784820Z ##[group]Run make -f Makefile.sync clean sync
2025-04-12T19:07:49.7786229Z [36;1mmake -f Makefile.sync clean sync[0m
2025-04-12T19:07:49.7787506Z [36;1mgit diff --compact-summary --exit-code[0m
2025-04-12T19:07:49.7838771Z shell: /usr/bin/bash -e {0}
2025-04-12T19:07:49.7839828Z ##[endgroup]
2025-04-12T19:07:49.7972092Z git clone https://github.com/ggerganov/llama.cpp.git llama/vendor
2025-04-12T19:07:49.7981858Z Cloning into 'llama/vendor'...
2025-04-12T19:07:57.5253920Z git -C llama/vendor fetch
2025-04-12T19:07:58.0597558Z git -C llama/vendor checkout -f 7538246e7ce0606694c38055cc2fc9f60535be6c
2025-04-12T19:07:58.4690372Z Note: switching to '7538246e7ce0606694c38055cc2fc9f60535be6c'.
2025-04-12T19:07:58.4691302Z 
2025-04-12T19:07:58.4691816Z You are in 'detached HEAD' state. You can look around, make experimental
2025-04-12T19:07:58.4693382Z changes and commit them, and you can discard any commits you make in this
2025-04-12T19:07:58.4694514Z state without impacting any branches by switching back to a branch.
2025-04-12T19:07:58.4695294Z 
2025-04-12T19:07:58.4695717Z If you want to create a new branch to retain commits you create, you may
2025-04-12T19:07:58.4696724Z do so (now or later) by using -c with the switch command. Example:
2025-04-12T19:07:58.4697341Z 
2025-04-12T19:07:58.4697602Z   git switch -c <new-branch-name>
2025-04-12T19:07:58.4698062Z 
2025-04-12T19:07:58.4698302Z Or undo this operation with:
2025-04-12T19:07:58.4698670Z 
2025-04-12T19:07:58.4698891Z   git switch -
2025-04-12T19:07:58.4699164Z 
2025-04-12T19:07:58.4699631Z Turn off this advice by setting config variable advice.detachedHead to false
2025-04-12T19:07:58.4700272Z 
2025-04-12T19:07:58.4700620Z HEAD is now at 7538246e cuda : add f32 to bf16 copy op (#12806)
2025-04-12T19:07:58.4713662Z rm -f llama/patches/0001-cuda.patched llama/patches/0002-pretokenizer.patched llama/patches/0003-embeddings.patched llama/patches/0004-clip-unicode.patched llama/patches/0005-solar-pro.patched llama/patches/0006-conditional-fattn.patched llama/patches/0007-add-mllama-support.patched llama/patches/0008-add-unpad-operator.patched llama/patches/0009-fix-deepseek-deseret-regex.patched llama/patches/0010-Maintain-ordering-for-rules-for-grammar.patched llama/patches/0011-sort-devices-by-score.patched llama/patches/0012-add-phony-target-ggml-cpu-for-all-cpu-variants.patched llama/patches/0013-remove-amx.patched llama/patches/0014-fix-string-arr-kv-loading.patched llama/patches/0015-ollama-debug-tensor.patched llama/patches/0016-add-model-quantizations.patched llama/patches/0017-add-op_neg.patched llama/patches/0018-fix-compiler-error-in-clip.h.patched
2025-04-12T19:07:58.4722052Z sed -e 's|@FETCH_HEAD@|7538246e7ce0606694c38055cc2fc9f60535be6c|' llama/build-info.cpp.in > llama/build-info.cpp
2025-04-12T19:07:58.5105412Z Applying: cuda
2025-04-12T19:07:58.5425377Z Applying: pretokenizer
2025-04-12T19:07:58.5724819Z Applying: embeddings
2025-04-12T19:07:58.6035908Z Applying: clip-unicode
2025-04-12T19:07:58.6469685Z Applying: solar-pro
2025-04-12T19:07:58.6790003Z Applying: conditional-fattn
2025-04-12T19:07:58.7388038Z Applying: add mllama support
2025-04-12T19:07:58.7996477Z Applying: add unpad operator
2025-04-12T19:07:58.8365466Z Applying: fix deepseek deseret regex
2025-04-12T19:07:58.8673123Z Applying: maintain ordering for rules for grammar
2025-04-12T19:07:58.8979006Z Applying: sort devices by score
2025-04-12T19:07:58.9296069Z Applying: add phony target ggml-cpu for all cpu variants
2025-04-12T19:07:58.9606133Z Applying: remove amx
2025-04-12T19:07:58.9956129Z Applying: fix string arr kv loading
2025-04-12T19:07:59.0289579Z Applying: ollama debug tensor
2025-04-12T19:07:59.0547347Z Applying: add model quantizations
2025-04-12T19:07:59.0808941Z Applying: add op_neg
2025-04-12T19:07:59.0991767Z Applying: fix compiler error in clip.h
2025-04-12T19:07:59.1021700Z rsync -arvzc -f "merge llama/llama.cpp/.rsync-filter" llama/vendor/ llama/llama.cpp
2025-04-12T19:07:59.1082701Z sending incremental file list
2025-04-12T19:07:59.1104894Z ./
2025-04-12T19:07:59.1124606Z common/
2025-04-12T19:07:59.1124895Z examples/
2025-04-12T19:07:59.1125102Z examples/llava/
2025-04-12T19:07:59.1125316Z examples/llava/clip.cpp
2025-04-12T19:07:59.1132062Z include/
2025-04-12T19:07:59.1132241Z src/
2025-04-12T19:07:59.1544961Z 
2025-04-12T19:07:59.1545359Z sent 29,960 bytes  received 242 bytes  60,404.00 bytes/sec
2025-04-12T19:07:59.1546071Z total size is 3,245,211  speedup is 107.45
2025-04-12T19:07:59.1556538Z rsync -arvzc -f "merge ml/backend/ggml/ggml/.rsync-filter" llama/vendor/ggml/ ml/backend/ggml/ggml
2025-04-12T19:07:59.1590421Z sending incremental file list
2025-04-12T19:07:59.1628137Z ./
2025-04-12T19:07:59.1673732Z cmake/
2025-04-12T19:07:59.1674077Z include/
2025-04-12T19:07:59.1674357Z src/
2025-04-12T19:07:59.1674623Z src/ggml-blas/
2025-04-12T19:07:59.1674934Z src/ggml-cpu/
2025-04-12T19:07:59.1675154Z src/ggml-cpu/amx/
2025-04-12T19:07:59.1675356Z src/ggml-cpu/cmake/
2025-04-12T19:07:59.1675564Z src/ggml-cpu/llamafile/
2025-04-12T19:07:59.1675781Z src/ggml-cuda/
2025-04-12T19:07:59.1676158Z src/ggml-cuda/template-instances/
2025-04-12T19:07:59.1676421Z src/ggml-cuda/vendors/
2025-04-12T19:07:59.1676690Z src/ggml-hip/
2025-04-12T19:07:59.1676922Z src/ggml-metal/
2025-04-12T19:07:59.2079219Z 
2025-04-12T19:07:59.2079580Z sent 14,132 bytes  received 923 bytes  30,110.00 bytes/sec
2025-04-12T19:07:59.2080323Z total size is 4,221,339  speedup is 280.39
2025-04-12T19:07:59.3790591Z  llama/llama.cpp/examples/llava/clip.cpp | 8 ++++----
2025-04-12T19:07:59.3791312Z  1 file changed, 4 insertions(+), 4 deletions(-)
2025-04-12T19:07:59.5450498Z ##[error]Process completed with exit code 1.
